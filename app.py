import streamlit as st
import datetime
import base64
import os
import sys

# Version tracking
APP_VERSION = "1.0.0"

# Resolve page icon robustly for both dev and PyInstaller-frozen environments
def _resolve_resource_path(rel_path: str) -> str:
    try:
        if getattr(sys, "frozen", False):  # PyInstaller
            base_path = getattr(sys, "_MEIPASS", os.path.dirname(sys.executable))
        else:
            base_path = os.path.dirname(__file__)
        return os.path.join(base_path, rel_path)
    except Exception:
        return rel_path

_page_icon = "ðŸ“Š"  # safe default
if not getattr(sys, "frozen", False):  # Only use image path in dev, not in packaged app
    try:
        _icon_path = _resolve_resource_path("assets/hand_logo.png")
        if os.path.exists(_icon_path):
            _page_icon = _icon_path
    except Exception:
        _page_icon = "ðŸ“Š"

# Set page config must be the first Streamlit command
st.set_page_config(layout="wide", page_title="Amazon Advertising Dashboard", page_icon=_page_icon)



import plotly.express as px
import plotly.graph_objects as go
import pandas as pd
import numpy as np
import json
import os
import re
import glob
import time
import io
import math
import traceback
from datetime import datetime, timedelta
from pathlib import Path
from collections import defaultdict
from io import StringIO
from urllib.parse import quote
from typing import Dict, List, Tuple, Optional, Any, Union, Set
import importlib
import uuid
import functools
from contextlib import contextmanager
import streamlit.components.v1 as components
from database import db_manager
from openpyxl import Workbook
from openpyxl.styles import PatternFill, Font, Alignment, NamedStyle
from openpyxl.formatting.rule import ColorScaleRule
from openpyxl.utils.dataframe import dataframe_to_rows
from openpyxl.utils import get_column_letter
try:
    from supabase_store import (
        is_supabase_configured,
        get_current_user_id,
        sign_in,
        sign_up,
        sign_out,
        apply_session_from_state,
        list_client_names as sb_list_client_names,
        fetch_client_config as sb_fetch_client_config,
        upsert_client_config as sb_upsert_client_config,
        list_sessions as sb_list_sessions,
        save_session as sb_save_session,
        fetch_session as sb_fetch_session,
        delete_session as sb_delete_session,
    )
except Exception:
    # Supabase not available locally; provide no-op placeholders so local runs are unaffected
    def is_supabase_configured() -> bool:
        return False

    def get_current_user_id():
        return None

    def sign_in(email: str, password: str):
        return False, "Supabase not available"

    def sign_up(email: str, password: str):
        return False, "Supabase not available"

    def sign_out():
        return None

    def apply_session_from_state():
        return None

    def sb_list_client_names(uid):
        return []

    def sb_fetch_client_config(uid, client_name):
        return None

    def sb_upsert_client_config(uid, client_name, config):
        return False

    def sb_list_sessions(uid, client_name):
        return []

    def sb_save_session(uid, client_name, filename, metadata, session_data):
        return False

    def sb_fetch_session(uid, client_name, filename):
        return None

    def sb_delete_session(uid, client_name, filename):
        return False

# --- Enhanced Campaign Filtering Function ---
def phrase_match(value, filter_value):
    """Enhanced phrase matching for campaign names with pipe separators"""
    if not isinstance(value, str):
        return False
    pattern = re.escape(filter_value)
    # Match as a whole phrase (preceded/followed by |, start/end, or whitespace)
    regex = rf"(^|[|\s]){pattern}([|\s]|$)"
    return re.search(regex, value, re.IGNORECASE) is not None or value.strip().lower() == filter_value.strip().lower()



# --- Performance Monitoring Utilities ---
@contextmanager
def performance_timer(operation_name):
    """Context manager to time operations and log performance metrics."""
    start_time = time.time()
    try:
        yield
    finally:
        elapsed_time = time.time() - start_time
        if 'debug_messages' in st.session_state:
            st.session_state.debug_messages.append(f"[PERFORMANCE] {operation_name}: {elapsed_time:.3f} seconds")

def performance_monitor(func):
    """Decorator to monitor function performance."""
    @functools.wraps(func)
    def wrapper(*args, **kwargs):
        with performance_timer(f"{func.__name__}"):
            return func(*args, **kwargs)
    return wrapper

# --- Cache Performance Tracking ---
def track_cache_hit(operation_name):
    """Track cache hit for performance monitoring."""
    if 'cache_stats' not in st.session_state:
        st.session_state.cache_stats = {'hits': 0, 'misses': 0}
    st.session_state.cache_stats['hits'] += 1
    if 'debug_messages' in st.session_state:
        st.session_state.debug_messages.append(f"[CACHE HIT] {operation_name}")

def track_cache_miss(operation_name):
    """Track cache miss for performance monitoring."""
    if 'cache_stats' not in st.session_state:
        st.session_state.cache_stats = {'hits': 0, 'misses': 0}
    st.session_state.cache_stats['misses'] += 1
    if 'debug_messages' in st.session_state:
        st.session_state.debug_messages.append(f"[CACHE MISS] {operation_name}")

def get_cache_stats():
    """Get cache performance statistics."""
    if 'cache_stats' not in st.session_state:
        return {'hits': 0, 'misses': 0, 'hit_rate': 0}
    
    stats = st.session_state.cache_stats
    total = stats['hits'] + stats['misses']
    hit_rate = (stats['hits'] / total * 100) if total > 0 else 0
    
    return {
        'hits': stats['hits'],
        'misses': stats['misses'],
        'total': total,
        'hit_rate': hit_rate
    }

# --- Helper Functions for Expandable/Downloadable Sections ---
def create_expandable_section(title, key=None):
    """
    Creates an expandable section with a title and expand/collapse button.
    
    Args:
        title: The title of the section
        key: Optional unique key for the section (will be generated if not provided)
        
    Returns:
        is_expanded: Boolean indicating if the section is expanded
        section_key: The unique key for the section
    """
    if key is None:
        # Generate a unique key if none provided
        key = f"section_{str(uuid.uuid4())[:8]}"
        
    # Initialize session state for this section if it doesn't exist
    if f"expanded_{key}" not in st.session_state:
        st.session_state[f"expanded_{key}"] = False
    
    # Create a container for the header with title and button
    col1, col2 = st.columns([0.85, 0.15])
    
    with col1:
        st.markdown(f"#### {title}")
    
    with col2:
        if st.session_state[f"expanded_{key}"]:
            if st.button("Collapse", key=f"collapse_btn_{key}"):
                st.session_state[f"expanded_{key}"] = False
                st.rerun()
        else:
            if st.button("Expand", key=f"expand_btn_{key}"):
                st.session_state[f"expanded_{key}"] = True
                st.rerun()
    
    return st.session_state[f"expanded_{key}"], key

def get_table_download_link(df, filename, button_text="Download Data"):
    """
    Generates a link to download the dataframe as a CSV file.
    
    Args:
        df: Pandas DataFrame to download
        filename: Name of the file to download
        button_text: Text to display on the download button
        
    Returns:
        None (displays the download button directly)
    """
    csv = df.to_csv(index=False)
    b64 = base64.b64encode(csv.encode()).decode()
    href = f'<a href="data:file/csv;base64,{b64}" download="{filename}.csv" class="download-button">{button_text}</a>'
    st.markdown(href, unsafe_allow_html=True)

def get_figure_download_link(fig, filename, button_text="Download Chart"):
    """
    Generates a link to download a Plotly figure as an HTML file.
    
    Args:
        fig: Plotly figure to download
        filename: Name of the file to download
        button_text: Text to display on the download button
        
    Returns:
        None (displays the download button directly)
    """
    buffer = io.StringIO()
    fig.write_html(buffer)
    html_bytes = buffer.getvalue().encode()
    b64 = base64.b64encode(html_bytes).decode()
    href = f'<a href="data:text/html;base64,{b64}" download="{filename}.html" class="download-button">{button_text}</a>'
    st.markdown(href, unsafe_allow_html=True)

def expandable_dataframe(df, title, column_config=None, use_styling_func=None, styling_args=None):
    """
    Creates an expandable section containing a dataframe with download option.
    
    Args:
        df: Pandas DataFrame to display
        title: Title of the section
        column_config: Optional column configuration for st.dataframe
        use_styling_func: Optional styling function to apply (e.g., style_acos)
        styling_args: Optional arguments to pass to the styling function
        
    Returns:
        None
    """
    is_expanded, section_key = create_expandable_section(title)
    
    if is_expanded:
        # Create a container for the dataframe and download button
        col1, col2 = st.columns([0.85, 0.15])
        
        with col2:
            # Add download button
            get_table_download_link(df, f"{title.lower().replace(' ', '_')}")
        
        # Display the dataframe with appropriate styling
        if use_styling_func and callable(use_styling_func):
            if styling_args:
                use_styling_func(df, **styling_args)
            else:
                use_styling_func(df)
        else:
            st.dataframe(df, use_container_width=True, column_config=column_config, hide_index=True)

def expandable_chart(fig, title):
    """
    Creates an expandable section containing a Plotly chart with download option.
    
    Args:
        fig: Plotly figure to display
        title: Title of the section
        
    Returns:
        None
    """
    is_expanded, section_key = create_expandable_section(title)
    
    if is_expanded:
        # Create a container for the chart and download button
        col1, col2 = st.columns([0.85, 0.15])
        
        with col2:
            # Add download button
            get_figure_download_link(fig, f"{title.lower().replace(' ', '_')}")
        
        # Display the chart
        st.plotly_chart(fig, use_container_width=True, config={'responsive': True})

# Add CSS for enhanced UI components
st.markdown("""
<style>
/* Download buttons */
.download-button {
    display: inline-block;
    padding: 0.5em 1em;
    background-color: #4CAF50;
    color: white;
    text-align: center;
    text-decoration: none;
    font-size: 14px;
    border-radius: 4px;
    cursor: pointer;
    transition: background-color 0.3s;
    box-shadow: 0 2px 4px rgba(0,0,0,0.1);
}
.download-button:hover {
    background-color: #45a049;
    box-shadow: 0 4px 8px rgba(0,0,0,0.15);
}

/* Enhanced section headers */
.main-section-header {
    font-size: 1.5em;
    font-weight: bold;
    color: #1f77b4;
    text-shadow: 1px 1px 2px rgba(0,0,0,0.1);
}

/* Improved metrics display */
.metric-container {
    background-color: #f8f9fa;
    padding: 1rem;
    border-radius: 8px;
    border-left: 4px solid #1f77b4;
    margin: 0.5rem 0;
}

/* Warning and info boxes */
.warning-box {
    background-color: #fff3cd;
    border: 1px solid #ffeaa7;
    border-radius: 4px;
    padding: 1rem;
    margin: 1rem 0;
}

.info-box {
    background-color: #d1ecf1;
    border: 1px solid #b8daff;
    border-radius: 4px;
    padding: 1rem;
    margin: 1rem 0;
}

/* Enhanced table styling */
.dataframe {
    font-size: 0.9em;
}

/* Progress indicators */
.progress-text {
    font-size: 0.8em;
    color: #666;
    font-style: italic;
}
</style>
""", unsafe_allow_html=True)

# Global color gradient functions for styling tables
def color_gradient_blue(val, min_val, max_val, scale_max=75):
    # Blue gradient: from dark gray (#23272f) to blue (#2563eb)
    if pd.isnull(val):
        return ''
    # Scale from 0% to scale_max% (default: 75%)
    norm = min(max(val, 0), scale_max) / scale_max
    norm = norm ** 0.5  # sqrt scaling for more visual spread
    r = int(35 + (37-35)*norm)
    g = int(39 + (99-39)*norm)
    b = int(47 + (235-47)*norm)
    return f'background-color: rgb({r},{g},{b}); color: #fff;'

def color_gradient_green(val, min_val, max_val, scale_max=75):
    # Green gradient: from dark gray (#23272f) to deep green (#15803d)
    if pd.isnull(val):
        return ''
    # Scale from 0% to scale_max% (default: 75%)
    norm = min(max(val, 0), scale_max) / scale_max
    norm = norm ** 0.5  # sqrt scaling for more visual spread
    r = int(35 + (21-35)*norm)    # 35 to 21
    g = int(39 + (128-39)*norm)   # 39 to 128
    b = int(47 + (61-47)*norm)    # 47 to 61
    return f'background-color: rgb({r},{g},{b}); color: #fff;'

def color_gradient_orange(val, min_val, max_val, scale_max=75):
    # Orange gradient: from dark gray (#23272f) to orange (#ea580c)
    if pd.isnull(val):
        return ''
    # Scale from 0% to scale_max% (default: 75%)
    norm = min(max(val, 0), scale_max) / scale_max
    norm = norm ** 0.5  # sqrt scaling for more visual spread
    r = int(35 + (234-35)*norm)   # 35 to 234
    g = int(39 + (88-39)*norm)    # 39 to 88
    b = int(47 + (12-47)*norm)    # 47 to 12
    return f'background-color: rgb({r},{g},{b}); color: #fff;'

# Helper function to apply ACoS styling to a specific column
# This function was removed as part of removing ACoS coloring based on user goals
def apply_acos_styling(styled_df, numeric_df, acos_column, target, use_avg_fallback=False):
    # Function removed - returning unstyled dataframe
    return styled_df

# --- Helper Functions ---
def check_file_size_warning(uploaded_file):
    """Check uploaded file size and warn if it might cause memory issues."""
    if uploaded_file is not None:
        file_size_mb = uploaded_file.size / (1024 * 1024)
        if file_size_mb > 50:
            st.warning(f"âš ï¸ Large file detected ({file_size_mb:.1f} MB). Processing may take longer and use significant memory.")
            if file_size_mb > 100:
                st.error(f"ðŸš¨ Very large file ({file_size_mb:.1f} MB). This may cause the application to run out of memory on some systems.")
                return False
    return True

def show_loading_spinner(message="Loading data..."):
    """Display a consistent loading spinner with the given message.
    
    Args:
        message: Text to display next to the spinner
    
    Returns:
        A Streamlit spinner context manager that can be used in a with statement
    """
    return st.spinner(message)

# --- Ensure session state keys are initialized ---
if 'current_page' not in st.session_state:
    st.session_state.current_page = "home"  # Set default page
    
# --- Cache Management Functions ---
def clear_caches():
    """Clear all st.cache_data caches and database caches to force data refresh"""
    st.cache_data.clear()
    
    # Also clear database cache for current client
    if 'client_config' in st.session_state and st.session_state.client_config:
        client_name = st.session_state.client_config.get('client_name', 'unknown')
        db_manager.clear_client_cache(client_name)
    
    if 'debug_messages' in st.session_state:
        st.session_state.debug_messages.append("[INFO] All data caches and database caches cleared")
        
if 'cache_initialized' not in st.session_state:
    st.session_state.cache_initialized = True
    # Initialize cache timestamp to track when data was last refreshed
    st.session_state.last_cache_refresh = datetime.now()
    
    # Cleanup old database cache entries (older than 7 days)
    try:
        db_manager.cleanup_old_cache(days_old=7)
    except Exception as e:
        if 'debug_messages' in st.session_state:
            st.session_state.debug_messages.append(f"[DB] Cache cleanup failed: {str(e)}")

# Global ACoS styling functions moved to top level
def clean_acos(val):
    if pd.isnull(val):
        return 0.0
    try:
        # Remove any CSS, percent, commas, whitespace, and newlines
        if isinstance(val, str):
            val = val.replace('\n', '').replace('\r', '')
            val = val.split(';')[0]
            val = val.replace('%', '').replace(',', '').strip()
        return float(val)
    except (ValueError, TypeError, AttributeError):
        # If conversion fails, return 0
        return 0.0

# Helper function to safely convert values to numeric, handling both string and numeric inputs
def safe_convert_to_numeric(series):
    if series.dtype == 'object':
        return pd.to_numeric(series.str.replace('[$,]', '', regex=True), errors='coerce')
    else:
        return pd.to_numeric(series, errors='coerce')

def calculate_acos_range_distribution(df, num_ranges=5):
    """
    Calculate the distribution of targets across different ACoS ranges.
    
    Args:
        df: DataFrame containing targeting data with 'ACoS', 'Spend', and sales columns
        num_ranges: Number of ACoS ranges to create (default: 5)
        
    Returns:
        DataFrame with ACoS ranges, count of targets, spend, and sales in each range
    """
    if df.empty or 'ACoS' not in df.columns or 'Spend' not in df.columns:
        return pd.DataFrame(columns=['ACoS Range', 'Number of Targets', '% of Total Spend', '% of Total Ad Sales', 'Spend', 'Ad Sales'])
    
    # Clean and convert ACoS values to numeric
    df = df.copy()
    df['ACoS_numeric'] = df['ACoS'].apply(lambda x: clean_acos(x))
    df['Spend_numeric'] = df['Spend'].replace('[\$,]', '', regex=True).astype(float)
    
    # Determine which sales column to use
    sales_col = None
    if 'sd_attribution_choice' in st.session_state and st.session_state.sd_attribution_choice == "Sales (Views & Clicks)" and 'Sponsored Display' in str(df.get('Campaign Type', '')):
        sales_col = 'Sales (Views & Clicks)' if 'Sales (Views & Clicks)' in df.columns else 'Ad Sales'
    else:
        sales_col = 'Ad Sales' if 'Ad Sales' in df.columns else 'Sales'
    
    if sales_col not in df.columns:
        # If no sales column is found, add a placeholder
        df[sales_col] = 0
    
    df['Sales_numeric'] = df[sales_col].replace('[\$,]', '', regex=True).astype(float)
    
    # Initialize results dictionary
    results = []
    total_spend = df['Spend_numeric'].sum()
    total_sales = df['Sales_numeric'].sum()
    
    # Create No Sales row data (will add at the end)
    no_sales_df = df[df['Sales_numeric'] == 0]
    no_sales_count = len(no_sales_df)
    no_sales_spend = no_sales_df['Spend_numeric'].sum()
    no_sales_spend_percentage = (no_sales_spend / total_spend * 100) if total_spend > 0 else 0
    
    # Get data with sales for dynamic range calculation
    df_with_sales = df[df['Sales_numeric'] > 0]
    
    if len(df_with_sales) == 0:
        # If there are no targets with sales, return just the No Sales row
        results_df = pd.DataFrame(results)
    else:
        # Calculate dynamic ranges based on the data
        # Always have 0% as the lower bound of the first range and 100%+ as the last range
        
        # Get ACoS values for targets with sales
        acos_values = df_with_sales['ACoS_numeric'].dropna()
        
        # Check if we have ACoS goals in the client config to help inform our ranges
        has_acos_goal = False
        acos_goal = None
        if 'client_config' in st.session_state and st.session_state.client_config:
            goals = st.session_state.client_config.get('goals', {})
            if goals:
                # Check for branded, non-branded, or account-wide goals
                acos_goal = goals.get('branded_acos') or goals.get('non_branded_acos') or goals.get('account_wide_acos')
                if acos_goal is not None:
                    try:
                        acos_goal = float(acos_goal)
                        has_acos_goal = True
                    except (ValueError, TypeError):
                        has_acos_goal = False
        
        # Determine the upper bound for dynamic ranges
        max_for_ranges = min(100, acos_values.quantile(0.95)) if len(acos_values) > 0 else 100
        
        # Create dynamic ranges (plus the 100%+ range) based on user selection
        if has_acos_goal and 0 < acos_goal < 100:
            # If we have an ACoS goal, create ranges centered around it
            # Half of ranges below the goal, one goal range, and half of ranges above the goal
            
            # Calculate how many ranges should be below and above the goal
            ranges_per_side = num_ranges // 2
            
            # Determine range size based on the goal
            range_size_below = acos_goal / (ranges_per_side + 1)  # +1 for the goal range
            range_size_above = (max_for_ranges - acos_goal) / (ranges_per_side + 1)  # +1 for the goal range
            
            # Create ranges dynamically
            ranges = []
            
            # First range always starts at 0
            lower_bound = 0
            
            # Create ranges below the goal
            for i in range(ranges_per_side):
                upper_bound = acos_goal - (ranges_per_side - i) * range_size_below
                ranges.append((lower_bound, upper_bound))
                lower_bound = upper_bound
            
            # Add the goal range
            ranges.append((lower_bound, acos_goal + range_size_above))
            lower_bound = acos_goal + range_size_above
            
            # Create ranges above the goal
            for i in range(ranges_per_side - 1):
                upper_bound = acos_goal + (i + 2) * range_size_above
                ranges.append((lower_bound, upper_bound))
                lower_bound = upper_bound
            
            # Add the range up to 100%
            ranges.append((lower_bound, 100))
            
            # Always add the 100%+ range
            ranges.append((100, float('inf')))
            
            # Create range labels without decimals
            range_labels = []
            for i, (lower, upper) in enumerate(ranges):
                if i == 0:
                    # First range is always 0-X%
                    range_labels.append(f"0-{int(upper)}%")
                elif upper == float('inf'):
                    # Last range is always 100%+
                    range_labels.append("100%+")
                elif upper == 100:
                    # Range ending at 100%
                    range_labels.append(f"{int(lower)}-100%")
                else:
                    # All other ranges
                    range_labels.append(f"{int(lower)}-{int(upper)}%")
        else:
            # If no goal, create evenly distributed ranges
            step = max_for_ranges / num_ranges  # num_ranges plus the 100%+ range
            
            ranges = []
            for i in range(num_ranges):
                if i == 0:
                    # First range always starts at 0
                    ranges.append((0, step))
                elif i == num_ranges - 1:
                    # Last range before 100%+
                    ranges.append((i * step, 100))
                else:
                    # Middle ranges
                    ranges.append((i * step, (i + 1) * step))
            
            # Always add the 100%+ range
            ranges.append((100, float('inf')))
            
            # Create range labels without decimals
            range_labels = []
            for i, (lower, upper) in enumerate(ranges):
                if i == 0:
                    # First range is always 0-X%
                    range_labels.append(f"0-{int(upper)}%")
                elif upper == float('inf'):
                    # Last range is always 100%+
                    range_labels.append("100%+")
                elif upper == 100:
                    # Range ending at 100%
                    range_labels.append(f"{int(lower)}-100%")
                else:
                    # All other ranges
                    range_labels.append(f"{int(lower)}-{int(upper)}%")
        
        # Calculate counts and spend for each range, excluding those with no sales
        for i, (lower, upper) in enumerate(ranges):
            range_df = df_with_sales[(df_with_sales['ACoS_numeric'] >= lower) & (df_with_sales['ACoS_numeric'] < upper)] if upper != float('inf') else df_with_sales[df_with_sales['ACoS_numeric'] >= lower]
            count = len(range_df)
            spend = range_df['Spend_numeric'].sum()
            sales = range_df['Sales_numeric'].sum()
            spend_percentage = (spend / total_spend * 100) if total_spend > 0 else 0
            sales_percentage = (sales / total_sales * 100) if total_sales > 0 else 0
            
            results.append({
                'ACoS Range': range_labels[i],
                'Number of Targets': count,
                'Spend': spend,
                '% of Total Spend': spend_percentage,
                'Ad Sales': sales,
                '% of Total Ad Sales': sales_percentage
            })
        
        # Now add the No Sales row at the end
        results.append({
            'ACoS Range': 'No Sales',
            'Number of Targets': no_sales_count,
            'Spend': no_sales_spend,
            '% of Total Spend': no_sales_spend_percentage,
            'Ad Sales': 0,
            '% of Total Ad Sales': 0
        })
        
        # Convert to DataFrame
        results_df = pd.DataFrame(results)
    
    # Format the columns
    results_df['Number of Targets'] = results_df['Number of Targets'].apply(lambda x: f"{x:,}" if x >= 1000 else str(x))
    results_df['Spend'] = results_df['Spend'].apply(lambda x: f"${x:,.2f}")
    results_df['Ad Sales'] = results_df['Ad Sales'].apply(lambda x: f"${x:,.2f}")
    results_df['% of Total Spend'] = results_df['% of Total Spend'].apply(lambda x: f"{x:.2f}%")
    results_df['% of Total Ad Sales'] = results_df['% of Total Ad Sales'].apply(lambda x: f"{x:.2f}%")
    
    # Reorder columns
    results_df = results_df[['ACoS Range', 'Number of Targets', '% of Total Spend', '% of Total Ad Sales', 'Spend', 'Ad Sales']]
    
    # Add debug information
    if 'debug_messages' in st.session_state:
        st.session_state.debug_messages.append(f"[ACoS Distribution] Generated {len(results_df)-1} dynamic ACoS ranges plus 'No Sales' row with {num_ranges} user-selected ranges")
    
    return results_df

# This function was removed as part of removing ACoS coloring based on user goals
def get_acos_color(acos, target_acos):
    # Function removed - returning empty style
    return ''

def style_acos(df, target_acos=None, column_config=None, use_avg_as_fallback=False, title=None, use_expander=False):
    """Display a DataFrame. ACoS coloring has been removed.
    
    Args:
        df: DataFrame to display
        target_acos: Parameter kept for backward compatibility but no longer used
        column_config: Optional column configuration for st.dataframe
        use_avg_as_fallback: Parameter kept for backward compatibility but no longer used
        title: Optional title for expandable section
        use_expander: Whether to use expandable section
    """
    try:
        if df.empty:
            st.session_state.debug_messages.append("[style_acos debug] DataFrame is empty.")
            if use_expander and title:
                is_expanded, section_key = create_expandable_section(title)
                if is_expanded:
                    col1, col2 = st.columns([0.85, 0.15])
                    with col2:
                        get_table_download_link(df, f"{title.lower().replace(' ', '_')}")
                    return st.dataframe(df, use_container_width=True, column_config=column_config, hide_index=True) if column_config else st.dataframe(df, use_container_width=True, hide_index=True)
            else:
                return st.dataframe(df, use_container_width=True, column_config=column_config, hide_index=True) if column_config else st.dataframe(df, use_container_width=True, hide_index=True)
        
        # Make a copy to avoid modifying the original
        df = df.copy()
        
        # Display the dataframe without ACoS styling
        if use_expander and title:
            is_expanded, section_key = create_expandable_section(title)
            if is_expanded:
                # Create a container for the dataframe and download button
                col1, col2 = st.columns([0.85, 0.15])
                with col2:
                    # Add download button
                    get_table_download_link(df, f"{title.lower().replace(' ', '_')}")
                
                # Display the dataframe without styling
                return st.dataframe(df, use_container_width=True, column_config=column_config, hide_index=True) if column_config else st.dataframe(df, use_container_width=True, hide_index=True)
        else:
            return st.dataframe(df, use_container_width=True, column_config=column_config, hide_index=True) if column_config else st.dataframe(df, use_container_width=True, hide_index=True)
    except Exception as e:
        if 'debug_messages' in st.session_state:
            st.session_state.debug_messages.append(f"[style_acos debug] Exception in style_acos: {e}")
        if use_expander and title:
            is_expanded, section_key = create_expandable_section(title)
            if is_expanded:
                col1, col2 = st.columns([0.85, 0.15])
                with col2:
                    get_table_download_link(df, f"{title.lower().replace(' ', '_')}")
                return st.dataframe(df, use_container_width=True, column_config=column_config, hide_index=True) if column_config else st.dataframe(df, use_container_width=True, hide_index=True)
        else:
            return st.dataframe(df, use_container_width=True, column_config=column_config, hide_index=True) if column_config else st.dataframe(df, use_container_width=True, hide_index=True)


# --- Helper Functions for Consistent ACoS Range Distribution ---

def get_consistent_acos_ranges(all_dataframes, num_ranges):
    """Calculate consistent ACoS ranges based on combined data from all dataframes."""
    combined_data = []
    for df in all_dataframes:
        if not df.empty and 'ACoS' in df.columns:
            df_copy = df.copy()
            df_copy['ACoS_numeric'] = df_copy['ACoS'].apply(lambda x: clean_acos(x))
            # Get sales column
            sales_col = 'Ad Sales' if 'Ad Sales' in df_copy.columns else 'Sales'
            if sales_col in df_copy.columns:
                df_copy['Sales_numeric'] = df_copy[sales_col].replace('[\$,]', '', regex=True).astype(float)
                combined_data.append(df_copy[df_copy['Sales_numeric'] > 0])
    
    if not combined_data:
        return None, None
    
    combined_df = pd.concat(combined_data, ignore_index=True)
    if combined_df.empty:
        return None, None
    
    acos_values = combined_df['ACoS_numeric'].dropna()
    if len(acos_values) == 0:
        return None, None
    
    # Simple evenly distributed ranges for consistency
    max_acos = min(100, acos_values.quantile(0.95))
    step = max_acos / num_ranges
    
    ranges = []
    for i in range(num_ranges):
        if i == 0:
            ranges.append((0, step))
        elif i == num_ranges - 1:
            ranges.append((i * step, 100))
        else:
            ranges.append((i * step, (i + 1) * step))
    ranges.append((100, float('inf')))  # 100%+ range
    
    # Create labels
    labels = []
    for lower, upper in ranges:
        if upper == float('inf'):
            labels.append("100%+")
        elif upper == 100:
            labels.append(f"{int(lower)}-100%")
        else:
            labels.append(f"{int(lower)}-{int(upper)}%")
    
    return ranges, labels

def calculate_acos_distribution_with_ranges(df, ranges, labels):
    """Calculate distribution using predefined ranges."""
    if df.empty or not ranges or not labels:
        return pd.DataFrame()
    
    df = df.copy()
    df['ACoS_numeric'] = df['ACoS'].apply(lambda x: clean_acos(x))
    df['Spend_numeric'] = df['Spend'].replace('[\$,]', '', regex=True).astype(float)
    
    # Determine which sales column to use
    sales_col = None
    if 'sd_attribution_choice' in st.session_state and st.session_state.sd_attribution_choice == "Sales (Views & Clicks)" and 'Sponsored Display' in str(df.get('Campaign Type', '')):
        sales_col = 'Sales (Views & Clicks)' if 'Sales (Views & Clicks)' in df.columns else 'Ad Sales'
    else:
        sales_col = 'Ad Sales' if 'Ad Sales' in df.columns else 'Sales'
    
    if sales_col not in df.columns:
        df[sales_col] = 0
    
    df['Sales_numeric'] = df[sales_col].replace('[\$,]', '', regex=True).astype(float)
    
    # Initialize results
    results = []
    total_spend = df['Spend_numeric'].sum()
    total_sales = df['Sales_numeric'].sum()
    
    # Calculate for each range
    df_with_sales = df[df['Sales_numeric'] > 0]
    for i, (lower, upper) in enumerate(ranges):
        if upper == float('inf'):
            range_df = df_with_sales[df_with_sales['ACoS_numeric'] >= lower]
        else:
            range_df = df_with_sales[(df_with_sales['ACoS_numeric'] >= lower) & (df_with_sales['ACoS_numeric'] < upper)]
        
        count = len(range_df)
        spend = range_df['Spend_numeric'].sum()
        sales = range_df['Sales_numeric'].sum()
        spend_percentage = (spend / total_spend * 100) if total_spend > 0 else 0
        sales_percentage = (sales / total_sales * 100) if total_sales > 0 else 0
        
        results.append({
            'ACoS Range': labels[i],
            'Number of Targets': count,
            'Spend': spend,
            '% of Total Spend': spend_percentage,
            'Ad Sales': sales,
            '% of Total Ad Sales': sales_percentage
        })
    
    # Add No Sales row
    no_sales_df = df[df['Sales_numeric'] == 0]
    no_sales_count = len(no_sales_df)
    no_sales_spend = no_sales_df['Spend_numeric'].sum()
    no_sales_spend_percentage = (no_sales_spend / total_spend * 100) if total_spend > 0 else 0
    
    results.append({
        'ACoS Range': 'No Sales',
        'Number of Targets': no_sales_count,
        'Spend': no_sales_spend,
        '% of Total Spend': no_sales_spend_percentage,
        'Ad Sales': 0,
        '% of Total Ad Sales': 0
    })
    
    # Convert to DataFrame and format
    results_df = pd.DataFrame(results)
    results_df['Number of Targets'] = results_df['Number of Targets'].apply(lambda x: f"{x:,}" if x >= 1000 else str(x))
    results_df['Spend'] = results_df['Spend'].apply(lambda x: f"${x:,.2f}")
    results_df['Ad Sales'] = results_df['Ad Sales'].apply(lambda x: f"${x:,.2f}")
    results_df['% of Total Spend'] = results_df['% of Total Spend'].apply(lambda x: f"{x:.2f}%")
    results_df['% of Total Ad Sales'] = results_df['% of Total Ad Sales'].apply(lambda x: f"{x:.2f}%")
    
    return results_df

# --- End Helper Functions for Consistent ACoS Range Distribution ---

# --- Browser LocalStorage Integration ---
def is_cloud_environment():
    """Detect if running on Streamlit Cloud or similar cloud environment.
    
    Returns True only if running on Streamlit Community Cloud or similar hosted environment.
    Returns False for local development (even when running streamlit run locally).
    """
    # Check for definitive cloud environment indicators
    # Note: STREAMLIT_SERVER_PORT is set locally too, so we don't use it
    return (
        os.environ.get('STREAMLIT_SHARING_MODE') is not None or
        os.environ.get('HOME', '').startswith('/home/appuser') or
        os.environ.get('HOSTNAME', '').startswith('streamlit-') or
        'streamlit.io' in os.environ.get('HOSTNAME', '')
    )

def use_supabase():
    """Return True when running in cloud and Supabase credentials are present."""
    try:
        # Allow user to disable Supabase temporarily if network issues occur
        if st.session_state.get('disable_supabase', False):
            return False
        return is_cloud_environment() and is_supabase_configured()
    except Exception:
        return False

# --- Lazy import helpers to speed up initial load ---
@functools.lru_cache(maxsize=None)
def _lazy_mod(name: str):
    return importlib.import_module(name)

def get_plt():
    return _lazy_mod('matplotlib.pyplot')

def get_wordcloud():
    mod = _lazy_mod('wordcloud')
    return mod.WordCloud, mod.STOPWORDS

def get_linear_regression():
    return _lazy_mod('sklearn.linear_model').LinearRegression

def get_localStorage_value(key):
    """Get a value from browser localStorage."""
    # Check if we already have this value in session state
    session_key = f'_ls_cache_{key}'
    if session_key in st.session_state:
        return st.session_state[session_key]
    
    html_code = f"""
    <script>
        const value = localStorage.getItem('{key}');
        window.parent.postMessage({{
            type: 'streamlit:setComponentValue',
            value: value
        }}, '*');
    </script>
    """
    
    result = components.html(html_code, height=0)
    
    # Cache the result if it's a string (valid value)
    if isinstance(result, str):
        st.session_state[session_key] = result
    
    return result

def set_localStorage_value(key, value):
    """Set a value in browser localStorage."""
    # Escape the value for JavaScript
    import json
    json_value = json.dumps(value)
    
    html_code = f"""
    <script>
        localStorage.setItem('{key}', {json_value});
        window.parent.postMessage({{
            type: 'streamlit:setComponentValue',
            value: 'success'
        }}, '*');
    </script>
    """
    
    components.html(html_code, height=0)
    
    # Clear the cache so next get will fetch fresh value
    session_key = f'_ls_cache_{key}'
    if session_key in st.session_state:
        del st.session_state[session_key]

def remove_localStorage_value(key):
    """Remove a value from browser localStorage."""
    html_code = f"""
    <script>
        localStorage.removeItem('{key}');
        window.parent.postMessage({{
            type: 'streamlit:setComponentValue',
            value: 'removed'
        }}, '*');
    </script>
    """
    
    components.html(html_code, height=0)
    
    # Clear the cache
    session_key = f'_ls_cache_{key}'
    if session_key in st.session_state:
        del st.session_state[session_key]

def get_all_localStorage_keys():
    """Get all keys from browser localStorage that match our app prefix."""
    html_code = """
    <script>
        const keys = [];
        for (let i = 0; i < localStorage.length; i++) {
            const key = localStorage.key(i);
            if (key && key.startsWith('amazon_dashboard_')) {
                keys.push(key);
            }
        }
        window.parent.postMessage({
            type: 'streamlit:setComponentValue',
            value: JSON.stringify(keys)
        }, '*');
    </script>
    """
    
    result = components.html(html_code, height=0)
    if result:
        try:
            return json.loads(result)
        except:
            return []
    return []

# --- User Data Directory Helper ---
def get_user_data_dir():
    """Get a safe directory for user data that works on both Windows and Mac, including work computers."""
    import platform
    import os
    from pathlib import Path
    
    try:
        # Try to determine the best location for user data
        system = platform.system()
        
        if system == "Windows":
            # Try APPDATA first, fallback to user profile
            if os.environ.get('APPDATA'):
                base_dir = Path(os.environ['APPDATA']) / 'AmazonDashboard'
            else:
                base_dir = Path.home() / 'AppData' / 'Roaming' / 'AmazonDashboard'
        else:  # macOS and Linux
            # Try user Application Support, fallback to home directory
            if system == "Darwin":  # macOS
                base_dir = Path.home() / 'Library' / 'Application Support' / 'AmazonDashboard'
            else:  # Linux
                base_dir = Path.home() / '.AmazonDashboard'
        
        # Test if we can write to this location
        try:
            base_dir.mkdir(parents=True, exist_ok=True)
            # Test write permissions by creating a temporary file
            test_file = base_dir / 'write_test.tmp'
            test_file.write_text('test')
            test_file.unlink()  # delete test file
            return str(base_dir)
        except (PermissionError, OSError):
            # If that fails, try Documents folder
            docs_dir = Path.home() / 'Documents' / 'AmazonDashboard'
            try:
                docs_dir.mkdir(parents=True, exist_ok=True)
                test_file = docs_dir / 'write_test.tmp'
                test_file.write_text('test')
                test_file.unlink()
                return str(docs_dir)
            except (PermissionError, OSError):
                # Last resort: try current directory (original behavior)
                return '.'
    except Exception:
        # If all else fails, use current directory
        return '.'

def migrate_old_data():
    """Automatically migrate data from old location to new user data directory."""
    try:
        user_data_dir = get_user_data_dir()
        
        # Skip migration if we're using current directory (fallback mode)
        if user_data_dir == '.':
            return
            
        # Check if old data exists in current directory
        old_clients_dir = './clients'
        old_sessions_dir = './client_sessions'
        old_db_file = './audit_cache.db'
        
        new_clients_dir = os.path.join(user_data_dir, 'clients')
        new_sessions_dir = os.path.join(user_data_dir, 'client_sessions')
        new_db_file = os.path.join(user_data_dir, 'audit_cache.db')
        
        migrated_anything = False
        
        # Migrate client configs
        if os.path.exists(old_clients_dir) and os.listdir(old_clients_dir):
            if not os.path.exists(new_clients_dir):
                os.makedirs(new_clients_dir, exist_ok=True)
                import shutil
                for file in os.listdir(old_clients_dir):
                    if file.endswith('.json'):
                        shutil.copy2(os.path.join(old_clients_dir, file), 
                                   os.path.join(new_clients_dir, file))
                        migrated_anything = True
        
        # Migrate sessions
        if os.path.exists(old_sessions_dir) and os.listdir(old_sessions_dir):
            if not os.path.exists(new_sessions_dir):
                os.makedirs(new_sessions_dir, exist_ok=True)
                import shutil
                for item in os.listdir(old_sessions_dir):
                    old_path = os.path.join(old_sessions_dir, item)
                    new_path = os.path.join(new_sessions_dir, item)
                    if os.path.isdir(old_path):
                        shutil.copytree(old_path, new_path, dirs_exist_ok=True)
                        migrated_anything = True
                    elif item.endswith('.json'):
                        shutil.copy2(old_path, new_path)
                        migrated_anything = True
        
        # Migrate database
        if os.path.exists(old_db_file) and not os.path.exists(new_db_file):
            import shutil
            shutil.copy2(old_db_file, new_db_file)
            migrated_anything = True
        
        if migrated_anything and 'debug_messages' in st.session_state:
            st.session_state.debug_messages.append("[MIGRATION] Automatically migrated data from old location to user data directory")
            
    except Exception as e:
        if 'debug_messages' in st.session_state:
            st.session_state.debug_messages.append(f"[MIGRATION] Migration failed: {str(e)}")

# Constants
USER_DATA_DIR = get_user_data_dir()
CLIENT_CONFIG_DIR = os.path.join(USER_DATA_DIR, 'clients')

# --- Restore Supabase session (only once per session to avoid redundant overhead) ---
if use_supabase():
    if '_sb_session_applied' not in st.session_state:
        apply_session_from_state()
        st.session_state._sb_session_applied = True

# --- Authentication (Supabase) ---
if use_supabase():
    with st.sidebar:
        st.markdown("### Account")
        current_uid = get_current_user_id()
        if current_uid:
            st.success("Signed in")
            if st.button("Sign out", key="sb_sign_out"):
                sign_out()
                try:
                    clear_client_caches()
                    get_saved_sessions.clear()
                except Exception:
                    pass
                st.rerun()
        else:
            # Persist last-typed credentials across mode switches and reruns
            if 'last_typed_email' not in st.session_state:
                st.session_state.last_typed_email = ''
            if 'last_typed_password' not in st.session_state:
                st.session_state.last_typed_password = ''
            # Initialize widget state from last typed values only once
            if 'auth_email' not in st.session_state:
                st.session_state.auth_email = st.session_state.last_typed_email
            if 'auth_password' not in st.session_state:
                st.session_state.auth_password = st.session_state.last_typed_password

            auth_mode = st.radio("Select", ["Sign in", "Create account"], key="auth_mode")
            email = st.text_input("Email", key="auth_email")
            password = st.text_input("Password", type="password", key="auth_password")
            # Continuously remember last-typed values for auto-population
            st.session_state.last_typed_email = st.session_state.get('auth_email', '')
            st.session_state.last_typed_password = st.session_state.get('auth_password', '')
            if auth_mode == "Sign in":
                if st.button("Sign in", key="auth_sign_in_btn"):
                    ok, msg = sign_in(email, password)
                    if ok:
                        st.success(msg)
                        try:
                            clear_client_caches()
                            get_saved_sessions.clear()
                        except Exception:
                            pass
                        st.rerun()
                    else:
                        st.error(msg)
            else:
                if st.button("Create account", key="auth_sign_up_btn"):
                    ok, msg = sign_up(email, password)
                    if ok:
                        st.success(msg)
                        try:
                            clear_client_caches()
                            get_saved_sessions.clear()
                        except Exception:
                            pass
                        st.rerun()
                    else:
                        # Hide Supabase cooldown text; auto-switch to Sign in and rerun
                        if isinstance(msg, str) and ("For security purposes" in msg or "request this after" in msg):
                            st.session_state.auth_mode = "Sign in"
                            st.rerun()
                        else:
                            st.error(msg)

    if not get_current_user_id():
        st.title("Amazon Advertising Dashboard")
        st.info("Please sign in to access your clients and sessions.")
        st.stop()

# Lightweight file logger for packaged builds
def log_app_event(message: str):
    """Append a timestamped line to a log file in the user's data directory.
    Never raises; safe to call anywhere.
    """
    try:
        from datetime import datetime
        log_path = os.path.join(USER_DATA_DIR, 'app.log')
        ts = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        with open(log_path, 'a', encoding='utf-8') as lf:
            lf.write(f"[{ts}] {message}\n")
    except Exception:
        pass

# Auto-migrate old data on startup
if 'migration_checked' not in st.session_state:
    migrate_old_data()
    st.session_state.migration_checked = True

# --- Client Management Functions ---

def clear_client_caches():
    """Helper to clear all client-related caches across all storage layers."""
    # Clear filesystem cache (local only)
    try:
        _load_client_config_from_file.clear()
    except Exception:
        pass
    
    # SQLite and Supabase caches are self-managing via TTL
    # Session state caches are cleared per-key as needed

def get_existing_clients():
    """Returns a list of client names from the config directory or localStorage.
    
    Caching is handled at the supabase_store layer with multi-tier caching
    (session_state -> SQLite -> Supabase) for optimal performance.
    """
    if is_cloud_environment():
        # Prefer Supabase per-user storage in cloud
        if use_supabase():
            uid = get_current_user_id()
            if not uid:
                return []
            return sb_list_client_names(uid)
        # Fallback: browser localStorage (legacy)
        if 'client_list' not in st.session_state:
            stored_list = get_localStorage_value('amazon_dashboard_client_list')
            if stored_list:
                try:
                    st.session_state.client_list = json.loads(stored_list)
                except:
                    st.session_state.client_list = []
            else:
                st.session_state.client_list = []
        # Merge any in-session imported clients so they appear during this session
        session_only_clients = []
        try:
            if 'in_memory_clients' in st.session_state and isinstance(st.session_state.in_memory_clients, dict):
                session_only_clients = list(st.session_state.in_memory_clients.keys())
        except Exception:
            session_only_clients = []
        base_list = st.session_state.get('client_list', [])
        if session_only_clients:
            # Deduplicate while preserving alphabetical order
            merged = sorted(set(base_list) | set(session_only_clients))
            return merged
        return base_list
    else:
        # Use filesystem locally
        if not os.path.exists(CLIENT_CONFIG_DIR):
            os.makedirs(CLIENT_CONFIG_DIR)
        clients = [f.replace('.json', '') for f in os.listdir(CLIENT_CONFIG_DIR) if f.endswith('.json')]
        log_app_event(f"get_existing_clients() found {len(clients)} clients in {CLIENT_CONFIG_DIR}")
        return sorted(clients)

def load_client_config(client_name):
    """Loads the configuration for a given client from localStorage or filesystem.
    
    Note: No caching decorator for cloud mode because get_localStorage_value uses
    components.html which needs to render on each run to return values.
    Session state provides the caching layer instead.
    """
    if is_cloud_environment():
        cache_key = f'client_config_cache_{client_name}'
        # Cloud: prefer Supabase
        if use_supabase():
            if cache_key in st.session_state:
                return st.session_state[cache_key]
            uid = get_current_user_id()
            if not uid:
                return None
            # sb_fetch_client_config now has session-level caching built in
            config = sb_fetch_client_config(uid, client_name)
            if config is not None:
                st.session_state[cache_key] = config
            return config
        # Cloud without Supabase: use session-only in-memory store first
        try:
            if 'in_memory_clients' in st.session_state and isinstance(st.session_state.in_memory_clients, dict):
                if client_name in st.session_state.in_memory_clients:
                    config = st.session_state.in_memory_clients[client_name]
                    st.session_state[cache_key] = config
                    return config
        except Exception:
            pass
        # Legacy: browser localStorage path
        if cache_key in st.session_state:
            return st.session_state[cache_key]
        stored_config = get_localStorage_value(f'amazon_dashboard_client_{client_name}')
        if stored_config is None or not isinstance(stored_config, str):
            st.session_state[f'{cache_key}_loading'] = True
            return None
        if f'{cache_key}_loading' in st.session_state:
            del st.session_state[f'{cache_key}_loading']
        if stored_config:
            try:
                config = json.loads(stored_config)
                st.session_state[cache_key] = config
                return config
            except (json.JSONDecodeError, TypeError):
                st.error(f"Error reading configuration for {client_name}. It might be corrupted.")
                if isinstance(stored_config, str):
                    st.error(f"Debug - stored_config (first 200 chars): {stored_config[:200]}")
                else:
                    st.error(f"Debug - stored_config type: {type(stored_config).__name__}")
                return None
        return None
    else:
        # Use filesystem locally - can use caching here
        return _load_client_config_from_file(client_name)

@st.cache_data(ttl=300, show_spinner=False)
def _load_client_config_from_file(client_name):
    """Helper function for filesystem loading with caching."""
    filepath = os.path.join(CLIENT_CONFIG_DIR, f"{client_name}.json")
    log_app_event(f"Attempting to load client config from: {filepath}")
    
    if os.path.exists(filepath):
        try:
            with open(filepath, 'r') as f:
                config = json.load(f)
                log_app_event(f"Successfully loaded config for {client_name}")
                return config
        except json.JSONDecodeError as e:
            error_msg = f"JSON decode error for {client_name}: {str(e)}"
            st.error(f"Error reading configuration file for {client_name}. It might be corrupted.")
            log_app_event(error_msg)
            return None
        except Exception as e:
            error_msg = f"Unexpected error loading {client_name}: {str(e)}"
            st.error(error_msg)
            log_app_event(error_msg)
            return None
    else:
        log_app_event(f"Config file not found for {client_name} at {filepath}")
        return None

def get_campaigns_from_bulk_file(bulk_data):
    """Extracts unique campaign names and types from the bulk file data."""
    campaign_data = {}
    campaign_type_map = {
        'Sponsored Products Campaigns': 'Sponsored Products',
        'Sponsored Brands Campaigns': 'Sponsored Brands',
        'Sponsored Display Campaigns': 'Sponsored Display'
    }
    
    for sheet_name, df in bulk_data.items():
        if sheet_name in campaign_type_map:
            campaign_type = campaign_type_map[sheet_name]
            
            if 'Campaign Name' in df.columns and 'State' in df.columns and 'Entity' in df.columns:
                all_campaigns = df[df['Entity'] == 'Campaign']
                for _, row in all_campaigns.drop_duplicates(subset=['Campaign Name']).iterrows():
                    campaign_name = row['Campaign Name']
                    if campaign_name not in campaign_data:
                        campaign_data[campaign_name] = campaign_type
    return campaign_data

def save_client_config(client_name, config_data):
    """Saves the configuration for a given client to localStorage or filesystem.

    Only persists and toggles ``st.session_state.settings_updated`` when the
    incoming ``config_data`` differs from what is already stored. This avoids
    unnecessary cache invalidation and re-processing on the Advertising Audit page
    when the user opens *Client Settings* and clicks *Save* without making any real
    changes.
    """
    # Utility to canonicalise config for a *stable*, order-insensitive comparison
    def _normalise(obj):
        """Recursively sort dict keys and list items so ordering differences
        do not trigger a false-positive change detection."""
        if isinstance(obj, dict):
            return {k: _normalise(obj[k]) for k in sorted(obj)}
        if isinstance(obj, list):
            # Sort lists *after* normalising their items so that nested dicts
            # are comparable via JSON serialisation.
            return sorted((_normalise(i) for i in obj), key=lambda x: json.dumps(x, sort_keys=True))
        return obj

    if is_cloud_environment():
        # Cloud: prefer Supabase per-user storage
        if use_supabase():
            uid = get_current_user_id()
            if not uid:
                st.error("Please sign in to save client settings.")
                st.session_state.settings_updated = False
                return
            previous_data = sb_fetch_client_config(uid, client_name)
            if previous_data is not None and _normalise(previous_data) == _normalise(config_data):
                st.session_state.settings_updated = False
                return
            ok = sb_upsert_client_config(uid, client_name, config_data)
            if not ok:
                st.error("Failed to save client configuration.")
                st.session_state.settings_updated = False
                return
            # Update cache
            cache_key = f'client_config_cache_{client_name}'
            st.session_state[cache_key] = config_data
            # Refresh clients cache
            clear_client_caches()
            st.session_state.settings_updated = True
            return
        # Cloud without Supabase: session-only fallback with best-effort localStorage write
        try:
            # Maintain in-memory map
            if 'in_memory_clients' not in st.session_state or not isinstance(st.session_state.in_memory_clients, dict):
                st.session_state.in_memory_clients = {}
            st.session_state.in_memory_clients[client_name] = config_data
        except Exception:
            pass
        # Try legacy localStorage write (may be unsupported on Streamlit Cloud)
        try:
            previous_data = None
            stored_config = get_localStorage_value(f'amazon_dashboard_client_{client_name}')
            if stored_config:
                try:
                    previous_data = json.loads(stored_config)
                except Exception:
                    previous_data = None
            if previous_data is None or _normalise(previous_data) != _normalise(config_data):
                set_localStorage_value(f'amazon_dashboard_client_{client_name}', config_data)
        except Exception:
            # Ignore localStorage failures in cloud; we rely on in-session memory
            pass
        cache_key = f'client_config_cache_{client_name}'
        st.session_state[cache_key] = config_data
        # Merge into visible client list for this session
        try:
            if 'client_list' not in st.session_state or not isinstance(st.session_state.client_list, list):
                st.session_state.client_list = []
            if client_name not in st.session_state.client_list:
                st.session_state.client_list.append(client_name)
                try:
                    set_localStorage_value('amazon_dashboard_client_list', st.session_state.client_list)
                except Exception:
                    pass
        except Exception:
            pass
        clear_client_caches()
        st.session_state.clients_list_changed = True
        st.session_state.settings_updated = True
    else:
        # Use filesystem locally
        if not os.path.exists(CLIENT_CONFIG_DIR):
            os.makedirs(CLIENT_CONFIG_DIR)

        filepath = os.path.join(CLIENT_CONFIG_DIR, f"{client_name}.json")

        # Load the previously-saved config (if available) to compare.
        previous_data = None
        if os.path.exists(filepath):
            try:
                with open(filepath, "r") as f:
                    previous_data = json.load(f)
            except Exception:
                # Corrupted or unreadable file â€“ treat as changed so we overwrite it.
                previous_data = None

        # Compare *normalised* versions to ignore inconsequential ordering changes.
        if previous_data is not None and _normalise(previous_data) == _normalise(config_data):
            # Ensure the page isn't falsely refreshed by clearing any stale change flag.
            st.session_state.settings_updated = False
            return  # No meaningful change; skip write & flag.

        # Persist the new configuration to disk.
        with open(filepath, "w") as f:
            json.dump(config_data, f, indent=4)

        # Clear caches so the new/updated client appears in the list
        clear_client_caches()

        # Flag downstream pages to refresh because something actually changed.
        st.session_state.settings_updated = True

# --- Session Management Functions ---

# Constants for session management
CLIENT_SESSIONS_DIR = os.path.join(USER_DATA_DIR, 'client_sessions')

def ensure_session_directory(client_name):
    """Ensures the session directory exists for a given client."""
    client_session_dir = os.path.join(CLIENT_SESSIONS_DIR, client_name)
    if not os.path.exists(client_session_dir):
        os.makedirs(client_session_dir)
    return client_session_dir

@st.cache_data(ttl=300, show_spinner=False)  # Cache for 5 minutes
def get_saved_sessions(client_name):
    """Returns a list of saved session files for a given client from localStorage or filesystem."""
    if is_cloud_environment():
        # Cloud: prefer Supabase
        if use_supabase():
            uid = get_current_user_id()
            if not uid:
                return []
            return sb_list_sessions(uid, client_name)
        # Legacy: browser localStorage
        sessions_key = f'amazon_dashboard_sessions_{client_name}'
        stored_sessions = get_localStorage_value(sessions_key)
        if stored_sessions:
            try:
                return json.loads(stored_sessions)
            except:
                return []
        return []
    else:
        # Use filesystem locally
        client_session_dir = os.path.join(CLIENT_SESSIONS_DIR, client_name)
        if not os.path.exists(client_session_dir):
            return []
        
        sessions = []
        for file in os.listdir(client_session_dir):
            if file.endswith('.json'):
                filepath = os.path.join(client_session_dir, file)
                try:
                    with open(filepath, 'r') as f:
                        session_data = json.load(f)
                        sessions.append({
                            'filename': file,
                            'filepath': filepath,
                            'display_name': session_data.get('session_name', file.replace('.json', '')),
                            'timestamp': session_data.get('timestamp', 'Unknown'),
                            'created_date': session_data.get('created_date', 'Unknown'),
                            'description': session_data.get('description', 'No description'),
                            'data_types': session_data.get('data_types', [])
                        })
                except (json.JSONDecodeError, KeyError) as e:
                    st.session_state.debug_messages.append(f"Error reading session file {file}: {str(e)}")
                    continue
        
        # Sort by timestamp (newest first)
        sessions.sort(key=lambda x: x['timestamp'], reverse=True)
        return sessions

def save_audit_session(client_name, session_name=None, description=""):
    """Saves the current audit session data for a client to localStorage or filesystem."""
    if not client_name:
        st.error("No client selected for saving session.")
        return False
    
    # Generate session name if not provided
    if not session_name:
        timestamp = datetime.now().strftime("%m/%d/%Y")
        session_name = f"Audit Session {timestamp}"
    
    # Prepare session data
    session_data = {
        'session_name': session_name,
        'client_name': client_name,
        'timestamp': datetime.now().isoformat(),
        'created_date': datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        'description': description,
        'data_types': [],
        'bulk_data': None,
        'sales_report_data': None,
        'is_companion_data': st.session_state.get('is_companion_data', False),
        'sd_attribution_choice': st.session_state.get('sd_attribution_choice', 'Sales')
    }
    
    # Save bulk data if available
    if 'bulk_data' in st.session_state and st.session_state.bulk_data is not None:
        # Convert DataFrames to dict format for JSON serialization
        bulk_data_serializable = {}
        if isinstance(st.session_state.bulk_data, dict):
            for sheet_name, df in st.session_state.bulk_data.items():
                if isinstance(df, pd.DataFrame):
                    bulk_data_serializable[sheet_name] = df.to_dict('records')
        session_data['bulk_data'] = bulk_data_serializable
        data_type = "Companion Data" if st.session_state.get('is_companion_data', False) else "Bulk File"
        session_data['data_types'].append(data_type)
    
    # Save sales report data if available
    if 'sales_report_data' in st.session_state and st.session_state.sales_report_data is not None:
        if isinstance(st.session_state.sales_report_data, pd.DataFrame):
            session_data['sales_report_data'] = st.session_state.sales_report_data.to_dict('records')
        session_data['data_types'].append("Sales Report")
    
    # Create filename from session name (sanitized)
    safe_session_name = re.sub(r'[^\w\-_\.]', '_', session_name)
    filename = f"{safe_session_name}.json"
    
    try:
        if is_cloud_environment():
            # Cloud: prefer Supabase
            if use_supabase():
                uid = get_current_user_id()
                if not uid:
                    st.error("Please sign in to save sessions.")
                    return False
                session_metadata = {
                    'filename': filename,
                    'display_name': session_data.get('session_name', filename.replace('.json', '')),
                    'timestamp': session_data.get('timestamp', 'Unknown'),
                    'created_date': session_data.get('created_date', 'Unknown'),
                    'description': session_data.get('description', 'No description'),
                    'data_types': session_data.get('data_types', []),
                }
                ok = sb_save_session(uid, client_name, filename, session_metadata, session_data)
                if not ok:
                    st.error("Failed to save session.")
                    return False
                get_saved_sessions.clear()
            else:
                # Legacy: browser localStorage
                sessions_key = f'amazon_dashboard_sessions_{client_name}'
                stored_sessions = get_localStorage_value(sessions_key)
                sessions_list = json.loads(stored_sessions) if stored_sessions else []
                session_metadata = {
                    'filename': filename,
                    'display_name': session_data.get('session_name', filename.replace('.json', '')),
                    'timestamp': session_data.get('timestamp', 'Unknown'),
                    'created_date': session_data.get('created_date', 'Unknown'),
                    'description': session_data.get('description', 'No description'),
                    'data_types': session_data.get('data_types', [])
                }
                sessions_list = [s for s in sessions_list if s.get('filename') != filename]
                sessions_list.append(session_metadata)
                sessions_list.sort(key=lambda x: x.get('timestamp', ''), reverse=True)
                set_localStorage_value(f'amazon_dashboard_session_data_{client_name}_{filename}', session_data)
                set_localStorage_value(sessions_key, sessions_list)
                get_saved_sessions.clear()
        else:
            # Use filesystem locally
            client_session_dir = ensure_session_directory(client_name)
            filepath = os.path.join(client_session_dir, filename)
            
            with open(filepath, 'w') as f:
                json.dump(session_data, f, indent=2)
            
            # Clear the cache to refresh the session list
            get_saved_sessions.clear()
        
        return True
        
    except Exception as e:
        st.error(f"Error saving session: {str(e)}")
        return False

def load_audit_session(client_name, session_filename):
    """Loads a saved audit session for a client from localStorage or filesystem."""
    if not client_name or not session_filename:
        st.error("Invalid client name or session filename.")
        return False
    
    try:
        if is_cloud_environment():
            # Cloud: prefer Supabase
            if use_supabase():
                uid = get_current_user_id()
                if not uid:
                    st.error("Please sign in to load sessions.")
                    return False
                session_data = sb_fetch_session(uid, client_name, session_filename)
                if session_data is None:
                    st.error("Session file not found.")
                    return False
            else:
                # Legacy: browser localStorage
                session_key = f'amazon_dashboard_session_data_{client_name}_{session_filename}'
                stored_data = get_localStorage_value(session_key)
                if not stored_data:
                    st.error("Session file not found.")
                    return False
                session_data = json.loads(stored_data)
        else:
            # Use filesystem locally
            filepath = os.path.join(CLIENT_SESSIONS_DIR, client_name, session_filename)
            
            if not os.path.exists(filepath):
                st.error("Session file not found.")
                return False
            
            with open(filepath, 'r') as f:
                session_data = json.load(f)
        
        # Clear existing session data
        st.session_state.bulk_data = None
        st.session_state.sales_report_data = None
        st.session_state.is_companion_data = False
        
        # Load bulk data if available
        if session_data.get('bulk_data'):
            bulk_data_restored = {}
            for sheet_name, records in session_data['bulk_data'].items():
                if records:  # Only restore if there are records
                    bulk_data_restored[sheet_name] = pd.DataFrame(records)
            st.session_state.bulk_data = bulk_data_restored if bulk_data_restored else None
        
        # Load sales report data if available
        if session_data.get('sales_report_data'):
            st.session_state.sales_report_data = pd.DataFrame(session_data['sales_report_data'])
        
        # Restore other session state variables
        st.session_state.is_companion_data = session_data.get('is_companion_data', False)
        st.session_state.sd_attribution_choice = session_data.get('sd_attribution_choice', 'Sales')
        
        # Clear caches to ensure fresh analysis
        clear_caches()
        st.session_state.last_cache_refresh = datetime.now()
        
        return True
        
    except Exception as e:
        st.error(f"Error loading session: {str(e)}")
        return False

def delete_audit_session(client_name, session_filename):
    """Deletes a saved audit session for a client from localStorage or filesystem."""
    if not client_name or not session_filename:
        return False
    
    try:
        if is_cloud_environment():
            # Cloud: prefer Supabase
            if use_supabase():
                uid = get_current_user_id()
                if not uid:
                    return False
                ok = sb_delete_session(uid, client_name, session_filename)
                if ok:
                    get_saved_sessions.clear()
                return ok
            else:
                # Legacy: browser localStorage
                remove_localStorage_value(f'amazon_dashboard_session_data_{client_name}_{session_filename}')
                sessions_key = f'amazon_dashboard_sessions_{client_name}'
                stored_sessions = get_localStorage_value(sessions_key)
                if stored_sessions:
                    sessions_list = json.loads(stored_sessions)
                    sessions_list = [s for s in sessions_list if s.get('filename') != session_filename]
                    set_localStorage_value(sessions_key, sessions_list)
                get_saved_sessions.clear()
                return True
        else:
            # Use filesystem locally
            filepath = os.path.join(CLIENT_SESSIONS_DIR, client_name, session_filename)
            
            if not os.path.exists(filepath):
                return False
            
            os.remove(filepath)
            # Clear the cache to refresh the session list
            get_saved_sessions.clear()
            return True
    except Exception as e:
        return False

# --- Data Processing Functions ---

@st.cache_data(ttl=3600, show_spinner=False)  # Cache for 1 hour
def extract_asins_from_sales_report(sales_df):
    """
    Extracts ASINs and their titles from a sales report DataFrame.
    Returns a dictionary mapping ASINs to their titles.
    """
    if sales_df is None or sales_df.empty:
        st.session_state.debug_messages.append("Cannot extract ASINs: Sales report is empty")
        return {}
    
    # Check for ASIN columns with different possible names
    asin_column = None
    possible_asin_columns = ['ASIN', '(Child) ASIN', 'child asin', 'asin', '(Parent) ASIN', 'parent asin', 'sku']
    
    for col_name in possible_asin_columns:
        if col_name in sales_df.columns:
            asin_column = col_name
            st.session_state.debug_messages.append(f"Found ASIN column: {asin_column}")
            break
    
    # Also check for case-insensitive matches
    if asin_column is None:
        for col in sales_df.columns:
            col_lower = str(col).lower().strip()
            if any(name.lower() in col_lower for name in possible_asin_columns):
                asin_column = col
                st.session_state.debug_messages.append(f"Found case-insensitive ASIN column match: {asin_column}")
                break
    
    if asin_column is None:
        st.session_state.debug_messages.append("Cannot extract ASINs: No ASIN column found in sales report")
        st.session_state.debug_messages.append(f"Available columns: {list(sales_df.columns)}")
        return {}
    
    asin_title_map = {}
    standard_asin_pattern = re.compile(r'(B[0-9A-Z]{9})', re.IGNORECASE)
    
    # Ensure we have a Title column
    title_column = None
    possible_title_columns = ['Title', 'Product Title', 'title', 'product name', 'item name']
    
    for col_name in possible_title_columns:
        if col_name in sales_df.columns:
            title_column = col_name
            break
    
    # Also check for case-insensitive matches
    if title_column is None:
        for col in sales_df.columns:
            col_lower = str(col).lower().strip()
            if any(name.lower() in col_lower for name in possible_title_columns):
                title_column = col
                break
    
    if title_column is None:
        # Create a placeholder title column
        sales_df['Title'] = 'Title not available'
        title_column = 'Title'
    
    # Process each row to extract ASINs and titles
    for _, row in sales_df.iterrows():
        asin = str(row[asin_column]).strip()
        
        # Skip invalid ASINs
        if not asin or pd.isna(asin) or asin.lower() == 'nan' or asin == '':
            continue
            
        # First try to extract standard ASINs using regex pattern
        asin_matches = standard_asin_pattern.findall(asin)
        if asin_matches:
            asin = asin_matches[0].upper()  # Standardize to uppercase
        else:
            # If it doesn't match the standard pattern but has non-empty content, keep it as is
            # This will capture non-standard ASINs that still have valid sales data
            asin = asin.upper()  # Just standardize to uppercase
            st.session_state.debug_messages.append(f"Found non-standard ASIN format: {asin}")
        
        # Get the title
        title = str(row[title_column]).strip()
        if pd.isna(title) or title.lower() == 'nan' or title == '':
            title = 'Title not available'
            
        asin_title_map[asin] = title
    
    st.session_state.debug_messages.append(f"Extracted {len(asin_title_map)} ASINs with titles from sales report")
    return asin_title_map

@st.cache_data(ttl=3600, show_spinner=False)  # Cache for 1 hour
def process_sales_report(uploaded_file):
    """
    Reads an SC or VC sales report (CSV or XLSX) and extracts relevant sales data.
    Simplified version with maximum compatibility for different file formats.
    Handles Vendor Central reports with metadata row at the top.
    """
    # Check for cached data first
    client_name = st.session_state.get('client_config', {}).get('client_name', 'unknown')
    
    # Get file content for hashing
    uploaded_file.seek(0)
    file_content = uploaded_file.read()
    uploaded_file.seek(0)  # Reset for processing
    
    # Try to get cached data
    cached_data = db_manager.get_cached_sales_report(client_name, file_content)
    if cached_data is not None:
        return cached_data
    
    try:
        # Step 1: Detect if this is a Vendor Central report with metadata row
        file_name = uploaded_file.name
        uploaded_file.seek(0)
        
        # Read the first few lines to check for header
        header_row_index = 0  # Default to first row
        
        if file_name.endswith('.csv'):
            # Read first 10 rows to check for header
            try:
                preview_df = pd.read_csv(uploaded_file, header=None, nrows=10, encoding='utf-8')
            except UnicodeDecodeError:
                uploaded_file.seek(0)
                preview_df = pd.read_csv(uploaded_file, header=None, nrows=10, encoding='latin1')
        elif file_name.endswith('.xlsx'):
            preview_df = pd.read_excel(uploaded_file, header=None, nrows=10)
        else:
            return None  # Unsupported file format
        
        # Check for metadata row (first row contains equals signs or specific Vendor Central patterns)
        if len(preview_df) > 0:
            first_row_str = ' '.join([str(x) for x in preview_df.iloc[0].values if pd.notna(x)])
            # Check for equals signs (general metadata indicator) or specific VC patterns
            if '=' in first_row_str or 'Program=' in first_row_str or 'Distributor View=' in first_row_str:
                header_row_index = 1  # Skip the metadata row
                st.session_state.debug_messages.append(f"Detected metadata row with equals sign or VC pattern, using row {header_row_index} as header")
        
        # Step 2: Read the file with the correct header row
        uploaded_file.seek(0)
        
        if file_name.endswith('.csv'):
            # Try a few common encodings for CSV files
            for encoding in ['utf-8', 'latin1', 'ISO-8859-1']:
                try:
                    df = pd.read_csv(uploaded_file, header=header_row_index, encoding=encoding)
                    if not df.empty:
                        break
                except Exception as e:
                    st.session_state.debug_messages.append(f"Error with encoding {encoding}: {str(e)}")
                    uploaded_file.seek(0)
                    continue
        elif file_name.endswith('.xlsx'):
            df = pd.read_excel(uploaded_file, header=header_row_index)
        else:
            return None  # Unsupported file format
            
        if df is None or df.empty:
            st.session_state.debug_messages.append("File loaded but DataFrame is empty")
            return None
        
        # Step 3: Basic data cleaning
        df = df.dropna(how='all')  # Drop rows that are all NaN
        
        # Debug output to help troubleshoot
        original_columns = list(df.columns)
        st.session_state.debug_messages.append(f"Available columns after loading: {original_columns}")
        
        # Step 4: Create standardized DataFrame with proper column mapping
        standardized_df = pd.DataFrame()
        
        # Helper function for case-insensitive column matching
        def find_column_case_insensitive(target_patterns, available_columns):
            """
            Find a column that matches any of the target patterns (case-insensitive).
            Returns the actual column name from available_columns, or None if not found.
            """
            for col in available_columns:
                col_clean = str(col).lower().strip()
                for pattern in target_patterns:
                    pattern_clean = str(pattern).lower().strip()
                    if col_clean == pattern_clean:
                        return col
            return None
        
        # Helper function for partial case-insensitive matching
        def find_column_partial_match(target_patterns, available_columns):
            """
            Find a column that partially matches any of the target patterns (case-insensitive).
            Returns the actual column name from available_columns, or None if not found.
            """
            for col in available_columns:
                col_clean = str(col).lower().strip()
                for pattern in target_patterns:
                    pattern_clean = str(pattern).lower().strip()
                    if pattern_clean in col_clean:
                        return col
            return None
        
        # Standard column mappings for non-sales columns (case-insensitive)
        column_mappings = {
            'ASIN': ['ASIN', '(Child) ASIN', 'child asin'],
            'Parent ASIN': ['(Parent) ASIN', 'Parent ASIN', 'parent asin'],
            'Title': ['Title', 'Product Title', 'product name', 'item name'],
            'Sessions': ['Sessions', 'Sessions - Total', 'Glance Views', 'Page Views', 'traffic', 'views'],
            'Orders': ['Ordered Units', 'Shipped Units', 'Units Sold', 'units', 'orders', 'quantity']
        }
        
        # Sales column patterns (case-insensitive)
        sales_patterns = [
            'Ordered Revenue', 'Shipped Revenue', 'Ordered Product Sales', 
            'Shipped Product Sales', 'Product Sales', 'Gross Product Sales', 
            'Shipped COGS', 'Total Sales', 'Net Sales', 'Revenue'
        ]
        
        # Find all sales columns using case-insensitive matching
        found_sales_columns = []
        for pattern in sales_patterns:
            # First try exact case-insensitive match
            exact_match = find_column_case_insensitive([pattern], original_columns)
            if exact_match and exact_match not in found_sales_columns:
                found_sales_columns.append(exact_match)
                continue
            
            # Then try partial match for columns containing sales/revenue keywords
            for col in original_columns:
                col_lower = str(col).lower().strip()
                pattern_lower = pattern.lower().strip()
                if (pattern_lower in col_lower and 
                    ('sales' in col_lower or 'revenue' in col_lower) and 
                    col not in found_sales_columns):
                    found_sales_columns.append(col)
                    break
        
        st.session_state.debug_messages.append(f"Found {len(found_sales_columns)} sales columns: {found_sales_columns}")
        
        # Map standard columns using case-insensitive matching
        for std_col, possible_names in column_mappings.items():
            # Try case-insensitive exact match first
            matched_col = find_column_case_insensitive(possible_names, original_columns)
            
            if matched_col:
                # Found an exact case-insensitive match
                if std_col in ['Sessions', 'Orders']:
                    # Clean and convert numeric columns
                    values = df[matched_col].astype(str).str.replace(r'[$,Â£â‚¬]', '', regex=True)
                    standardized_df[std_col] = pd.to_numeric(values, errors='coerce').fillna(0)
                else:
                    # String columns
                    standardized_df[std_col] = df[matched_col].astype(str)
                
                st.session_state.debug_messages.append(f"Mapped {matched_col} to {std_col} (case-insensitive exact match)")
                continue
            
            # If no exact match, try partial matching
            matched_col = find_column_partial_match(possible_names, original_columns)
            
            if matched_col:
                # Found a partial case-insensitive match
                if std_col in ['Sessions', 'Orders']:
                    # Clean and convert numeric columns
                    values = df[matched_col].astype(str).str.replace(r'[$,Â£â‚¬]', '', regex=True)
                    standardized_df[std_col] = pd.to_numeric(values, errors='coerce').fillna(0)
                else:
                    # String columns
                    standardized_df[std_col] = df[matched_col].astype(str)
                
                st.session_state.debug_messages.append(f"Mapped {matched_col} to {std_col} (case-insensitive partial match)")
                continue
            
            # Log if no match found
            st.session_state.debug_messages.append(f"No match found for {std_col} in available columns")
        
        # Add ALL sales columns with their original names
        for sales_col in found_sales_columns:
            if sales_col in df.columns:
                # Clean and convert numeric columns
                values = df[sales_col].astype(str).str.replace(r'[$,Â£â‚¬]', '', regex=True)
                standardized_df[sales_col] = pd.to_numeric(values, errors='coerce').fillna(0)
                st.session_state.debug_messages.append(f"Preserved original sales column: {sales_col}")
        
        # Check if we found the essential columns
        if 'ASIN' not in standardized_df.columns:
            st.session_state.debug_messages.append("ASIN column not found in the report")
            return None
            
        # If no title column found, use ASIN as title
        if 'Title' not in standardized_df.columns:
            standardized_df['Title'] = standardized_df['ASIN']
            
        # If no sales columns found, add a placeholder Total Sales column
        if not found_sales_columns:
            standardized_df['Total Sales'] = 0.0
            st.session_state.debug_messages.append("No sales columns found, adding placeholder Total Sales column")
            found_sales_columns = ['Total Sales']
        
        # Step 5: Clean up and return the standardized DataFrame
        # Remove any rows with missing or invalid ASINs
        standardized_df = standardized_df.dropna(subset=['ASIN'])
        standardized_df = standardized_df[standardized_df['ASIN'].str.strip() != '']
        
        # Aggregate by ASIN (in case there are duplicates)
        agg_dict = {
            'Title': 'first',
        }
        
        # Add Parent ASIN to aggregation if it exists
        if 'Parent ASIN' in standardized_df.columns:
            agg_dict['Parent ASIN'] = 'first'
        
        # Add all sales columns to aggregation
        for sales_col in found_sales_columns:
            if sales_col in standardized_df.columns:
                agg_dict[sales_col] = 'sum'
        
        if 'Sessions' in standardized_df.columns:
            agg_dict['Sessions'] = 'sum'
        if 'Orders' in standardized_df.columns:
            agg_dict['Orders'] = 'sum'
            
        result_df = standardized_df.groupby('ASIN', as_index=False).agg(agg_dict)
        
        # Ensure we have at least one row
        if len(result_df) > 0:
            # Cache the processed data
            db_manager.cache_sales_report(client_name, file_content, result_df)
            
            st.session_state.debug_messages.append(f"Successfully processed sales report with {len(result_df)} unique ASINs")
            return result_df
        else:
            st.session_state.debug_messages.append("No valid data rows found after processing")
            return None
            
    except Exception as e:
        import traceback
        error_details = traceback.format_exc()
        st.session_state.debug_messages.append(f"Error processing sales report: {str(e)}")
        st.session_state.debug_messages.append(f"Error details: {error_details}")
        return None



@st.cache_data(ttl=3600, show_spinner=False)  # Cache for 1 hour
def detect_and_persist_sku_asin_mappings(bulk_data):
    """
    Detect SKU-ASIN mappings from bulk file data and persist them to Branded ASINs.
    This allows us to maintain SKU mappings over time for Seller Central clients.
    """
    if not bulk_data or 'client_config' not in st.session_state:
        return
    
    # Initialize branded_asins_data if not present
    if 'branded_asins_data' not in st.session_state.client_config:
        st.session_state.client_config['branded_asins_data'] = {}
    
    sku_asin_mappings = {}
    
    # Look for SKU-ASIN mappings in all bulk file sheets using same logic as Performance by ASIN
    for sheet_name, df in bulk_data.items():
        if not isinstance(df, pd.DataFrame) or df.empty:
            continue
            
        # Find SKU and ASIN columns (case-insensitive, same logic as Performance by ASIN)
        sku_col = None
        asin_col = None
        entity_col = None
        
        # Find SKU column (exact case-insensitive match)
        for col in df.columns:
            if col.upper() == 'SKU':
                sku_col = col
                break
        
        # Find ASIN column (prioritize ASIN (INFORMATIONAL ONLY))
        for col in df.columns:
            if col.upper() == 'ASIN (INFORMATIONAL ONLY)':
                asin_col = col
                break
            elif col.upper() == 'ASIN':
                asin_col = col
        
        # Find Entity column
        for col in df.columns:
            if col.upper() == 'ENTITY':
                entity_col = col
                break
        
        # Extract SKU-ASIN mappings if both columns found
        if sku_col and asin_col:
            # Use same filtering logic as Performance by ASIN
            if entity_col:
                # Try the strict Entity filter first (case insensitive)
                product_ad_rows = df[
                    (df[entity_col].astype(str).str.strip().str.lower().isin(['product ad', 'product ads', 'productad', 'productads'])) &
                    (df[sku_col].notna()) &
                    (df[asin_col].notna())
                ]
                
                # If no results with Entity filter, try without it
                if product_ad_rows.empty:
                    product_ad_rows = df[
                        (df[sku_col].notna()) &
                        (df[asin_col].notna())
                    ]
            else:
                # No Entity column, just filter by SKU and ASIN presence
                product_ad_rows = df[
                    (df[sku_col].notna()) &
                    (df[asin_col].notna())
                ]
            
            # Extract mappings from filtered rows
            for _, row in product_ad_rows.iterrows():
                sku = str(row[sku_col]).strip()
                asin = str(row[asin_col]).strip().upper()
                
                if sku and asin and sku != 'nan' and asin != 'NAN':
                    sku_asin_mappings[asin] = sku
                    
            if len(product_ad_rows) > 0:
                st.session_state.debug_messages.append(f"Found {len(product_ad_rows)} rows with SKU data in sheet '{sheet_name}'")
    
    # Persist new SKU mappings to Branded ASINs data
    if sku_asin_mappings:
        updated_count = 0
        for asin, sku in sku_asin_mappings.items():
            # Update existing ASIN entries with SKU data
            if asin in st.session_state.client_config['branded_asins_data']:
                current_sku = st.session_state.client_config['branded_asins_data'][asin].get('sku', '')
                if not current_sku or current_sku != sku:
                    st.session_state.client_config['branded_asins_data'][asin]['sku'] = sku
                    updated_count += 1
            # Create new ASIN entry if it doesn't exist (for Seller Central clients)
            else:
                st.session_state.client_config['branded_asins_data'][asin] = {
                    'product_title': 'Title not available',
                    'product_group': '',
                    'sku': sku
                }
                updated_count += 1
        
        if updated_count > 0:
            # Save the updated config
            save_client_config(st.session_state.selected_client_name, st.session_state.client_config)
            st.session_state.debug_messages.append(f"Persisted {updated_count} SKU-ASIN mappings to Branded ASINs data")

def process_bulk_data(uploaded_file):
    """
    Reads all sheets from the bulk advertising file, cleans data,
    and returns a dictionary of DataFrames.
    """
    # Check for cached data first
    client_name = st.session_state.get('client_config', {}).get('client_name', 'unknown')
    
    # Get file content for hashing
    uploaded_file.seek(0)
    file_content = uploaded_file.read()
    uploaded_file.seek(0)  # Reset for processing
    
    # Try to get cached data
    cached_data = db_manager.get_cached_bulk_data(client_name, file_content)
    if cached_data:
        track_cache_hit("bulk_data_processing")
        return cached_data
    else:
        track_cache_miss("bulk_data_processing")
    
    bulk_data = {}
    # These are the primary campaign sheets we'll use for campaign analysis
    campaign_sheets = [
        'Sponsored Products Campaigns',
        'Sponsored Brands Campaigns',
        'Sponsored Display Campaigns', 
        'SB Multi Ad Group Campaigns',  # Added SB Multi Ad Group Campaigns as a relevant sheet
    ]

    numeric_cols_pattern = [
        'Impressions', 'Clicks', 'Spend', 'Orders', 'Total Sales', 'Sales', 'ROAS',
        'Cost Per Click (CPC)', 'Click-Thru Rate (CTR)',
        '7 Day Total Orders (#)', '7 Day Total Sales', 'Sales (Views & Clicks)'
    ]
    
    # Define the sheets that require campaign-level filtering
    campaign_sheets_to_filter = [
        'Sponsored Products Campaigns',
        'Sponsored Brands Campaigns',
        'Sponsored Display Campaigns',
        'SB Multi Ad Group Campaigns'  # Added SB Multi Ad Group Campaigns to filtering list
    ]

    try:
        # Use a more efficient approach to read Excel
        xls = pd.ExcelFile(uploaded_file)
        
        # Helper function to find actual sheet name case-insensitively
        def find_actual_sheet_name(target_name):
            """Find the actual sheet name by case-insensitive matching."""
            target_lower = target_name.lower()
            for sheet_name in all_sheet_names:
                if sheet_name.lower() == target_lower:
                    return sheet_name
            return None
        
        # Helper function to find actual sheet name case-insensitively
        def find_actual_sheet_name(target_name):
            """Find the actual sheet name by case-insensitive matching."""
            target_lower = target_name.lower()
            for sheet_name in all_sheet_names:
                if sheet_name.lower() == target_lower:
                    return sheet_name
            return None
        
        # Get all the sheets available in the file
        all_sheet_names = xls.sheet_names

        # Find actual sheet names case-insensitively for later use
        sp_sheet = find_actual_sheet_name('Sponsored Products Campaigns')
        sb_multi_sheet = find_actual_sheet_name('SB Multi Ad Group Campaigns')
        sb_standard_sheet = find_actual_sheet_name('Sponsored Brands Campaigns')
        sd_sheet = find_actual_sheet_name('Sponsored Display Campaigns')
        st.session_state.debug_messages.append(f"[Bulk File Processing] Found {len(all_sheet_names)} sheets in bulk file: {', '.join(all_sheet_names)}")
        
        # Check if we have both SB sheet types
        has_sb_multi = any(sheet.lower() == 'sb multi ad group campaigns' for sheet in all_sheet_names)
        has_sb_standard = any(sheet.lower() == 'sponsored brands campaigns' for sheet in all_sheet_names)
        
        # Log what we found for debugging
        if has_sb_multi:
            st.session_state.debug_messages.append("Found 'SB Multi Ad Group Campaigns' sheet - will use this for Sponsored Brands data")
        if has_sb_standard:
            if has_sb_multi:
                st.session_state.debug_messages.append("Also found 'Sponsored Brands Campaigns' sheet - will be ignored in favor of Multi Ad Group data")
            else:
                st.session_state.debug_messages.append("Found 'Sponsored Brands Campaigns' sheet")
        
        # Process ALL sheets in the file to ensure we don't miss any data
        # This is important for search term data which might be in non-standard sheets
        for sheet_name in all_sheet_names:
            # Skip sheets containing 'RAS' or 'Portfolio' in their names
            if 'RAS' in sheet_name or 'Portfolio' in sheet_name:
                st.session_state.debug_messages.append(f"Skipping sheet: {sheet_name} (contains 'RAS' or 'Portfolio')")
                continue
                
            st.session_state.debug_messages.append(f"Processing sheet: {sheet_name}")
            try:
                # Read the sheet
                df = pd.read_excel(xls, sheet_name=sheet_name)
                
                # Skip empty sheets
                if df.empty:
                    st.session_state.debug_messages.append(f"Sheet {sheet_name} is empty, skipping")
                    continue
                    
                # Store the sheet data in our dictionary
                bulk_data[sheet_name] = df
                st.session_state.debug_messages.append(f"Added sheet {sheet_name} to bulk_data dictionary ({len(df)} rows, {len(df.columns)} columns)")
            except Exception as e:
                st.session_state.debug_messages.append(f"Error processing sheet {sheet_name}: {str(e)}")
                continue
        
        # Now process the primary campaign sheets with additional cleaning
        campaign_sheet_list = []
        
        # Add the campaign sheets in the order we want to process them

        
        if sp_sheet:

        
            campaign_sheet_list.append(sp_sheet)
            
        # For Sponsored Brands, prioritize Multi Ad Group if available

            
        if has_sb_multi and sb_multi_sheet:

            
            campaign_sheet_list.append(sb_multi_sheet)

            
        elif has_sb_standard and sb_standard_sheet:

            
            campaign_sheet_list.append(sb_standard_sheet)
            
        if sd_sheet:

            
            campaign_sheet_list.append(sd_sheet)
        
        # Apply additional processing to campaign sheets
        for sheet_name in campaign_sheet_list:
            st.session_state.debug_messages.append(f"Applying campaign processing to sheet: {sheet_name}") # Debug
            try:
                # We already loaded the sheet in the first pass, so get it from bulk_data
                if sheet_name in bulk_data:
                    df = bulk_data[sheet_name].copy()
                else:
                    st.session_state.debug_messages.append(f"Sheet {sheet_name} not found in bulk_data, skipping campaign processing")
                    continue

                # Ensure required numeric columns exist and handle potential errors
                for col_pattern in numeric_cols_pattern:
                    if col_pattern in df.columns:
                        df[col_pattern] = pd.to_numeric(df[col_pattern], errors='coerce').fillna(0)
                
                # Convert column names to strings
                df.columns = df.columns.astype(str)
                
                # Rename columns for consistency
                column_renames = {}
                # Do not rename 'Sales' to 'Total Sales' as these are different metrics
                # 'Sales' in bulk file refers to ad-attributed sales
                # 'Total Sales' should only come from the sales report
                if 'Campaign' in df.columns and 'Campaign Name' not in df.columns:
                    column_renames['Campaign'] = 'Campaign Name'
                
                if column_renames:
                    df = df.rename(columns=column_renames)
                
                # Add campaign type efficiently
                if 'Sponsored Product' in sheet_name:
                    campaign_type = 'SP'
                elif 'Sponsored Brand' in sheet_name or sheet_name.lower() == 'sb multi ad group campaigns':
                    campaign_type = 'SB'
                elif 'Sponsored Display' in sheet_name:
                    campaign_type = 'SD'
                else:
                    campaign_type = 'Unknown'
                
                df['Campaign Type'] = campaign_type
                
                # Store processed data
                # For Multi Ad Group, store it under the standard Sponsored Brands key
                if sheet_name.lower() == 'sb multi ad group campaigns':
                    sheet_key = 'Sponsored Brands Campaigns'
                    st.session_state.debug_messages.append(f"Storing 'SB Multi Ad Group Campaigns' data as 'Sponsored Brands Campaigns'")
                    # Update the data in our dictionary
                    bulk_data[sheet_key] = df
                    # IMPORTANT: Remove the original Multi sheet to prevent double counting elsewhere
                    # We will represent SB data under the normalized 'Sponsored Brands Campaigns' key only
                    try:
                        if sheet_name in bulk_data:
                            del bulk_data[sheet_name]
                            st.session_state.debug_messages.append(
                                "Removed original 'SB Multi Ad Group Campaigns' sheet from bulk_data to prevent double counting"
                            )
                    except Exception:
                        # Best-effort cleanup; proceed even if deletion fails
                        pass
                else:
                    # Update the existing data in our dictionary using normalized keys

                    # This ensures case-insensitive access to data

                    if sp_sheet and sheet_name == sp_sheet:

                        bulk_data['Sponsored Products Campaigns'] = df

                    elif sb_standard_sheet and sheet_name == sb_standard_sheet:

                        bulk_data['Sponsored Brands Campaigns'] = df  

                    elif sd_sheet and sheet_name == sd_sheet:

                        bulk_data['Sponsored Display Campaigns'] = df

                    else:

                        bulk_data[sheet_name] = df
                
                st.session_state.debug_messages.append(f"Applied campaign processing to {sheet_name}: {len(df)} rows")
                
            except Exception as e:
                st.session_state.debug_messages.append(f"Error applying campaign processing to sheet '{sheet_name}': {str(e)}")
        
        if not bulk_data:
            st.warning("No data could be processed from any sheets.")
            return None
        
        # Detect and persist SKU-ASIN mappings for Seller Central clients
        detect_and_persist_sku_asin_mappings(bulk_data)
        
        # Cache the processed data
        db_manager.cache_bulk_data(client_name, file_content, bulk_data)
        
        st.session_state.debug_messages.append(f"Successfully processed {len(bulk_data)} sheets from bulk file")
        return bulk_data
        
    except Exception as e:
        st.error(f"Error processing bulk file: {str(e)}")
        return None

@st.cache_data(ttl=3600, show_spinner=False)  # Cache for 1 hour
def process_companion_asin_data(uploaded_file):
    """
    Processes companion ASIN export CSV file and converts it to bulk data format.
    """
    try:
        # Read the CSV file
        df = pd.read_csv(uploaded_file)
        st.session_state.debug_messages.append(f"[Companion ASIN] Loaded CSV with {len(df)} rows and {len(df.columns)} columns")
        
        if df.empty:
            st.warning("Companion ASIN file is empty.")
            return None
            
        # Map companion columns to bulk data format
        column_mapping = {
            'campaign_name': 'Campaign Name',
            'asin': 'ASIN',
            'impressions': 'Impressions',
            'clicks': 'Clicks',
            'spend': 'Spend',
            'sales': 'Sales',
            'orders': 'Orders',
            'cpc': 'Cost Per Click (CPC)',
            'ctr': 'Click-Thru Rate (CTR)',
            'acos': 'ACoS',
            'roas': 'ROAS',
            'kind': 'Campaign Type Raw',
            'campaign_targeting_type': 'Targeting Type',
            'ad_group_name': 'Ad Group Name',
            'campaign_id': 'Campaign ID'
        }
        
        # Rename columns
        df_mapped = df.rename(columns=column_mapping)
        
        # Map campaign types
        campaign_type_mapping = {'sp': 'SP', 'sb': 'SB', 'sd': 'SD'}
        df_mapped['Campaign Type'] = df_mapped['Campaign Type Raw'].map(campaign_type_mapping)
        
        # Add Entity column based on ASIN presence (for targeting analysis)
        df_mapped['Entity'] = 'Product Ad'  # ASIN data represents product ads, not targeting
        df_mapped['Product Targeting Expression'] = df_mapped['ASIN']  # Use ASIN as targeting expression
        
        # Add Product column to identify the ad type
        df_mapped['Product'] = df_mapped['Campaign Type'].map({
            'SP': 'Sponsored Products',
            'SB': 'Sponsored Brands', 
            'SD': 'Sponsored Display'
        })
        
        # Add State column (assume enabled for companion data)
        df_mapped['State'] = 'enabled'
        
        # Ensure numeric columns are properly formatted
        numeric_cols = ['Impressions', 'Clicks', 'Spend', 'Sales', 'Orders', 'Cost Per Click (CPC)', 'Click-Thru Rate (CTR)', 'ACoS', 'ROAS']
        for col in numeric_cols:
            if col in df_mapped.columns:
                df_mapped[col] = pd.to_numeric(df_mapped[col], errors='coerce').fillna(0)
        
        # Convert percentage columns
        if 'Click-Thru Rate (CTR)' in df_mapped.columns:
            df_mapped['Click-Thru Rate (CTR)'] = df_mapped['Click-Thru Rate (CTR)'] * 100  # Convert to percentage
        if 'ACoS' in df_mapped.columns:
            df_mapped['ACoS'] = df_mapped['ACoS'] * 100  # Convert to percentage
            
        # Split data by campaign type to create separate sheets
        bulk_data = {}
        
        for campaign_type in ['SP', 'SB', 'SD']:
            type_df = df_mapped[df_mapped['Campaign Type'] == campaign_type].copy()
            if not type_df.empty:
                if campaign_type == 'SP':
                    asin_sheet_name = 'Sponsored Products ASIN Data'
                elif campaign_type == 'SB':
                    asin_sheet_name = 'Sponsored Brands ASIN Data'
                else:  # SD
                    asin_sheet_name = 'Sponsored Display ASIN Data'
                
                # Create ASIN data sheet for Product Analysis
                bulk_data[asin_sheet_name] = type_df
                st.session_state.debug_messages.append(f"[Companion ASIN] Created {asin_sheet_name} with {len(type_df)} ASIN rows")
        
        st.session_state.debug_messages.append(f"[Companion ASIN] Successfully processed companion ASIN data into {len(bulk_data)} sheets")
        return bulk_data
        
    except Exception as e:
        st.error(f"Error processing companion ASIN file: {str(e)}")
        st.session_state.debug_messages.append(f"[Companion ASIN] Error: {str(e)}")
        return None

@st.cache_data(ttl=3600, show_spinner=False)  # Cache for 1 hour
def process_companion_search_term_data(uploaded_file):
    """
    Processes companion Search Term export CSV file and converts it to bulk data format.
    """
    try:
        # Read the CSV file
        df = pd.read_csv(uploaded_file)
        st.session_state.debug_messages.append(f"[Companion Search Term] Loaded CSV with {len(df)} rows and {len(df.columns)} columns")
        
        if df.empty:
            st.warning("Companion Search Term file is empty.")
            return None
            
        # Map companion columns to bulk data format
        column_mapping = {
            'campaign_name': 'Campaign Name',
            'query': 'Customer Search Term',
            'target': 'Keyword Text',
            'match_type': 'Match Type',
            'impressions': 'Impressions',
            'clicks': 'Clicks',
            'spend': 'Spend',
            'sales': 'Sales',
            'orders': 'Orders',
            'cpc': 'Cost Per Click (CPC)',
            'ctr': 'Click-Thru Rate (CTR)',
            'acos': 'ACoS',
            'roas': 'ROAS',
            'kind': 'Campaign Type Raw',
            'campaign_targeting_type': 'Targeting Type',
            'ad_group_name': 'Ad Group Name',
            'campaign_id': 'Campaign ID',
            'keyword_id': 'Keyword ID'
        }
        
        # Rename columns
        df_mapped = df.rename(columns=column_mapping)
        
        # Universal text cleaning function for search terms and other text fields
        def clean_text_field(text):
            """Clean text fields by handling Unicode escape sequences and other encoding issues."""
            if pd.isna(text) or text == '':
                return text
            
            text = str(text)
            
            try:
                # Handle Unicode escape sequences (like \u0027 for apostrophe)
                import codecs
                text = codecs.decode(text, 'unicode_escape')
                
                # Handle common JSON escape sequences
                text = text.replace('\\"', '"')  # Escaped quotes
                text = text.replace('\\/', '/')   # Escaped forward slash
                text = text.replace('\\\\', '\\') # Escaped backslash
                
                # Remove any remaining backslash artifacts
                import re
                text = re.sub(r'\\u[0-9a-fA-F]{4}', lambda m: chr(int(m.group(0)[2:], 16)), text)
                
                # Clean up any remaining escape sequences
                text = text.replace('\\n', ' ').replace('\\r', ' ').replace('\\t', ' ')
                
                # Normalize whitespace
                text = ' '.join(text.split())
                
            except Exception as e:
                # If cleaning fails, return the original text
                st.session_state.debug_messages.append(f"[Text Cleaning] Warning: Could not clean text '{text[:50]}...': {str(e)}")
                pass
            
            return text
        
        # Apply text cleaning to search term and target columns
        if 'Customer Search Term' in df_mapped.columns:
            df_mapped['Customer Search Term'] = df_mapped['Customer Search Term'].apply(clean_text_field)
            st.session_state.debug_messages.append("[Companion Search Term] Applied text cleaning to Customer Search Term column")
        
        if 'Keyword Text' in df_mapped.columns:
            df_mapped['Keyword Text'] = df_mapped['Keyword Text'].apply(clean_text_field)
            st.session_state.debug_messages.append("[Companion Search Term] Applied text cleaning to Keyword Text column")
        
        # Map campaign types
        campaign_type_mapping = {'sp': 'SP', 'sb': 'SB', 'sd': 'SD'}
        df_mapped['Campaign Type'] = df_mapped['Campaign Type Raw'].map(campaign_type_mapping)
        
        # Handle target column translation and entity determination
        def process_target_and_entity(row):
            targeting_type = str(row.get('Targeting Type', '')).lower()
            match_type = str(row.get('Match Type', '')).lower()
            target_value = str(row.get('target', '')).lower()
            
            # Debug logging for auto targeting detection
            if targeting_type == 'auto':
                st.session_state.debug_messages.append(f"[Companion] Found auto targeting: targeting_type='{targeting_type}', match_type='{match_type}'")
            
            # Handle special target values first
            if target_value == 'asincategorysameAs':
                return 'Product Targeting', 'Category Target', 'Category Target'
            elif target_value == 'asinexpandedfrom':
                return 'Product Targeting', 'ASIN Expanded', 'Product Targeting'
            elif target_value == 'asinsameas':
                return 'Product Targeting', 'ASIN Target', 'Product Targeting'
            # Check if it's auto targeting (from campaign_targeting_type column)
            elif targeting_type == 'auto':
                return 'Product Targeting', row.get('Keyword Text', ''), 'Auto'
            # Check if it's keyword targeting (broad, exact, phrase)
            elif match_type in ['broad', 'exact', 'phrase']:
                # Capitalize the match type for proper display
                capitalized_match_type = row.get('Match Type', '').title()
                return 'Keyword', row.get('Keyword Text', ''), capitalized_match_type
            # Check if it's explicitly product targeting
            elif match_type == 'target' or targeting_type == 'manual' and target_value:
                return 'Product Targeting', row.get('Keyword Text', ''), 'Product Targeting'
            else:
                # Default to keyword for anything else (preserves original match type)
                return 'Keyword', row.get('Keyword Text', ''), row.get('Match Type', '')
        
        # Apply the function to get entity, targeting expression, and match type
        df_mapped[['Entity', 'Targeting Expression', 'Match Type']] = df_mapped.apply(
            lambda row: pd.Series(process_target_and_entity(row)), axis=1
        )
        
        # Add Target column for compatibility with targeting performance display
        df_mapped['Target'] = df_mapped['Targeting Expression']
        
        # Add Target Type column based on Entity
        df_mapped['Target Type'] = df_mapped['Entity'].apply(
            lambda x: 'Keyword' if x == 'Keyword' else 'Product Targeting'
        )
        
        # Add State column (assume enabled for companion data)
        df_mapped['State'] = 'enabled'
        
        # Add Product column to identify the ad type
        df_mapped['Product'] = df_mapped['Campaign Type'].map({
            'SP': 'Sponsored Products',
            'SB': 'Sponsored Brands', 
            'SD': 'Sponsored Display'
        })
        

        
        # Ensure numeric columns are properly formatted
        numeric_cols = ['Impressions', 'Clicks', 'Spend', 'Sales', 'Orders', 'Cost Per Click (CPC)', 'Click-Thru Rate (CTR)', 'ACoS', 'ROAS']
        for col in numeric_cols:
            if col in df_mapped.columns:
                df_mapped[col] = pd.to_numeric(df_mapped[col], errors='coerce').fillna(0)
        
        # Convert percentage columns
        if 'Click-Thru Rate (CTR)' in df_mapped.columns:
            df_mapped['Click-Thru Rate (CTR)'] = df_mapped['Click-Thru Rate (CTR)'] * 100  # Convert to percentage
        if 'ACoS' in df_mapped.columns:
            df_mapped['ACoS'] = df_mapped['ACoS'] * 100  # Convert to percentage
            
        # Create both search term sheets and campaign sheets by campaign type
        bulk_data = {}
        
        for campaign_type in ['SP', 'SB', 'SD']:
            type_df = df_mapped[df_mapped['Campaign Type'] == campaign_type].copy()
            if not type_df.empty:
                # Create search term sheet
                if campaign_type == 'SP':
                    search_sheet_name = 'Sponsored Products Search Term Report'
                    campaign_sheet_name = 'Sponsored Products Campaigns'
                elif campaign_type == 'SB':
                    search_sheet_name = 'Sponsored Brands Search Term Report'
                    campaign_sheet_name = 'Sponsored Brands Campaigns'
                else:  # SD
                    search_sheet_name = 'Sponsored Display Search Term Report'
                    campaign_sheet_name = 'Sponsored Display Campaigns'
                
                # Add search term sheet
                bulk_data[search_sheet_name] = type_df
                
                # Create campaign-level aggregation for overview metrics
                campaign_agg = type_df.groupby('Campaign Name').agg({
                    'Impressions': 'sum',
                    'Clicks': 'sum',
                    'Spend': 'sum',
                    'Sales': 'sum',
                    'Orders': 'sum'
                }).reset_index()
                
                # Add required columns for campaign sheet
                campaign_agg['Entity'] = 'Campaign'
                campaign_agg['State'] = 'enabled'
                campaign_agg['Product'] = type_df['Product'].iloc[0] if not type_df.empty else ''
                campaign_agg['Campaign Type'] = campaign_type
                
                # Calculate derived metrics
                campaign_agg['Cost Per Click (CPC)'] = campaign_agg['Spend'] / campaign_agg['Clicks'].replace(0, 1)
                campaign_agg['Click-Thru Rate (CTR)'] = (campaign_agg['Clicks'] / campaign_agg['Impressions'].replace(0, 1)) * 100
                campaign_agg['ACoS'] = (campaign_agg['Spend'] / campaign_agg['Sales'].replace(0, 1)) * 100
                campaign_agg['ROAS'] = campaign_agg['Sales'] / campaign_agg['Spend'].replace(0, 1)
                
                bulk_data[campaign_sheet_name] = campaign_agg
                st.session_state.debug_messages.append(f"[Companion Search Term] Created {search_sheet_name} with {len(type_df)} rows and {campaign_sheet_name} with {len(campaign_agg)} campaigns")
        
        st.session_state.debug_messages.append(f"[Companion Search Term] Successfully processed companion search term data into {len(bulk_data)} sheets")
        return bulk_data
        
    except Exception as e:
        st.error(f"Error processing companion search term file: {str(e)}")
        st.session_state.debug_messages.append(f"[Companion Search Term] Error: {str(e)}")
        return None

@st.cache_data(ttl=3600, show_spinner=False)  # Cache for 1 hour
def process_companion_targeting_data(uploaded_file):
    """
    Processes companion Targeting export CSV file and converts it to bulk data format.
    This replaces Search Term report data when targets are being looked at instead of search terms.
    Includes Sponsored Display ad type data that wasn't in Search Term exports.
    
    Key features:
    - Maps targeting export columns to standard bulk data format
    - Handles different expression types (negativeexact, asinsameas, etc.)
    - Creates Campaign sheets for targeting performance analysis
    - Includes Sponsored Display data missing from Search Term exports
    - Takes priority over Search Term data for targeting analysis
    """
    try:
        # Read the CSV file
        df = pd.read_csv(uploaded_file)
        st.session_state.debug_messages.append(f"[Companion Targeting] Loaded CSV with {len(df)} rows and {len(df.columns)} columns")
        
        if df.empty:
            st.warning("Companion Targeting file is empty.")
            return None
            
        # First, let's determine which sales/orders columns are available
        available_cols = df.columns.tolist()
        st.session_state.debug_messages.append(f"[Companion Targeting] Available columns: {available_cols}")
        
        # Map companion columns to bulk data format based on companion_targeting.csv structure
        column_mapping = {
            'campaign_name': 'Campaign Name',
            'text': 'Keyword Text',  # The 'text' column contains the targeting text
            'match_type': 'Match Type',
            'impressions': 'Impressions',
            'clicks': 'Clicks',
            'spend': 'Spend',
            'cpc': 'Cost Per Click (CPC)',
            'ctr': 'Click-Thru Rate (CTR)',
            'acos': 'ACoS',
            'roas': 'ROAS',
            'kind': 'Campaign Type Raw',
            'campaign_targeting_type': 'Targeting Type',
            'ad_group_name': 'Ad Group Name',
            'campaign_id': 'Campaign ID',
            'id': 'Keyword ID',
            'expression_type': 'Expression Type'
        }
        
        # Dynamically map sales and orders columns based on what's available
        # Priority: attributed_sales_14d > sales > attributed_sales_7d
        if 'attributed_sales_14d' in available_cols:
            column_mapping['attributed_sales_14d'] = 'Sales'
        elif 'sales' in available_cols:
            column_mapping['sales'] = 'Sales'
        elif 'attributed_sales_7d' in available_cols:
            column_mapping['attributed_sales_7d'] = 'Sales'
            
        # Priority: attributed_conversions_14d > orders > attributed_conversions_7d
        if 'attributed_conversions_14d' in available_cols:
            column_mapping['attributed_conversions_14d'] = 'Orders'
        elif 'orders' in available_cols:
            column_mapping['orders'] = 'Orders'
        elif 'attributed_conversions_7d' in available_cols:
            column_mapping['attributed_conversions_7d'] = 'Orders'
        
        # Rename columns
        df_mapped = df.rename(columns=column_mapping)
        
        # Universal text cleaning function for targeting text and other text fields
        def clean_text_field(text):
            """Clean text fields by handling Unicode escape sequences and other encoding issues."""
            if pd.isna(text) or text == '':
                return text
            
            text = str(text)
            
            try:
                # Handle Unicode escape sequences (like \u0027 for apostrophe)
                import codecs
                text = codecs.decode(text, 'unicode_escape')
                
                # Handle common JSON escape sequences
                text = text.replace('\\"', '"')  # Escaped quotes
                text = text.replace('\\/', '/')   # Escaped forward slash
                text = text.replace('\\\\', '\\') # Escaped backslash
                
                # Remove any remaining backslash artifacts
                import re
                text = re.sub(r'\\u[0-9a-fA-F]{4}', lambda m: chr(int(m.group(0)[2:], 16)), text)
                
                # Clean up any remaining escape sequences
                text = text.replace('\\n', ' ').replace('\\r', ' ').replace('\\t', ' ')
                
                # Normalize whitespace
                text = ' '.join(text.split())
                
            except Exception as e:
                # If cleaning fails, return the original text
                st.session_state.debug_messages.append(f"[Text Cleaning] Warning: Could not clean text '{text[:50]}...': {str(e)}")
                pass
            
            return text
        
        # Apply text cleaning to targeting text column
        if 'Keyword Text' in df_mapped.columns:
            df_mapped['Keyword Text'] = df_mapped['Keyword Text'].apply(clean_text_field)
            st.session_state.debug_messages.append("[Companion Targeting] Applied text cleaning to Keyword Text column")
        
        # Map campaign types
        campaign_type_mapping = {'sp': 'SP', 'sb': 'SB', 'sd': 'SD'}
        df_mapped['Campaign Type'] = df_mapped['Campaign Type Raw'].map(campaign_type_mapping)
        
        # Handle target column translation and entity determination
        def process_target_and_entity(row):
            targeting_type = str(row.get('Targeting Type', '')).lower()
            match_type = str(row.get('Match Type', '')).lower()
            expression_type = str(row.get('Expression Type', '')).lower()
            target_value = str(row.get('Keyword Text', ''))
            campaign_name = str(row.get('Campaign Name', '')).lower()
            kind = str(row.get('Campaign Type Raw', '')).lower()
            campaign_targeting_type = str(row.get('campaign_targeting_type', '')).lower()
            
            # Handle auto targeting based on campaign_targeting_type column
            if campaign_targeting_type == 'auto':
                # Check for specific auto target types based on target value content
                target_value_lower = target_value.lower()
                
                if 'asinsubstituterelated' in target_value_lower:
                    clean_target = 'Auto - Substitute'
                elif 'querybroadrelmatches' in target_value_lower:
                    clean_target = 'Auto - Loose Match'
                elif 'queryhighrelmatches' in target_value_lower:
                    clean_target = 'Auto - Close Match'
                elif 'asinaccessoryrelated' in target_value_lower:
                    clean_target = 'Auto - Compliments'
                else:
                    # Check if the auto target contains an ASIN and format it
                    import re
                    asin_match = re.search(r'B0[A-Z0-9]{8}', target_value.upper())
                    if asin_match:
                        asin = asin_match.group(0)
                        clean_target = f'ASIN={asin}'
                    else:
                        # Generic auto targeting
                        clean_target = target_value if target_value else 'Auto Targeting'
                
                return 'Product Targeting', clean_target, 'Auto'

            # Special handling for Sponsored Display (SD) targeting
            if kind == 'sd' and target_value:
                import re
                import json
                
                try:
                    # Handle remarketing targets (views/purchases)
                    if 'views' in target_value.lower() or 'purchases' in target_value.lower():
                        # Extract the type (views or purchases) and lookback days
                        remarketing_type = 'Views' if 'views' in target_value.lower() else 'Purchases'
                        
                        # Try to extract lookback days using regex
                        # Look for patterns like "value\":\"7\", "value\":\"14\", etc.
                        days_match = re.search(r'"value":\s*"(\d+)"', target_value)
                        if days_match:
                            days = days_match.group(1)
                            clean_target = f"{remarketing_type} Remarketing - {days}d"
                        else:
                            # Fallback: look for standalone numbers at the end
                            days_match = re.search(r'(\d+)', target_value)
                            if days_match:
                                days = days_match.group(1)
                                clean_target = f"{remarketing_type} Remarketing - {days}d"
                            else:
                                clean_target = f"{remarketing_type} Remarketing"
                        
                        # Determine match type based on campaign name
                        if isinstance(campaign_name, str) and 'brand' in campaign_name.lower():
                            match_type_result = 'Remarketing - Branded'
                        else:
                            match_type_result = 'Remarketing - Competitor'
                        
                        return 'Product Targeting', clean_target, match_type_result
                    
                    # Handle ASIN targeting (starts with B0 and is 10 chars)
                    asin_match = re.search(r'B0[A-Z0-9]{8}', target_value.upper())
                    if asin_match:
                        asin = asin_match.group(0)
                        clean_target = f"ASIN = {asin}"
                        return 'Product Targeting', clean_target, 'Product Target'
                    
                    # Handle category targeting
                    if 'asincategorysameAs'.lower() in target_value.lower() or 'similarProduct'.lower() in target_value.lower():
                        clean_target = 'SD Category Target'
                        return 'Product Targeting', clean_target, 'Category Targeting'
                        
                except Exception as e:
                    # If JSON parsing or regex fails, fall through to default handling
                    st.session_state.debug_messages.append(f"[SD Targeting] Error processing SD target '{target_value[:50]}...': {str(e)}")
            
            # Default handling for non-SD or non-special cases
            target_value_lower = target_value.lower()
            
            # Handle special expression types and targeting types
            if expression_type == 'negativeexact' or expression_type == 'negativephrase' or expression_type == 'negativebroad':
                # Handle negative keywords
                return 'Keyword', target_value, match_type.title()
            elif targeting_type == 'auto':
                # Auto targeting
                return 'Product Targeting', target_value, 'Auto'
            elif match_type in ['broad', 'exact', 'phrase']:
                # Regular keyword targeting
                return 'Keyword', target_value, match_type.title()
            elif match_type == 'target' or expression_type == 'asinsameas' or expression_type == 'asincategorysameAs':
                # Product targeting
                return 'Product Targeting', target_value, 'Product Target'
            else:
                # Default to keyword for unknown cases
                return 'Keyword', target_value, row.get('Match Type', '').title()
        
        # Apply target processing to each row
        targeting_results = df_mapped.apply(process_target_and_entity, axis=1, result_type='expand')
        df_mapped['Entity'] = targeting_results[0]
        df_mapped['Product Targeting Expression'] = targeting_results[1]  # For compatibility with existing analysis
        df_mapped['Match Type'] = targeting_results[2]
        
        # For product targeting, also set Targeting Expression for SD compatibility
        product_targeting_mask = df_mapped['Entity'] == 'Product Targeting'
        df_mapped.loc[product_targeting_mask, 'Targeting Expression'] = df_mapped.loc[product_targeting_mask, 'Product Targeting Expression']
        
        # For keywords, set the Keyword Text column properly for targeting analysis
        keyword_mask = df_mapped['Entity'] == 'Keyword'
        df_mapped.loc[keyword_mask, 'Keyword Text'] = df_mapped.loc[keyword_mask, 'Product Targeting Expression']
        
        # Add State column (assume enabled for companion data)
        df_mapped['State'] = 'enabled'
        
        # Add Product column to identify the ad type
        df_mapped['Product'] = df_mapped['Campaign Type'].map({
            'SP': 'Sponsored Products',
            'SB': 'Sponsored Brands', 
            'SD': 'Sponsored Display'
        })
        
        # Ensure numeric columns are properly formatted
        numeric_cols = ['Impressions', 'Clicks', 'Spend', 'Sales', 'Orders', 'Cost Per Click (CPC)', 'Click-Thru Rate (CTR)', 'ACoS', 'ROAS']
        for col in numeric_cols:
            if col in df_mapped.columns:
                df_mapped[col] = pd.to_numeric(df_mapped[col], errors='coerce').fillna(0)
        
        # Convert percentage columns
        if 'Click-Thru Rate (CTR)' in df_mapped.columns:
            df_mapped['Click-Thru Rate (CTR)'] = df_mapped['Click-Thru Rate (CTR)'] * 100  # Convert to percentage
        if 'ACoS' in df_mapped.columns:
            df_mapped['ACoS'] = df_mapped['ACoS'] * 100  # Convert to percentage
            
        # Create campaign sheets by campaign type (these will be used by targeting performance)
        bulk_data = {}
        
        for campaign_type in ['SP', 'SB', 'SD']:
            type_df = df_mapped[df_mapped['Campaign Type'] == campaign_type].copy()
            if not type_df.empty:
                if campaign_type == 'SP':
                    sheet_name = 'Sponsored Products Campaigns'
                elif campaign_type == 'SB':
                    sheet_name = 'Sponsored Brands Campaigns'
                else:  # SD
                    sheet_name = 'Sponsored Display Campaigns'
                
                bulk_data[sheet_name] = type_df
                st.session_state.debug_messages.append(f"[Companion Targeting] Created {sheet_name} with {len(type_df)} rows")
        
        st.session_state.debug_messages.append(f"[Companion Targeting] Successfully processed companion targeting data into {len(bulk_data)} sheets")
        return bulk_data
        
    except Exception as e:
        st.error(f"Error processing companion targeting file: {str(e)}")
        st.session_state.debug_messages.append(f"[Companion Targeting] Error: {str(e)}")
        return None

@st.cache_data(ttl=3600, show_spinner=False)  # Cache for 1 hour
def process_companion_data(asin_file, search_term_file, targeting_file=None):
    """
    Combines companion ASIN, Search Term, and Targeting data into a unified bulk data structure.
    The Targeting Export replaces Search Term data for targeting analysis and includes Sponsored Display data.
    """
    # Check for cached data first
    client_name = st.session_state.get('client_config', {}).get('client_name', 'unknown')
    
    # Create combined file content for hashing
    combined_content = b""
    if asin_file:
        asin_file.seek(0)
        combined_content += asin_file.read()
        asin_file.seek(0)
    if search_term_file:
        search_term_file.seek(0)
        combined_content += search_term_file.read()
        search_term_file.seek(0)
    if targeting_file:
        targeting_file.seek(0)
        combined_content += targeting_file.read()
        targeting_file.seek(0)
    
    # Try to get cached data
    cached_data = db_manager.get_cached_bulk_data(client_name, combined_content)
    if cached_data:
        return cached_data
    
    try:
        combined_bulk_data = {}
        asin_data = None
        search_term_data = None
        targeting_data = None
        
        # Process ASIN data
        if asin_file is not None:
            asin_data = process_companion_asin_data(asin_file)
            if asin_data:
                combined_bulk_data.update(asin_data)
                st.session_state.debug_messages.append(f"[Companion Combined] Added ASIN data: {list(asin_data.keys())}")
        
        # Process Targeting data (this has priority over Search Term data for targeting analysis)
        if targeting_file is not None:
            targeting_data = process_companion_targeting_data(targeting_file)
            if targeting_data:
                # Targeting data creates Campaign sheets which take priority for targeting performance analysis
                combined_bulk_data.update(targeting_data)
                st.session_state.debug_messages.append(f"[Companion Combined] Added Targeting data: {list(targeting_data.keys())}")
        
        # Process Search Term data (only add if targeting data not provided, or add search term reports)
        if search_term_file is not None:
            search_term_data = process_companion_search_term_data(search_term_file)
            if search_term_data:
                if targeting_data:
                    # If we have targeting data, only add Search Term reports, not campaign sheets
                    # since targeting data provides better campaign targeting information
                    for sheet_name, data in search_term_data.items():
                        if 'Search Term' in sheet_name and 'Campaigns' not in sheet_name:
                            combined_bulk_data[sheet_name] = data
                            st.session_state.debug_messages.append(f"[Companion Combined] Added Search Term report: {sheet_name}")
                else:
                    # No targeting data, use search term data for campaigns
                    # If we have both ASIN and Search Term data, prioritize Search Term data for campaigns
                    if asin_data:
                        # Clear any campaign sheets from ASIN data since Search Term data should handle campaigns
                        for sheet_name in ['Sponsored Products Campaigns', 'Sponsored Brands Campaigns', 'Sponsored Display Campaigns']:
                            if sheet_name in combined_bulk_data:
                                del combined_bulk_data[sheet_name]
                                st.session_state.debug_messages.append(f"[Companion Combined] Removed ASIN campaign data for {sheet_name}")
                        
                        # Add all search term data (both search term reports and campaign sheets)
                        combined_bulk_data.update(search_term_data)
                        st.session_state.debug_messages.append(f"[Companion Combined] Added all Search Term data: {list(search_term_data.keys())}")
                    else:
                        # Only search term data, add everything
                        combined_bulk_data.update(search_term_data)
                        st.session_state.debug_messages.append(f"[Companion Combined] Added Search Term data only: {list(search_term_data.keys())}")
        
        # If we only have ASIN data and no search term or targeting data, create campaign aggregation
        if asin_data and not search_term_data and not targeting_data:
            st.session_state.debug_messages.append("[Companion Combined] Only ASIN data provided, creating campaign aggregation")
            # We need to create campaign sheets from ASIN data since search term processing won't do it
            for campaign_type in ['SP', 'SB', 'SD']:
                if campaign_type == 'SP':
                    campaign_sheet_name = 'Sponsored Products Campaigns'
                elif campaign_type == 'SB':
                    campaign_sheet_name = 'Sponsored Brands Campaigns'
                else:  # SD
                    campaign_sheet_name = 'Sponsored Display Campaigns'
                
                # Find ASIN data for this campaign type (it would be in the combined_bulk_data already)
                # We need to aggregate from any existing data
                # For now, create empty campaign sheets since ASIN data alone doesn't provide campaign-level aggregation
                campaign_df = pd.DataFrame(columns=['Campaign Name', 'Entity', 'State', 'Product', 'Campaign Type', 
                                                  'Impressions', 'Clicks', 'Spend', 'Sales', 'Orders',
                                                  'Cost Per Click (CPC)', 'Click-Thru Rate (CTR)', 'ACoS', 'ROAS'])
                combined_bulk_data[campaign_sheet_name] = campaign_df
                st.session_state.debug_messages.append(f"[Companion Combined] Created empty {campaign_sheet_name} for ASIN-only data")
        
        if not combined_bulk_data:
            st.warning("No data could be processed from companion files.")
            return None
        
        # Cache the processed data
        db_manager.cache_bulk_data(client_name, combined_content, combined_bulk_data)
        
        # Mark this as companion data for UI purposes
        st.session_state.is_companion_data = True
        
        st.session_state.debug_messages.append(f"[Companion Combined] Successfully combined companion data with {len(combined_bulk_data)} sheets")
        return combined_bulk_data
        
    except Exception as e:
        st.error(f"Error processing companion data: {str(e)}")
        st.session_state.debug_messages.append(f"[Companion Combined] Error: {str(e)}")
        return None

@st.cache_data(ttl=3600, show_spinner="Classifying campaigns...")  # Cache for 1 hour
def classify_branded_campaigns(bulk_data, client_settings):
    """Classify campaigns as branded or non-branded based on targeting.
    Uses the global sales attribution choice from session state.
    
    Args:
        bulk_data: Dictionary of DataFrames from uploaded reports
        client_settings: Dictionary containing 'Branded Terms' and 'Branded ASINs'
        
    Returns:
        Dictionary with 'Branded' and 'Non-Branded' DataFrames for each campaign type
    """
    classified = {
        'Sponsored Products': {'Branded': [], 'Non-Branded': []},
        'Sponsored Brands': {'Branded': [], 'Non-Branded': []},
        'Sponsored Display': {'Branded': [], 'Non-Branded': []}
    }
    
    branded_terms = client_settings.get('Branded Terms', [])
    branded_asins = client_settings.get('Branded ASINs', [])
    st.session_state.debug_messages.append(f"Classifying with Branded Terms: {branded_terms}") # Log branded terms
    
    campaign_sheets = [
        'Sponsored Products Campaigns',
        'Sponsored Brands Campaigns',
        'Sponsored Display Campaigns'
    ]
    
    for sheet_name in campaign_sheets:
        if sheet_name not in bulk_data:
            st.session_state.debug_messages.append(f"Campaign sheet {sheet_name} not found in bulk data. Skipping classification.")
            continue
            
        df = bulk_data[sheet_name]
        # Ensure 'Entity' and 'Spend' columns exist and handle potential errors
        if 'Entity' not in df.columns:
             st.session_state.debug_messages.append(f"'{sheet_name}' is missing 'Entity' column. Skipping classification for this sheet.")
             continue
        if 'Spend' not in df.columns:
             st.session_state.debug_messages.append(f"'{sheet_name}' is missing 'Spend' column. Spend calculations might be inaccurate.")
             # Optionally add a Spend column with 0 if missing, or handle differently
             df['Spend'] = 0 
             
        # Determine which sales column to use based on global attribution choice
        if 'sd_attribution_choice' in st.session_state and st.session_state.sd_attribution_choice == "Sales (Views & Clicks)" and 'Sponsored Display' in sheet_name:
            sales_patterns = ['Sales (Views & Clicks)', 'Total Sales (Views & Clicks)', 'Sales', 'Total Sales']
            orders_patterns = ['Orders (Views & Clicks)', 'Orders', 'Total Orders']
            st.session_state.debug_messages.append(f"Classification: Using (Views & Clicks) attribution for {sheet_name}")
        else:
            sales_patterns = ['Sales', 'Total Sales', 'Sales (Views & Clicks)']
            orders_patterns = ['Orders', 'Total Orders', 'Orders (Views & Clicks)']
            
        # Find the appropriate sales column
        sales_col = None
        for pattern in sales_patterns:
            if pattern in df.columns:
                sales_col = pattern
                st.session_state.debug_messages.append(f"Classification: Using '{sales_col}' for sales in {sheet_name}")
                break
                
        # If sales column not found, add a placeholder
        if sales_col is None:
            st.session_state.debug_messages.append(f"Warning: No sales column found in {sheet_name}. Adding placeholder.")
            df['Sales'] = 0
            sales_col = 'Sales'

        # Filter for rows where Entity is 'Keyword' or 'Product Targeting'
        targets_df = df[df['Entity'].isin(['Keyword', 'Product Targeting'])]
        st.session_state.debug_messages.append(f"Processing {len(targets_df)} targeting rows in {sheet_name}")

        for index, row in targets_df.iterrows(): # Use index for better logging
            is_branded = False # Default
            classification_reason = "No matching criteria"
            # Safely get spend, convert to numeric, handle errors by coercing to NaN, then fill NaN with 0
            spend_val = row.get('Spend', 0)
            row_spend = pd.to_numeric(spend_val, errors='coerce')
            if pd.isna(row_spend):
                row_spend = 0 # Replace NaN with 0

            # Keyword targeting check
            if pd.notna(row.get('Keyword Text')):
                keyword = str(row['Keyword Text']).lower()
                match_found = False
                for brand_term in branded_terms:
                    # Ensure brand_term is not None or empty before checking
                    if brand_term and isinstance(brand_term, str) and brand_term.lower() in keyword:
                        is_branded = True
                        match_found = True
                        classification_reason = f"Keyword '{keyword}' matched term '{brand_term}'"
                        break # Stop checking terms once a match is found
                if not match_found:
                     classification_reason = f"Keyword '{keyword}' did not match any branded terms."
                # Log keyword check result regardless of match
                st.session_state.debug_messages.append(f"  Row Index: {index}, Type: Keyword, Target: '{keyword}', Spend: {row_spend:.2f}, IsBranded: {is_branded}, Reason: {classification_reason}")

            # Product targeting check
            elif pd.notna(row.get('Product Targeting Expression') or row.get('Targeting Expression')):
                 target_expr_val = row.get('Product Targeting Expression') or row.get('Targeting Expression')
                 target_expr = str(target_expr_val) # Ensure string conversion
                 # Ensure asin is not None or empty before checking
                 asin_match_found = any(asin and isinstance(asin, str) and asin in target_expr for asin in branded_asins)
                 expanded_present = 'expanded' in target_expr.lower()
                 exact_product_present = 'exact-product' in target_expr.lower()
                  
                 # Check if this is a Sponsored Display campaign
                 is_sponsored_display = 'Sponsored Display' in sheet_name
                  
                 # Special case for Sponsored Display with exact-product targeting
                 if is_sponsored_display and exact_product_present:
                     is_branded = True
                     classification_reason = f"Target '{target_expr}' contains 'exact-product' in Sponsored Display campaign (Branded)"
                 elif asin_match_found and not expanded_present:
                     is_branded = True
                     classification_reason = f"Target '{target_expr}' matched a branded ASIN and not 'expanded'"
                 elif asin_match_found and expanded_present:
                      classification_reason = f"Target '{target_expr}' matched ASIN but contained 'expanded' (Non-Branded)"
                      is_branded = False # Explicitly set to False
                 elif not asin_match_found:
                      classification_reason = f"Target '{target_expr}' did not match any branded ASINs."
                      is_branded = False # Explicitly set to False
                 else: # Should not happen based on logic, but good fallback
                      classification_reason = f"Target '{target_expr}' - unusual case"
                      is_branded = False # Default to Non-Branded
                 # Log product target check result
                 st.session_state.debug_messages.append(f"  Row Index: {index}, Type: Product Target, Target: '{target_expr}', Spend: {row_spend:.2f}, IsBranded: {is_branded}, Reason: {classification_reason}")

            else:
                # Row has a Bid but neither Keyword nor Product Target? Log this edge case.
                classification_reason = "Row has Bid but no Keyword/Product Target text"
                st.session_state.debug_messages.append(f"  Row Index: {index}, Type: Unknown/No Target, Spend: {row_spend:.2f}, IsBranded: {is_branded}, Reason: {classification_reason}")
                is_branded = False # Default to Non-Branded if unclear

            campaign_type = sheet_name.replace(' Campaigns', '')
            key = 'Branded' if is_branded else 'Non-Branded'
            # Make sure the row being appended has the numeric spend
            row_copy = row.copy()
            row_copy['Spend'] = row_spend
            classified[campaign_type][key].append(row_copy)
    
    # Convert lists to DataFrames
    for campaign_type in classified:
        for brand_type in classified[campaign_type]: # 'Branded' or 'Non-Branded'
            rows = classified[campaign_type][brand_type]
            # Check if we have any data to convert
            if not rows:
                st.session_state.debug_messages.append(f"  No data for {brand_type} in {campaign_type}. Creating empty DataFrame.")
                classified[campaign_type][brand_type] = pd.DataFrame()
                continue
            
            # Convert list of rows to DataFrame
            df = pd.DataFrame(rows)
            classified[campaign_type][brand_type] = df
            
            st.session_state.debug_messages.append(f"  Processing {brand_type} ({len(df)} rows) for {campaign_type}")

            # Aggregate Spend
            current_spend = 0.0
            if 'Spend' in df.columns:
                 # Ensure conversion to numeric, coercing errors, and filling NaN before sum
                 numeric_spend = pd.to_numeric(df['Spend'], errors='coerce').fillna(0)
                 current_spend = numeric_spend.sum()
                 st.session_state.debug_messages.append(f"    Adding {current_spend:.2f} Spend to {brand_type} (Total: {current_spend:.2f})")
            else:
                 st.session_state.debug_messages.append(f"    '{'Spend'}' column missing in {brand_type} {campaign_type} DataFrame. Spend not added.")

            # Aggregate Sales
            current_sales = 0.0
            if 'Sales' in df.columns:
                numeric_sales = pd.to_numeric(df['Sales'], errors='coerce').fillna(0)
                current_sales = numeric_sales.sum()
                st.session_state.debug_messages.append(f"    Adding {current_sales:.2f} Sales to {brand_type} (Total: {current_sales:.2f})")
            else:
                 st.session_state.debug_messages.append(f"    '{'Sales'}' column missing. Sales not added.")

            # Aggregate Impressions
            current_impressions = 0.0
            if 'Impressions' in df.columns:
                numeric_impressions = pd.to_numeric(df['Impressions'], errors='coerce').fillna(0)
                current_impressions = numeric_impressions.sum()
                st.session_state.debug_messages.append(f"    Adding {current_impressions:.0f} Impressions to {brand_type} (Total: {current_impressions:.0f})")
            else:
                 st.session_state.debug_messages.append(f"    '{'Impressions'}' column missing. Impressions not added.")

            # Aggregate Clicks
            current_clicks = 0.0
            if 'Clicks' in df.columns:
                numeric_clicks = pd.to_numeric(df['Clicks'], errors='coerce').fillna(0)
                current_clicks = numeric_clicks.sum()
                st.session_state.debug_messages.append(f"    Adding {current_clicks:.0f} Clicks to {brand_type} (Total: {current_clicks:.0f})")
            else:
                 st.session_state.debug_messages.append(f"    '{'Clicks'}' column missing. Clicks not added.")

    return classified

@st.cache_data(ttl=3600, show_spinner="Calculating KPIs...")  # Cache for 1 hour
def calculate_branded_kpis(classified_campaigns):
    """Calculate KPIs for branded vs non-branded campaigns.
    
    Args:
        classified_campaigns: Output from classify_branded_campaigns()
        
    Returns:
        Dictionary of KPIs for branded and non-branded campaigns
    """
    kpis = {
        'branded': {
            'Total Spend': 0,
            'Total Ad Sales': 0,
            'ACoS': 0,
            'ROAS': 0,
            'CPC': 0,
            'CVR': 0,
            'CTR': 0,
            'Total Impressions': 0,
            'Total Clicks': 0,
            'AOV': 0,
            'CPA': 0,
            'Total Orders': 0
        },
        'non_branded': {
            'Total Spend': 0,
            'Total Ad Sales': 0,
            'ACoS': 0,
            'ROAS': 0,
            'CPC': 0,
            'CVR': 0,
            'CTR': 0,
            'Total Impressions': 0,
            'Total Clicks': 0,
            'AOV': 0,
            'CPA': 0,
            'Total Orders': 0
        }
    }
    
    # Track the last DataFrame for each brand type for CVR/AOV calculations
    last_df = {'Branded': None, 'Non-Branded': None}
    
    for campaign_type in classified_campaigns:
        for brand_type in classified_campaigns[campaign_type]:
            df = classified_campaigns[campaign_type][brand_type]
            if not isinstance(df, pd.DataFrame) or df.empty:
                continue
                
            # Store the last non-empty DataFrame for each brand type
            last_df[brand_type] = df
                
            # Map the brand_type to the lowercase key used in kpis
            kpi_key = 'branded' if brand_type == 'Branded' else 'non_branded'
                
            # Ensure numeric conversion for all metrics
            if 'Spend' in df.columns:
                kpis[kpi_key]['Total Spend'] += pd.to_numeric(df['Spend'], errors='coerce').fillna(0).sum()
            if 'Sales' in df.columns:
                kpis[kpi_key]['Total Ad Sales'] += pd.to_numeric(df['Sales'], errors='coerce').fillna(0).sum()
            if 'Impressions' in df.columns:
                kpis[kpi_key]['Total Impressions'] += pd.to_numeric(df['Impressions'], errors='coerce').fillna(0).sum()
            if 'Clicks' in df.columns:
                kpis[kpi_key]['Total Clicks'] += pd.to_numeric(df['Clicks'], errors='coerce').fillna(0).sum()
            if 'Orders' in df.columns:
                kpis[kpi_key]['Total Orders'] += pd.to_numeric(df['Orders'], errors='coerce').fillna(0).sum()
    
    # Calculate derived metrics
    for brand_type in kpis:
        if kpis[brand_type]['Total Spend'] > 0:
            kpis[brand_type]['ACoS'] = (kpis[brand_type]['Total Spend'] / kpis[brand_type]['Total Ad Sales']) * 100 \
                if kpis[brand_type]['Total Ad Sales'] > 0 else 0
            kpis[brand_type]['ROAS'] = kpis[brand_type]['Total Ad Sales'] / kpis[brand_type]['Total Spend'] \
                if kpis[brand_type]['Total Spend'] > 0 else 0
            kpis[brand_type]['CPC'] = kpis[brand_type]['Total Spend'] / kpis[brand_type]['Total Clicks'] \
                if kpis[brand_type]['Total Clicks'] > 0 else 0
            kpis[brand_type]['CPA'] = kpis[brand_type]['Total Spend'] / (kpis[brand_type]['Total Clicks'] * kpis[brand_type]['CVR'] / 100) \
                if kpis[brand_type]['Total Clicks'] > 0 and kpis[brand_type]['CVR'] > 0 else 0
        
        if kpis[brand_type]['Total Impressions'] > 0:
            kpis[brand_type]['CTR'] = (kpis[brand_type]['Total Clicks'] / kpis[brand_type]['Total Impressions']) * 100
            
        # Calculate CVR and AOV based on Total Orders
        if kpis[brand_type]['Total Clicks'] > 0:
            kpis[brand_type]['CVR'] = (kpis[brand_type]['Total Orders'] / kpis[brand_type]['Total Clicks']) * 100 \
                if kpis[brand_type]['Total Orders'] > 0 else 0
            kpis[brand_type]['AOV'] = kpis[brand_type]['Total Ad Sales'] / kpis[brand_type]['Total Orders'] \
                if kpis[brand_type]['Total Orders'] > 0 else 0
    
    return kpis

@st.cache_data(ttl=3600, show_spinner="Analyzing targeting performance...", hash_funcs={"_thread.RLock": lambda _: None})  # Cache for 1 hour
def get_targeting_performance_data(bulk_data, client_config):
    # Include the Sales Attribution choice in the cache key
    import re  # Import re module at the top of the function
    sd_attribution = st.session_state.get('sd_attribution_choice', 'Sales')
    st.session_state.debug_messages.append(f"Using Sales Attribution model: {sd_attribution} for targeting performance data")
    # Function description moved to a comment to prevent it from showing as a tooltip
    # Processes bulk data to calculate KPIs per target, classified as branded or non-branded.
    # Handles keyword and product targeting from SP, SB, and SD campaigns.
    # Uses the global sales attribution choice from session state.
    st.session_state.debug_messages.append(f"Starting targeting performance analysis v8") # Debug version

    if bulk_data is None or client_config is None:
        st.session_state.debug_messages.append("Targeting analysis skipped: Missing bulk data or client config.")
        return pd.DataFrame(), pd.DataFrame()

    # Check for cached analysis result
    client_name = client_config.get('client_name', 'unknown')
    analysis_input = {
        'bulk_data_hash': hash(str(bulk_data)),
        'client_config_hash': hash(str(client_config)),
        'sd_attribution': sd_attribution
    }
    
    cached_result = db_manager.get_cached_analysis_result(client_name, 'targeting_performance', analysis_input)
    if cached_result:
        track_cache_hit("targeting_performance_analysis")
        st.session_state.debug_messages.append("Retrieved cached targeting performance analysis")
        return cached_result
    else:
        track_cache_miss("targeting_performance_analysis")

    # --- Configuration Extraction ---
    # Safely extract branded terms (lowercase)
    branded_terms = [str(term).strip().lower() for term in client_config.get('branded_keywords', []) if str(term).strip()] if client_config.get('branded_keywords') else []

    # Safely extract branded ASINs (uppercase) from the correct source: branded_asins_data
    branded_asins_data = client_config.get('branded_asins_data', {})
    branded_asins = {str(asin).strip().upper() for asin in branded_asins_data.keys() if str(asin).strip()} # Use a set for faster lookups
    
    # Extract campaign product groups for filtering
    campaign_product_groups = {}
    if 'campaign_tags_data' in client_config:
        for campaign_name, campaign_info in client_config.get('campaign_tags_data', {}).items():
            product_group = campaign_info.get('tag_1', '') or 'Untagged Group'
            if product_group:
                # Store campaign names as uppercased, trimmed keys for robust lookup
                campaign_product_groups[str(campaign_name).strip().upper()] = product_group

    st.session_state.debug_messages.append(f"Found {len(branded_terms)} branded terms: {list(branded_terms)[:5]}...")
    st.session_state.debug_messages.append(f"Found {len(branded_asins)} branded ASINs from 'branded_asins_data': {list(branded_asins)[:5]}...")
    st.session_state.debug_messages.append(f"Found {len(campaign_product_groups)} campaigns with product groups")

    # --- Data Collection ---
    collected_targets = []

    # Determine which sheets to use for targeting data by scanning all uploaded sheets
    # We consider a sheet a "campaign/targeting" sheet if it has an Entity column
    # containing any of the valid targeting entity types.
    is_companion_data = st.session_state.get('is_companion_data', False)

    def find_entity_column(df):
        # Prefer 'Entity', but allow 'Record Type' seen in many bulk exports
        for c in df.columns:
            cl = str(c).strip().lower()
            if cl == 'entity' or cl == 'record type':
                return c
        return None

    def sheet_has_targeting_rows(df):
        entity_col = find_entity_column(df)
        if not entity_col:
            return False
        valid_entities = ['keyword', 'product targeting', 'product target', 'contextual targeting', 'audience targeting']
        try:
            mask = df[entity_col].fillna('').astype(str).str.strip().str.lower().apply(
                lambda x: any(entity in x for entity in valid_entities)
            )
            return bool(mask.any())
        except Exception:
            return False
    # Dynamically detect all campaign/targeting sheets present in bulk_data
    campaign_sheets_with_targets = []
    for sheet_name, df in bulk_data.items():
        if isinstance(df, pd.DataFrame) and not df.empty and sheet_has_targeting_rows(df):
            campaign_sheets_with_targets.append(sheet_name)

    using_campaign_sheets = len(campaign_sheets_with_targets) > 0

    if using_campaign_sheets:
        targeting_sheets = campaign_sheets_with_targets
        st.session_state.debug_messages.append(
            f"[Targeting Perf] Using dynamically detected targeting sheets: {targeting_sheets}"
        )
    else:
        # No sheets with explicit targeting rows; there may still be Search Term Reports available
        # Leave targeting_sheets empty so loop below will skip; All-mode will still pull data from get_search_term_data()
        targeting_sheets = []
        st.session_state.debug_messages.append(
            "[Targeting Perf] No sheets with targeting rows detected; relying on Search Term data only."
        )

    # Precompile ASIN regex for efficiency
    asin_regex = re.compile(r'(B[0-9A-Z]{9})')

    for sheet_name in targeting_sheets:
        if sheet_name not in bulk_data or bulk_data[sheet_name].empty:
            st.session_state.debug_messages.append(f"Skipping sheet: {sheet_name} (Not found or empty)")
            continue

        df = bulk_data[sheet_name]
        # Detect Sponsored Brands broadly by name to relax Ad Group requirement
        sb_sheet = ('brand' in sheet_name.lower()) or ('sb' in sheet_name.lower())
        st.session_state.debug_messages.append(f"Processing sheet: {sheet_name} with {len(df)} rows")

        # Dynamically find column names (case-insensitive)
        def find_col(potential_names):
            for name in potential_names:
                col = next((c for c in df.columns if c.strip().lower() == name.lower()), None)
                if col:
                    return col
            return None

        entity_col = find_col(['Entity'])
        # Column mapping depends on whether we're using Campaign sheets or Search Term Reports
        if using_campaign_sheets:
            # Using Campaign sheets (from Targeting Export or bulk data)
            keyword_text_col = find_col(['Keyword Text', 'Keyword'])
            product_targeting_col_sp_sb = find_col(['Product Targeting Expression', 'Keyword Text'])
            targeting_expression_col_sd = find_col(['Targeting Expression', 'Keyword Text', 'Product Targeting Expression'])
        else:
            # Using Search Term Reports (fallback for companion data without Targeting Export)
            keyword_text_col = find_col(['Keyword Text', 'Customer Search Term'])
            product_targeting_col_sp_sb = find_col(['Targeting Expression', 'Product Targeting Expression'])
            targeting_expression_col_sd = find_col(['Targeting Expression'])
        state_col = find_col(['State'])
        campaign_col = find_col(['Campaign Name (Informational Only)', 'Campaign Name', 'Campaign']) # Prioritize informational
        adgroup_col = find_col(['Ad Group Name', 'Ad Group'])
        bid_col = find_col(['Bid', 'Max Bid'])
        match_type_col = find_col(['Match Type']) # Add Match Type column
        spend_col = find_col(['Spend', 'Cost'])
        # Use global sales attribution choice to determine which sales column to prioritize
        if 'sd_attribution_choice' in st.session_state and st.session_state.sd_attribution_choice == "Sales (Views & Clicks)" and 'Sponsored Display' in sheet_name:
            sales_patterns = ['Sales (Views & Clicks)', 'Total Sales (Views & Clicks)', 'Sales', 'Total Sales', '14 Day Total Sales', '7 Day Total Sales']
            orders_patterns = ['Orders (Views & Clicks)', 'Orders', 'Total Orders', '14 Day Total Orders (#)', '7 Day Total Orders (#)']
        else:
            sales_patterns = ['Sales', 'Total Sales', '14 Day Total Sales', '7 Day Total Sales', 'Sales (Views & Clicks)']
            orders_patterns = ['Orders', '14 Day Total Orders (#)', '7 Day Total Orders (#)', 'Orders (Views & Clicks)']
            
        st.session_state.debug_messages.append(f"Targeting: Using sales patterns for {sheet_name}: {sales_patterns[:2]}...")
        sales_col = find_col(sales_patterns)
        orders_col = find_col(orders_patterns)
        impressions_col = find_col(['Impressions'])
        clicks_col = find_col(['Clicks'])

        # Essential columns check - require entity and some campaign column; do NOT require State universally
        required_cols = [entity_col, campaign_col]
        # Only require Ad Group for non-SB sheets if present; skip hard requirement to avoid dropping data
        # (Some bulk variants omit Ad Group even for SP)
        
        if not all(required_cols):
            missing = []
            for name, col_var in [('Entity/Record Type', entity_col), ('Campaign', campaign_col)]:
                if not col_var:
                    missing.append(name)
            # No longer treat Ad Group as required
            
            st.session_state.debug_messages.append(f"Error: Missing essential columns {missing} in {sheet_name}")
            continue

        # Filter for relevant entities (case-insensitive) and status
        # Expand valid entities to include variations seen in different campaign types
        valid_entities = ['keyword', 'product targeting', 'product target', 'contextual targeting', 'audience targeting']
        try:
            # Make filtering more robust to different casing and potential NaNs
            entity_mask = df[entity_col].fillna('').astype(str).str.strip().str.lower().apply(
                lambda x: any(entity in x for entity in valid_entities)
            )
            st.session_state.debug_messages.append(f"Entity types found in {sheet_name}: {df[entity_col].fillna('').astype(str).str.strip().str.lower().unique().tolist()}")
            # Remove state filtering - include all data regardless of state
            df_filtered = df[entity_mask]
        except KeyError as e:
             st.session_state.debug_messages.append(f"Error applying filters in {sheet_name}: Missing column {e}")
             continue
        st.session_state.debug_messages.append(f"Filtered to {len(df_filtered)} 'Keyword' or 'Product Targeting' rows in {sheet_name}")
        st.session_state.debug_messages.append(f"Filtered to {len(df_filtered)} 'Keyword' or 'Product Targeting' rows in {sheet_name}")

        # Process each relevant row
        for _, row in df_filtered.iterrows():
            entity_type = str(row[entity_col]).strip().lower()
            target_str = None
            classification = 'Non-Branded' # Default
            classification_reason = "Default - No specific match"

            # Skip negatives at the entity level (e.g., 'Negative Product Targeting')
            if 'negative' in entity_type:
                continue

            # --- Extract Target String ---
            # Handle keyword targeting
            if 'keyword' in entity_type and keyword_text_col and pd.notna(row[keyword_text_col]):
                target_str = str(row[keyword_text_col]).strip()
                entity_type = 'keyword'  # Normalize entity type
            # Handle product targeting (including variations)
            elif any(target_type in entity_type for target_type in ['product targeting', 'targeting', 'contextual targeting', 'audience targeting', 'product', 'asin', 'category']):
                # For Campaign sheets (from companion targeting), prioritize cleaned target names
                if using_campaign_sheets:
                    # First check for cleaned target names in Keyword Text (which contains cleaned targets from companion processing)
                    if keyword_text_col and pd.notna(row[keyword_text_col]):
                        potential_target = str(row[keyword_text_col]).strip()
                        # Use this if it's not raw JSON
                        if not (potential_target.startswith('[{') and potential_target.endswith('}]')):
                            target_str = potential_target
                    
                    # If we still don't have a clean target, try Product Targeting Expression
                    if not target_str and product_targeting_col_sp_sb and pd.notna(row[product_targeting_col_sp_sb]):
                        target_str = str(row[product_targeting_col_sp_sb]).strip()
                        # If this looks like raw JSON, try other columns for cleaned names
                        if target_str.startswith('[{') and target_str.endswith('}]'):
                            if targeting_expression_col_sd and pd.notna(row[targeting_expression_col_sd]):
                                alt_target = str(row[targeting_expression_col_sd]).strip()
                                if not (alt_target.startswith('[{') and alt_target.endswith('}]')):
                                    target_str = alt_target
                    
                    # If still no clean target, try Targeting Expression
                    if not target_str and targeting_expression_col_sd and pd.notna(row[targeting_expression_col_sd]):
                        target_str = str(row[targeting_expression_col_sd]).strip()
                else:
                    # For Search Term Reports, use the original logic
                    if product_targeting_col_sp_sb and pd.notna(row[product_targeting_col_sp_sb]):
                        target_str = str(row[product_targeting_col_sp_sb]).strip()
                    elif targeting_expression_col_sd and pd.notna(row[targeting_expression_col_sd]):
                        target_str = str(row[targeting_expression_col_sd]).strip()
                
                # Fallback options if we still have raw JSON or no target_str
                if not target_str or (target_str.startswith('[{') and target_str.endswith('}]')):
                    # For SD campaigns, also check Targeting ID as a fallback
                    if 'Targeting ID' in df.columns and pd.notna(row.get('Targeting ID')):
                        target_str = str(row.get('Targeting ID')).strip()
                    # Check for ASIN column as a fallback for product targeting
                    elif 'ASIN' in df.columns and pd.notna(row.get('ASIN')):
                        target_str = str(row.get('ASIN')).strip()
                    # Check for Target ID column as a fallback
                    elif 'Target ID' in df.columns and pd.notna(row.get('Target ID')):
                        target_str = str(row.get('Target ID')).strip()
                    # Special handling for Sponsored Brands auto-tagging using Creative ASINs
                    elif 'Sponsored Brands' in sheet_name and 'Creative ASINs' in df.columns and pd.notna(row.get('Creative ASINs')):
                        creative_asins = [asin.strip().upper() for asin in str(row.get('Creative ASINs')).split(',') if asin.strip()]
                        if creative_asins:
                            # If all ASINs are branded, classify as branded
                            if set(creative_asins).issubset(branded_asins):
                                is_branded = True
                                classification_reason = f"All Creative ASINs ({creative_asins}) are branded. Classified as Branded."
                            else:
                                is_branded = False
                                classification_reason = f"Not all Creative ASINs ({creative_asins}) are branded. Classified as Non-Branded."
                            target_str = ','.join(creative_asins) # For traceability
                # Normalize entity type for consistent processing
                entity_type = 'product targeting'

            if not target_str:
                # st.session_state.debug_messages.append(f"Skipping row: Could not extract target string for entity '{entity_type}'")
                continue # Skip if no target string could be extracted

            # --- Universal Auto Targeting Cleanup ---
            # Apply the same cleanup logic as in search term data
            original_target = target_str
            campaign_name = str(row.get(campaign_col, '')).strip() if pd.notna(row.get(campaign_col)) else ''
            
            # Detect campaign type from campaign name or sheet name
            campaign_type = 'SP'  # Default
            if 'SB' in campaign_name.upper() or 'SPONSORED BRAND' in campaign_name.upper() or 'Sponsored Brands' in sheet_name:
                campaign_type = 'SB'
            elif 'SD' in campaign_name.upper() or 'SPONSORED DISPLAY' in campaign_name.upper() or 'Sponsored Display' in sheet_name:
                campaign_type = 'SD'
            
            # Handle JSON format targeting expressions first
            if target_str.startswith('[{') and target_str.endswith('}]'):
                try:
                    import json
                    parsed = json.loads(target_str)
                    if isinstance(parsed, list) and len(parsed) > 0 and isinstance(parsed[0], dict):
                        target_type = parsed[0].get('type', '')
                        target_value = parsed[0].get('value', '')
                        
                        # Auto targeting cleanup
                        if target_type in ['queryBroadRelMatches', 'queryHighRelMatches', 'asinSubstituteRelated', 'asinAccessoryRelated']:
                            if target_type == 'queryBroadRelMatches':
                                target_str = 'Auto - Loose Match'
                            elif target_type == 'queryHighRelMatches':
                                target_str = 'Auto - Close Match'
                            elif target_type == 'asinSubstituteRelated':
                                target_str = 'Auto - Substitute'
                            elif target_type == 'asinAccessoryRelated':
                                target_str = 'Auto - Compliments'
                        
                        # ASIN targeting cleanup
                        elif target_type == 'asinSameAs' and target_value:
                            if 'expanded' in target_str.lower():
                                target_str = f'ASIN-Expanded = {target_value}'
                            else:
                                target_str = f'ASIN = {target_value}'
                        
                        # Category targeting cleanup
                        elif target_type == 'asinCategorySameAs' and target_value:
                            target_str = f'{campaign_type} Category Targeting'
                        
                        # Other JSON types - extract value if available
                        elif target_value:
                            target_str = target_value
                            
                except Exception as e:
                    # If JSON parsing fails, continue with original logic
                    st.session_state.debug_messages.append(f"[Targeting Performance] JSON parsing failed for target: {target_str[:50]}... Error: {e}")
                    pass
            
            # Handle non-JSON format targeting expressions
            else:
                # Auto targeting cleanup for simple strings
                if target_str in ['queryBroadRelMatches', 'queryHighRelMatches', 'asinSubstituteRelated', 'asinAccessoryRelated']:
                    if target_str == 'queryBroadRelMatches':
                        target_str = 'Auto - Loose Match'
                    elif target_str == 'queryHighRelMatches':
                        target_str = 'Auto - Close Match'
                    elif target_str == 'asinSubstituteRelated':
                        target_str = 'Auto - Substitute'
                    elif target_str == 'asinAccessoryRelated':
                        target_str = 'Auto - Compliments'
                
                # ASIN detection and formatting for non-JSON format
                elif 'asinSameAs' in target_str or 'asinsameas' in target_str.lower():
                    asin_pattern = re.compile(r'B0[A-Z0-9]{8}', re.IGNORECASE)  # Case-insensitive ASIN pattern
                    asin_match = asin_pattern.search(target_str)
                    
                    if asin_match:
                        asin = asin_match.group().upper()  # Convert to uppercase for consistency
                        if 'expanded' in target_str.lower():
                            target_str = f'ASIN-Expanded = {asin}'
                        else:
                            target_str = f'ASIN = {asin}'
                    else:
                        # Enhanced fallback - try to extract any ASIN-like pattern
                        # Look for patterns like b0xxxxxxxx (10 chars starting with b0)
                        fallback_pattern = re.compile(r'b0[a-z0-9]{8}', re.IGNORECASE)
                        fallback_match = fallback_pattern.search(target_str)
                        
                        if fallback_match:
                            asin = fallback_match.group().upper()
                            if 'expanded' in target_str.lower():
                                target_str = f'ASIN-Expanded = {asin}'
                            else:
                                target_str = f'ASIN = {asin}'
                        else:
                            # Final fallback if no ASIN found
                            if 'expanded' in target_str.lower():
                                target_str = 'ASIN-Expanded Target'
                            else:
                                target_str = 'ASIN Target'
                
                # Category targeting (campaign-specific)
                elif 'category=' in target_str.lower() or 'categorysameas' in target_str.lower():
                    target_str = f'{campaign_type} Category Targeting'
            
            # SD-specific remarketing logic (applies to both JSON and non-JSON)
            if campaign_type == 'SD':
                if 'views' in target_str.lower() or 'purchases' in target_str.lower():
                    # Extract lookback days if present
                    days_match = re.search(r'(\d+)', target_str)
                    if days_match:
                        days = days_match.group(1)
                        if 'views' in target_str.lower():
                            target_str = f'Views Remarketing - {days}d'
                        elif 'purchases' in target_str.lower():
                            target_str = f'Purchases Remarketing - {days}d'
                    else:
                        if 'views' in target_str.lower():
                            target_str = 'Views Remarketing'
                        elif 'purchases' in target_str.lower():
                            target_str = 'Purchases Remarketing'
            
            # Log cleanup if target was changed
            if target_str != original_target:
                st.session_state.debug_messages.append(f"[Targeting Performance] Cleaned target: '{original_target}' â†’ '{target_str}'")

            # --- Classification Logic ---
            target_lower = target_str.lower() # For keyword matching
            is_branded = False # Reset for each target
                            
            if entity_type == 'keyword':
                # Exact match check first for branded terms
                if target_lower in branded_terms:
                    is_branded = True
                    classification_reason = f"Keyword '{target_str}' exactly matched a branded term."
                else:
                    # Check if any branded term is a substring (e.g., "brand shoe" contains "brand")
                    if any(term in target_lower for term in branded_terms):
                         is_branded = True
                         classification_reason = f"Keyword '{target_str}' contained a branded term."
                    else:
                         classification_reason = f"Keyword '{target_str}' did not match any branded terms."

            elif entity_type == 'product targeting':
                # --- SPECIAL CASE FOR EXACT-PRODUCT TARGETING IN SD ---
                # Check if this is a Sponsored Display campaign with exact-product targeting
                is_sponsored_display = 'Sponsored Display' in sheet_name
                target_lower = target_str.lower()
                contains_exact_product = 'exact-product' in target_lower or 'exact-product' in original_target.lower()
                
                if is_sponsored_display and contains_exact_product:
                    is_branded = True
                    classification_reason = f"Target '{target_str}' contains 'exact-product' in Sponsored Display campaign (Branded)"
                    if 'debug_messages' in st.session_state:
                        st.session_state.debug_messages.append(f"[Targeting Performance] HIGHEST PRIORITY: Classified '{target_str}' as BRANDED due to exact-product")
                # --- ENHANCED AUTO TARGET LOGIC FOR SP ---
                elif sheet_name == 'Sponsored Products Campaigns':
                    # Check for Auto targeting patterns
                    target_lower = target_str.strip().lower()
                    
                    # Direct auto targeting terms
                    if target_lower in {'loose-match', 'close-match', 'complements', 'substitutes', 'auto - loose match', 'auto - close match', 'auto - substitute', 'auto - compliments'}:
                        entity_type = 'auto targeting'
                        match_type = 'Auto'
                        classification_reason = f"Target '{target_str}' matched Auto term for SP. Classified as 'Auto' target."
                    
                    # Check for auto targeting patterns in target expressions
                    elif any(pattern in target_lower for pattern in ['querybroadrelmatches', 'queryhighrelmatches', 'asinsubstituterelated', 'asinaccessoryrelated']):
                        entity_type = 'auto targeting'
                        match_type = 'Auto'
                        classification_reason = f"Target '{target_str}' contains Auto targeting pattern for SP. Classified as 'Auto' target."
                    
                    # Check for targeting type column with 'auto' value
                    elif 'Targeting Type' in df.columns and pd.notna(row.get('Targeting Type')) and str(row.get('Targeting Type')).strip().lower() == 'auto':
                        entity_type = 'auto targeting'
                        match_type = 'Auto'
                        classification_reason = f"Target '{target_str}' has Targeting Type 'Auto' for SP. Classified as 'Auto' target."
                # Continue with normal product targeting logic if not exact-product
                else:
                    found_asins = set(asin_regex.findall(target_str.upper())) # Use set for uniqueness

                    if found_asins:
                        # Check if ALL found ASINs are in the defined branded ASIN list
                        if found_asins.issubset(branded_asins):
                             # Check for 'expanded' non-branded)
                            if 'expanded' in target_lower:
                                is_branded = False
                                classification_reason = f"Target '{target_str}' contained only branded ASIN(s) {found_asins} but also 'expanded'."
                            else:
                                is_branded = True
                                classification_reason = f"Target '{target_str}' contained ONLY branded ASIN(s): {found_asins}"
                        else:
                            # If the target contains *any* non-branded ASIN, classify as Non-Branded
                            non_branded_found = found_asins - branded_asins
                            branded_found = found_asins.intersection(branded_asins)
                            is_branded = False # Explicitly non-branded
                            classification_reason = f"Target '{target_str}' contained non-branded ASIN(s): {non_branded_found}."
                            if branded_found:
                                classification_reason += f" (Also contained branded: {branded_found})"

                    else:
                        # If the target string doesn't contain any recognizable ASINs
                        # Check if it contains any branded *terms* as a fallback (e.g., targeting categories)
                        if any(term in target_lower for term in branded_terms):
                            is_branded = True
                            classification_reason = f"Product Target '{target_str}' (no ASINs found) contained a branded term."
                        else:
                            classification_reason = f"Product Target '{target_str}' contained no recognizable ASINs or branded terms."

            # --- COMPANION EXPORTS SPECIAL LOGIC ---
            # For Companion Exports, apply additional classification rules
            if st.session_state.get('is_companion_data', False):
                # Rule 1: "Remarketing - Branded" Match Type should be considered Branded
                if match_type_col and pd.notna(row.get(match_type_col)):
                    current_match_type = str(row.get(match_type_col)).strip()
                    if current_match_type == 'Remarketing - Branded':
                        is_branded = True
                        classification_reason = f"Companion Export: Match Type '{current_match_type}' classified as Branded"
                        st.session_state.debug_messages.append(f"[Companion Export] Classified '{target_str}' as BRANDED due to Match Type 'Remarketing - Branded'")
                
                # Rule 2: Check ASIN targets against Branded ASINs for both Targeting and Search Term Performance
                # This applies to both Target column (Targeting Performance) and Search Term column (Search Term Performance)
                asin_pattern_companion = re.compile(r'(B[0-9A-Z]{9})', re.IGNORECASE)
                
                # Check Target column for ASINs
                target_asins = set(asin_pattern_companion.findall(target_str.upper()))
                if target_asins:
                    # Check if any of the ASINs found are branded
                    branded_asins_found = target_asins.intersection(branded_asins)
                    if branded_asins_found:
                        is_branded = True
                        classification_reason = f"Companion Export: Target '{target_str}' contains branded ASIN(s): {branded_asins_found}"
                        st.session_state.debug_messages.append(f"[Companion Export] Classified '{target_str}' as BRANDED due to branded ASIN(s): {branded_asins_found}")

            # --- Universal ASIN Classification Safety Check ---
            # Final check to ensure any target containing ONLY branded ASINs is classified as Branded
            if not is_branded:  # Only apply if not already branded
                all_asins_in_target = set(asin_regex.findall(target_str.upper()))
                if all_asins_in_target and all_asins_in_target.issubset(branded_asins) and 'expanded' not in target_str.lower():
                    is_branded = True
                    classification_reason = f'Safety Check: Target {target_str} contains ONLY branded ASIN(s): {all_asins_in_target}'
                    st.session_state.debug_messages.append(f'[Universal ASIN Safety Check] Reclassified {target_str} as BRANDED')

            classification = 'Branded' if is_branded else 'Non-Branded' 

            # --- Data Extraction ---
            try:
                # Extract metrics, handling potential missing columns and non-numeric data
                spend = pd.to_numeric(row.get(spend_col, 0), errors='coerce') or 0
                sales = pd.to_numeric(row.get(sales_col, 0), errors='coerce') or 0
                orders = pd.to_numeric(row.get(orders_col, 0), errors='coerce') or 0
                impressions = pd.to_numeric(row.get(impressions_col, 0), errors='coerce') or 0
                clicks = pd.to_numeric(row.get(clicks_col, 0), errors='coerce') or 0

                # Calculate KPIs
                if sales > 0:
                    acos = (spend / sales) * 100
                else:
                    acos = 0
                roas = sales / spend if spend > 0 else 0
                cpc = spend / clicks if clicks > 0 else 0
                ctr = (clicks / impressions) * 100 if impressions > 0 else 0
                cvr = (orders / clicks) * 100 if clicks > 0 else 0
                aov = sales / orders if orders > 0 else 0
                cpa = spend / orders if orders > 0 else 0 # Cost Per Acquisition/Order

                # Get match type - enhanced logic for better detection
                if entity_type == 'keyword' and match_type_col and pd.notna(row.get(match_type_col)):
                    match_type = str(row.get(match_type_col)).strip()
                    # Skip negative keywords
                    if 'negative' in match_type.lower():
                        continue
                # Enhanced Auto targeting detection
                elif entity_type == 'auto targeting':
                    match_type = 'Auto'
                # For companion data, check if we have a Match Type column with 'Auto' value
                elif match_type_col and pd.notna(row.get(match_type_col)):
                    provided_match_type = str(row.get(match_type_col)).strip()
                    if provided_match_type == 'Auto':
                        match_type = 'Auto'
                        entity_type = 'auto targeting'  # Update entity type as well
                    elif provided_match_type in ['Remarketing - Branded', 'Remarketing - Competitor']:
                        match_type = provided_match_type
                    else:
                        match_type = provided_match_type
                elif sheet_name == 'Sponsored Display Campaigns':
                    # Use Match Type column if available (from companion targeting processing)
                    if match_type_col and pd.notna(row.get(match_type_col)):
                        match_type = str(row.get(match_type_col)).strip()
                    else:
                        # Default for SD if no match type available
                        match_type = 'Product Target'
                else:
                    # Check if product targeting contains 'category' for special classification
                    if entity_type == 'product targeting' and target_str and 'category' in target_str.lower():
                        match_type = 'Category Targeting'
                    else:
                        # Roll up all other product targeting types to 'Product Target' for consistency
                        match_type = 'Product Target' 
                
                # Handle Ad Group differently for Sponsored Brands
                ad_group_value = 'N/A'
                if adgroup_col and pd.notna(row.get(adgroup_col)):
                    ad_group_value = row.get(adgroup_col)
                elif sb_sheet:
                    ad_group_value = 'SB Campaign'
                    
                # Get campaign name for product group lookup
                campaign_name = row.get(campaign_col, 'N/A')
                # Get product group for this campaign (case-insensitive, whitespace-trimmed lookup)
                # Only populate Product Group if there are actually product groups defined in Campaign Tagging
                product_group = ''
                if campaign_product_groups:  # Only if there are product groups defined
                    campaign_name_upper = str(campaign_name).strip().upper()
                    product_group = campaign_product_groups.get(campaign_name_upper, '')
                
                # Debug match type assignment
                if 'debug_messages' in st.session_state:
                    st.session_state.debug_messages.append(f"[Match Type Debug] Target: '{target_str}' | Entity: '{entity_type}' | Match Type: '{match_type}' | Sheet: '{sheet_name}'")
                
                
                # Fix Target Type for Sponsored Display remarketing campaigns
                target_type_to_use = entity_type.title()
                
                # Check if this is a Sponsored Display campaign - use Targeting Expression to determine type
                if sheet_name == 'Sponsored Display Campaigns' and entity_type == 'product targeting':
                    # Get the targeting expression to determine if it's remarketing or product targeting
                    targeting_expression = ''
                    if targeting_expression_col_sd and pd.notna(row.get(targeting_expression_col_sd)):
                        targeting_expression = str(row.get(targeting_expression_col_sd, '')).lower()
                    
                    # First, check if it's category targeting (highest priority)
                    is_category_targeting = ('category' in targeting_expression or 
                                           'category' in target_str.lower() or
                                           'asinCategorySameAs' in str(row.get(targeting_expression_col_sd, '')))
                    
                    if is_category_targeting:
                        # It's category targeting
                        target_type_to_use = 'Product Targeting'
                        match_type = 'Category Targeting'
                        st.session_state.debug_messages.append(f"[Targeting Performance] Category targeting: '{target_str}' (Expression: {targeting_expression[:50]})")
                        
                    else:
                        # Check if it's remarketing (contains 'views' or 'purchases')
                        is_remarketing = 'views' in targeting_expression or 'purchases' in targeting_expression
                        
                        if is_remarketing:
                            # Check if it's category-based remarketing (contains both remarketing and category)
                            is_category_remarketing = 'category' in targeting_expression
                            
                            if is_category_remarketing:
                                # Category-based remarketing - extract lookback period and create descriptive target name
                                lookback_match = re.search(r'lookback=(\d+)', targeting_expression)
                                lookback_days = lookback_match.group(1) if lookback_match else '30'  # default to 30 if not found
                                
                                if 'views' in targeting_expression:
                                    target_str = f'Views Remarketing - {lookback_days}d'
                                else:  # purchases
                                    target_str = f'Purchases Remarketing - {lookback_days}d'
                                
                                # Category-based remarketing is always competitor
                                target_type_to_use = 'Remarketing - Competitor'
                                match_type = 'Remarketing - Competitor'
                                st.session_state.debug_messages.append(f"[Targeting Performance] Category remarketing: Updated target to '{target_str}' (Expression: {targeting_expression[:50]})")
                            else:
                                # Product-based remarketing - determine if branded or competitor based on expression content
                                if 'exact-product' in targeting_expression:
                                    target_type_to_use = 'Remarketing - Branded'
                                    match_type = 'Remarketing - Branded'
                                elif 'similar-product' in targeting_expression:
                                    target_type_to_use = 'Remarketing - Competitor'
                                    match_type = 'Remarketing - Competitor'
                                else:
                                    # Fallback to branded classification if no specific product type found
                                    if is_branded:
                                        target_type_to_use = 'Remarketing - Branded'
                                        match_type = 'Remarketing - Branded'
                                    else:
                                        target_type_to_use = 'Remarketing - Competitor'
                                        match_type = 'Remarketing - Competitor'
                                st.session_state.debug_messages.append(f"[Targeting Performance] Product remarketing: Updated Target Type and Match Type for '{target_str}' to '{target_type_to_use}' (Expression: {targeting_expression[:50]})")
                        else:
                            # It's product targeting - determine branded/non-branded
                            if 'similar-product' in targeting_expression:
                                # Similar product is always non-branded
                                is_branded = False
                                classification = 'Non-Branded'
                                classification_reason = 'Similar-product targeting is classified as Non-Branded'
                            elif 'exact-product' in targeting_expression:
                                # Exact product is branded, but we still check ASINs if available
                                is_branded = True
                                classification = 'Branded'
                                classification_reason = 'Exact-product targeting is classified as Branded'
                            # For other product targeting, classification remains as determined by ASIN check
                            match_type = 'Product Target'
                            st.session_state.debug_messages.append(f"[Targeting Performance] Product targeting: '{target_str}' classified as {classification} (Expression: {targeting_expression[:50]})")
                
                # --- Enabled flag detection (status/state + entity gating) ---
                try:
                    lower_map = {c.lower(): c for c in df.columns}
                    status_candidates = ['state', 'targeting state', 'target state', 'status', 'target status']
                    status_col_name = next((lower_map[c] for c in status_candidates if c in lower_map), None)
                    state_val = str(row.get(status_col_name, '')).strip().lower() if status_col_name else ''
                    enabled_base = (state_val == 'enabled')
                except Exception:
                    enabled_base = False

                # Consider only target-level entities for enabled counting
                entity_ok = any(tok in entity_type for tok in ['keyword', 'product targeting', 'auto targeting', 'contextual targeting', 'audience targeting'])
                is_enabled = bool(enabled_base and entity_ok)

                collected_targets.append({
                    'Campaign': campaign_name,
                    'Ad Group': ad_group_value,
                    'Product Group': product_group,
                    'Target Type': target_type_to_use,
                    'Target': target_str,
                    'Match Type': match_type,  # Use the match type as determined above
                    'Classification': classification,
                    'Classification Reason': classification_reason, # Added for debugging
                    'Spend': spend,
                    'Ad Sales': sales,
                    'Orders': orders,
                    'Impressions': impressions,
                    'Clicks': clicks,
                    'ACoS': round(acos, 2),
                    'ROAS': roas,
                    'CPC': cpc,
                    'CTR': ctr,
                    'CVR': cvr,
                    'AOV': aov,
                    'CPA': cpa,
                    'Bid': pd.to_numeric(row.get(bid_col, 0), errors='coerce') or 0, # Add Bid
                    'Sheet Source': sheet_name, # Added for debugging
                    'Is Enabled': is_enabled
                })
            except Exception as e:
                 st.session_state.debug_messages.append(f"Error processing row data in {sheet_name} for target '{target_str}': {e}")

    # Process the collected targets
    targets_df = pd.DataFrame(collected_targets)
    
    if targets_df.empty:
        st.session_state.debug_messages.append("No targeting data found after processing.")
        return pd.DataFrame(), pd.DataFrame()
        
    # Log the distribution of target types for debugging
    if 'Target Type' in targets_df.columns:
        target_type_counts = targets_df['Target Type'].value_counts().to_dict()
        st.session_state.debug_messages.append(f"Target type distribution: {target_type_counts}")
    
    if 'Match Type' in targets_df.columns:
        match_type_counts = targets_df['Match Type'].value_counts().to_dict()
        st.session_state.debug_messages.append(f"Match type distribution: {match_type_counts}")
    
    # Ensure we're using 'Ad Sales' as the standard column name
    # But keep 'Sales' for backward compatibility with other functions
    if 'Sales' in targets_df.columns and 'Ad Sales' not in targets_df.columns:
        targets_df['Ad Sales'] = targets_df['Sales']
    elif 'Ad Sales' in targets_df.columns and 'Sales' not in targets_df.columns:
        targets_df['Sales'] = targets_df['Ad Sales']
        
    # Split into branded and non-branded
    branded_targets_df = targets_df[targets_df['Classification'] == 'Branded'].copy()
    non_branded_targets_df = targets_df[targets_df['Classification'] == 'Non-Branded'].copy()

    # Debug: Show total and sample rows
    st.session_state.debug_messages.append(
        f"[Targeting Classification] Total targets: {len(targets_df)}, Branded: {len(branded_targets_df)}, Non-Branded: {len(non_branded_targets_df)}"
    )

    if not non_branded_targets_df.empty:
        sample_cols = [col for col in ['Campaign','Target','Match Type','Spend','Ad Sales','ACoS'] if col in non_branded_targets_df.columns]
        if sample_cols:
            st.session_state.debug_messages.append(
                f"[Non-Branded Sample Rows] {non_branded_targets_df[sample_cols].head(10).to_dict(orient='records')}"
            )
        else:
            st.session_state.debug_messages.append("[Non-Branded Sample Rows] No sample columns available in non_branded_targets_df.")
    else:
        st.session_state.debug_messages.append("[Non-Branded Sample Rows] No non-branded rows found.")

    # Cache the analysis result
    result = (branded_targets_df, non_branded_targets_df)
    db_manager.cache_analysis_result(client_name, 'targeting_performance', analysis_input, result)

    return branded_targets_df, non_branded_targets_df

@st.cache_data(ttl=3600, show_spinner="Processing data...", hash_funcs={"_thread.RLock": lambda _: None})  # Cache for 1 hour
def get_search_term_data(bulk_data, client_config=None):
    # Include the Sales Attribution choice in the cache key
    sd_attribution = st.session_state.get('sd_attribution_choice', 'Sales')
    st.session_state.debug_messages.append(f"Using Sales Attribution model: {sd_attribution} for search term data")
    # Function description moved to a comment to prevent it from showing as a tooltip
    # Extracts and processes search term data from bulk advertising files.
    # Handles both Sponsored Products and Sponsored Brands search terms.
    # Classifies search terms as Branded or Non-Branded based on client configuration.
    if not bulk_data or not isinstance(bulk_data, dict):
        return pd.DataFrame()

    # Check for cached analysis result
    if client_config:
        client_name = client_config.get('client_name', 'unknown')
        analysis_input = {
            'bulk_data_hash': hash(str(bulk_data)),
            'client_config_hash': hash(str(client_config)),
            'sd_attribution': sd_attribution
        }
        
        cached_result = db_manager.get_cached_analysis_result(client_name, 'search_term_data', analysis_input)
        if cached_result is not None:
            st.session_state.debug_messages.append("Retrieved cached search term data analysis")
            return cached_result
        
    st.session_state.debug_messages.append(f"Starting Search Term Performance")
    
    # Initialize an empty list to store search term data
    search_term_data = []
    
    # Get targeting data to match with search terms
    targeting_data = None
    # Check if we have both branded and non-branded targeting data in session state
    if 'branded_targets_df' in st.session_state and 'non_branded_targets_df' in st.session_state:
        branded_df = st.session_state.branded_targets_df
        non_branded_df = st.session_state.non_branded_targets_df
        if not branded_df.empty or not non_branded_df.empty:
            targeting_data = pd.concat([branded_df, non_branded_df], ignore_index=True)
            st.session_state.debug_messages.append(f"[Search Term Performance] Loaded targeting data: {len(targeting_data)} rows")
    
    # Extract campaign product groups for filtering
    campaign_product_groups = {}
    if client_config and 'campaign_tags_data' in client_config:
        for campaign_name, campaign_info in client_config.get('campaign_tags_data', {}).items():
            product_group = campaign_info.get('tag_1', '') or 'Untagged Group'
            if product_group:
                campaign_product_groups[campaign_name.upper()] = product_group
        st.session_state.debug_messages.append(f"[Search Term Performance] Found {len(campaign_product_groups)} campaigns with product groups")
    
    # Check each sheet in the bulk file for search term data
    for sheet_name, df in bulk_data.items():
        if not isinstance(df, pd.DataFrame) or df.empty:
            continue
            
        # Look for Search Term column (case insensitive)
        # Check for both 'Search Term' and 'Customer Search Term'
        search_term_col = None
        for col in df.columns:
            if col.lower() == 'search term' or col.lower() == 'customer search term':
                search_term_col = col
                st.session_state.debug_messages.append(f"[Search Term Performance] Found column: {col}")
                break
        
        # Look for Campaign Name column (case insensitive) - prioritize informational name over ID
        campaign_col = None
        # First, try to find 'Campaign Name (Informational Only)' specifically
        for col in df.columns:
            if col.lower() == 'campaign name (informational only)':
                campaign_col = col
                st.session_state.debug_messages.append(f"[Search Term Performance] Found informational campaign column: {col}")
                break
                
        # If not found, try 'Campaign Name'
        if not campaign_col:
            for col in df.columns:
                if col.lower() == 'campaign name':
                    campaign_col = col
                    st.session_state.debug_messages.append(f"[Search Term Performance] Found campaign name column: {col}")
                    break
                    
        # Last resort, use 'Campaign' column
        if not campaign_col:
            for col in df.columns:
                if col.lower() == 'campaign':
                    campaign_col = col
                    st.session_state.debug_messages.append(f"[Search Term Performance] Found campaign column: {col}")
                    break
        
        # Debug available columns
        st.session_state.debug_messages.append(f"[Search Term Performance] Available columns: {', '.join(df.columns)}")
                
        if search_term_col is None:
            continue
            
        st.session_state.debug_messages.append(f"Found search term data in sheet: {sheet_name}")
        
        # Identify campaign type (Sponsored Products or Sponsored Brands)
        campaign_type = None
        if 'Product' in df.columns:
            # Get unique values in the Product column
            product_values = df['Product'].unique()
            for product in product_values:
                if isinstance(product, str):
                    if 'sponsored products' in product.lower():
                        campaign_type = 'Sponsored Products'
                        break
                    elif 'sponsored brands' in product.lower():
                        campaign_type = 'Sponsored Brands'
                        break
        
        # If campaign type couldn't be determined from Product column, try to infer from sheet name
        if campaign_type is None:
            if 'product' in sheet_name.lower() or 'sp' in sheet_name.lower():
                campaign_type = 'Sponsored Products'
            elif 'brand' in sheet_name.lower() or 'sb' in sheet_name.lower():
                campaign_type = 'Sponsored Brands'
            else:
                campaign_type = 'Unknown'
                
        st.session_state.debug_messages.append(f"Campaign type for sheet {sheet_name}: {campaign_type}")
        
        # Create a copy of the dataframe with only the columns we need
        search_df = df.copy()
        
        # Add campaign type column if not already present
        if 'Campaign Type' not in search_df.columns:
            search_df['Campaign Type'] = campaign_type
            
        # Ensure required columns exist
        required_cols = ['Campaign', search_term_col, 'Target', 'Match Type', 'Impressions', 'Clicks', 'Spend', 'Orders', 'Sales']
        for col in required_cols:
            if col not in search_df.columns:
                search_df[col] = None
                
        # If we found a campaign column in the original data, use it to populate the Campaign column
        if campaign_col and campaign_col != 'Campaign':
            # Make sure to handle any NaN or empty values
            search_df['Campaign'] = df[campaign_col].fillna('')
            # Replace empty strings with None so we can detect them later
            search_df.loc[search_df['Campaign'] == '', 'Campaign'] = None
            st.session_state.debug_messages.append(f"[Search Term Performance] Populated Campaign column from {campaign_col}, found {search_df['Campaign'].notna().sum()} non-empty values")
                
        # Rename the search term column to a standard name
        search_df = search_df.rename(columns={search_term_col: 'Search Term'})
        
        # Handle Target column for companion data - use 'Keyword Text' if available and Target is empty
        if 'Target' in search_df.columns and search_df['Target'].isnull().all() and 'Keyword Text' in search_df.columns:
            search_df['Target'] = search_df['Keyword Text']
            st.session_state.debug_messages.append(f"[Search Term Performance] Populated Target column from Keyword Text")
        
        # Helper function to safely convert values to float
        def safe_convert_to_float(val):
            if pd.isna(val):
                return 0.0
            try:
                # Remove any non-numeric characters except decimal point
                return float(str(val).replace('$', '').replace(',', '').replace('%', ''))
            except:
                return 0.0
                
        # Calculate metrics if possible
        if 'Clicks' in search_df.columns and 'Spend' in search_df.columns and search_df['Clicks'].sum() > 0:
            # Apply safe conversion to ensure numeric values
            spend_values = search_df['Spend'].apply(safe_convert_to_float)
            clicks_values = search_df['Clicks'].apply(safe_convert_to_float)
            search_df['CPC'] = spend_values / clicks_values.replace(0, float('nan'))
            # Replace NaN with 0
            search_df['CPC'] = search_df['CPC'].fillna(0)
        else:
            search_df['CPC'] = 0
            
        # Determine which sales column to use based on attribution choice
        sales_col = None
        if 'sd_attribution_choice' in st.session_state and st.session_state.sd_attribution_choice == "Sales (Views & Clicks)" and campaign_type == 'Sponsored Display':
            sales_col = 'Sales (Views & Clicks)' if 'Sales (Views & Clicks)' in search_df.columns else 'Sales'
        else:
            sales_col = 'Sales' if 'Sales' in search_df.columns else None
            
        # If we have a valid sales column, calculate ACoS and CVR
        if sales_col and 'Spend' in search_df.columns:
            # Create Ad Sales column for consistency with targeting performance
            search_df['Ad Sales'] = search_df[sales_col]
            
            # Apply safe conversion to numeric values
            spend_values = search_df['Spend'].apply(safe_convert_to_float)
            sales_values = search_df['Ad Sales'].apply(safe_convert_to_float)
            
            # Calculate ACoS
            search_df['ACoS'] = (spend_values / sales_values.replace(0, float('nan')) * 100).fillna(0)
            # Format ACoS as percentage with 2 decimal places
            search_df['ACoS'] = search_df['ACoS'].apply(lambda x: round(float(x), 2) if pd.notnull(x) else 0)
            
            # Calculate CVR (Conversion Rate)
            orders_values = search_df['Orders'].apply(safe_convert_to_float)
            clicks_values = search_df['Clicks'].apply(safe_convert_to_float)
            search_df['CVR'] = (orders_values / clicks_values.replace(0, float('nan')) * 100).fillna(0)
            # Format CVR as percentage with 2 decimal places
            search_df['CVR'] = search_df['CVR'].apply(lambda x: round(float(x), 2) if pd.notnull(x) else 0)
            
            # Calculate CTR (Click-Through Rate)
            clicks_values = search_df['Clicks'].apply(safe_convert_to_float)
            impressions_values = search_df['Impressions'].apply(safe_convert_to_float)
            search_df['CTR'] = (clicks_values / impressions_values.replace(0, float('nan')) * 100).fillna(0)
            # Format CTR as percentage with 2 decimal places
            search_df['CTR'] = search_df['CTR'].apply(lambda x: round(float(x), 2) if pd.notnull(x) else 0)
            
            # Calculate ROAS
            sales_values = search_df['Ad Sales'].apply(safe_convert_to_float)
            spend_values = search_df['Spend'].apply(safe_convert_to_float)
            search_df['ROAS'] = (sales_values / spend_values.replace(0, float('nan'))).fillna(0)
            # Format ROAS with 2 decimal places
            search_df['ROAS'] = search_df['ROAS'].apply(lambda x: round(float(x), 2) if pd.notnull(x) else 0)
            
            # Calculate AOV (Average Order Value)
            sales_values = search_df['Ad Sales'].apply(safe_convert_to_float)
            orders_values = search_df['Orders'].apply(safe_convert_to_float)
            search_df['AOV'] = (sales_values / orders_values.replace(0, float('nan'))).fillna(0)
            # Format AOV with 2 decimal places
            search_df['AOV'] = search_df['AOV'].apply(lambda x: round(float(x), 2) if pd.notnull(x) else 0)
            
            # Format CPC with 2 decimal places
            if 'CPC' in search_df.columns:
                search_df['CPC'] = search_df['CPC'].apply(lambda x: round(float(x), 2) if pd.notnull(x) else 0)
                
            # Ensure all numeric columns are properly formatted
            for col in ['Spend', 'Ad Sales', 'Orders', 'Impressions', 'Clicks']:
                if col in search_df.columns:
                    search_df[col] = search_df[col].apply(safe_convert_to_float)
            
            # Add product group information to search terms
            # Only populate if there are actually product groups defined in Campaign Tagging
            if 'Campaign' in search_df.columns:
                if campaign_product_groups:  # Only if there are product groups defined
                    search_df['Product Group'] = search_df['Campaign'].apply(
                        lambda campaign: campaign_product_groups.get(str(campaign).upper(), '') or 'Untagged Group' if pd.notna(campaign) else 'Untagged Group'
                    )
                else:
                    search_df['Product Group'] = 'Untagged Group'  # Use 'Untagged Group' if no product groups defined
        
        # Add Sheet Source column for Ad Type filtering
        search_df['Sheet Source'] = sheet_name
        
        # Add to our collection
        search_term_data.append(search_df)
    
    # Process Sponsored Display targeting data and add it to search term data
    # This will map SD targeting data to the Search Terms data source
    sd_targeting_data = []
    for sheet_name, df in bulk_data.items():
        if not isinstance(df, pd.DataFrame) or df.empty:
            continue
            
        # Only process Sponsored Display sheets
        if 'sponsored display' not in sheet_name.lower() and 'sd' not in sheet_name.lower():
            continue
            
        st.session_state.debug_messages.append(f"Processing Sponsored Display targeting data from sheet: {sheet_name}")
        
        # Look for targeting expression column
        targeting_expression_col = None
        for col in df.columns:
            if col.lower() in ['targeting expression', 'product targeting expression']:
                targeting_expression_col = col
                st.session_state.debug_messages.append(f"[SD Search Term Performance] Found targeting column: {col}")
                break
                
        # Look for Campaign Name column - prioritize informational name over ID
        campaign_col = None
        # First, try to find 'Campaign Name (Informational Only)' specifically
        for col in df.columns:
            if col.lower() == 'campaign name (informational only)':
                campaign_col = col
                st.session_state.debug_messages.append(f"[SD Search Term Performance] Found informational campaign column: {col}")
                break
                
        # If not found, try 'Campaign Name'
        if not campaign_col:
            for col in df.columns:
                if col.lower() == 'campaign name':
                    campaign_col = col
                    st.session_state.debug_messages.append(f"[SD Search Term Performance] Found campaign name column: {col}")
                    break
                    
        # Last resort, use 'Campaign' column
        if not campaign_col:
            for col in df.columns:
                if col.lower() == 'campaign':
                    campaign_col = col
                    st.session_state.debug_messages.append(f"[SD Search Term Performance] Found campaign column: {col}")
                    break
        
        # Debug available columns
        st.session_state.debug_messages.append(f"[SD Search Term Performance] Available columns: {', '.join(df.columns)}")

                
        # Skip if no targeting expression column found
        if targeting_expression_col is None:
            continue
            
        # Filter for relevant entities (targeting)
        entity_col = next((c for c in df.columns if c.lower() == 'entity'), None)
        if entity_col:
            valid_entities = ['product targeting', 'product target', 'contextual targeting', 'audience targeting']
            entity_mask = df[entity_col].fillna('').astype(str).str.strip().str.lower().apply(
                lambda x: any(entity in x for entity in valid_entities)
            )
            state_col = next((c for c in df.columns if c.lower() == 'state'), None)
            df_filtered = df
            if state_col:
                df_filtered = df[entity_mask]
            else:
                df_filtered = df[entity_mask]
            
        # Process each row with targeting expression
        for _, row in df_filtered.iterrows():
            if pd.isna(row.get(targeting_expression_col)):
                continue
                
            target_expr = str(row.get(targeting_expression_col)).strip()
            
            # Skip empty targeting expressions
            if not target_expr:
                continue
                
            # Create a search term-like entry from the targeting expression
            sd_entry = {}
            
            # Set campaign
            campaign_value = row.get(campaign_col) if campaign_col else None
            
            # Try harder to find a campaign name
            if not (campaign_value and pd.notna(campaign_value) and str(campaign_value).strip() != ''):
                # Try alternative campaign columns
                for alt_col in df.columns:
                    if 'campaign' in alt_col.lower() and alt_col != campaign_col:
                        alt_value = row.get(alt_col)
                        if alt_value and pd.notna(alt_value) and str(alt_value).strip() != '':
                            campaign_value = alt_value
                            st.session_state.debug_messages.append(f"[SD Search Term Performance] Found campaign in alternate column: {alt_col}")
                            break
            
            if campaign_value and pd.notna(campaign_value) and str(campaign_value).strip() != '':
                sd_entry['Campaign'] = str(campaign_value).strip()
            else:
                # Log debug info if we're falling back to 'Unknown Campaign'
                st.session_state.debug_messages.append(
                    f"[SD Search Term Performance] Using 'Unknown Campaign' for targeting expression: {target_expr[:50]}..."
                )
                sd_entry['Campaign'] = 'Unknown Campaign'
                
            # Use targeting expression as the search term
            sd_entry['Search Term'] = target_expr
            sd_entry['Target'] = target_expr
            sd_entry['Campaign Type'] = 'Sponsored Display'
            
            # Extract match type if available
            match_type_col = next((c for c in df.columns if c.lower() == 'match type'), None)
            if match_type_col and pd.notna(row.get(match_type_col)):
                sd_entry['Match Type'] = row.get(match_type_col)
            else:
                # Determine match type based on targeting expression
                if 'exact-product' in target_expr.lower():
                    sd_entry['Match Type'] = 'Remarketing - Branded'
                elif 'views' in target_expr.lower() or 'purchases' in target_expr.lower():
                    sd_entry['Match Type'] = 'Remarketing'
                elif 'category=' in target_expr.lower():
                    sd_entry['Match Type'] = 'Category Target'
                else:
                    sd_entry['Match Type'] = 'Product Target'
            
            # Determine Target Type based on the specified logic
            targeting_type_col = next((c for c in df.columns if c.lower() == 'targeting type'), None)
            if targeting_type_col and pd.notna(row.get(targeting_type_col)) and str(row.get(targeting_type_col)).strip().lower() == 'auto':
                sd_entry['Target Type'] = 'Auto'
            elif 'retargeting' in str(sd_entry.get('Match Type', '')).lower():
                sd_entry['Target Type'] = 'Remarketing'
            elif 'remarketing' in str(sd_entry.get('Match Type', '')).lower():
                sd_entry['Target Type'] = 'Remarketing'
            elif 'category' in str(sd_entry.get('Match Type', '')).lower():
                sd_entry['Target Type'] = 'Category Targeting'
            else:
                sd_entry['Target Type'] = 'Product Targeting'
            
            # Extract performance metrics
            for metric in ['Impressions', 'Clicks', 'Spend', 'Orders']:
                metric_col = next((c for c in df.columns if c.lower() == metric.lower()), None)
                if metric_col and pd.notna(row.get(metric_col)):
                    try:
                        sd_entry[metric] = float(str(row.get(metric_col)).replace('$', '').replace(',', ''))
                    except:
                        sd_entry[metric] = 0
                else:
                    sd_entry[metric] = 0
            
            # Determine sales column based on attribution choice
            sales_col = None
            if 'sd_attribution_choice' in st.session_state and st.session_state.sd_attribution_choice == "Sales (Views & Clicks)":
                sales_patterns = ['Sales (Views & Clicks)', 'Total Sales (Views & Clicks)', 'Sales', 'Total Sales']
            else:
                sales_patterns = ['Sales', 'Total Sales', 'Sales (Views & Clicks)', 'Total Sales (Views & Clicks)']
                
            for pattern in sales_patterns:
                col = next((c for c in df.columns if c.lower() == pattern.lower()), None)
                if col and pd.notna(row.get(col)):
                    sales_col = col
                    break
                    
            if sales_col:
                try:
                    sd_entry['Ad Sales'] = float(str(row.get(sales_col)).replace('$', '').replace(',', ''))
                    sd_entry['Sales'] = sd_entry['Ad Sales']
                except:
                    sd_entry['Ad Sales'] = 0
                    sd_entry['Sales'] = 0
            else:
                sd_entry['Ad Sales'] = 0
                sd_entry['Sales'] = 0
                
            # Calculate derived metrics
            if sd_entry['Clicks'] > 0:
                sd_entry['CPC'] = sd_entry['Spend'] / sd_entry['Clicks']
            else:
                sd_entry['CPC'] = 0
                
            if sd_entry['Ad Sales'] > 0:
                sd_entry['ACoS'] = (sd_entry['Spend'] / sd_entry['Ad Sales']) * 100
                sd_entry['ROAS'] = sd_entry['Ad Sales'] / sd_entry['Spend'] if sd_entry['Spend'] > 0 else 0
            else:
                sd_entry['ACoS'] = 0
                sd_entry['ROAS'] = 0
                
            if sd_entry['Clicks'] > 0:
                sd_entry['CVR'] = (sd_entry['Orders'] / sd_entry['Clicks']) * 100
            else:
                sd_entry['CVR'] = 0
                
            if sd_entry['Impressions'] > 0:
                sd_entry['CTR'] = (sd_entry['Clicks'] / sd_entry['Impressions']) * 100
            else:
                sd_entry['CTR'] = 0
                
            if sd_entry['Orders'] > 0:
                sd_entry['AOV'] = sd_entry['Ad Sales'] / sd_entry['Orders']
            else:
                sd_entry['AOV'] = 0
                
            # Get product group for this campaign (case-insensitive lookup)
            # Only populate if there are actually product groups defined in Campaign Tagging
            campaign_name = sd_entry.get('Campaign', '')
            product_group = 'Untagged Group'  # Default to 'Untagged Group'
            if campaign_product_groups and campaign_name and campaign_name.upper() in campaign_product_groups:
                product_group = campaign_product_groups[campaign_name.upper()] or 'Untagged Group'
            
            # Add product group to the entry
            sd_entry['Product Group'] = product_group
            
            # Add Sheet Source column for Ad Type filtering
            sd_entry['Sheet Source'] = sheet_name
            
            # Add to SD targeting data
            sd_targeting_data.append(sd_entry)
    
    if sd_targeting_data:
        st.session_state.debug_messages.append(f"Added {len(sd_targeting_data)} Sponsored Display targeting entries to search term data")
        # Convert to DataFrame and add to search term data
        sd_df = pd.DataFrame(sd_targeting_data)
        search_term_data.append(sd_df)
    
    # Combine all search term data
    combined_df = pd.DataFrame()  # Initialize to empty DataFrame
    if search_term_data:
        combined_df = pd.concat(search_term_data, ignore_index=True)
        st.session_state.debug_messages.append(f"Combined search term data: {len(combined_df)} rows")
        # Remove any exact duplicate rows across all columns
        before_dedup = len(combined_df)
        combined_df = combined_df.drop_duplicates()
        after_dedup = len(combined_df)
        st.session_state.debug_messages.append(f"Dropped {before_dedup - after_dedup} exact duplicate search term rows (now {after_dedup} rows)")
        
        # Remove duplicate Sponsored Display targets
        if "Campaign Type" in combined_df.columns:
            sd_mask = combined_df["Campaign Type"] == "Sponsored Display"
            if sd_mask.any():
                original_sd_count = sd_mask.sum()
                sd_deduplicated = combined_df[sd_mask].drop_duplicates()
                combined_df = pd.concat([
                    combined_df[~sd_mask],
                    sd_deduplicated
                ], ignore_index=True)
                new_sd_count = len(sd_deduplicated)
                st.session_state.debug_messages.append(f"Deduplicated Sponsored Display targets (ALL columns must match): {original_sd_count} -> {new_sd_count} (removed {original_sd_count - new_sd_count} duplicates)")
        
        # Sort by Ad Sales by default (descending)
        combined_df['Ad_Sales_Numeric'] = combined_df['Ad Sales'].replace('[$,]', '', regex=True).astype(float)
        combined_df = combined_df.sort_values(by='Ad_Sales_Numeric', ascending=False)
        combined_df = combined_df.drop(columns=['Ad_Sales_Numeric'])
        
        # Classify search terms as Branded or Non-Branded
        combined_df['Is_Branded'] = False  # Default to Non-Branded
        
        # Get branded terms and ASINs from client config if available
        branded_terms = []
        branded_asins = set()
        
        if client_config is not None:
            # Extract branded terms (case-insensitive) - check both key formats
            if 'Branded Terms' in client_config:
                if isinstance(client_config['Branded Terms'], list):
                    branded_terms = [str(term).strip().lower() for term in client_config['Branded Terms'] if str(term).strip()]
                elif isinstance(client_config['Branded Terms'], str):
                    branded_terms = [str(term).strip().lower() for term in client_config['Branded Terms'].split(',') if str(term).strip()]
            # Fallback to legacy key name if the new format isn't found
            elif 'branded_keywords' in client_config:
                branded_terms = [str(term).strip().lower() for term in client_config.get('branded_keywords', []) 
                                if str(term).strip()]
            
            # Extract branded ASINs (uppercase for consistency) - use correct data source
            branded_asins = set()
            
            # Primary source: branded_asins_data (current format)
            if 'branded_asins_data' in client_config:
                branded_asins_data = client_config.get('branded_asins_data', {})
                branded_asins = {str(asin).strip().upper() for asin in branded_asins_data.keys() if str(asin).strip()}
            # Fallback to old format for backward compatibility
            elif 'Branded ASINs' in client_config:
                if isinstance(client_config['Branded ASINs'], list):
                    branded_asins = {str(asin).strip().upper() for asin in client_config['Branded ASINs'] if str(asin).strip()}
                elif isinstance(client_config['Branded ASINs'], str):
                    branded_asins = {str(asin).strip().upper() for asin in client_config['Branded ASINs'].split(',') if str(asin).strip()}
            
            st.session_state.debug_messages.append(f"[Search Term Classification] Found {len(branded_terms)} branded terms and {len(branded_asins)} branded ASINs")
        
        # Compile regex for finding ASINs (starting with B0 and 10 characters long)
        asin_pattern = re.compile(r'(B[0-9A-Z]{9})', re.IGNORECASE)
        
        # Classify each search term (enhanced to check both search term and target)
        for idx, row in combined_df.iterrows():
            search_term = str(row['Search Term']).strip() if pd.notna(row['Search Term']) else ''
            target = str(row.get('Target', '')).strip() if pd.notna(row.get('Target')) else ''
            
            # Skip empty search terms
            if not search_term:
                continue
                
            is_branded = False
            
            # Check if the search term contains an ASIN
            asin_matches = asin_pattern.findall(search_term.upper())
            
            if asin_matches:
                # Check if any of the ASINs found are branded
                for asin in asin_matches:
                    if asin.upper() in branded_asins:
                        is_branded = True
                        st.session_state.debug_messages.append(f"[Search Term Classification] Term '{search_term}' classified as Branded (contains branded ASIN {asin})")
                        break
            
            # Also check if the target contains branded ASINs
            if not is_branded and target:
                target_asin_matches = asin_pattern.findall(target.upper())
                for asin in target_asin_matches:
                    if asin.upper() in branded_asins:
                        is_branded = True
                        st.session_state.debug_messages.append(f"[Search Term Classification] Term '{search_term}' classified as Branded (target '{target}' contains branded ASIN {asin})")
                        break
            
            # Check if the search term contains any branded terms
            if not is_branded:
                search_term_lower = search_term.lower()
                for term in branded_terms:
                    if term.lower() in search_term_lower:
                        is_branded = True
                        st.session_state.debug_messages.append(f"[Search Term Classification] Term '{search_term}' classified as Branded (contains branded term '{term}')")
                        break
            
            # Set the classification
            if is_branded:
                combined_df.at[idx, 'Is_Branded'] = True
        
        # --- COMPANION EXPORTS SPECIAL LOGIC FOR SEARCH TERMS ---
        # For Companion Exports, apply additional classification rules
        if st.session_state.get('is_companion_data', False):
            st.session_state.debug_messages.append("[Search Term Classification] Applying Companion Export special rules")
            companion_reclassified_count = 0
            
            for idx, row in combined_df.iterrows():
                search_term = str(row['Search Term']).strip() if pd.notna(row['Search Term']) else ''
                match_type = str(row.get('Match Type', '')).strip() if pd.notna(row.get('Match Type')) else ''
                
                # Skip empty search terms
                if not search_term:
                    continue
                
                # Rule 1: "Remarketing - Branded" Match Type should be considered Branded
                if match_type == 'Remarketing - Branded':
                    if not combined_df.at[idx, 'Is_Branded']:  # Only reclassify if not already branded
                        combined_df.at[idx, 'Is_Branded'] = True
                        companion_reclassified_count += 1
                        st.session_state.debug_messages.append(f"[Companion Export Search Term] Classified '{search_term}' as BRANDED due to Match Type 'Remarketing - Branded'")
                
                # Rule 2: Check Search Term for ASINs against Branded ASINs
                if not combined_df.at[idx, 'Is_Branded']:  # Only check if not already branded
                    asin_pattern_companion = re.compile(r'(B[0-9A-Z]{9})', re.IGNORECASE)
                    search_term_asins = set(asin_pattern_companion.findall(search_term.upper()))
                    
                    if search_term_asins:
                        # Check if any of the ASINs found are branded
                        branded_asins_found = search_term_asins.intersection(branded_asins)
                        if branded_asins_found:
                            combined_df.at[idx, 'Is_Branded'] = True
                            companion_reclassified_count += 1
                            st.session_state.debug_messages.append(f"[Companion Export Search Term] Classified '{search_term}' as BRANDED due to branded ASIN(s): {branded_asins_found}")
            
            if companion_reclassified_count > 0:
                st.session_state.debug_messages.append(f"[Companion Export Search Term] Reclassified {companion_reclassified_count} search terms as Branded")
        
        # Log classification results
        branded_count = combined_df['Is_Branded'].sum()
        non_branded_count = len(combined_df) - branded_count
        st.session_state.debug_messages.append(f"[Search Term Classification] Results: {branded_count} Branded terms, {non_branded_count} Non-Branded terms")
        
        # Update Target Type for Sponsored Display remarketing campaigns based on Is_Branded classification
        sd_remarketing_updates = 0
        if 'Campaign Type' in combined_df.columns and 'Target Type' in combined_df.columns:
            sd_remarketing_mask = (combined_df['Campaign Type'] == 'Sponsored Display') & (combined_df['Target Type'] == 'Remarketing')
            for idx in combined_df[sd_remarketing_mask].index:
                is_branded = combined_df.at[idx, 'Is_Branded']
                if is_branded:
                    combined_df.at[idx, 'Target Type'] = 'Remarketing - Branded'
                    combined_df.at[idx, 'Match Type'] = 'Remarketing - Branded'
                else:
                    combined_df.at[idx, 'Target Type'] = 'Remarketing - Competitor'
                    combined_df.at[idx, 'Match Type'] = 'Remarketing - Competitor'
                sd_remarketing_updates += 1
        
        if sd_remarketing_updates > 0:
            st.session_state.debug_messages.append(f"[Sponsored Display Remarketing] Updated {sd_remarketing_updates} remarketing campaigns with branded/competitor classification")
        
        # Match with targeting data to populate Target and Match Type
        if targeting_data is not None:
            st.session_state.debug_messages.append(f"Attempting to match search terms with targeting data")
            
            # Create a mapping from Campaign + Target to Match Type
            campaign_target_to_match_type = {}
            campaign_to_targets = {}
            
            for _, row in targeting_data.iterrows():
                campaign = str(row['Campaign']).strip() if pd.notna(row['Campaign']) else ''
                target = str(row['Target']).strip() if pd.notna(row['Target']) else ''
                match_type = str(row['Match Type']).strip() if pd.notna(row['Match Type']) else ''
                target_type = str(row['Target Type']).strip() if pd.notna(row['Target Type']) else ''
                
                # Store the match type for this campaign + target combination
                campaign_target_to_match_type[(campaign, target)] = (match_type, target_type)
                
                # Also store all targets for each campaign for fuzzy matching
                if campaign not in campaign_to_targets:
                    campaign_to_targets[campaign] = []
                campaign_to_targets[campaign].append((target, match_type, target_type))
            
            # Function to find the best target match for a search term within a campaign
            def find_best_target_match(campaign, search_term):
                if campaign in campaign_to_targets:
                    targets = campaign_to_targets[campaign]
                    
                    # First try exact match
                    for target, match_type, target_type in targets:
                        if target.lower() == search_term.lower():
                            return target, match_type, target_type
                    
                    # Then try contains match for broad match keywords
                    for target, match_type, target_type in targets:
                        if target_type.lower() == 'keyword' and match_type.lower() == 'broad' and search_term.lower() in target.lower():
                            return target, match_type, target_type
                    
                    # Then try contains match for phrase match keywords
                    for target, match_type, target_type in targets:
                        if target_type.lower() == 'keyword' and match_type.lower() == 'phrase' and search_term.lower() in target.lower():
                            return target, match_type, target_type
                    
                    # If no match found, return the first target for this campaign
                    if targets:
                        return targets[0]
                
                return None, None, None
            
            # Apply the matching to populate Target and Match Type
            matched_count = 0
            for idx, row in combined_df.iterrows():
                campaign = str(row['Campaign']).strip() if pd.notna(row['Campaign']) else ''
                search_term = str(row['Search Term']).strip() if pd.notna(row['Search Term']) else ''
                current_target = str(row['Target']).strip() if pd.notna(row['Target']) else ''
                
                # If we already have a target, try to find its match type directly
                if current_target and campaign:
                    if (campaign, current_target) in campaign_target_to_match_type:
                        match_type, target_type = campaign_target_to_match_type[(campaign, current_target)]
                        combined_df.at[idx, 'Match Type'] = match_type
                        combined_df.at[idx, 'Target Type'] = target_type
                        matched_count += 1
                        continue
                
                # Otherwise try to find the best target match for this search term
                target, match_type, target_type = find_best_target_match(campaign, search_term)
                
                if target is not None:
                    if not current_target or current_target == 'None':
                        combined_df.at[idx, 'Target'] = target
                    combined_df.at[idx, 'Match Type'] = match_type
                    combined_df.at[idx, 'Target Type'] = target_type
                    matched_count += 1
            
            st.session_state.debug_messages.append(f"Matched {matched_count} search terms with targeting data")
        
        # Apply universal auto targeting cleanup to all search term data
        st.session_state.debug_messages.append("Applying universal auto targeting cleanup to search term data")
        cleanup_count = 0
        
        for idx, row in combined_df.iterrows():
            target = str(row.get('Target', '')).strip() if pd.notna(row.get('Target')) else ''
            campaign = str(row.get('Campaign', '')).strip() if pd.notna(row.get('Campaign')) else ''
            
            if target:
                original_target = target
                cleaned_target = target
                
                # Detect campaign type from campaign name
                campaign_type = 'SP'  # Default
                if 'SB' in campaign.upper() or 'SPONSORED BRAND' in campaign.upper():
                    campaign_type = 'SB'
                elif 'SD' in campaign.upper() or 'SPONSORED DISPLAY' in campaign.upper():
                    campaign_type = 'SD'
                
                # Handle JSON format targeting expressions first
                if target.startswith('[{') and target.endswith('}]'):
                    try:
                        import json
                        parsed = json.loads(target)
                        if isinstance(parsed, list) and len(parsed) > 0 and isinstance(parsed[0], dict):
                            target_type = parsed[0].get('type', '')
                            target_value = parsed[0].get('value', '')
                            
                            # Auto targeting cleanup
                            if target_type in ['queryBroadRelMatches', 'queryHighRelMatches', 'asinSubstituteRelated', 'asinAccessoryRelated']:
                                if target_type == 'queryBroadRelMatches':
                                    cleaned_target = 'Auto - Loose Match'
                                elif target_type == 'queryHighRelMatches':
                                    cleaned_target = 'Auto - Close Match'
                                elif target_type == 'asinSubstituteRelated':
                                    cleaned_target = 'Auto - Substitute'
                                elif target_type == 'asinAccessoryRelated':
                                    cleaned_target = 'Auto - Compliments'
                                
                                # Update Match Type and Target Type for auto targets
                                combined_df.at[idx, 'Match Type'] = 'Auto'
                                combined_df.at[idx, 'Target Type'] = 'Auto'
                            
                            # ASIN targeting cleanup
                            elif target_type == 'asinSameAs' and target_value:
                                if 'expanded' in target.lower():
                                    cleaned_target = f'ASIN-Expanded = {target_value}'
                                else:
                                    cleaned_target = f'ASIN = {target_value}'
                            
                            # Category targeting cleanup
                            elif target_type == 'asinCategorySameAs' and target_value:
                                cleaned_target = f'{campaign_type} Category Targeting'
                            
                            # Other JSON types - extract value if available
                            elif target_value:
                                cleaned_target = target_value
                                
                    except Exception as e:
                        # If JSON parsing fails, continue with original logic
                        st.session_state.debug_messages.append(f"[Search Term Performance] JSON parsing failed for target: {target[:50]}... Error: {e}")
                        pass
                
                # Handle non-JSON format targeting expressions
                else:
                    # Auto targeting cleanup for simple strings
                    if target in ['queryBroadRelMatches', 'queryHighRelMatches', 'asinSubstituteRelated', 'asinAccessoryRelated']:
                        if target == 'queryBroadRelMatches':
                            cleaned_target = 'Auto - Loose Match'
                        elif target == 'queryHighRelMatches':
                            cleaned_target = 'Auto - Close Match'
                        elif target == 'asinSubstituteRelated':
                            cleaned_target = 'Auto - Substitute'
                        elif target == 'asinAccessoryRelated':
                            cleaned_target = 'Auto - Compliments'
                        
                        # Update Match Type and Target Type for auto targets
                        combined_df.at[idx, 'Match Type'] = 'Auto'
                        combined_df.at[idx, 'Target Type'] = 'Auto'
                    
                    # ASIN detection and formatting for non-JSON format
                    elif 'asinSameAs' in target or 'asinsameas' in target.lower():
                        asin_pattern = re.compile(r'B0[A-Z0-9]{8}', re.IGNORECASE)  # Case-insensitive ASIN pattern
                        search_term = str(row.get('Search Term', '')).strip() if pd.notna(row.get('Search Term')) else ''
                        
                        # Try to find ASIN in search term first, then in target
                        asin_match = asin_pattern.search(search_term) or asin_pattern.search(target)
                        
                        if asin_match:
                            asin = asin_match.group().upper()  # Convert to uppercase for consistency
                            if 'expanded' in target.lower():
                                cleaned_target = f'ASIN-Expanded = {asin}'
                            else:
                                cleaned_target = f'ASIN = {asin}'
                        else:
                            # Enhanced fallback - try to extract any ASIN-like pattern from search term
                            # Look for patterns like b0xxxxxxxx (10 chars starting with b0)
                            fallback_pattern = re.compile(r'b0[a-z0-9]{8}', re.IGNORECASE)
                            fallback_match = fallback_pattern.search(search_term)
                            
                            if fallback_match:
                                asin = fallback_match.group().upper()
                                if 'expanded' in target.lower():
                                    cleaned_target = f'ASIN-Expanded = {asin}'
                                else:
                                    cleaned_target = f'ASIN = {asin}'
                            else:
                                # Final fallback if no ASIN found
                                if 'expanded' in target.lower():
                                    cleaned_target = 'ASIN-Expanded Target'
                                else:
                                    cleaned_target = 'ASIN Target'
                    
                    # Category targeting (campaign-specific)
                    elif 'category=' in target.lower() or 'categorysameas' in target.lower():
                        cleaned_target = f'{campaign_type} Category Targeting'
                
                # SD-specific remarketing logic (applies to both JSON and non-JSON)
                if campaign_type == 'SD':
                    if 'views' in target.lower() or 'purchases' in target.lower():
                        # Extract lookback days if present
                        days_match = re.search(r'(\d+)', target)
                        if days_match:
                            days = days_match.group(1)
                            if 'views' in target.lower():
                                cleaned_target = f'Views Remarketing - {days}d'
                            elif 'purchases' in target.lower():
                                cleaned_target = f'Purchases Remarketing - {days}d'
                        else:
                            if 'views' in target.lower():
                                cleaned_target = 'Views Remarketing'
                            elif 'purchases' in target.lower():
                                cleaned_target = 'Purchases Remarketing'
                
                # Update the target if it was cleaned
                if cleaned_target != original_target:
                    combined_df.at[idx, 'Target'] = cleaned_target
                    cleanup_count += 1
        
        if cleanup_count > 0:
            st.session_state.debug_messages.append(f"Cleaned up {cleanup_count} auto targeting expressions in search term data")
            
    # Cache the analysis result
    if client_config:
        db_manager.cache_analysis_result(client_name, 'search_term_data', analysis_input, combined_df)
    
    return combined_df
        
@st.cache_data(ttl=3600, show_spinner="Generating word cloud...")  # Cache for 1 hour
def generate_search_term_wordcloud(search_terms_df, filter_type='all', remove_asins=False):
    """
    Generates a word cloud visualization from search term data.
    
    Args:
        search_terms_df: DataFrame containing search term data with 'Search Term' and 'Is_Branded' columns
        filter_type: 'all', 'branded', or 'non-branded' to filter the search terms
        remove_asins: If True, removes terms that appear to be ASINs (start with B0 and are 10 chars long)
        
    Returns:
        matplotlib figure with the word cloud visualization
    """
    # Lazy import heavy libs
    plt = get_plt()
    WordCloud, STOPWORDS = get_wordcloud()

    if search_terms_df is None or search_terms_df.empty or 'Search Term' not in search_terms_df.columns:
        # Create an empty figure with a message
        fig, ax = plt.subplots(figsize=(10, 6))
        ax.text(0.5, 0.5, "No search term data available", 
                horizontalalignment='center', verticalalignment='center', fontsize=14)
        ax.axis('off')
        return fig
    
    # Filter data based on type
    if filter_type == 'branded' and 'Is_Branded' in search_terms_df.columns:
        filtered_df = search_terms_df[search_terms_df['Is_Branded'] == True].copy()
        title = 'Branded Search Terms'
    elif filter_type == 'non-branded' and 'Is_Branded' in search_terms_df.columns:
        filtered_df = search_terms_df[search_terms_df['Is_Branded'] == False].copy()
        title = 'Non-Branded Search Terms'
    else:
        filtered_df = search_terms_df.copy()
        title = 'All Search Terms'
    
    if filtered_df.empty:
        # Create an empty figure with a message
        fig, ax = plt.subplots(figsize=(10, 6))
        ax.text(0.5, 0.5, f"No {filter_type.replace('-', ' ')} search terms found", 
                horizontalalignment='center', verticalalignment='center', fontsize=14)
        ax.axis('off')
        return fig
    
    # Prepare the text data
    if 'Spend' in filtered_df.columns and 'Clicks' in filtered_df.columns:
        # Weight by spend or clicks to emphasize important terms
        # Use spend by default, fall back to clicks if any terms have zero spend
        if filtered_df['Spend'].sum() > 0:
            weighted_terms = {}
            for _, row in filtered_df.iterrows():
                if pd.notna(row['Search Term']) and row['Search Term'] != '':
                    term = str(row['Search Term']).lower()
                    # Skip ASINs if remove_asins is True
                    if remove_asins and len(term) == 10 and term.startswith('b0'):
                        continue
                    weight = float(row['Spend']) if pd.notna(row['Spend']) else 0
                    weighted_terms[term] = weighted_terms.get(term, 0) + weight
        else:
            weighted_terms = {}
            for _, row in filtered_df.iterrows():
                if pd.notna(row['Search Term']) and row['Search Term'] != '':
                    term = str(row['Search Term']).lower()
                    # Skip ASINs if remove_asins is True
                    if remove_asins and len(term) == 10 and term.startswith('b0'):
                        continue
                    weight = float(row['Clicks']) if pd.notna(row['Clicks']) else 0
                    weighted_terms[term] = weighted_terms.get(term, 0) + weight
    else:
        # If no weight columns, just count frequency
        weighted_terms = {}
        for _, row in filtered_df.iterrows():
            if pd.notna(row['Search Term']) and row['Search Term'] != '':
                term = str(row['Search Term']).lower()
                # Skip ASINs if remove_asins is True
                if remove_asins and len(term) == 10 and term.startswith('b0'):
                    continue
                weighted_terms[term] = weighted_terms.get(term, 0) + 1
    
    if not weighted_terms:
        # Create an empty figure with a message
        fig, ax = plt.subplots(figsize=(10, 6))
        ax.text(0.5, 0.5, f"No valid {filter_type.replace('-', ' ')} search terms found", 
                horizontalalignment='center', verticalalignment='center', fontsize=14)
        ax.axis('off')
        return fig
    
    # Create a set of stopwords (common words to exclude)
    stopwords = set(STOPWORDS)
    stopwords.update(['amazon', 'com', 'www'])  # Add custom stopwords if needed
    
    # Generate the word cloud with bright colors (similar to target word cloud)
    wordcloud = WordCloud(
        width=800, 
        height=400,
        background_color='#181818',  # Dark background to match the target word cloud
        colormap='summer',  # Bright yellow/green colors, similar to target word cloud
        stopwords=stopwords,
        max_words=100,
        collocations=False,  # Avoid repeated word pairs
        contour_width=2,
        contour_color='#bfa23a'  # Gold border like the target word cloud
    ).generate_from_frequencies(weighted_terms)
    
    # Create a figure and display the word cloud
    fig, ax = plt.subplots(figsize=(10, 6))
    ax.imshow(wordcloud, interpolation='bilinear')
    ax.set_title(title, fontsize=16, color='white')
    ax.axis('off')
    fig.tight_layout(pad=0)
    
    # Set figure background to match the target word cloud
    fig.patch.set_facecolor('#181818')
    
    return fig

@st.cache_data(ttl=3600, show_spinner="Processing campaign performance data...", hash_funcs={"_thread.RLock": lambda _: None})
def get_campaign_performance_data(bulk_data, client_config=None):
    """
    Extracts and processes campaign-level performance data from bulk advertising files.
    Only processes rows where Entity="Campaign" (case-insensitive) and excludes Portfolio and RAS sheets.
    
    Args:
        bulk_data: Dictionary containing bulk file data
        client_config: Configuration data including campaign tags
        
    Returns:
        DataFrame with campaign performance metrics
    """
    if not bulk_data or not isinstance(bulk_data, dict):
        return pd.DataFrame()

    # Check for cached analysis result
    if client_config:
        client_name = client_config.get('client_name', 'unknown')
        analysis_input = {
            'bulk_data_hash': hash(str(bulk_data)),
            'client_config_hash': hash(str(client_config))
        }
        
        cached_result = db_manager.get_cached_analysis_result(client_name, 'campaign_performance_data', analysis_input)
        if cached_result is not None:
            st.session_state.debug_messages.append("Retrieved cached campaign performance data analysis")
            return cached_result
    
    st.session_state.debug_messages.append(f"Starting Campaign Performance data extraction")
    
    campaign_data = []
    
    # Helper function to safely convert values to float
    def safe_convert_to_float(val):
        if pd.isna(val):
            return 0.0
        try:
            return float(str(val).replace('$', '').replace(',', '').replace('%', ''))
        except:
            return 0.0
    
    # Helper function to find column case-insensitively
    def find_column(df, column_names):
        df_columns_lower = [col.lower() for col in df.columns]
        for name in column_names:
            for i, col_lower in enumerate(df_columns_lower):
                if col_lower == name.lower():
                    return df.columns[i]
        return None
    
    # Process each sheet in bulk data
    for sheet_name, df in bulk_data.items():
        if not isinstance(df, pd.DataFrame) or df.empty:
            continue
        
        # Skip Portfolio and RAS sheets as requested
        if any(exclude_sheet.lower() in sheet_name.lower() for exclude_sheet in ['Portfolio', 'RAS']):
            st.session_state.debug_messages.append(f"Skipping sheet {sheet_name} - Portfolio or RAS sheet")
            continue
        
        
        # Skip original SB Multi Ad Group Campaigns sheet to prevent double counting
        # (it's already been processed and stored as 'Sponsored Brands Campaigns')
        if sheet_name.lower() == 'sb multi ad group campaigns':
            st.session_state.debug_messages.append(f"Skipping sheet {sheet_name} - data already processed as 'Sponsored Brands Campaigns' to prevent double counting")
            continue

        # For Companion data, skip ASIN and Search Term reports to avoid double counting
        # Only process Campaign sheets for campaign performance
        if st.session_state.get('is_companion_data', False):
            if ('ASIN' in sheet_name and 'Campaign' not in sheet_name) or                ('Search Term' in sheet_name and 'Campaign' not in sheet_name):
                st.session_state.debug_messages.append(f"Skipping sheet {sheet_name} - Companion ASIN/Search Term report (avoiding double count)")
                continue
        
        # Handle different data types: Campaign vs Companion data
        is_companion = st.session_state.get('is_companion_data', False)
        
        if is_companion:
            # For companion data, use all rows as they're already at the campaign level
            campaign_rows = df
            st.session_state.debug_messages.append(f"Sheet {sheet_name}: Using companion data - {len(campaign_rows)} rows (no Entity filtering needed)")
        else:
            # Traditional bulk data processing with Entity filtering
            entity_col = find_column(df, ['Entity'])
            if not entity_col:
                st.session_state.debug_messages.append(f"Skipping sheet {sheet_name} - no Entity column found")
                continue
            
            # Filter for rows where Entity equals "Campaign" (case-insensitive)
            entity_mask = df[entity_col].fillna('').astype(str).str.strip().str.lower() == 'campaign'
            campaign_rows = df[entity_mask]
            
            if campaign_rows.empty:
                st.session_state.debug_messages.append(f"Skipping sheet {sheet_name} - no Campaign entities found")
                continue
            
            st.session_state.debug_messages.append(f"Sheet {sheet_name}: Found {len(campaign_rows)} Campaign entity rows out of {len(df)} total rows")
        
        # Find required columns - now using campaign_rows instead of df
        campaign_col = find_column(campaign_rows, ['Campaign Name (Informational Only)', 'Campaign Name', 'Campaign'])
        product_col = find_column(campaign_rows, ['Product'])
        spend_col = find_column(campaign_rows, ['Spend'])
        clicks_col = find_column(campaign_rows, ['Clicks'])
        orders_col = find_column(campaign_rows, ['Orders'])
        campaign_state_col = find_column(campaign_rows, ['Campaign State (Informational Only)', 'Campaign State'])
        
        # Determine sales column based on attribution choice and product type
        sales_col = None
        if 'sd_attribution_choice' in st.session_state and st.session_state.sd_attribution_choice == "Sales (Views & Clicks)":
            # Check if this is a Sponsored Display sheet - different logic for companion vs bulk data
            is_sponsored_display = False
            
            if is_companion:
                # For companion data, check the 'kind' column
                kind_col = find_column(campaign_rows, ['kind', 'Campaign Type Raw'])
                if kind_col and not campaign_rows[kind_col].empty:
                    kind_values = campaign_rows[kind_col].dropna().unique()
                    for kind in kind_values:
                        if isinstance(kind, str) and kind.lower().strip() == 'sd':
                            is_sponsored_display = True
                            break
            else:
                # For bulk data, check the Product column
                if product_col and not campaign_rows[product_col].empty:
                    product_values = campaign_rows[product_col].dropna().unique()
                    for product in product_values:
                        if isinstance(product, str) and 'sponsored display' in product.lower():
                            is_sponsored_display = True
                            break
            
            if is_sponsored_display:
                sales_col = find_column(campaign_rows, ['Sales (Views & Clicks)', 'Sales'])
            else:
                sales_col = find_column(campaign_rows, ['Sales'])
        else:
            sales_col = find_column(campaign_rows, ['Sales'])
        
        if not campaign_col or not sales_col or not spend_col:
            st.session_state.debug_messages.append(f"Skipping sheet {sheet_name} - missing required columns (Campaign: {campaign_col}, Sales: {sales_col}, Spend: {spend_col})")
            continue
        
        # Determine ad type - different logic for companion vs bulk data
        ad_type = 'Unknown'
        
        if is_companion:
            # For companion data, use the 'kind' column
            kind_col = find_column(campaign_rows, ['kind', 'Campaign Type Raw'])
            if kind_col and not campaign_rows[kind_col].empty:
                kind_values = campaign_rows[kind_col].dropna().unique()
                for kind in kind_values:
                    if isinstance(kind, str):
                        kind_lower = kind.lower().strip()
                        if kind_lower == 'sp':
                            ad_type = 'SP'
                            break
                        elif kind_lower == 'sb':
                            ad_type = 'SB'
                            break
                        elif kind_lower == 'sd':
                            ad_type = 'SD'
                            break
        else:
            # For bulk data, use the Product column
            if product_col and not campaign_rows[product_col].empty:
                product_values = campaign_rows[product_col].dropna().unique()
                for product in product_values:
                    if isinstance(product, str):
                        if 'sponsored products' in product.lower():
                            ad_type = 'SP'
                            break
                        elif 'sponsored brands' in product.lower():
                            ad_type = 'SB'
                            break
                        elif 'sponsored display' in product.lower():
                            ad_type = 'SD'
                            break
        
        st.session_state.debug_messages.append(f"Processing sheet {sheet_name} - Ad Type: {ad_type}, Campaign rows: {len(campaign_rows)}")
        
        # Process each campaign row in the sheet
        for _, row in campaign_rows.iterrows():
            campaign_name = str(row[campaign_col]).strip() if pd.notna(row[campaign_col]) else ''
            if not campaign_name:
                continue
            
            # Extract metrics
            spend = safe_convert_to_float(row[spend_col]) if spend_col else 0
            ad_sales = safe_convert_to_float(row[sales_col]) if sales_col else 0
            clicks = safe_convert_to_float(row[clicks_col]) if clicks_col else 0
            orders = safe_convert_to_float(row[orders_col]) if orders_col else 0
            
            # Calculate derived metrics
            acos = (spend / ad_sales * 100) if ad_sales > 0 else 0
            roas = (ad_sales / spend) if spend > 0 else 0
            cpc = (spend / clicks) if clicks > 0 else 0
            cvr = (orders / clicks * 100) if clicks > 0 else 0
            aov = (ad_sales / orders) if orders > 0 else 0
            
            # Get product group from campaign tagging
            product_group = ''
            if client_config and 'campaign_tags_data' in client_config:
                campaign_info = client_config['campaign_tags_data'].get(campaign_name, {})
                product_group = campaign_info.get('tag_1', '') or 'Untagged Group'
            if not product_group:
                product_group = 'Untagged Group'
            
            campaign_data.append({
                'Ad Type': ad_type,
                'Product Group': product_group,
                'Campaign': campaign_name,
                'Spend': spend,
                'Ad Sales': ad_sales,
                'ACoS': acos,
                'ROAS': roas,
                'CPC': cpc,
                'CVR': cvr,
                'AOV': aov,
                'Clicks': clicks,
                'Orders': orders
            })
    
    if not campaign_data:
        st.session_state.debug_messages.append("No campaign performance data found")
        return pd.DataFrame()
    
    # Create DataFrame and aggregate by campaign (in case there are multiple rows per campaign across different sheets)
    df = pd.DataFrame(campaign_data)
    
    # Group by Campaign, Ad Type, and Product Group, summing metrics
    groupby_cols = ['Campaign', 'Ad Type', 'Product Group']
    agg_dict = {
        'Spend': 'sum',
        'Ad Sales': 'sum',
        'Clicks': 'sum',
        'Orders': 'sum'
    }
    
    grouped_df = df.groupby(groupby_cols, as_index=False).agg(agg_dict)
    
    # Recalculate derived metrics after aggregation
    grouped_df['ACoS'] = grouped_df.apply(
        lambda row: (row['Spend'] / row['Ad Sales'] * 100) if row['Ad Sales'] > 0 else 0, axis=1
    )
    grouped_df['ROAS'] = grouped_df.apply(
        lambda row: (row['Ad Sales'] / row['Spend']) if row['Spend'] > 0 else 0, axis=1
    )
    grouped_df['CPC'] = grouped_df.apply(
        lambda row: (row['Spend'] / row['Clicks']) if row['Clicks'] > 0 else 0, axis=1
    )
    grouped_df['CVR'] = grouped_df.apply(
        lambda row: (row['Orders'] / row['Clicks'] * 100) if row['Clicks'] > 0 else 0, axis=1
    )
    grouped_df['AOV'] = grouped_df.apply(
        lambda row: (row['Ad Sales'] / row['Orders']) if row['Orders'] > 0 else 0, axis=1
        )
    # Calculate percentage columns
    total_spend = grouped_df['Spend'].sum()
    total_ad_sales = grouped_df['Ad Sales'].sum()
    
    grouped_df['% Ad Spend'] = grouped_df.apply(
        lambda row: (row['Spend'] / total_spend * 100) if total_spend > 0 else 0, axis=1
    )
    grouped_df['% Ad Sales'] = grouped_df.apply(
        lambda row: (row['Ad Sales'] / total_ad_sales * 100) if total_ad_sales > 0 else 0, axis=1
    )
    
    st.session_state.debug_messages.append(f"Campaign Performance: Generated {len(grouped_df)} campaign records")
    
    # Cache the analysis result
    if client_config:
        db_manager.cache_analysis_result(client_name, 'campaign_performance_data', analysis_input, grouped_df)
    
    return grouped_df 

# --- UI Functions ---
# --- Main App Logic ---

# Page config has been moved to the top of the file

# Load and display the main logo

# Load and display the main logo
def display_logo():
    logo_path = "assets/hand_logo.png"
    if os.path.exists(logo_path):
        logo = Image.open(logo_path)
        return logo
    else:
        st.error(f"Logo file not found at {logo_path}")
        return None

# Custom CSS to implement a dark charcoal gray theme that's compatible with charts
custom_css = r'''
<style>
/* Import professional fonts */
@import url('https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap');

/* Base theme elements */
.stApp {
background-color: #1E1E1E;
font-family: 'Inter', -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Open Sans', 'Helvetica Neue', sans-serif;
}

/* Custom styled message boxes */
.success-box {
    background-color: rgba(38, 166, 91, 0.15);
    border-left: 4px solid #26a65b;
    padding: 10px 15px;
    border-radius: 4px;
    margin: 10px 0;
    color: #e0e0e0;
}

.info-box {
    background-color: rgba(52, 152, 219, 0.15);
    border-left: 4px solid #3498db;
    padding: 10px 15px;
    border-radius: 4px;
    margin: 10px 0;
    color: #e0e0e0;
}

.warning-box {
    background-color: rgba(241, 196, 15, 0.15);
    border-left: 4px solid #f1c40f;
    padding: 10px 15px;
    border-radius: 4px;
    margin: 10px 0;
    color: #e0e0e0;
}

/* Typography for headings and text */
h1, h2, h3, h4, h5, h6 {
font-family: 'Inter', -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif !important;
font-weight: 600 !important;
letter-spacing: -0.01em !important;
color: #FFFFFF !important;
}

/* HERO TITLE (main dashboard title) */
.dashboard-hero-title {
font-weight: 800 !important;
color: #F0F0F0 !important;
font-size: 2.8rem !important;
margin-top: 1.5rem !important;
margin-bottom: 2.5rem !important;
letter-spacing: -0.02em !important;
text-shadow: 0 2px 8px rgba(0,0,0,0.15);
display: inline-block;
vertical-align: middle;
}
.dashboard-hero-title .client-name {
    font-weight: 900 !important;
    color: #2563eb !important;
    font-size: 3.2rem !important;
    letter-spacing: -0.03em !important;
    text-shadow: 0 2px 12px #10204055, 0 0px 2px #2563eb55;
}
.dashboard-hero-title {
    font-weight: 800 !important;
    color: #fff !important;
    font-size: 2.7rem !important;
    letter-spacing: -0.02em !important;
    text-shadow: 0 2px 12px #08080855, 0 0px 2px #2563eb33;
    display: inline-block;
    vertical-align: middle;
}
/* Header container for title and logo */
.header-container {
    display: flex;
    align-items: center;
    justify-content: space-between;
    margin-bottom: 1.0rem !important;
    animation: fadeInHeader 0.7s cubic-bezier(.4,2,.6,1);
}
@keyframes fadeInHeader {
    0% { opacity: 0; transform: translateY(-30px); }
    100% { opacity: 1; transform: translateY(0); }
}
/* Logo styling in header */
.header-logo {
    max-height: 80px;
    float: right;
    filter: drop-shadow(0 2px 8px rgba(40,40,40,0.45));
    transition: transform 0.2s cubic-bezier(.4,2,.6,1);
}
.header-logo:hover {
    transform: scale(1.04) rotate(-2deg);
}
/* Thin blue divider below header */
.header-divider {
    width: 100%;
    height: 3px;
    background: linear-gradient(90deg, #2563eb 30%, #60a5fa 100%);
    border: none;
    margin: 0 0 1.2rem 0;
    opacity: 0.95;
    border-radius: 2px;
}
/* Section headers for main areas */
.main-section-header {
    background: linear-gradient(90deg, #f7d774 0%, #bfa23a 80%, #fffbe7 100%);
    -webkit-background-clip: text;
    -webkit-text-fill-color: transparent;
    background-clip: text;
    text-fill-color: transparent;
    font-size: 2.2rem !important;
    font-weight: 900 !important;
    letter-spacing: 0.04em !important;
    text-shadow: 0 2px 8px rgba(191,162,58,0.10), 0 1px 1px #00000033;

font-size: 2.1rem !important;
font-weight: 800 !important;
color: #BFA23A !important;
margin-top: 2.8rem !important;
margin-bottom: 1.4rem !important;
letter-spacing: -0.01em !important;
text-shadow: 0 2px 8px rgba(0,0,0,0.10);
display: block;
}
/* Increase spacing between dashboard sections */
.dashboard-section {
margin-top: 2.8rem !important;
margin-bottom: 2.0rem !important;
padding-top: 0.5rem !important;
padding-bottom: 0.5rem !important;
}
/* Add extra spacing after navigation and attribution */
.stButton, .attribution {
    margin-bottom: 1rem;
}

/* Navigation bar styling */
.audit-nav-container {
    position: fixed;
    top: 0;
    left: 0;
    right: 0;
    z-index: 9999;
    background-color: rgba(30, 30, 30, 0.95);
    padding: 12px 16px;
    margin-bottom: 20px;
    box-shadow: 0 4px 12px rgba(0, 0, 0, 0.5);
    display: flex;
    flex-wrap: wrap;
    gap: 12px;
    align-items: center;
    justify-content: flex-start;
    backdrop-filter: blur(10px);
    -webkit-backdrop-filter: blur(10px);
    border-bottom: 1px solid rgba(255, 215, 0, 0.2);
    width: 100%;
    max-width: 100%;
    overflow-x: auto;
    white-space: nowrap;
}

/* Add padding to the top of the main content to account for fixed navbar */
.main .block-container {
    padding-top: 70px !important;
}

/* Fix for Streamlit's sidebar and main content layout */
.stApp {
    overflow-x: hidden;
}

/* Ensure the nav bar stays on top of all Streamlit elements */
.stApp > header {
    z-index: 998;
}

/* Add some space above the first section header */
.dashboard-section:first-of-type {
    margin-top: 20px;
}

.audit-nav-link {
    color: #cccccc;
    text-decoration: none;
    padding: 8px 14px;
    border-radius: 6px;
    font-size: 15px;
    font-weight: 500;
    transition: all 0.2s ease;
    white-space: nowrap;
    letter-spacing: 0.02em;
    border: 1px solid transparent;
}

.audit-nav-link:hover {
    background-color: #333333;
    color: #ffd700;
    transform: translateY(-2px);
    border: 1px solid rgba(255, 215, 0, 0.3);
    box-shadow: 0 4px 8px rgba(0, 0, 0, 0.2);
}

/* Active class disabled per user request */
/*.audit-nav-link.active {
    background-color: rgba(255, 215, 0, 0.15);
    color: #ffd700;
    border: 1px solid rgba(255, 215, 0, 0.3);
    font-weight: 600;
}*/

/* Section anchor styling */
.section-anchor {
    display: block;
    position: relative;
    top: -100px;
    visibility: hidden;
}

/* Table filter dropdown styling */
.stDataFrame [data-testid="column-actions-menu"],
.st-df [data-testid="column-actions-menu"] {
    background-color: #f8f9fa !important;
    border-radius: 6px !important;
    border: 1px solid #e9ecef !important;
    box-shadow: 0 4px 15px rgba(0,0,0,0.1) !important;
}

/* Menu items inside the dropdown */
.stDataFrame [data-testid="column-actions-menu"] button,
.st-df [data-testid="column-actions-menu"] button {
    color: #495057 !important;
    font-size: 0.85rem !important;
    padding: 8px 12px !important;
    border-radius: 4px !important;
    margin: 2px 0 !important;
    transition: background-color 0.2s ease !important;
}

/* Hover effect for menu items */
.stDataFrame [data-testid="column-actions-menu"] button:hover,
.st-df [data-testid="column-actions-menu"] button:hover {
    background-color: #e9ecef !important;
    color: #2563eb !important;
}

/* Header row styling for better filter button appearance */
.stDataFrame th button,
.st-df th button {
    border: none !important;
    background: transparent !important;
}

/* Filter button icon color */
.stDataFrame th [data-testid="StyledIcon"],
.st-df th [data-testid="StyledIcon"] {
    color: #6c757d !important;
}

/* Default Streamlit headings (for reference, not used for main sections) */
h1 {
font-size: 1.8rem !important;
letter-spacing: -0.02em !important;
margin-bottom: 1rem !important;
}
h2 {
font-size: 1.5rem !important;
letter-spacing: -0.015em !important;
margin-top: 1.5rem !important;
margin-bottom: 0.75rem !important;
}
h3 {
font-size: 1.3rem !important;
margin-top: 1.2rem !important;
margin-bottom: 0.6rem !important;
}
p, div, span, li {
font-family: 'Inter', -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif !important;
font-weight: 400 !important;
font-size: 1rem !important;
line-height: 1.5 !important;
}
/* Section headers styling */
.main .block-container h1, .main .block-container h2 {
padding-bottom: 0.3rem !important;
border-bottom: 1px solid rgba(255, 255, 255, 0.1) !important;
}

/* Dataframe and table styling */
.stDataFrame, .dataframe, .st-df {
    border: none !important;
    box-shadow: 0 4px 12px rgba(0,0,0,0.08) !important;
    border-radius: 8px !important;
    overflow: hidden !important;
    background-color: #2C2C2C !important;
    color: #F0F0F0 !important;
    font-family: 'Inter', sans-serif !important;
}

.stDataFrame th {
    background-color: #333333 !important;
    color: #FFFFFF !important;
    font-weight: 600 !important;
    padding: 0.75rem 1rem !important;
    font-size: 0.9rem !important;
    letter-spacing: 0.01em !important;
    text-transform: uppercase !important;
    font-variant: small-caps !important;
}

/* Table filter dropdown styling */
.stDataFrame [data-testid="column-actions-menu"],
.st-df [data-testid="column-actions-menu"] {
    background-color: #3a3a3a !important;
    border-radius: 6px !important;
    border: 1px solid #4a4a4a !important;
    box-shadow: 0 4px 15px rgba(0,0,0,0.3) !important;
}

/* Menu items inside the dropdown */
.stDataFrame [data-testid="column-actions-menu"] button,
.st-df [data-testid="column-actions-menu"] button {
    color: #e0e0e0 !important;
    font-size: 0.85rem !important;
    padding: 8px 12px !important;
    border-radius: 4px !important;
    margin: 2px 0 !important;
    transition: background-color 0.2s ease !important;
}

/* Hover effect for menu items */
.stDataFrame [data-testid="column-actions-menu"] button:hover,
.st-df [data-testid="column-actions-menu"] button:hover {
    background-color: #4a4a4a !important;
    color: #BFA23A !important;
}

/* Header row styling for better filter button appearance */
.stDataFrame th button,
.st-df th button {
    border: none !important;
    background: transparent !important;
}

/* Filter button icon color */
.stDataFrame th [data-testid="StyledIcon"],
.st-df th [data-testid="StyledIcon"] {
    color: #BFA23A !important;
}

.stDataFrame td {
background-color: #2C2C2C !important;
color: #F0F0F0 !important;
padding: 0.5rem 1rem !important;
font-size: 0.95rem !important;
}

.stDataFrame tr:nth-child(even) td {
background-color: #333333 !important;
}

/* Widget styling */
.stSelectbox, .stMultiselect, .stSlider {
background-color: #333333 !important;
color: #F0F0F0 !important;
border-radius: 4px !important;
font-family: 'Inter', sans-serif !important;
}

/* Button styling - keep original colors but improve contrast */
.stButton button {
font-family: 'Inter', sans-serif !important;
font-weight: 500 !important;
font-size: 0.95rem !important;
letter-spacing: 0.01em !important;
border: none !important;
box-shadow: 0 2px 4px rgba(0, 0, 0, 0.2) !important;
border-radius: 4px !important;
padding: 0.4rem 1rem !important;
}

/* Making all text consistent */
p, div, span, li, label, a, .stMarkdown, .stText {
font-family: 'Inter', sans-serif !important;
}

/* Standardize all small texts */
.stText p, .stMarkdown p, .stMarkdown ul li, .stInfoMessage p {
font-size: 0.95rem !important;
line-height: 1.5 !important;
color: #E0E0E0 !important;
margin-bottom: 0.75rem !important;
}
    
/* Expander styling */
.streamlit-expanderHeader {
background-color: #333333 !important;
color: #FFFFFF !important;
border-radius: 4px !important;
font-family: 'Inter', sans-serif !important;
font-weight: 500 !important;
font-size: 1rem !important;
}
    
.streamlit-expanderContent {
background-color: #2C2C2C !important;
border: 1px solid #444444 !important;
border-top: none !important;
border-radius: 0 0 4px 4px !important;
padding: 1rem !important;
}
    
/* Tab styling */
.stTabs [data-baseweb="tab-list"] {
background-color: #1E1E1E !important;
border-radius: 4px 4px 0 0 !important;
font-family: 'Inter', sans-serif !important;
}
    
.stTabs [data-baseweb="tab"] {
color: #CCCCCC !important;
background-color: #1E1E1E !important;
font-weight: 500 !important;
font-size: 0.95rem !important;
letter-spacing: 0.01em !important;
padding: 0.5rem 1rem !important;
}

.stTabs [aria-selected="true"] {
color: #FFFFFF !important;
background-color: #1E1E1E !important;
font-weight: 600 !important;
border-bottom: 2px solid #3B82F6 !important;
}

/* Metric styling */
[data-testid="stMetric"] {
background-color: #262626 !important;
border-radius: 6px !important;
padding: 1rem !important;
box-shadow: 0 2px 4px rgba(0, 0, 0, 0.1) !important;
}

[data-testid="stMetric"] label {
color: #CCCCCC !important;
font-family: 'Inter', sans-serif !important;
font-weight: 500 !important;
font-size: 0.75rem !important;
letter-spacing: 0.03em !important;
text-transform: uppercase !important;
margin-bottom: 0.3rem !important;
}

[data-testid="stMetric"] .value {
color: #FFFFFF !important;
font-family: 'Inter', sans-serif !important;
font-weight: 700 !important;
font-size: 1.8rem !important;
letter-spacing: -0.01em !important;
line-height: 1.2 !important;
}

/* Info, warning, error box styling */
.stAlert {
border-radius: 4px !important;
font-family: 'Inter', sans-serif !important;
}

.stAlert [data-baseweb="notification"] {
border-radius: 4px !important;
}

/* Scrollbar styling */
::-webkit-scrollbar {
width: 10px;
height: 10px;
}

::-webkit-scrollbar-track {
background: #262626;
}

::-webkit-scrollbar-thumb {
background: #555555;
border-radius: 5px;
}

::-webkit-scrollbar-thumb:hover {
background: #777777;
}

/* Page title and app heading */
.stApp [data-testid="stAppViewContainer"] > div:first-child h1 {
font-weight: 700 !important;
font-size: 2rem !important;
letter-spacing: -0.03em !important;
color: #3B82F6 !important;
margin-bottom: 1.5rem !important;
}

/* File uploader styling */
.stUploader {
border: 1px dashed #555555 !important;
border-radius: 6px !important;
padding: 1rem !important;
background-color: #262626 !important;
margin-bottom: 0.75rem !important;
height: 90px !important;
}

.stUploader > div > div {
font-family: 'Inter', sans-serif !important;
display: flex !important;
flex-direction: column !important;
justify-content: center !important;
align-items: center !important;
height: 100% !important;
}

.stUploader [data-testid="stFileUploadDropzone"] {
height: 100% !important;
min-height: 90px !important;
display: flex !important;
align-items: center !important;
justify-content: center !important;
}

.stUploader span p {
font-family: 'Inter', sans-serif !important;
font-size: 0.9rem !important;
color: #CCCCCC !important;
margin-bottom: 0.25rem !important;
}

/* File Uploader button style */
.stUploader button {
font-family: 'Inter', sans-serif !important;
font-weight: 500 !important;
font-size: 0.9rem !important;
padding: 0.35rem 0.9rem !important;
border-radius: 4px !important;
margin-top: 0.5rem !important;
background-color: #333333 !important;
border: 1px solid #555555 !important;
color: #FFFFFF !important;
}

/* Dashboard section headers */
.main .block-container > div > div > div > div > h3 {
font-weight: 600 !important;
color: #E5E5E5 !important;
border-left: 3px solid #3B82F6 !important;
padding-left: 0.5rem !important;
margin-top: 1.5rem !important;
margin-bottom: 1rem !important;
}

/* Modal styling for session management */
.session-modal {
    background-color: #2C2C2C !important;
    border: 2px solid #4CAF50 !important;
    border-radius: 10px !important;
    padding: 20px !important;
    margin: 10px 0 !important;
    box-shadow: 0 8px 32px rgba(0, 0, 0, 0.6) !important;
}

/* Improved input field styling for session forms */
.stTextInput > div > div > input {
    background-color: #3C3C3C !important;
    border: 2px solid #555555 !important;
    border-radius: 8px !important;
    color: #FFFFFF !important;
    padding: 12px 16px !important;
    font-size: 1.1rem !important;
    font-weight: 500 !important;
}

.stTextInput > div > div > input:focus {
    border-color: #4CAF50 !important;
    box-shadow: 0 0 0 3px rgba(76, 175, 80, 0.2) !important;
    outline: none !important;
}

/* Text area styling for session descriptions */
.stTextArea > div > div > textarea {
    background-color: #3C3C3C !important;
    border: 2px solid #555555 !important;
    border-radius: 8px !important;
    color: #FFFFFF !important;
    padding: 12px 16px !important;
    font-size: 1rem !important;
    font-family: 'Inter', sans-serif !important;
}

.stTextArea > div > div > textarea:focus {
    border-color: #4CAF50 !important;
    box-shadow: 0 0 0 3px rgba(76, 175, 80, 0.2) !important;
    outline: none !important;
}

/* Improve selectbox styling for session selection */
.stSelectbox > div > div > div {
    background-color: #3C3C3C !important;
    border: 2px solid #555555 !important;
    border-radius: 8px !important;
    color: #FFFFFF !important;
}

/* --- Streamlit Cloud overlay fixes --- */
/* Some cloud builds render keyboard hint badges as <kbd> elements that can overlap inputs. */
.stApp kbd { display: none !important; }
div[data-baseweb="select"] kbd { display: none !important; }
.streamlit-expanderHeader kbd { display: none !important; }

/* Prevent any pseudo-content overlays from appearing on selects/expanders */
div[data-baseweb="select"]::before,
div[data-baseweb="select"]::after,
.streamlit-expanderHeader::before,
.streamlit-expanderHeader::after { content: none !important; }

/* Ensure select internals stay above any stray badges */
div[data-baseweb="select"] > div { position: relative !important; z-index: 0 !important; }
div[data-baseweb="select"] [role="combobox"] { position: relative !important; z-index: 1 !important; }

/* Hide any stray "key" text that appears in expander headers or widget containers */
/* This targets the specific issue where Streamlit Cloud renders internal keys as visible text */

/* Target the expander header structure - hide any leading text before the icon */
.streamlit-expanderHeader > div:first-child {
    display: flex !important;
    align-items: center !important;
}

/* Hide text nodes that appear before the expander icon/label */
.streamlit-expanderHeader > div:first-child > *:first-child:not(svg):not([data-testid]) {
    display: none !important;
}

/* More aggressive: target any small text at the start of expander headers */
.streamlit-expanderHeader::before {
    content: '' !important;
    display: none !important;
}

/* Hide leading spans in expander headers that might contain keys */
.streamlit-expanderHeader span[style*="font-size: 0"] {
    display: none !important;
}

/* For selectboxes and other widgets, hide any leading text that looks like a key */
div[data-baseweb="select"] > div:first-child > span:first-child {
    display: none !important;
}

/* Nuclear option: hide all text content in expander headers, then re-show only the label */
.streamlit-expanderHeader {
    font-size: 0 !important;
}
.streamlit-expanderHeader svg,
.streamlit-expanderHeader [data-testid="stMarkdownContainer"],
.streamlit-expanderHeader p,
.streamlit-expanderHeader div[data-testid] {
    font-size: 1rem !important;
}
</style>

<script>
// Remove stray "key" text nodes from expander headers
(function() {
    function cleanExpanderHeaders() {
        const headers = document.querySelectorAll('.streamlit-expanderHeader');
        headers.forEach(header => {
            // Iterate through child nodes and remove text nodes that start with "key"
            const walker = document.createTreeWalker(
                header,
                NodeFilter.SHOW_TEXT,
                null,
                false
            );
            
            let nodesToRemove = [];
            let node;
            while (node = walker.nextNode()) {
                if (node.nodeValue && node.nodeValue.trim().startsWith('key')) {
                    nodesToRemove.push(node);
                }
            }
            
            nodesToRemove.forEach(n => n.remove());
        });
    }
    
    // Run on load
    if (document.readyState === 'loading') {
        document.addEventListener('DOMContentLoaded', cleanExpanderHeaders);
    } else {
        cleanExpanderHeaders();
    }
    
    // Re-run periodically to catch dynamically added expanders
    setInterval(cleanExpanderHeaders, 500);
    
    // Also run on any DOM mutations
    const observer = new MutationObserver(cleanExpanderHeaders);
    observer.observe(document.body, { childList: true, subtree: true });
})();
</script>
'''

# Initialize session state if needed
if 'client_config' not in st.session_state:
    st.session_state.client_config = None
if 'bulk_data' not in st.session_state: 
    st.session_state.bulk_data = None 
if 'sales_report_data' not in st.session_state:
    st.session_state.sales_report_data = None
if 'asin_perf_df' not in st.session_state:
    st.session_state.asin_perf_df = None
if 'debug_messages' not in st.session_state:
    st.session_state.debug_messages = []
if 'active_tab' not in st.session_state:
    st.session_state.active_tab = "client_overview"
if 'is_companion_data' not in st.session_state:
    st.session_state.is_companion_data = False

# Apply custom CSS
st.markdown(custom_css, unsafe_allow_html=True)

# We'll add the logo in the header section alongside the title text later

# Create a dark sidebar with client selection
with st.sidebar:
    st.markdown("<h2 style='text-align: center;'>Client Selection</h2>", unsafe_allow_html=True)
    # Optional: allow disabling Supabase to fall back to session-only storage in cloud
    try:
        if is_cloud_environment() and is_supabase_configured():
            prev = bool(st.session_state.get('disable_supabase', False))
            toggled = st.checkbox("Disable Supabase (session-only)", value=prev, help="If you're seeing hangs or network issues, use session-only storage. Data will not persist across reloads.")
            if toggled != prev:
                st.session_state.disable_supabase = toggled
                # Clear caches so UI reflects new storage mode
                try:
                    clear_client_caches()
                except Exception:
                    pass
                st.rerun()
    except Exception:
        pass
    
    # Cache is automatically reset after 1 hour or when new data is uploaded
    # Also clear cache if a client was just saved/imported
    if st.session_state.get('clients_list_changed', False):
        clear_client_caches()
        st.session_state.clients_list_changed = False
        log_app_event("Cleared client list cache due to changes")
    
    try:
        existing_clients = get_existing_clients()
    except Exception as e:
        st.error(f"Error loading client list: {str(e)}")
        existing_clients = []

    # Load client if selected previously
    if 'selected_client_name' in st.session_state and st.session_state.client_config is None:
        # Prevent infinite rerun loop if client fails to load
        if 'client_load_failed' not in st.session_state and 'client_load_attempt_count' not in st.session_state:
            st.session_state.client_load_attempt_count = 0
        
        if 'client_load_failed' not in st.session_state and st.session_state.get('client_load_attempt_count', 0) < 3:
            try:
                client_name = st.session_state.selected_client_name
                st.session_state.client_load_attempt_count += 1
                log_app_event(f"Auto-loading client (attempt {st.session_state.client_load_attempt_count}): {client_name}")
                
                # Show status message during auto-load
                with st.spinner(f'Restoring client session: {client_name}...'):
                    loaded_config = load_client_config(client_name)
                
                if loaded_config is None:
                    st.error(f"âŒ Failed to load client '{client_name}'. The configuration file may be missing or corrupted.")
                    log_app_event(f"Failed to load client '{client_name}' - config returned None")
                    st.session_state.client_load_failed = True
                    st.session_state.selected_client_name = None
                    st.session_state.client_load_attempt_count = 0
                else:
                    st.session_state.client_config = loaded_config
                    st.session_state.client_load_attempt_count = 0
                    log_app_event(f"Auto-loaded client successfully: {client_name}")
            except Exception as e:
                st.error(f"Error auto-loading client '{client_name}': {str(e)}")
                log_app_event(f"Error auto-loading client '{client_name}': {str(e)}")
                st.session_state.client_load_failed = True
                st.session_state.selected_client_name = None
                st.session_state.client_load_attempt_count = 0
        elif st.session_state.get('client_load_attempt_count', 0) >= 3:
            st.error(f"âŒ Failed to load client after 3 attempts. Please try selecting the client again.")
            log_app_event(f"Failed to load client after 3 attempts")
            st.session_state.selected_client_name = None
            st.session_state.client_load_attempt_count = 0

    # Radio button for action choice
    st.markdown("### Action")
    
    # Always show Import Client Data option
    choice = st.radio("Client Action", ["Load Existing Client", "Create New Client", "Import Client Data"], label_visibility="collapsed")

    if choice == "Load Existing Client":
        if existing_clients:
            st.markdown("### Select Client")
            selected_client = st.selectbox("Select Client", existing_clients, label_visibility="collapsed")
            col1, col2 = st.columns(2)
            
            with col1:
                if st.button("Load Client", use_container_width=True):
                    try:
                        log_app_event(f"Load Client clicked for: {selected_client}")
                        
                        # Clear any previous load failure flag
                        if 'client_load_failed' in st.session_state:
                            del st.session_state.client_load_failed
                        
                        # Try to load the client config
                        loaded_config = load_client_config(selected_client)
                        
                        if loaded_config is None:
                            st.error(f"âŒ Failed to load client '{selected_client}'. The configuration file may be missing or corrupted.")
                            log_app_event(f"Failed to load client '{selected_client}' - config returned None")
                            # Don't set selected_client_name if load failed
                        else:
                            st.session_state.selected_client_name = selected_client
                            st.session_state.client_config = loaded_config
                            st.session_state.bulk_data = None 
                            st.session_state.sales_report_data = None
                            st.session_state.current_page = 'file_uploads'  # Redirect to File Uploads page
                            log_app_event(f"Client loaded successfully: {selected_client}")
                            st.success(f"âœ… Client '{selected_client}' loaded successfully!")
                            import time
                            time.sleep(0.5)  # Brief pause to show success message
                            st.rerun()
                    except Exception as e:
                        st.error(f"Error loading client: {str(e)}")
                        import traceback
                        log_app_event(f"Error loading client '{selected_client}': {str(e)}\n{traceback.format_exc()}")
                        st.error(traceback.format_exc())
            
            with col2:
                if st.button("Delete Client", use_container_width=True):
                    # Set up confirmation state
                    st.session_state.confirm_delete = True
                    st.session_state.client_to_delete = selected_client
                    st.rerun()
                    
            # Handle delete confirmation
            if 'confirm_delete' in st.session_state and st.session_state.confirm_delete:
                st.warning(f"Are you sure you want to delete '{st.session_state.client_to_delete}'? This cannot be undone.")
                col1, col2 = st.columns(2)
                
                with col1:
                    if st.button("Yes, Delete", use_container_width=True):
                        # Store the client name before clearing session state
                        deleted_client_name = st.session_state.client_to_delete
                        
                        # Get the client's config file path in the user data directory
                        client_file = Path(os.path.join(CLIENT_CONFIG_DIR, f"{st.session_state.client_to_delete}.json"))
                        
                        # Delete the client file
                        if client_file.exists():
                            client_file.unlink()  # Delete the file
                            
                            # Clear caches to immediately refresh the client list
                            clear_caches()
                            
                            # Reset session state
                            if 'selected_client_name' in st.session_state and st.session_state.selected_client_name == st.session_state.client_to_delete:
                                st.session_state.selected_client_name = None
                                st.session_state.client_config = None
                            
                            # Reset confirmation state
                            st.session_state.confirm_delete = False
                            st.session_state.client_to_delete = None
                            st.success(f"Client '{deleted_client_name}' has been deleted.")
                            st.rerun()  # Refresh to update the client list
                        else:
                            st.error(f"Client file '{deleted_client_name}.json' not found. It may have already been deleted.")
                            # Reset confirmation state even if file doesn't exist
                            st.session_state.confirm_delete = False
                            st.session_state.client_to_delete = None
                            st.rerun()
                
                with col2:
                    if st.button("Cancel", use_container_width=True):
                        # Reset confirmation state
                        st.session_state.confirm_delete = False
                        st.session_state.client_to_delete = None
                        st.rerun()
            

        else:
            st.warning("No existing clients found.")
            st.info(f"Amazon Dashboard v{APP_VERSION} - Create a client to get started.")

    elif choice == "Create New Client":
        new_client_name = st.text_input("New Client Name")
        if st.button("Create Client"):
            clean_name = new_client_name.strip()
            if clean_name and clean_name not in existing_clients:
                new_config = {
                    "client_name": clean_name,
                    "branded_keywords": [],
                    "branded_asins": [],
                    "product_groups": {},
                    "goals": {
                        "account_wide_acos": None,
                        "branded_acos": None,
                        "non_branded_acos": None
                    }
                }
                save_client_config(clean_name, new_config)
                st.session_state.selected_client_name = clean_name
                st.session_state.client_config = new_config
                st.session_state.bulk_data = None 
                st.session_state.sales_report_data = None
                st.session_state.current_page = 'file_uploads'  # Redirect to File Uploads page
                st.success(f"Client '{clean_name}' created and loaded.")
                st.rerun()
            elif not clean_name:
                st.error("Please enter a client name.")
            else:
                st.error(f"Client '{clean_name}' already exists.")
    
    elif choice == "Import Client Data":
        st.markdown("### Import Client Data")
        
        if is_cloud_environment():
            st.info("ðŸŒ **Cloud Mode**: Import your client data backup file to restore your settings and configurations.")
        else:
            st.info("ðŸ’» **Local Mode**: Import client data from a backup file.")
        
        st.markdown("---")
        
        # Import section
        st.subheader("ðŸ“¥ Import Client Data")
        st.markdown("Upload one or more previously exported backup files to restore your client configurations.")
        
        uploaded_backups = st.file_uploader("Choose backup file(s)", type=['json'], accept_multiple_files=True, key="client_backup_import")
        
        if uploaded_backups:
            # Merge all uploaded files into a single import_data structure
            merged_import_data = {
                'version': '1.0',
                'export_date': datetime.now().isoformat(),
                'clients': {}
            }
            
            file_errors = []
            files_processed = 0
            
            with st.spinner(f"Loading {len(uploaded_backups)} backup file(s)..."):
                for uploaded_file in uploaded_backups:
                    try:
                        file_data = json.load(uploaded_file)
                        
                        # Validate individual file
                        if 'clients' not in file_data or not isinstance(file_data.get('clients'), dict):
                            file_errors.append(f"{uploaded_file.name}: Invalid backup file format")
                            continue
                        
                        # Merge clients from this file
                        for client_name, client_config in file_data['clients'].items():
                            merged_import_data['clients'][client_name] = client_config
                        
                        files_processed += 1
                        
                    except json.JSONDecodeError:
                        file_errors.append(f"{uploaded_file.name}: Invalid JSON format")
                    except Exception as e:
                        file_errors.append(f"{uploaded_file.name}: {str(e)}")
                
                # Show file processing results
                if file_errors:
                    with st.expander(f"âš ï¸ {len(file_errors)} file(s) had errors", expanded=True):
                        for error in file_errors:
                            st.error(f"âŒ {error}")
                
                if files_processed > 0:
                    st.success(f"âœ… Successfully loaded {files_processed} backup file(s)")
                
                # Use the merged data for import processing
                import_data = merged_import_data
                
                try:
                    # Validate merged backup
                    if not import_data['clients']:
                        st.error("âŒ No valid client data found in uploaded files.")
                    
                    else:
                        # Detect duplicates
                        duplicate_clients = []
                        new_clients = []
                        
                        for client_name in import_data['clients'].keys():
                            if client_name in existing_clients:
                                duplicate_clients.append(client_name)
                            else:
                                new_clients.append(client_name)
                        
                        # Show what will be imported
                        st.markdown("**Contents:**")
                        st.write(f"- **Total clients in backup:** {len(import_data['clients'])}")
                        st.write(f"- âœ… **New clients:** {len(new_clients)}")
                        st.write(f"- âš ï¸ **Duplicate clients:** {len(duplicate_clients)}")
                        
                        # Show expandable lists for large imports (collapsed by default to prevent render issues)
                        if new_clients:
                            with st.expander(f"ðŸ“‹ View New Clients ({len(new_clients)})", expanded=False):
                                if len(new_clients) <= 10:
                                    st.write(", ".join(sorted(new_clients)))
                                else:
                                    # Use dataframe for better display
                                    st.dataframe(
                                        pd.DataFrame({"Client Name": sorted(new_clients)}),
                                        use_container_width=True,
                                        hide_index=True,
                                        height=min(300, len(new_clients) * 35 + 38)
                                    )
                        
                        if duplicate_clients:
                            with st.expander(f"âš ï¸ View Duplicate Clients ({len(duplicate_clients)})", expanded=False):
                                if len(duplicate_clients) <= 10:
                                    st.write(", ".join(sorted(duplicate_clients)))
                                else:
                                    # Use dataframe for better display
                                    st.dataframe(
                                        pd.DataFrame({"Client Name": sorted(duplicate_clients)}),
                                        use_container_width=True,
                                        hide_index=True,
                                        height=min(300, len(duplicate_clients) * 35 + 38)
                                    )
                        
                        import_mode = st.radio(
                            "Import mode:",
                            ["Merge (keep existing clients)", "Replace (overwrite all clients)"],
                            help="Merge will add to existing clients. Replace will delete all current clients first."
                        )
                        
                        # Handle duplicate resolution in Merge mode - SIMPLIFIED APPROACH
                        duplicate_action_choice = "skip_all"  # default
                        if "Merge" in import_mode and duplicate_clients:
                            st.markdown("---")
                            st.markdown("### ðŸ”„ Duplicate Client Resolution")
                            st.info(f"{len(duplicate_clients)} client(s) already exist. Choose how to handle all duplicates:")
                            
                            # Single choice for ALL duplicates instead of per-client widgets
                            duplicate_action_choice = st.radio(
                                "Action for all duplicate clients:",
                                ["skip_all", "overwrite_all"],
                                format_func=lambda x: "Skip all (keep existing)" if x == "skip_all" else "Overwrite all (replace with backup)",
                                help="This action will apply to all duplicate clients to avoid creating too many widgets.",
                                key="bulk_duplicate_action"
                            )
                            
                            st.markdown("---")
                        
                        if st.button("ðŸ”„ Import Clients", type="primary", use_container_width=True):
                            try:
                                
                                # Replace mode: clear existing clients
                                if "Replace" in import_mode:
                                    if is_cloud_environment():
                                        # Clear localStorage
                                        for client_name in existing_clients:
                                            remove_localStorage_value(f'amazon_dashboard_client_{client_name}')
                                            # Clear session state cache
                                            cache_key = f'client_config_cache_{client_name}'
                                            if cache_key in st.session_state:
                                                del st.session_state[cache_key]
                                        remove_localStorage_value('amazon_dashboard_client_list')
                                        st.session_state.client_list = []
                                    else:
                                        # Clear filesystem
                                        if os.path.exists(CLIENT_CONFIG_DIR):
                                            for file in os.listdir(CLIENT_CONFIG_DIR):
                                                if file.endswith('.json'):
                                                    os.remove(os.path.join(CLIENT_CONFIG_DIR, file))
                                
                                # Import clients with progress indication
                                imported_count = 0
                                skipped_count = 0
                                overwritten_count = 0
                                failed_imports = []  # Track failures instead of showing errors in loop
                                
                                total_clients = len(import_data['clients'])
                                progress_bar = st.progress(0)
                                status_text = st.empty()
                                
                                # Check Supabase auth BEFORE loop to prevent crashes
                                if use_supabase() and not get_current_user_id():
                                    progress_bar.empty()
                                    status_text.empty()
                                    st.error("âŒ Import failed: Not logged in to Supabase. Please sign in or disable Supabase in the sidebar.")
                                    raise ValueError("Supabase authentication required")
                                
                                for idx, (client_name, client_config) in enumerate(import_data['clients'].items(), 1):
                                    # Update progress less frequently for large imports (every 10% or every client if <10 clients)
                                    if total_clients <= 10 or idx % max(1, total_clients // 10) == 0 or idx == total_clients:
                                        progress_bar.progress(idx / total_clients)
                                        status_text.text(f"Importing {idx}/{total_clients}...")
                                    
                                    # In Merge mode, check duplicate action (bulk decision)
                                    if "Merge" in import_mode and client_name in duplicate_clients:
                                        if duplicate_action_choice == "skip_all":
                                            skipped_count += 1
                                            continue
                                        else:
                                            # Overwrite all
                                            try:
                                                save_client_config(client_name, client_config)
                                                overwritten_count += 1
                                                imported_count += 1
                                            except Exception as e:
                                                failed_imports.append((client_name, str(e)))
                                                skipped_count += 1
                                    else:
                                        # New client or Replace mode
                                        try:
                                            save_client_config(client_name, client_config)
                                            imported_count += 1
                                        except Exception as e:
                                            failed_imports.append((client_name, str(e)))
                                            skipped_count += 1
                                
                                # Clear progress indicators
                                progress_bar.empty()
                                status_text.empty()
                                
                                # Show any import failures
                                if failed_imports:
                                    with st.expander(f"âš ï¸ {len(failed_imports)} client(s) failed to import", expanded=True):
                                        for client_name, error in failed_imports[:10]:  # Show max 10
                                            st.error(f"âŒ {client_name}: {error}")
                                        if len(failed_imports) > 10:
                                            st.warning(f"... and {len(failed_imports) - 10} more")
                                
                                # Clear caches
                                clear_client_caches()
                                
                                # Clear Supabase session-state cache keys if using Supabase
                                if use_supabase():
                                    uid = get_current_user_id()
                                    if uid:
                                        # Clear the session-level cache for client names
                                        cache_key_names = f"_client_names_cache_{uid}"
                                        st.session_state.pop(cache_key_names, None)
                                        # Clear individual client config caches
                                        for client_name in import_data['clients'].keys():
                                            cache_key_config = f"_config_cache_{uid}_{client_name}"
                                            st.session_state.pop(cache_key_config, None)
                                            # Also clear the app-level cache
                                            app_cache_key = f'client_config_cache_{client_name}'
                                            st.session_state.pop(app_cache_key, None)
                                        # Clear the Streamlit @cache_data decorated functions
                                        try:
                                            sb_list_client_names.clear()
                                            sb_fetch_client_config.clear()
                                        except Exception:
                                            pass
                                
                                # Show detailed success message
                                if "Replace" in import_mode:
                                    st.success(f"âœ… Successfully imported {imported_count} client(s)!")
                                else:
                                    result_parts = []
                                    if imported_count - overwritten_count > 0:
                                        result_parts.append(f"{imported_count - overwritten_count} new")
                                    if overwritten_count > 0:
                                        result_parts.append(f"{overwritten_count} overwritten")
                                    if skipped_count > 0:
                                        result_parts.append(f"{skipped_count} skipped")
                                    
                                    st.success(f"âœ… Import complete: {', '.join(result_parts)} client(s)!")

                                # Auto-load first imported client and rerun to refresh UI
                                if imported_count > 0:
                                    should_rerun = False
                                    
                                    # In cloud without Supabase, localStorage fallback can be unreliable on Streamlit Cloud.
                                    # To avoid hangs and make the app usable, keep imported clients in-session and auto-load the first one.
                                    if is_cloud_environment() and not use_supabase():
                                        try:
                                            st.session_state.in_memory_clients = import_data.get('clients', {})
                                            first_client = None
                                            for name in st.session_state.in_memory_clients.keys():
                                                first_client = name
                                                break
                                            if first_client:
                                                st.warning("Cloud mode without Supabase: clients are available for this session only. Configure Supabase to persist across reloads.")
                                                st.session_state.selected_client_name = first_client
                                                st.session_state.client_config = st.session_state.in_memory_clients[first_client]
                                                # Refresh client list cache so the name shows up under Load Existing Client during this session
                                                try:
                                                    clear_client_caches()
                                                except Exception:
                                                    pass
                                                st.session_state.current_page = 'file_uploads'
                                                should_rerun = True
                                        except Exception:
                                            pass
                                    else:
                                        # For all other modes (local, cloud with Supabase), auto-load first client
                                        try:
                                            first_imported = list(import_data['clients'].keys())[0]
                                            st.session_state.selected_client_name = first_imported
                                            # Load the config to populate session state
                                            loaded_config = load_client_config(first_imported)
                                            if loaded_config:
                                                st.session_state.client_config = loaded_config
                                                st.session_state.current_page = 'file_uploads'
                                                should_rerun = True
                                        except Exception:
                                            pass
                                    
                                    if should_rerun:
                                        st.balloons()
                                        st.rerun()
                                
                                # Fallback: just show success and info message (if we didn't rerun)
                                st.balloons()
                                st.info("Please select 'Load Existing Client' to access your imported clients.")
                                
                            except Exception as e:
                                st.error(f"âŒ Error importing data: {str(e)}")
                                import traceback
                                st.error(traceback.format_exc())
                            
                except Exception as e:
                    st.error(f"âŒ Error importing data: {str(e)}")
                    import traceback
                    st.error(traceback.format_exc())
        
        st.markdown("---")
        
        # Export section (always available)
        if existing_clients:
            st.subheader("ðŸ“¤ Export Current Data")
            st.markdown("Select which clients to export as a backup file.")
            
            # Multi-select for clients
            selected_clients_to_export = st.multiselect(
                "Select clients to export:",
                options=sorted(existing_clients),
                default=sorted(existing_clients),
                help="Choose one or more clients to include in the backup file"
            )
            
            col1, col2 = st.columns(2)
            
            with col1:
                if st.button("Select All", use_container_width=True):
                    st.session_state.export_select_all = True
                    st.rerun()
            
            with col2:
                if st.button("Clear Selection", use_container_width=True):
                    st.session_state.export_select_all = False
                    st.rerun()
            
            # Handle select all/clear
            if 'export_select_all' in st.session_state:
                if st.session_state.export_select_all:
                    selected_clients_to_export = sorted(existing_clients)
                else:
                    selected_clients_to_export = []
                del st.session_state.export_select_all
            
            if selected_clients_to_export:
                st.info(f"ðŸ“Š {len(selected_clients_to_export)} client(s) selected for export")
                
                if st.button("ðŸ“¦ Export Selected Clients", type="primary", use_container_width=True):
                    try:
                        export_data = {
                            'version': '1.0',
                            'export_date': datetime.now().isoformat(),
                            'clients': {}
                        }
                        
                        # Export selected client configs
                        failed_clients = []
                        for client_name in selected_clients_to_export:
                            client_config = load_client_config(client_name)
                            if client_config:
                                export_data['clients'][client_name] = client_config
                            else:
                                failed_clients.append(client_name)
                        
                        # Warn if some clients couldn't be loaded
                        if failed_clients:
                            if use_supabase() and not get_current_user_id():
                                st.error(f"âŒ Unable to export {len(failed_clients)} client(s) - Not logged in to Supabase. Please sign in or disable Supabase in the sidebar.")
                            else:
                                st.warning(f"âš ï¸ Unable to load config for {len(failed_clients)} client(s): {', '.join(failed_clients[:3])}{'...' if len(failed_clients) > 3 else ''}")
                        
                        # Check if we have any clients to export
                        if not export_data['clients']:
                            st.error("âŒ No client data could be exported. Please check your authentication or storage settings.")
                            if use_supabase():
                                st.info("ðŸ’¡ Try disabling Supabase in the sidebar if you're not using it.")
                            raise ValueError("No client data available for export")
                        
                        # Create download button
                        export_json = json.dumps(export_data, indent=2)
                        file_size_bytes = len(export_json.encode('utf-8'))
                        file_size_mb = file_size_bytes / (1024 * 1024)
                        
                        # Create filename based on selection
                        if len(selected_clients_to_export) == len(existing_clients):
                            filename_prefix = "all_clients"
                        elif len(selected_clients_to_export) == 1:
                            filename_prefix = selected_clients_to_export[0].replace(' ', '_')
                        else:
                            filename_prefix = f"{len(selected_clients_to_export)}_clients"
                        
                        # Show file size info
                        if file_size_mb < 1:
                            st.info(f"ðŸ“¦ Export size: {file_size_bytes / 1024:.1f} KB")
                        else:
                            st.info(f"ðŸ“¦ Export size: {file_size_mb:.2f} MB")
                            if file_size_mb > 5:
                                st.warning("âš ï¸ Large export file! Import may take longer. Consider exporting fewer clients at once if you experience issues.")
                        
                        st.download_button(
                            label=f"ðŸ’¾ Download Backup ({len(selected_clients_to_export)} client{'s' if len(selected_clients_to_export) > 1 else ''})",
                            data=export_json,
                            file_name=f"amazon_dashboard_{filename_prefix}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json",
                            mime="application/json",
                            use_container_width=True
                        )
                        st.success(f"âœ… Export prepared! Click the download button above to save {len(selected_clients_to_export)} client(s).")
                        
                    except Exception as e:
                        st.error(f"âŒ Error exporting data: {str(e)}")
            else:
                st.warning("âš ï¸ Please select at least one client to export.")
            
            st.markdown("---")
        
        st.caption("ðŸ’¡ **Tip**: Export your data regularly to prevent data loss, especially when using cloud mode where data is stored in your browser.")

# --- Main Panel --- #
if st.session_state.client_config:
    config = st.session_state.client_config
    
    # Initialize sales attribution choice in session state if not exists
    if 'sd_attribution_choice' not in st.session_state:
        st.session_state.sd_attribution_choice = "Sales"

    # Create navigation in sidebar
    with st.sidebar:
        st.markdown("")
        file_uploads_btn = st.button("File Uploads", use_container_width=True)
        client_settings_btn = st.button("Client Settings Center", use_container_width=True)
        advertising_audit_btn = st.button("Advertising Audit", use_container_width=True)
        advertiser_actions_btn = st.button("Advertiser Actions", use_container_width=True)
        
        # Session Management Section
        st.markdown("---")
        st.markdown("### Audit Sessions")
        
        # Initialize session modal states if not exists
        if 'show_save_modal' not in st.session_state:
            st.session_state.show_save_modal = False
        if 'show_load_modal' not in st.session_state:
            st.session_state.show_load_modal = False
        
        # Save Audit Button
        col_save, col_load = st.columns(2)
        with col_save:
            if st.button("ðŸ’¾ Save Audit", use_container_width=True, help="Save current audit session"):
                st.session_state.show_save_modal = True
                st.rerun()
        
        with col_load:
            if st.button("ðŸ“‚ Load Audit", use_container_width=True, help="Load a saved audit session"):
                st.session_state.show_load_modal = True
                st.rerun()
        
        # Version display at bottom of sidebar
        st.markdown("---")
        st.markdown(f"<small style='color: #888; text-align: center; display: block;'>v{APP_VERSION}</small>", unsafe_allow_html=True)
        
        # Save Modal
        if st.session_state.show_save_modal:
            with st.container():
                st.markdown("#### ðŸ’¾ Save Current Session")
                st.markdown("---")
                
                # Check if there's data to save
                has_data = (st.session_state.get('bulk_data') is not None or 
                           st.session_state.get('sales_report_data') is not None)
                
                if has_data:
                    # Initialize session name in session state if not exists
                    if 'temp_session_name' not in st.session_state:
                        st.session_state.temp_session_name = f"Audit Session {datetime.now().strftime('%m/%d/%Y')}"
                    
                    # Use a more prominent input field
                    st.markdown("**Session Name:**")
                    session_name = st.text_input(
                        "Enter session name", 
                        value=st.session_state.temp_session_name,
                        key="session_name_input",
                        label_visibility="collapsed",
                        placeholder="Enter a name for this audit session..."
                    )
                    
                    st.markdown("**Description (Optional):**")
                    description = st.text_area(
                        "Enter description", 
                        key="session_description_input",
                        label_visibility="collapsed",
                        placeholder="Add notes about this session...",
                        height=80
                    )
                    
                    # Show what will be saved
                    st.markdown("**This session will include:**")
                    data_types = []
                    if st.session_state.get('bulk_data'):
                        data_type = "Companion Data" if st.session_state.get('is_companion_data', False) else "Bulk File"
                        data_types.append(f"âœ… {data_type}")
                    if st.session_state.get('sales_report_data') is not None:
                        data_types.append("âœ… Sales Report")
                    
                    if data_types:
                        for data_type in data_types:
                            st.markdown(data_type)
                    else:
                        st.markdown("âš ï¸ No data to save")
                    
                    st.markdown("---")
                    
                    col1, col2 = st.columns(2)
                    with col1:
                        if st.button("ðŸ’¾ Save Session", use_container_width=True, type="primary"):
                            if session_name.strip():  # Ensure name is not empty
                                client_name = st.session_state.client_config.get('client_name', '')
                                if save_audit_session(client_name, session_name.strip(), description.strip()):
                                    st.success(f"Session '{session_name}' saved successfully!")
                                    st.session_state.show_save_modal = False
                                    # Clear temp values
                                    if 'temp_session_name' in st.session_state:
                                        del st.session_state.temp_session_name
                                    st.rerun()
                                else:
                                    st.error("Failed to save session.")
                            else:
                                st.error("Please enter a session name.")
                    
                    with col2:
                        if st.button("âŒ Cancel", use_container_width=True):
                            st.session_state.show_save_modal = False
                            # Clear temp values
                            if 'temp_session_name' in st.session_state:
                                del st.session_state.temp_session_name
                            st.rerun()
                else:
                    st.warning("âš ï¸ No data available to save.")
                    st.markdown("Please upload your advertising data and/or sales report first.")
                    if st.button("Close", use_container_width=True):
                        st.session_state.show_save_modal = False
                        st.rerun()
        
        # Load Modal
        if st.session_state.show_load_modal:
            with st.container():
                st.markdown("#### ðŸ“‚ Load Saved Session")
                st.markdown("---")
                
                client_name = st.session_state.client_config.get('client_name', '')
                saved_sessions = get_saved_sessions(client_name)
                
                if saved_sessions:
                    st.markdown("**Select a session to load:**")
                    
                    # Session selection with better formatting
                    session_options = []
                    for session in saved_sessions:
                        # Parse and format the date
                        try:
                            # Parse the created_date which is in format "YYYY-MM-DD HH:MM:SS"
                            created_date = datetime.strptime(session['created_date'], '%Y-%m-%d %H:%M:%S')
                            formatted_date = created_date.strftime('%m/%d/%y')
                        except (ValueError, TypeError):
                            # Fallback if date parsing fails
                            formatted_date = "Unknown"
                        
                        option_text = f"{session['display_name']} - {formatted_date}"
                        session_options.append(option_text)
                    
                    selected_index = st.selectbox(
                        "Available sessions:",
                        range(len(session_options)),
                        format_func=lambda x: session_options[x],
                        label_visibility="collapsed"
                    )
                    
                    if selected_index is not None and selected_index < len(saved_sessions):
                        selected_session = saved_sessions[selected_index]
                        
                        st.markdown("---")
                        st.markdown("**Session Details:**")
                        
                        # Show session details in a nicer format
                        if selected_session['description']:
                            st.markdown(f"ðŸ“ **Description:** {selected_session['description']}")
                        
                        st.markdown(f"ðŸ“Š **Contains:** {', '.join(selected_session['data_types'])}")
                        st.markdown(f"ðŸ“… **Created:** {selected_session['created_date']}")
                        
                        st.markdown("---")
                        
                        col1, col2, col3 = st.columns(3)
                        with col1:
                            if st.button("ðŸ“‚ Load Audit", use_container_width=True, type="primary"):
                                if load_audit_session(client_name, selected_session['filename']):
                                    st.success(f"Session '{selected_session['display_name']}' loaded successfully!")
                                    st.session_state.show_load_modal = False
                                    st.rerun()
                                else:
                                    st.error("Failed to load session.")
                        
                        with col2:
                            if st.button("ðŸ—‘ï¸ Delete", use_container_width=True):
                                if delete_audit_session(client_name, selected_session['filename']):
                                    st.success("Session deleted successfully!")
                                    st.rerun()
                                else:
                                    st.error("Failed to delete session.")
                        
                        with col3:
                            if st.button("âŒ Cancel", use_container_width=True):
                                st.session_state.show_load_modal = False
                                st.rerun()
                else:
                    st.info("ðŸ“­ No saved sessions found for this client.")
                    st.markdown("Upload some data and create your first saved session!")
                    if st.button("Close", use_container_width=True):
                        st.session_state.show_load_modal = False
                        st.rerun()
        
        # Add indicators for uploaded data
        st.markdown("")
        if 'bulk_data' in st.session_state and st.session_state.bulk_data:
            data_type = "Companion Data" if st.session_state.get('is_companion_data', False) else "Bulk File"
            st.success(f"âœ“ {data_type} loaded")
        
        if 'sales_report_data' in st.session_state and st.session_state.sales_report_data is not None:
            st.success("âœ“ Sales Report loaded")
            
        # Show overall status when both are loaded
        if ('bulk_data' in st.session_state and st.session_state.bulk_data and 
            'sales_report_data' in st.session_state and st.session_state.sales_report_data is not None):
            st.info("ðŸ“Š All data ready for analysis")
        
    # Default to file uploads page
    if 'current_page' not in st.session_state:
        st.session_state.current_page = "file_uploads"
    
    # Handle navigation
    if file_uploads_btn:
        st.session_state.current_page = "file_uploads"
    if client_settings_btn:
        st.session_state.current_page = "client_settings"
    if advertising_audit_btn:
        st.session_state.current_page = "advertising_audit"
    if advertiser_actions_btn:
        st.session_state.current_page = "advertiser_actions"
    
    # Initialize sales attribution choice in session state if not exists (moved from global to Advertising Audit section)
    
    # Display the selected page
    if st.session_state.current_page == "file_uploads":
        # Create two columns for the two types of uploads
        col1, col2 = st.columns(2)
        
        # Initialize file keys for upload tracking
        if 'last_bulk_file_key' not in st.session_state:
            st.session_state.last_bulk_file_key = None
        if 'last_companion_file_key' not in st.session_state:
            st.session_state.last_companion_file_key = None
        if 'last_sales_file_key' not in st.session_state:
            st.session_state.last_sales_file_key = None
        
        # Initialize upload method choice if not exists
        if 'upload_method' not in st.session_state:
            st.session_state.upload_method = "Bulk File"
        
        # Add navigation links to the respective tabs in the Advertising Audit section AFTER the file upload sections
        
        with col1:
            st.markdown("### Advertising Data Upload")
            
            # Radio button to choose upload method
            upload_method = st.radio(
                "Choose upload method:",
                ["Bulk File", "Companion"],
                key="upload_method_radio",
                horizontal=True
            )
            
            # Add a clear data button so users can intentionally clear data when needed
            if st.session_state.bulk_data is not None:
                if st.button("ðŸ—‘ï¸ Clear Current Data", key="clear_data_btn", help="Clear currently loaded data to upload new files"):
                    st.session_state.bulk_data = None
                    st.session_state.is_companion_data = False
                    st.session_state.last_bulk_file_key = None
                    st.session_state.last_companion_file_key = None
                    st.success("Data cleared. You can now upload new files.")
                    st.rerun()
            
            # Update session state when radio button changes
            if upload_method != st.session_state.upload_method:
                st.session_state.upload_method = upload_method
                # Don't automatically clear data when changing upload methods
                # Users can use the "Clear Data" button if they want to start fresh
                st.rerun()
            
            if st.session_state.upload_method == "Bulk File":
                st.markdown("**Drag and drop file here**")
                st.markdown("*Limit 200MB per file â€¢ XLSX*")
                bulk_file_uploaded = st.file_uploader("Upload Bulk File", type=["xlsx"], key="bulk_file_uploader", label_visibility="collapsed")
                
                # Process bulk file upload
                if bulk_file_uploaded is not None:
                    # Create a unique key for this file to avoid reprocessing the same file
                    file_key = f"{bulk_file_uploaded.name}_{bulk_file_uploaded.size}"
                    
                    # Only process if this is a new file
                    if st.session_state.get('last_bulk_file_key') != file_key:
                        try:
                            with st.spinner('Processing bulk file...'):
                                # Clear caches when new data is uploaded
                                clear_caches()
                                st.session_state.last_cache_refresh = datetime.now()
                                processed_data = process_bulk_data(bulk_file_uploaded)
                                st.session_state.bulk_data = processed_data
                                st.session_state.is_companion_data = False
                                st.session_state.last_bulk_file_key = file_key
                                
                            st.success('âœ“ Bulk file processed successfully')
                        except Exception as e:
                            st.error(f"Error processing bulk file: {e}")
                    else:
                        st.success('âœ“ Bulk file processed successfully')
                
            else:  # Companion upload
                st.markdown("**Drag and drop files here**")
                st.markdown("*Limit 200MB per file â€¢ CSV, XLSX*")
                
                # Three file uploaders for companion exports
                asin_file = st.file_uploader("ASIN Export", type=["csv", "xlsx"], key="companion_asin_uploader")
                search_term_file = st.file_uploader("Search Term Export", type=["csv", "xlsx"], key="companion_search_term_uploader")
                targeting_file = st.file_uploader("Targeting Export", type=["csv", "xlsx"], key="companion_targeting_uploader")
                
                # Add a manual processing button to allow partial uploads
                process_clicked = st.button("âš™ï¸ Process Files", key="process_companion_btn", help="Process any of the Companion exports you've uploaded")
                if process_clicked:
                    if not (asin_file or search_term_file or targeting_file):
                        st.warning("Please upload at least one Companion export (ASIN, Search Term, or Targeting) to process.")
                    else:
                        # Create a unique key based on whichever files are provided
                        parts = []
                        if asin_file:
                            parts.append(f"{asin_file.name}_{asin_file.size}")
                        if search_term_file:
                            parts.append(f"{search_term_file.name}_{search_term_file.size}")
                        if targeting_file:
                            parts.append(f"{targeting_file.name}_{targeting_file.size}")
                        file_key = "|".join(parts)
                        
                        # Only process if these are new files or a different combination
                        if st.session_state.get('last_companion_file_key') != file_key:
                            try:
                                with st.spinner('Processing companion files...'):
                                    # Clear caches when new data is uploaded
                                    clear_caches()
                                    st.session_state.last_cache_refresh = datetime.now()
                                    processed_data = process_companion_data(asin_file, search_term_file, targeting_file)
                                    st.session_state.bulk_data = processed_data
                                    if processed_data is not None:
                                        st.session_state.is_companion_data = True
                                        st.session_state.last_companion_file_key = file_key
                                
                                if processed_data:
                                    st.success('âœ“ Companion files processed successfully')
                                    missing = []
                                    if not asin_file:
                                        missing.append("ASIN Export")
                                    if not search_term_file:
                                        missing.append("Search Term Export")
                                    if not targeting_file:
                                        missing.append("Targeting Export")
                                    if missing:
                                        st.info("Processed with partial data. Missing: " + ", ".join(missing))
                                else:
                                    st.warning("No data could be processed from the provided files.")
                                    # Reset companion indicators if nothing processed
                                    st.session_state.is_companion_data = False
                            except Exception as e:
                                st.error(f"Error processing companion files: {e}")
                        else:
                            st.success('âœ“ Companion files processed successfully')
            
            # Show description based on upload method
            if st.session_state.upload_method == "Bulk File":
                st.markdown("Pull for desired date range. Include SP and SB Search Term Reports")
            else:
                st.markdown("Upload any of the Companion exports (ASIN, Search Term, Targeting) from your in-house software. You can process with one or more files.")
            
            st.markdown("Used for the following:")
            st.markdown("* Advertising Audit - Ad Performance Metrics")
            st.markdown("* Advertising Audit - Product Allocation Analysis")
            st.markdown("* Advertising Audit - Targeting")
            st.markdown("* Client Settings Center - Updating Advertised ASINs")
            st.markdown("* Client Settings Center - Campaign Tagging")
        
        with col2:
            st.markdown("### Sales Report Upload")
            
            # Add info section to balance the radio buttons on the left
            st.markdown("**Report Type:** Auto-detected")
            st.caption("Vendor Central or Seller Central reports are automatically recognized")
            
            # Add a clear data button for sales report
            if st.session_state.sales_report_data is not None:
                if st.button("ðŸ—‘ï¸ Clear Sales Report", key="clear_sales_btn", help="Clear currently loaded sales report"):
                    st.session_state.sales_report_data = None
                    st.session_state.last_sales_file_key = None
                    st.success("Sales report data cleared.")
                    st.rerun()
            
            st.markdown("**Drag and drop file here**")
            st.markdown("*Limit 200MB per file â€¢ CSV, XLSX*")
            sales_report_uploaded = st.file_uploader("Upload Sales Report", type=["csv", "xlsx"], key="sales_report_uploader", label_visibility="collapsed")

            # Process sales report upload
            if sales_report_uploaded is not None:
                # Create a unique key for this file to avoid reprocessing the same file
                file_key = f"{sales_report_uploaded.name}_{sales_report_uploaded.size}"
                
                # Only process if this is a new file
                if st.session_state.get('last_sales_file_key') != file_key:
                    try:
                        with st.spinner('Processing sales report...'):
                            # Clear caches when new data is uploaded
                            clear_caches()
                            st.session_state.last_cache_refresh = datetime.now()
                            processed_data = process_sales_report(sales_report_uploaded)
                            
                            # Show detailed debugging info
                            if processed_data is not None:
                                st.write(f"DEBUG: Processed data returned with {len(processed_data)} rows and {list(processed_data.columns)} columns")
                                
                                # Extract ASINs and titles from the sales report
                                sales_asins_with_titles = extract_asins_from_sales_report(processed_data)
                                
                                # Get existing branded ASINs
                                existing_branded_asins = set()
                                if 'branded_asins_data' in st.session_state.client_config:
                                    existing_branded_asins = set(str(asin).upper() for asin in st.session_state.client_config['branded_asins_data'].keys())
                                
                                # Find new ASINs that aren't already in branded_asins_data
                                new_sales_asins = set(sales_asins_with_titles.keys()) - existing_branded_asins
                                
                                # Filter the dictionary to only include new ASINs
                                new_sales_asins_dict = {asin: sales_asins_with_titles[asin] for asin in new_sales_asins}
                                
                                # Store new ASINs in session state for user confirmation
                                if new_sales_asins:
                                    st.session_state.new_sales_asins = list(new_sales_asins)
                                    st.session_state.new_sales_asins_titles = new_sales_asins_dict
                                    st.session_state.show_new_sales_asins_prompt = True
                                    st.session_state.debug_messages.append(f"Found {len(new_sales_asins)} new ASINs from sales report")
                                else:
                                    st.session_state.new_sales_asins = []
                                    st.session_state.new_sales_asins_titles = {}
                                    st.session_state.show_new_sales_asins_prompt = False
                                    
                                # Also update titles for existing ASINs if better titles are available
                                if 'branded_asins_data' in st.session_state.client_config:
                                    for asin, title in sales_asins_with_titles.items():
                                        if asin in st.session_state.client_config['branded_asins_data'] and title != 'Title not available':
                                            current_title = st.session_state.client_config['branded_asins_data'][asin].get('product_title', '')
                                            if current_title == 'Title not available' or not current_title:
                                                st.session_state.client_config['branded_asins_data'][asin]['product_title'] = title
                                                st.session_state.debug_messages.append(f"Updated title for existing ASIN {asin}")
                            else:
                                st.write("DEBUG: Processed data is None")
                                
                            st.session_state.sales_report_data = processed_data
                            st.session_state.last_sales_file_key = file_key
                            
                        # Show success/error message based on processing result
                        if processed_data is not None and not processed_data.empty:
                            st.success('âœ“ Sales report processed successfully')
                        else:
                            st.warning('Sales report processed, but no data was extracted.')
                            
                    except Exception as e:
                        st.error(f"Error processing sales report: {e}")
                else:
                    # File already processed, just show success message
                    if st.session_state.sales_report_data is not None and not st.session_state.sales_report_data.empty:
                        st.success('âœ“ Sales report processed successfully')
                    else:
                        st.warning('Sales report processed, but no data was extracted.')
                
            st.markdown("Can be either Vendor Central Sales Report or Seller Central Business Reports By Child ASIN")
            st.markdown("Used for the following:")
            st.markdown("* Client Overview - Calculating Total Sales, TACoS, Sessions, etc.")
            st.markdown("* Client Settings Center - Updating Advertised ASINs")
            st.markdown("* Client Settings Center - Product Tagging")
        
        # Add navigation button to Advertising Audit when both files are uploaded
        if st.session_state.bulk_data is not None and st.session_state.sales_report_data is not None:
            st.markdown("---")
            st.markdown("### âœ… Files Ready for Analysis")
            st.markdown("Both advertising data and sales report have been uploaded successfully!")
            col_nav1, col_nav2, col_nav3 = st.columns([1, 2, 1])
            with col_nav2:
                if st.button("ðŸ“Š Go to Advertising Audit", key="nav_to_audit_both", use_container_width=True, type="primary"):
                    st.session_state.current_page = "advertising_audit"
                    st.rerun()
        elif st.session_state.bulk_data is not None or st.session_state.sales_report_data is not None:
            st.markdown("---")
            st.info("ðŸ’¡ Upload both files to access the full Advertising Audit analysis")
            col_nav1, col_nav2, col_nav3 = st.columns([1, 2, 1])
            with col_nav2:
                if st.button("ðŸ“Š View Advertising Audit", key="nav_to_audit_partial", use_container_width=True):
                    st.session_state.current_page = "advertising_audit"
                    st.rerun()
        
        # Display uploaded data viewer section
        st.markdown("---")
        
        # --- Show prompt for newly found ASINs (from sales report) ABOVE File Structure Checker ---
        # Add debug information to help troubleshoot
        st.session_state.debug_messages.append(f"show_new_sales_asins_prompt: {st.session_state.get('show_new_sales_asins_prompt', False)}")
        st.session_state.debug_messages.append(f"new_sales_asins count: {len(st.session_state.get('new_sales_asins', []))}")
        
        if st.session_state.get('show_new_sales_asins_prompt', False) and st.session_state.get('new_sales_asins', []):
            new_asins_count = len(st.session_state.new_sales_asins)
            st.warning(f"{new_asins_count} new Branded ASINs found in sales report. Add to Branded ASINs?")
            
            col1, col2 = st.columns(2)
            with col1:
                if st.button("Yes, add them", key="add_new_sales_asins_btn_1"):
                    # Initialize branded_asins_data if not present
                    if 'branded_asins_data' not in st.session_state.client_config:
                        st.session_state.client_config['branded_asins_data'] = {}
                    # Add new ASINs to branded_asins_data with their titles
                    added_count = 0
                    for asin in st.session_state.new_sales_asins:
                        # Standardize ASIN format
                        asin = str(asin).strip().upper()
                        if asin not in st.session_state.client_config['branded_asins_data']:
                            title = st.session_state.new_sales_asins_titles.get(asin, 'Title not available')
                            if isinstance(title, str) and title.strip() and title.lower() != 'nan':
                                title = title.strip()
                            else:
                                title = 'Title not available'
                            st.session_state.client_config['branded_asins_data'][asin] = {
                                'product_title': title,
                                'product_group': ''
                            }
                            added_count += 1
                    # Update the success message with the actual count of added ASINs
                    new_asins_count = added_count
                    # Save the updated config
                    save_client_config(st.session_state.selected_client_name, st.session_state.client_config)
                    # Clear the prompt flags
                    st.session_state.show_new_sales_asins_prompt = False
                    st.session_state.new_sales_asins = []
                    st.session_state.new_sales_asins_titles = {}
                    st.success(f"Added {new_asins_count} new ASINs to Branded ASINs list")
                    st.rerun()
            with col2:
                if st.button("No, ignore", key="ignore_new_sales_asins_btn_1"):
                    # Clear the prompt flags
                    st.session_state.show_new_sales_asins_prompt = False
                    st.session_state.new_sales_asins = []
                    st.rerun()
        # --- End ASIN prompt section ---
        # File Structure Checker section for uploaded files
        st.markdown("""
        <h3 style='font-family:"Inter","Roboto","Segoe UI",Arial,sans-serif; font-size:1.3rem; font-weight:600; color:#BFA23A; margin-top:1.5rem; margin-bottom:1.0rem;'>
            File Structure Checker
        </h3>""", unsafe_allow_html=True)
        st.markdown("This section allows you to verify the structure and columns of your uploaded files to ensure they're compatible with the dashboard.")
        
        # Create two columns for the two types of data
        data_col1, data_col2 = st.columns(2)
        
        with data_col1:
            st.markdown("### Bulk Advertising File Structure")
            
            # Show bulk data structure if available
            if 'bulk_data' in st.session_state and st.session_state.get('bulk_data') is not None:
                if isinstance(st.session_state.bulk_data, dict):
                    # Check for Search Term Reports
                    sp_search_term_sheet = None
                    sb_search_term_sheet = None
                    
                    # Look for sheets with 'Customer Search Term' or 'Search Term' columns
                    # Add debug information
                    st.session_state.debug_messages.append(f"Searching for search term data in {len(st.session_state.bulk_data)} sheets")
                    
                    # First pass: Identify all sheets with search term columns
                    search_term_sheets = []
                    for sheet_name, df in st.session_state.bulk_data.items():
                        if isinstance(df, pd.DataFrame) and not df.empty:
                            st.session_state.debug_messages.append(f"Examining sheet: {sheet_name} with {len(df)} rows and {len(df.columns)} columns")
                            
                            # Log all columns for debugging
                            st.session_state.debug_messages.append(f"Columns in {sheet_name}: {', '.join(df.columns)}")
                            
                            # Check for search term columns - with improved case-insensitive matching
                            has_search_term = False
                            search_term_col = None
                            
                            # Create a lowercase mapping of column names for case-insensitive matching
                            column_map = {col.lower(): col for col in df.columns}
                            
                            # Check for both 'search term' and 'customer search term' (case-insensitive)
                            if 'search term' in column_map:
                                search_term_col = column_map['search term']
                                has_search_term = True
                                st.session_state.debug_messages.append(f"Found 'Search Term' column (as '{search_term_col}') in sheet {sheet_name}")
                            elif 'customer search term' in column_map:
                                search_term_col = column_map['customer search term']
                                has_search_term = True
                                st.session_state.debug_messages.append(f"Found 'Customer Search Term' column (as '{search_term_col}') in sheet {sheet_name}")
                            
                            # If we found a search term column, add this sheet to our list
                            if has_search_term:
                                st.session_state.debug_messages.append(f"Adding sheet {sheet_name} to search term sheets list with column '{search_term_col}'")
                                search_term_sheets.append((sheet_name, df, search_term_col))
                    
                    # Log how many search term sheets we found
                    st.session_state.debug_messages.append(f"Found {len(search_term_sheets)} sheets with search term columns")
                    
                    # Second pass: Classify each search term sheet as SP or SB
                    for sheet_name, df, search_term_col in search_term_sheets:
                        # Determine if it's SP or SB based on sheet name or content
                        campaign_type = None
                        
                        # First check if 'Product' column exists
                        if 'Product' in df.columns:
                            st.session_state.debug_messages.append(f"Sheet {sheet_name} has 'Product' column. Checking values.")
                            
                            # Get unique values in the Product column - handle case sensitivity properly
                            try:
                                # First try to get the actual values
                                product_values = df['Product'].dropna().astype(str).unique()
                                product_values_str = ', '.join([str(val) for val in product_values[:5]])
                                st.session_state.debug_messages.append(f"Sample Product values: {product_values_str}")
                                
                                # Check for exact matches first
                                has_sp = any(val == 'Sponsored Products' for val in product_values)
                                has_sb = any(val == 'Sponsored Brands' for val in product_values)
                                
                                # If exact matches found, use them
                                if has_sp:
                                    campaign_type = 'Sponsored Products'
                                    st.session_state.debug_messages.append(f"Found exact match 'Sponsored Products' in Product column")
                                elif has_sb:
                                    campaign_type = 'Sponsored Brands'
                                    st.session_state.debug_messages.append(f"Found exact match 'Sponsored Brands' in Product column")
                                else:
                                    # If no exact matches, try case-insensitive search
                                    for product in product_values:
                                        product_lower = product.lower()
                                        if 'sponsored product' in product_lower:
                                            campaign_type = 'Sponsored Products'
                                            st.session_state.debug_messages.append(f"Found 'Sponsored Products' in Product column value: {product}")
                                            break
                                        elif 'sponsored brand' in product_lower:
                                            campaign_type = 'Sponsored Brands'
                                            st.session_state.debug_messages.append(f"Found 'Sponsored Brands' in Product column value: {product}")
                                            break
                            except Exception as e:
                                st.session_state.debug_messages.append(f"Error checking Product column: {str(e)}")
                                # Continue with other detection methods
                        
                        # If campaign type couldn't be determined from Product column, try to infer from sheet name
                        if campaign_type is None:
                            st.session_state.debug_messages.append(f"Could not determine campaign type from Product column for sheet {sheet_name}. Trying sheet name.")
                            sheet_name_lower = sheet_name.lower()
                            
                            if any(term in sheet_name_lower for term in ['product', 'sp', 'sponsored product']):
                                campaign_type = 'Sponsored Products'
                                st.session_state.debug_messages.append(f"Inferred 'Sponsored Products' from sheet name {sheet_name}")
                            elif any(term in sheet_name_lower for term in ['brand', 'sb', 'sponsored brand']):
                                campaign_type = 'Sponsored Brands'
                                st.session_state.debug_messages.append(f"Inferred 'Sponsored Brands' from sheet name {sheet_name}")
                        
                        # If still no campaign type, check for other columns that might indicate the type
                        if campaign_type is None and 'Campaign Type' in df.columns:
                            campaign_types = df['Campaign Type'].dropna().astype(str).unique()
                            campaign_types_str = ', '.join([str(val) for val in campaign_types[:5]])
                            st.session_state.debug_messages.append(f"Checking Campaign Type column values: {campaign_types_str}")
                            
                            for ct in campaign_types:
                                ct_lower = ct.lower()
                                if 'sponsored product' in ct_lower or 'sp' == ct_lower:
                                    campaign_type = 'Sponsored Products'
                                    st.session_state.debug_messages.append(f"Found 'Sponsored Products' in Campaign Type column")
                                    break
                                elif 'sponsored brand' in ct_lower or 'sb' == ct_lower:
                                    campaign_type = 'Sponsored Brands'
                                    st.session_state.debug_messages.append(f"Found 'Sponsored Brands' in Campaign Type column")
                                    break
                        
                        # Check for campaign name patterns if available
                        if campaign_type is None and 'Campaign' in df.columns:
                            campaign_names = df['Campaign'].dropna().astype(str).unique()
                            campaign_names_str = ', '.join([str(val) for val in campaign_names[:5]])
                            st.session_state.debug_messages.append(f"Checking Campaign column values: {campaign_names_str}")
                            
                            for name in campaign_names:
                                name_lower = name.lower()
                                if 'sp' in name_lower.split() or 'sponsored product' in name_lower:
                                    campaign_type = 'Sponsored Products'
                                    st.session_state.debug_messages.append(f"Found 'Sponsored Products' pattern in Campaign name")
                                    break
                                elif 'sb' in name_lower.split() or 'sponsored brand' in name_lower:
                                    campaign_type = 'Sponsored Brands'
                                    st.session_state.debug_messages.append(f"Found 'Sponsored Brands' pattern in Campaign name")
                                    break
                        
                        # As a last resort, check all other columns for campaign type indicators
                        if campaign_type is None and len(df) > 0:
                            st.session_state.debug_messages.append(f"Still could not determine campaign type. Examining all columns for clues.")
                            
                            # Check all columns for any indicators
                            for col in df.columns:
                                if col not in ['Search Term', 'Customer Search Term', search_term_col]:  # Skip the search term column
                                    try:
                                        col_values = df[col].dropna().astype(str).head(20).tolist()  # Check just first 20 values
                                        col_text = ' '.join(col_values).lower()
                                        
                                        if 'sponsored product' in col_text or ' sp ' in col_text:
                                            campaign_type = 'Sponsored Products'
                                            st.session_state.debug_messages.append(f"Found 'Sponsored Products' indicator in column {col}")
                                            break
                                        elif 'sponsored brand' in col_text or ' sb ' in col_text:
                                            campaign_type = 'Sponsored Brands'
                                            st.session_state.debug_messages.append(f"Found 'Sponsored Brands' indicator in column {col}")
                                            break
                                    except:
                                        # Skip columns that can't be processed
                                        continue
                        
                        # If we still can't determine, use a heuristic based on column structure
                        if campaign_type is None:
                            # Check for columns that are more common in SP vs SB reports
                            sp_indicator_cols = ['Match Type', 'Keyword', 'Target', 'Targeting']  
                            sb_indicator_cols = ['Keyword Status', 'Creative', 'Headline']
                            
                            sp_score = sum(1 for col in sp_indicator_cols if col in df.columns)
                            sb_score = sum(1 for col in sb_indicator_cols if col in df.columns)
                            
                            if sp_score > sb_score:
                                campaign_type = 'Sponsored Products'
                                st.session_state.debug_messages.append(f"Determined 'Sponsored Products' based on column structure (SP score: {sp_score}, SB score: {sb_score})")
                            elif sb_score > sp_score:
                                campaign_type = 'Sponsored Brands'
                                st.session_state.debug_messages.append(f"Determined 'Sponsored Brands' based on column structure (SP score: {sp_score}, SB score: {sb_score})")
                            else:
                                # Default to Sponsored Products if we still can't determine
                                campaign_type = 'Sponsored Products'
                                st.session_state.debug_messages.append(f"Could not determine campaign type for sheet {sheet_name}. Defaulting to Sponsored Products.")
                        
                        # Assign to appropriate variable based on campaign type
                        if campaign_type == 'Sponsored Products':
                            sp_search_term_sheet = (sheet_name, df, search_term_col)
                            st.session_state.debug_messages.append(f"âœ… Assigned sheet {sheet_name} as Sponsored Products search term data")
                        elif campaign_type == 'Sponsored Brands':
                            sb_search_term_sheet = (sheet_name, df, search_term_col)
                            st.session_state.debug_messages.append(f"âœ… Assigned sheet {sheet_name} as Sponsored Brands search term data")
                    # Create tabs for different sheets
                    sheet_names = list(st.session_state.bulk_data.keys())
                    if sheet_names:
                        st.success(f"âœ… Bulk file processed successfully with {len(sheet_names)} sheets")
                        
                        # Display SP Search Term Report section if found
                        if sp_search_term_sheet:
                            sheet_name, df, search_term_col = sp_search_term_sheet
                            with st.expander(f"ðŸ” SP Search Term Report ({len(df)} rows)", expanded=False):
                                st.success(f"âœ… Sponsored Products Search Term data found in sheet '{sheet_name}'")
                                st.markdown(f"**Search Term Column:** '{search_term_col}'")
                                
                                # Show column information
                                st.markdown("**Column Structure:**")
                                
                                # Create a dataframe with column information
                                col_info = []
                                required_cols = {
                                    search_term_col: ['customer search query', 'required'],
                                    'Campaign': ['campaign identification', 'required'],
                                    'Ad Group': ['ad group identification', 'recommended'],
                                    'Targeting': ['keyword targeting', 'recommended'],
                                    'Match Type': ['match type information', 'recommended'],
                                    'Impressions': ['performance metric', 'required'],
                                    'Clicks': ['performance metric', 'required'],
                                    'Spend': ['performance metric', 'required'],
                                    'Sales': ['performance metric', 'required'],
                                    'Orders': ['performance metric', 'recommended']
                                }
                                
                                for col in df.columns:
                                    data_sample = str(df[col].iloc[0]) if not df.empty else 'N/A'
                                    if len(data_sample) > 50:
                                        data_sample = data_sample[:47] + '...'
                                    
                                    status = 'âœ… Present' 
                                    if col in required_cols:
                                        purpose, importance = required_cols[col]
                                        if importance == 'required':
                                            status = 'âœ… Required - Present'
                                    else:
                                        purpose = 'additional data'
                                        
                                    col_info.append({
                                        'Column Name': col,
                                        'Data Type': str(df[col].dtype),
                                        'Purpose': purpose,
                                        'Sample Data': data_sample,
                                        'Status': status
                                    })
                                
                                # Check for missing required columns
                                for req_col, (purpose, importance) in required_cols.items():
                                    if importance == 'required' and req_col not in df.columns:
                                        # Check for alternative column names for Sales
                                        found = False
                                        if req_col == 'Sales':
                                            for alt in ['Ad Sales', 'Sales (Views & Clicks)']:
                                                if alt in df.columns:
                                                    found = True
                                                    break
                                        
                                        if not found:
                                            col_info.append({
                                                'Column Name': req_col,
                                                'Data Type': 'N/A',
                                                'Purpose': purpose,
                                                'Sample Data': 'N/A',
                                                'Status': 'âš ï¸ Required - Missing'
                                            })
                                
                                # Display the column information
                                col_df = pd.DataFrame(col_info)
                                st.dataframe(col_df, use_container_width=True)
                                
                                # Show sample data
                                st.markdown("**Sample Search Terms:**")
                                if len(df) > 0:
                                    sample_df = df.head(5)[[search_term_col, 'Impressions', 'Clicks', 'Spend', 'Orders']].copy() if all(col in df.columns for col in ['Impressions', 'Clicks', 'Spend', 'Orders']) else df.head(5)[[search_term_col]].copy()
                                    st.dataframe(sample_df, use_container_width=True)
                        else:
                            st.warning("âš ï¸ No Sponsored Products Search Term data found in the bulk file")
                            st.markdown("**Required Column:** 'Customer Search Term' or 'Search Term'")
                            st.markdown("Make sure your bulk file includes the Search Term Report for Sponsored Products campaigns.")
                        
                        # Display SB Search Term Report section if found
                        if sb_search_term_sheet:
                            sheet_name, df, search_term_col = sb_search_term_sheet
                            with st.expander(f"ðŸ” SB Search Term Report ({len(df)} rows)", expanded=False):
                                st.success(f"âœ… Sponsored Brands Search Term data found in sheet '{sheet_name}'")
                                st.markdown(f"**Search Term Column:** '{search_term_col}'")
                                
                                # Show column information
                                st.markdown("**Column Structure:**")
                                
                                # Create a dataframe with column information
                                col_info = []
                                required_cols = {
                                    search_term_col: ['customer search query', 'required'],
                                    'Campaign': ['campaign identification', 'required'],
                                    'Impressions': ['performance metric', 'required'],
                                    'Clicks': ['performance metric', 'required'],
                                    'Spend': ['performance metric', 'required'],
                                    'Sales': ['performance metric', 'required'],
                                    'Orders': ['performance metric', 'recommended']
                                }
                                
                                for col in df.columns:
                                    data_sample = str(df[col].iloc[0]) if not df.empty else 'N/A'
                                    if len(data_sample) > 50:
                                        data_sample = data_sample[:47] + '...'
                                    
                                    status = 'âœ… Present' 
                                    if col in required_cols:
                                        purpose, importance = required_cols[col]
                                        if importance == 'required':
                                            status = 'âœ… Required - Present'
                                    else:
                                        purpose = 'additional data'
                                        
                                    col_info.append({
                                        'Column Name': col,
                                        'Data Type': str(df[col].dtype),
                                        'Purpose': purpose,
                                        'Sample Data': data_sample,
                                        'Status': status
                                    })
                                
                                # Check for missing required columns
                                for req_col, (purpose, importance) in required_cols.items():
                                    if importance == 'required' and req_col not in df.columns:
                                        # Check for alternative column names for Sales
                                        found = False
                                        if req_col == 'Sales':
                                            for alt in ['Ad Sales', 'Sales (Views & Clicks)']:
                                                if alt in df.columns:
                                                    found = True
                                                    break
                                        
                                        if not found:
                                            col_info.append({
                                                'Column Name': req_col,
                                                'Data Type': 'N/A',
                                                'Purpose': purpose,
                                                'Sample Data': 'N/A',
                                                'Status': 'âš ï¸ Required - Missing'
                                            })
                                
                                # Display the column information
                                col_df = pd.DataFrame(col_info)
                                st.dataframe(col_df, use_container_width=True)
                                
                                # Show sample data
                                st.markdown("**Sample Search Terms:**")
                                if len(df) > 0:
                                    sample_df = df.head(5)[[search_term_col, 'Impressions', 'Clicks', 'Spend', 'Orders']].copy() if all(col in df.columns for col in ['Impressions', 'Clicks', 'Spend', 'Orders']) else df.head(5)[[search_term_col]].copy()
                                    st.dataframe(sample_df, use_container_width=True)
                        else:
                            st.warning("âš ï¸ No Sponsored Brands Search Term data found in the bulk file")
                            st.markdown("**Required Column:** 'Customer Search Term' or 'Search Term'")
                            st.markdown("Make sure your bulk file includes the Search Term Report for Sponsored Brands campaigns.")
                        
                        
                        # Track which sheets have already been displayed to avoid duplicates
                        displayed_sheets = set()
                        if sp_search_term_sheet:
                            displayed_sheets.add(sp_search_term_sheet[0])  # Add SP search term sheet name
                        if sb_search_term_sheet:
                            displayed_sheets.add(sb_search_term_sheet[0])  # Add SB search term sheet name
                            
                        # Display remaining sheets that haven't been shown yet
                        for sheet_name in sheet_names:
                            # Skip sheets that have already been displayed as search term reports
                            if sheet_name in displayed_sheets:
                                continue
                                
                            df = st.session_state.bulk_data[sheet_name]
                            if isinstance(df, pd.DataFrame) and not df.empty:
                                with st.expander(f"ðŸ“‹ {sheet_name} ({len(df)} rows)", expanded=False):
                                    # Show column information
                                    st.markdown("**Column Structure:**")
                                    
                                    # Create a dataframe with column information
                                    col_info = []
                                    required_cols = {
                                        'Campaign Name': ['campaign identification', 'required'],
                                        'Ad Group': ['ad group identification', 'required'],
                                        'Targeting': ['keyword or product targeting', 'recommended'],
                                        'Match Type': ['match type information', 'recommended'],
                                        'Impressions': ['performance metric', 'required'],
                                        'Clicks': ['performance metric', 'required'],
                                        'Spend': ['performance metric', 'required'],
                                        'Sales': ['performance metric', 'required'],
                                        'Orders': ['performance metric', 'recommended'],
                                        'ACoS': ['performance metric', 'recommended']
                                    }
                                    
                                    for col in df.columns:
                                        data_sample = str(df[col].iloc[0]) if not df.empty else 'N/A'
                                        if len(data_sample) > 50:
                                            data_sample = data_sample[:47] + '...'
                                        
                                        status = 'âœ… Present' 
                                        if col in required_cols:
                                            purpose, importance = required_cols[col]
                                            if importance == 'required':
                                                status = 'âœ… Required - Present'
                                        else:
                                            purpose = 'additional data'
                                            
                                        col_info.append({
                                            'Column Name': col,
                                            'Data Type': str(df[col].dtype),
                                            'Purpose': purpose,
                                            'Sample Data': data_sample,
                                            'Status': status
                                        })
                                    
                                    # Check for missing required columns
                                    for req_col, (purpose, importance) in required_cols.items():
                                        if importance == 'required' and req_col not in df.columns:
                                            # Check for alternative column names
                                            found = False
                                            if req_col == 'Sales':
                                                for alt in ['Ad Sales', 'Sales (Views & Clicks)']:
                                                    if alt in df.columns:
                                                        found = True
                                                        break
                                            
                                            if not found:
                                                col_info.append({
                                                    'Column Name': req_col,
                                                    'Data Type': 'N/A',
                                                    'Purpose': purpose,
                                                    'Sample Data': 'N/A',
                                                    'Status': 'âš ï¸ Required - Missing'
                                                })
                                    
                                    # Display the column information
                                    col_df = pd.DataFrame(col_info)
                                    st.dataframe(col_df, use_container_width=True)
                                    
                                    # Show data structure validation
                                    st.markdown("**Data Structure Validation:**")
                                    validations = []
                                    
                                    # Check for campaign name consistency
                                    if 'Campaign Name' in df.columns:
                                        campaign_count = len(df['Campaign Name'].unique())
                                        validations.append({
                                            'Check': 'Campaign Names',
                                            'Result': f'{campaign_count} unique campaigns found',
                                            'Status': 'âœ… Valid' if campaign_count > 0 else 'âš ï¸ No campaigns found'
                                        })
                                    
                                    # Check for numeric data in performance metrics
                                    for metric in ['Impressions', 'Clicks', 'Spend', 'Sales', 'Orders']:
                                        if metric in df.columns:
                                            try:
                                                # Try to convert to numeric
                                                numeric_col = pd.to_numeric(df[metric].astype(str).str.replace('[$,%]', '', regex=True), errors='coerce')
                                                valid_count = numeric_col.notna().sum()
                                                valid_percent = (valid_count / len(df)) * 100 if len(df) > 0 else 0
                                                
                                                validations.append({
                                                    'Check': f'{metric} Data',
                                                    'Result': f'{valid_count}/{len(df)} rows have valid numeric data ({valid_percent:.1f}%)',
                                                    'Status': 'âœ… Valid' if valid_percent > 90 else 'âš ï¸ Some invalid data'
                                                })
                                            except:
                                                validations.append({
                                                    'Check': f'{metric} Data',
                                                    'Result': 'Could not validate numeric data',
                                                    'Status': 'âš ï¸ Validation failed'
                                                })
                                    
                                    # Display validation results
                                    st.dataframe(pd.DataFrame(validations), use_container_width=True)
                            else:
                                st.warning(f"Sheet '{sheet_name}' is empty or not a valid DataFrame")
                    else:
                        st.warning("Bulk file processed but no valid sheets found")
                else:
                    st.warning("Bulk data is not in the expected format (dictionary of DataFrames)")
            else:
                st.info("No bulk advertising data has been uploaded yet.")
        
        with data_col2:
            st.markdown("### Sales Report File Structure")
            
            # Show sales report structure if available
            if st.session_state.get('sales_report_data') is not None:
                sales_df = st.session_state.sales_report_data
                if isinstance(sales_df, pd.DataFrame):
                    st.success(f"âœ… Sales report processed successfully with {len(sales_df)} rows and {len(sales_df.columns)} columns")
                    
                    with st.expander("ðŸ“‹ Sales Report Structure", expanded=False):
                        # Show column information
                        st.markdown("**Column Structure:**")
                        
                        # Create a dataframe with column information
                        col_info = []
                        required_cols = {
                            'ASIN': ['product identification', 'required'],
                            'SKU': ['seller SKU', 'recommended'],
                            'Product Title': ['product name', 'recommended'],
                            'Ordered Product Sales': ['sales amount', 'required'],
                            'Units Ordered': ['quantity sold', 'required'],
                            'Sessions': ['traffic data', 'recommended'],
                            'Session Percentage': ['traffic attribution', 'optional']
                        }
                        
                        # Check for alternative column names
                        sales_col_alternatives = {
                            'Ordered Product Sales': ['Total Sales', 'Sales', 'Revenue'],
                            'Units Ordered': ['Units Sold', 'Quantity', 'Units'],
                            'Product Title': ['Title', 'Item Name', 'Product Name']
                        }
                        
                        # Map actual columns to standard names
                        column_mapping = {}
                        for std_col, alternatives in sales_col_alternatives.items():
                            if std_col in sales_df.columns:
                                column_mapping[std_col] = std_col
                            else:
                                for alt in alternatives:
                                    if alt in sales_df.columns:
                                        column_mapping[std_col] = alt
                                        break
                        
                        for col in sales_df.columns:
                            data_sample = str(sales_df[col].iloc[0]) if not sales_df.empty else 'N/A'
                            if len(data_sample) > 50:
                                data_sample = data_sample[:47] + '...'
                            
                            # Determine if this column is a known required/recommended column or an alternative name
                            mapped_name = None
                            for std_col, alt_col in column_mapping.items():
                                if col == alt_col:
                                    mapped_name = std_col
                                    break
                            
                            if col in required_cols:
                                purpose, importance = required_cols[col]
                                status = 'âœ… Required - Present' if importance == 'required' else 'âœ… Recommended - Present'
                            elif mapped_name and mapped_name in required_cols:
                                purpose, importance = required_cols[mapped_name]
                                status = f'âœ… Alternative for {mapped_name}'
                            else:
                                purpose = 'additional data'
                                status = 'âœ… Present'
                                
                            col_info.append({
                                'Column Name': col,
                                'Data Type': str(sales_df[col].dtype),
                                'Purpose': purpose,
                                'Sample Data': data_sample,
                                'Status': status
                            })
                        
                        # Check for missing required columns
                        for req_col, (purpose, importance) in required_cols.items():
                            if importance == 'required' and req_col not in sales_df.columns and req_col not in column_mapping:
                                col_info.append({
                                    'Column Name': req_col,
                                    'Data Type': 'N/A',
                                    'Purpose': purpose,
                                    'Sample Data': 'N/A',
                                    'Status': 'âš ï¸ Required - Missing'
                                })
                        
                        # Display the column information
                        col_df = pd.DataFrame(col_info)
                        st.dataframe(col_df, use_container_width=True)
                        
                        # Show data structure validation
                        st.markdown("**Data Structure Validation:**")
                        validations = []
                        
                        # Check for ASIN format
                        asin_col = None
                        for col in sales_df.columns:
                            if col.upper() == 'ASIN' or 'ASIN' in col.upper():
                                asin_col = col
                                break
                        
                        if asin_col:
                            valid_asins = sales_df[asin_col].astype(str).str.match(r'^[A-Z0-9]{10}$').sum()
                            valid_percent = (valid_asins / len(sales_df)) * 100 if len(sales_df) > 0 else 0
                            validations.append({
                                'Check': 'ASIN Format',
                                'Result': f'{valid_asins}/{len(sales_df)} rows have valid ASIN format ({valid_percent:.1f}%)',
                                'Status': 'âœ… Valid' if valid_percent > 90 else 'âš ï¸ Some invalid ASINs'
                            })
                        
                        # Check for numeric data in sales metrics
                        for metric_std, alternatives in sales_col_alternatives.items():
                            if metric_std in ['Ordered Product Sales', 'Units Ordered']:
                                # Find the actual column name
                                metric_col = None
                                if metric_std in sales_df.columns:
                                    metric_col = metric_std
                                else:
                                    for alt in alternatives:
                                        if alt in sales_df.columns:
                                            metric_col = alt
                                            break
                                
                                if metric_col:
                                    try:
                                        # Try to convert to numeric
                                        numeric_col = pd.to_numeric(sales_df[metric_col].astype(str).str.replace('[$,%]', '', regex=True), errors='coerce')
                                        valid_count = numeric_col.notna().sum()
                                        valid_percent = (valid_count / len(sales_df)) * 100 if len(sales_df) > 0 else 0
                                        
                                        validations.append({
                                            'Check': f'{metric_std} Data',
                                            'Result': f'{valid_count}/{len(sales_df)} rows have valid numeric data ({valid_percent:.1f}%)',
                                            'Status': 'âœ… Valid' if valid_percent > 90 else 'âš ï¸ Some invalid data'
                                        })
                                    except:
                                        validations.append({
                                            'Check': f'{metric_std} Data',
                                            'Result': 'Could not validate numeric data',
                                            'Status': 'âš ï¸ Validation failed'
                                        })
                        
                        # Display validation results
                        st.dataframe(pd.DataFrame(validations), use_container_width=True)
                else:
                    st.warning("Sales report data is not in the expected format (DataFrame)")
            else:
                st.info("No sales report data has been uploaded yet.")
        
        # Show any targeting data structure if available
        if ('branded_targets_df' in st.session_state and st.session_state.get('branded_targets_df') is not None) or \
           ('non_branded_targets_df' in st.session_state and st.session_state.get('non_branded_targets_df') is not None):
            st.markdown("### Targeting Data Structure")
            
            targeting_tabs = st.tabs(["Branded Targeting", "Non-Branded Targeting"])
            
            with targeting_tabs[0]:
                if 'branded_targets_df' in st.session_state and st.session_state.get('branded_targets_df') is not None:
                    branded_df = st.session_state.branded_targets_df
                    if isinstance(branded_df, pd.DataFrame):
                        st.success(f"âœ… Branded targeting data processed successfully with {len(branded_df)} rows")
                        
                        with st.expander("ðŸ“‹ Branded Targeting Structure", expanded=False):
                            # Show column information
                            st.markdown("**Column Structure:**")
                            col_info = []
                            
                            for col in branded_df.columns:
                                data_sample = str(branded_df[col].iloc[0]) if not branded_df.empty else 'N/A'
                                if len(data_sample) > 50:
                                    data_sample = data_sample[:47] + '...'
                                
                                col_info.append({
                                    'Column Name': col,
                                    'Data Type': str(branded_df[col].dtype),
                                    'Sample Data': data_sample
                                })
                            
                            st.dataframe(pd.DataFrame(col_info), use_container_width=True)
                    else:
                        st.warning("Branded targeting data is not in the expected format (DataFrame)")
                else:
                    st.info("No branded targeting data available.")
            
            with targeting_tabs[1]:
                if 'non_branded_targets_df' in st.session_state and st.session_state.get('non_branded_targets_df') is not None:
                    non_branded_df = st.session_state.non_branded_targets_df
                    if isinstance(non_branded_df, pd.DataFrame):
                        st.success(f"âœ… Non-branded targeting data processed successfully with {len(non_branded_df)} rows")
                        
                        with st.expander("ðŸ“‹ Non-Branded Targeting Structure", expanded=False):
                            # Show column information
                            st.markdown("**Column Structure:**")
                            col_info = []
                            
                            for col in non_branded_df.columns:
                                data_sample = str(non_branded_df[col].iloc[0]) if not non_branded_df.empty else 'N/A'
                                if len(data_sample) > 50:
                                    data_sample = data_sample[:47] + '...'
                                
                                col_info.append({
                                    'Column Name': col,
                                    'Data Type': str(non_branded_df[col].dtype),
                                    'Sample Data': data_sample
                                })
                            
                            st.dataframe(pd.DataFrame(col_info), use_container_width=True)
                    else:
                        st.warning("Non-branded targeting data is not in the expected format (DataFrame)")
                else:
                    st.info("No non-branded targeting data available.")
        
        # Display ASIN prompts in the File Upload page
        st.markdown("---")
        
        # Check if we need to show the new branded ASINs prompt from bulk advertising file
        if st.session_state.get('show_new_asins_prompt', False) and st.session_state.get('new_branded_asins', []):
            new_asins_count = len(st.session_state.new_branded_asins)
            st.warning(f"{new_asins_count} new Branded ASINs found in bulk advertising file. Add to Branded ASINs?")
            
            col1, col2 = st.columns(2)
            with col1:
                if st.button("Yes, add them", key="add_new_asins_btn"):
                    # Initialize branded_asins_data if not present
                    if 'branded_asins_data' not in st.session_state.client_config:
                        st.session_state.client_config['branded_asins_data'] = {}
                    
                    # Add new ASINs to branded_asins_data
                    for asin in st.session_state.new_branded_asins:
                        if asin not in st.session_state.client_config['branded_asins_data']:
                            # Get title from sales report if available
                            title = st.session_state.new_branded_asins_titles.get(asin, 'Title not available')
                            
                            st.session_state.client_config['branded_asins_data'][asin] = {
                                'product_title': title,
                                'product_group': ''
                            }
                    
                    # Save the updated config
                    save_client_config(st.session_state.selected_client_name, st.session_state.client_config)
                    
                    # Clear the prompt flags
                    st.session_state.show_new_asins_prompt = False
                    st.session_state.new_branded_asins = []
                    
                    st.success(f"Added {new_asins_count} new ASINs to Branded ASINs list")
                    st.rerun()
            
            with col2:
                if st.button("No, ignore", key="ignore_new_asins_btn"):
                    # Clear the prompt flags
                    st.session_state.show_new_asins_prompt = False
                    st.session_state.new_branded_asins = []
                    st.rerun()
        
        # Check if we need to show the new branded ASINs prompt from sales report
        # Add debug information to help troubleshoot
        st.session_state.debug_messages.append(f"show_new_sales_asins_prompt: {st.session_state.get('show_new_sales_asins_prompt', False)}")
        st.session_state.debug_messages.append(f"new_sales_asins count: {len(st.session_state.get('new_sales_asins', []))}")
        
        if st.session_state.get('show_new_sales_asins_prompt', False) and st.session_state.get('new_sales_asins', []):
            new_asins_count = len(st.session_state.new_sales_asins)
            st.warning(f"{new_asins_count} new Branded ASINs found in sales report. Add to Branded ASINs?")
            
            col1, col2 = st.columns(2)
            with col1:
                if st.button("Yes, add them", key="add_new_sales_asins_btn_2"):
                    # Initialize branded_asins_data if not present
                    if 'branded_asins_data' not in st.session_state.client_config:
                        st.session_state.client_config['branded_asins_data'] = {}
                    
                    # Add new ASINs to branded_asins_data with their titles
                    added_count = 0
                    for asin in st.session_state.new_sales_asins:
                        # Standardize ASIN format
                        asin = str(asin).strip().upper()
                        
                        if asin not in st.session_state.client_config['branded_asins_data']:
                            title = st.session_state.new_sales_asins_titles.get(asin, 'Title not available')
                            if isinstance(title, str) and title.strip() and title.lower() != 'nan':
                                title = title.strip()
                            else:
                                title = 'Title not available'
                                
                            st.session_state.client_config['branded_asins_data'][asin] = {
                                'product_title': title,
                                'product_group': ''
                            }
                            added_count += 1
                    
                    # Update the success message with the actual count of added ASINs
                    new_asins_count = added_count
                    
                    # Save the updated config
                    save_client_config(st.session_state.selected_client_name, st.session_state.client_config)
                    
                    # Clear the prompt flags
                    st.session_state.show_new_sales_asins_prompt = False
                    st.session_state.new_sales_asins = []
                    st.session_state.new_sales_asins_titles = {}
                    
                    st.success(f"Added {new_asins_count} new ASINs to Branded ASINs list")
                    st.rerun()
            
            with col2:
                if st.button("No, ignore", key="ignore_new_sales_asins_btn_2"):
                    # Clear the prompt flags
                    st.session_state.show_new_sales_asins_prompt = False
                    st.session_state.new_sales_asins = []
                    st.session_state.new_sales_asins_titles = {}
                    st.rerun()
    
    elif st.session_state.current_page == "client_settings":
        st.markdown("""
    <h2 style='font-family:'Inter','Roboto','Segoe UI',Arial,sans-serif; font-size:2.2rem; font-weight:700; color:#2196f3; margin-top:1.2rem; margin-bottom:1.0rem;'>
        Client Settings Center
    </h2>""", unsafe_allow_html=True)
        
        # Create tabs for different settings
        settings_tab1, settings_tab2, settings_tab3, settings_tab4 = st.tabs(["Branded Terms", "Branded ASINs", "Campaign Tagging", "Data Backup"])
        
        with settings_tab1:
            st.markdown("""
    <h4 style='font-family:'Inter','Roboto','Segoe UI',Arial,sans-serif; font-size:0.98rem; font-weight:600; color:#6c757d; margin-top:1.4rem; margin-bottom:0.7rem;'>
        Branded Terms
    </h4>""", unsafe_allow_html=True)
            # Use a temporary session state variable for edits
            if "branded_terms_area_temp" not in st.session_state:
                st.session_state.branded_terms_area_temp = "\n".join(config.get("branded_keywords", []))

            st.text_area("Enter branded terms (one per line)", 
                         height=200, key="branded_terms_area_temp")

            col_save, col_cancel = st.columns([1,1])
            with col_save:
                if st.button("Save Changes", key="save_branded_terms"):
                    config["branded_keywords"] = [term.strip() for term in st.session_state.branded_terms_area_temp.split("\n") if term.strip()]
                    save_client_config(st.session_state.selected_client_name, config)
                    st.session_state.client_config = config
                    st.success("Branded terms updated.")
            with col_cancel:
                if st.button("Cancel", key="cancel_branded_terms"):
                    # Revert changes by resetting temp area to last saved config
                    st.session_state.branded_terms_area_temp = "\n".join(config.get("branded_keywords", []))
                    st.info("Changes discarded.")

        
        with settings_tab2:
            st.markdown("""
    <h4 style='font-family:'Inter','Roboto','Segoe UI',Arial,sans-serif; font-size:0.98rem; font-weight:600; color:#6c757d; margin-top:1.4rem; margin-bottom:0.7rem;'>
        Branded ASINs
    </h4>""", unsafe_allow_html=True)
            
            # Initialize branded_asins_data if not present in client config
            if 'branded_asins_data' not in st.session_state.client_config:
                st.session_state.client_config['branded_asins_data'] = {}
            
            # Get ASINs from sales report if available
            if 'sales_report_data' in st.session_state and st.session_state.sales_report_data is not None:
                # Debug the available columns and data types
                st.session_state.debug_messages.append(f"Sales report columns: {list(st.session_state.sales_report_data.columns)}")
                
                # Debug the data types of key columns and branded_asins_data status
                if not st.session_state.sales_report_data.empty:
                    st.session_state.debug_messages.append(f"Sales report data types: {st.session_state.sales_report_data.dtypes}")
                    
                # Log the current state of branded_asins_data
                branded_asins_count = len(st.session_state.client_config.get('branded_asins_data', {}))
                st.session_state.debug_messages.append(f"Current branded_asins_data count: {branded_asins_count}")
                
                # Determine the ASIN column name based on priority order
                # First try exact matches for 'ASIN' or '(Child) ASIN'
                asin_col = None
                
                # Priority order: '(Child) ASIN' first, then 'ASIN' exact match
                for col_name in ['(Child) ASIN', 'ASIN']:
                    if col_name in st.session_state.sales_report_data.columns:
                        asin_col = col_name
                        st.session_state.debug_messages.append(f"Using {asin_col} column from sales report")
                        break
                        
                # If no exact match found, look for any column containing 'ASIN' as fallback
                if asin_col is None:
                    for col in st.session_state.sales_report_data.columns:
                        if 'ASIN' in col and '(Parent)' not in col:  # Avoid using (Parent) ASIN
                            asin_col = col
                            st.session_state.debug_messages.append(f"Falling back to {asin_col} column")
                            break
                            
                # Last resort: use any ASIN column including (Parent) ASIN
                if asin_col is None:
                    for col in st.session_state.sales_report_data.columns:
                        if 'ASIN' in col:
                            asin_col = col
                            st.session_state.debug_messages.append(f"Last resort using {asin_col} column")
                            break
                
                # Get ASINs and titles from sales report
                if asin_col in st.session_state.sales_report_data.columns:
                    # Check if Title column exists, if not, try to find it with case-insensitive search
                    title_col = 'Title'
                    if 'Title' not in st.session_state.sales_report_data.columns:
                        for col in st.session_state.sales_report_data.columns:
                            if col.lower() == 'title' or 'title' in col.lower() or 'product name' in col.lower():
                                title_col = col
                                st.session_state.debug_messages.append(f"Found alternative title column: {title_col}")
                                break
                        
                    # If we still don't have a title column, create a placeholder
                    if title_col not in st.session_state.sales_report_data.columns:
                        st.session_state.sales_report_data[title_col] = 'Title not available'
                        st.session_state.debug_messages.append("No title column found, using placeholder")
                    # Debug: Log the total number of unique ASINs in the sales report
                    # First clean the ASINs to ensure proper counting
                    st.session_state.sales_report_data[asin_col] = st.session_state.sales_report_data[asin_col].astype(str).str.strip().str.upper()
                    unique_asins = st.session_state.sales_report_data[asin_col].unique()
                    st.session_state.debug_messages.append(f"Found {len(unique_asins)} unique ASINs in sales report. First 5: {list(unique_asins)[:5]}")
                    
                    # Log how many of these ASINs are already in the branded_asins_data
                    existing_asins = set(st.session_state.client_config.get('branded_asins_data', {}).keys())
                    new_asins_count = len(set(unique_asins) - existing_asins)
                    st.session_state.debug_messages.append(f"Of these, {new_asins_count} ASINs are not yet in branded_asins_data")
                    
                    # Use the identified title column
                    title_col = title_col if title_col in st.session_state.sales_report_data.columns else 'Title'
                    
                    # ASINs have already been converted to string and standardized above
                    
                    # Create a clean dataframe with ASIN and Title
                    sales_asins_df = st.session_state.sales_report_data[[asin_col, title_col]].copy()
                    sales_asins_df.columns = [asin_col, 'Title']  # Standardize column name
                    sales_asins_df = sales_asins_df.drop_duplicates().reset_index(drop=True)
                    
                    # Debug: Show a sample of the data
                    st.session_state.debug_messages.append(f"Sample of sales_asins_df:\n{sales_asins_df.head(3)}")
                    st.session_state.debug_messages.append(f"After drop_duplicates: {len(sales_asins_df)} unique ASIN/Title pairs")
                    
                    # Collect new ASINs from sales report
                    new_asins_from_sales = []
                    new_asins_titles = {}
                    skipped_count = 0
                    title_updates = 0  # Initialize title updates counter
                    
                    for _, row in sales_asins_df.iterrows():
                        asin = row[asin_col]
                        title = row['Title']
                        
                        # Debug: Check if ASIN is valid
                        if not asin or pd.isna(asin) or str(asin).strip() == '' or str(asin).lower() == 'nan':
                            st.session_state.debug_messages.append(f"Skipping empty or invalid ASIN")
                            skipped_count += 1
                            continue
                            
                        # Standardize ASIN format
                        asin = str(asin).strip().upper()
                        
                        # Check if title is valid
                        if pd.isna(title) or str(title).strip() == '' or str(title).lower() == 'nan':
                            title = 'Title not available'
                        else:
                            title = str(title).strip()
                        
                        # Only collect if not already in the data
                        if asin not in st.session_state.client_config['branded_asins_data']:
                            new_asins_from_sales.append(asin)
                            new_asins_titles[asin] = title
                        # Always update title if it's available from the sales report and not empty
                        elif asin in st.session_state.client_config['branded_asins_data'] and title and str(title).strip() != '' and title != 'nan':
                            # Debug: Log title update
                            old_title = st.session_state.client_config['branded_asins_data'][asin]['product_title']
                            st.session_state.debug_messages.append(f"Updating title for existing ASIN {asin}: '{old_title}' -> '{title}'")
                            st.session_state.client_config['branded_asins_data'][asin]['product_title'] = title
                    
                    # Debug: Log how many new ASINs were found and skipped
                    st.session_state.debug_messages.append(f"Found {len(new_asins_from_sales)} new ASINs in sales report, skipped {skipped_count} invalid ASINs")
                    
                    # If there are no new ASINs from sales report but there are titles to update, still do that
                    if not new_asins_from_sales and title_updates > 0:
                        st.session_state.debug_messages.append(f"No new ASINs to add, but updated {title_updates} existing ASINs with titles")
                    
                    # If the branded_asins_data is empty but we have ASINs in the sales report, add all of them
                    if not st.session_state.client_config['branded_asins_data'] and len(sales_asins_df) > 0:
                        st.session_state.debug_messages.append(f"Branded ASINs list is empty. Adding all {len(sales_asins_df)} ASINs from sales report.")
                        for _, row in sales_asins_df.iterrows():
                            asin = str(row[asin_col]).strip().upper()
                            title = row['Title']
                            if not asin or pd.isna(asin) or str(asin).strip() == '' or str(asin).lower() == 'nan':
                                continue
                                
                            if pd.isna(title) or str(title).strip() == '' or str(title).lower() == 'nan':
                                title = 'Title not available'
                            else:
                                title = str(title).strip()
                                
                            new_asins_from_sales.append(asin)
                            new_asins_titles[asin] = title
                    
                    # Store new ASINs in session state for user confirmation
                    if new_asins_from_sales:
                        st.session_state.new_sales_asins = new_asins_from_sales
                        st.session_state.new_sales_asins_titles = new_asins_titles
                        st.session_state.show_new_sales_asins_prompt = True
                        st.session_state.debug_messages.append(f"Setting show_new_sales_asins_prompt to True with {len(new_asins_from_sales)} ASINs")
                    else:
                        st.session_state.new_sales_asins = []
                        st.session_state.new_sales_asins_titles = {}
                        st.session_state.show_new_sales_asins_prompt = False
                    
                    # Save the updated config (for title updates only at this point)
                    save_client_config(st.session_state.selected_client_name, st.session_state.client_config)
                    
                    # Also update existing ASINs with titles from the sales report
                    title_updates = 0
                    for asin in st.session_state.client_config['branded_asins_data']:
                        asin_match = sales_asins_df[sales_asins_df[asin_col] == asin]
                        if not asin_match.empty and 'Title' in asin_match.columns:
                            title = asin_match['Title'].iloc[0]
                            if title and str(title).strip() != '' and str(title).lower() != 'nan' and title != 'Title not available':
                                if st.session_state.client_config['branded_asins_data'][asin]['product_title'] == 'Title not available':
                                    st.session_state.client_config['branded_asins_data'][asin]['product_title'] = title
                                    title_updates += 1
                    
                    if title_updates > 0:
                        st.session_state.debug_messages.append(f"Updated {title_updates} existing ASINs with titles from sales report")
                        save_client_config(st.session_state.selected_client_name, st.session_state.client_config)
            
            # Create DataFrame for display with columns in the specified order
            if st.session_state.client_config['branded_asins_data']:
                data = []
                for asin, info in st.session_state.client_config['branded_asins_data'].items():
                    data.append({
                        'SKU': info.get('sku', ''),  # Add SKU column (left of ASIN)
                        'ASIN': asin,
                        'Product Title': info.get('product_title', 'Title not available'),
                        'Product Group': info.get('product_group', ''),
                        'Tag 1': info.get('tag1', ''),
                        'Tag 2': info.get('tag2', ''),
                        'Tag 3': info.get('tag3', ''),
                        'Delete': False  # Add Delete column with default value False
                    })
                
                branded_asins_df = pd.DataFrame(data)

                # -----------------------------------------------------------------------------
                # Product Group Auto-Tagging Rules (based on Product Title)
                # -----------------------------------------------------------------------------
                # Initialize rule storage in session state
                # Sync Product Title â†’ Product Group tagging rules from client config each run
                if st.session_state.client_config and not st.session_state.get('asin_title_tag_rules'):
                    cfg_rules = st.session_state.client_config.get('asin_title_tag_rules', [])
                    st.session_state.asin_title_tag_rules = cfg_rules.copy()
                if 'asin_rule_modal_open' not in st.session_state:
                    st.session_state.asin_rule_modal_open = False

                # Helper to obtain Streamlit dialog/expander (mirrors Campaign Tagging implementation)
                from contextlib import nullcontext  # noqa: E402 â€“ local import ok in Streamlit script
                def get_asin_dialog(title):
                    dlg_fn = getattr(st, 'dialog', None)
                    if callable(dlg_fn):
                        dlg = dlg_fn(title)
                        if hasattr(dlg, '__enter__') and hasattr(dlg, '__exit__'):
                            return dlg
                    return st.expander(title, expanded=True)

                # Modal for creating / editing rules ------------------------------------------------
                from typing import Optional  # Added for type hints compatible with Python 3.9

                def open_asin_rule_modal(edit_idx: Optional[int] = None):
                    """Open the Product Group rule builder for ASIN titles."""
                    if edit_idx is not None:
                        st.session_state.temp_asin_rule = st.session_state.asin_title_tag_rules[edit_idx].copy()
                        st.session_state.temp_asin_rule['edit_idx'] = edit_idx
                    else:
                        st.session_state.temp_asin_rule = {
                            'conditions': [{'operator': 'contains', 'value': ''}],
                            'combine': 'AND',
                            'tag': ''
                        }
                    st.session_state.asin_rule_modal_open = True

                # Render the rule-builder modal when requested
                if st.session_state.asin_rule_modal_open:
                    with get_asin_dialog("Create / Edit Product Group Rule"):
                        rule = st.session_state.temp_asin_rule

                        st.markdown("### Product Group to assign")
                        rule['tag'] = st.text_input("Product Group", value=rule.get('tag', ''), key="asin_rule_modal_tag")

                        st.markdown("### Conditions (evaluated against Product Title)")
                        rule['combine'] = st.selectbox(
                            "Combine all conditions with",
                            ["AND", "OR"],
                            index=0 if rule.get('combine', 'AND') == 'AND' else 1,
                            key="asin_rule_modal_combine"
                        )

                        # Render conditions list ----------------------------------------------
                        remove_cond_indices: list[int] = []
                        for c_idx, cond in enumerate(rule['conditions']):
                            col_op, col_val, col_del = st.columns([0.25, 0.6, 0.15])
                            with col_op:
                                cond['operator'] = st.selectbox(
                                    "Operator",
                                    ["contains", "doesn't contain"],
                                    index=0 if cond.get('operator', 'contains') == 'contains' else 1,
                                    key=f"asin_cond_operator_{c_idx}"
                                )
                            with col_val:
                                cond['value'] = st.text_input(
                                    "Value",
                                    value=cond.get('value', ''),
                                    key=f"asin_cond_value_{c_idx}"
                                )
                            with col_del:
                                if st.button("ðŸ—‘ï¸", key=f"asin_cond_del_{c_idx}"):
                                    remove_cond_indices.append(c_idx)
                        # Remove marked conditions
                        for i in sorted(remove_cond_indices, reverse=True):
                            del rule['conditions'][i]

                        if st.button("Add Condition", key="asin_add_condition_btn"):
                            rule['conditions'].append({'operator': 'contains', 'value': ''})

                        col_save, col_cancel = st.columns(2)
                        with col_save:
                            if st.button("Save", key="asin_save_rule_btn"):
                                # Validate rule â€“ must have tag and at least one condition value
                                if rule['tag'] and any(cond['value'] for cond in rule['conditions']):
                                    if 'edit_idx' in rule:
                                        st.session_state.asin_title_tag_rules[rule['edit_idx']] = {
                                            k: rule[k] for k in ['conditions', 'combine', 'tag']
                                        }
                                    else:
                                        st.session_state.asin_title_tag_rules.append({
                                            k: rule[k] for k in ['conditions', 'combine', 'tag']
                                        })
                                    # Persist to client config
                                    st.session_state.client_config['asin_title_tag_rules'] = st.session_state.asin_title_tag_rules
                                    save_client_config(st.session_state.selected_client_name, st.session_state.client_config)
                                    st.session_state.asin_rule_modal_open = False
                                    st.session_state.temp_asin_rule = None
                                    st.rerun()
                                else:
                                    st.warning("Please provide at least one condition value and a Product Group.")
                        with col_cancel:
                            if st.button("Cancel", key="asin_cancel_rule_btn"):
                                st.session_state.asin_rule_modal_open = False
                                st.session_state.temp_asin_rule = None

                # Create an expander for the Bulk Tag ASINs by Rulesets section
                with st.expander("Bulk Tag ASINs by Rulesets", expanded=False):
                    # Option to only tag ASINs whose Product Group is blank
                    only_blank_asin_rule = st.checkbox("Only tag un-tagged ASINs", key="asin_rule_only_blank")
                    
                    # ----------------------------- Saved Rules list -----------------------------------
                    st.markdown("### Saved Rules")
                    removal_indices: list[int] = []
                    for idx, rule in enumerate(st.session_state.asin_title_tag_rules):
                        cond_strings = []
                        for cond in rule.get('conditions', []):
                            op = 'contains' if cond.get('operator') == 'contains' else "doesn't contain"
                            cond_strings.append(f"Product Title {op} \"{cond.get('value', '')}\"")
                        summary = (f" {rule.get('combine', 'AND')} ").join(cond_strings)
                        summary += f"  â†’  **{rule.get('tag', '')}**"
                        col_summary, col_edit, col_del = st.columns([0.8, 0.1, 0.1])
                        with col_summary:
                            st.markdown(summary)
                        with col_edit:
                            if st.button("âœï¸", key=f"asin_rule_edit_{idx}"):
                                open_asin_rule_modal(edit_idx=idx)
                        with col_del:
                            if st.button("ðŸ—‘ï¸", key=f"asin_rule_del_{idx}"):
                                removal_indices.append(idx)
                    for i in sorted(removal_indices, reverse=True):
                        del st.session_state.asin_title_tag_rules[i]
                    if removal_indices:
                        st.session_state.client_config['asin_title_tag_rules'] = st.session_state.asin_title_tag_rules
                        save_client_config(st.session_state.selected_client_name, st.session_state.client_config)

                    # Button to add a new rule
                    if st.button("Add Rule", key="asin_add_rule"):
                        open_asin_rule_modal()

                    # Apply rules to branded ASINs ------------------------------------------------------
                    if st.button("Apply Product Group Rules", key="apply_asin_rules"):
                        asins_updated = 0
                        for asin, asin_info in st.session_state.client_config['branded_asins_data'].items():
                            # Skip already-tagged ASINs when checkbox selected
                            if only_blank_asin_rule and asin_info.get('product_group', '').strip():
                                continue
                            title_lower = str(asin_info.get('product_title', '')).lower()
                            # Evaluate rules in order
                            for rule in st.session_state.asin_title_tag_rules:
                                tag_val = rule.get('tag', '').strip()
                                if not tag_val:
                                    continue
                                conditions = rule.get('conditions', [])
                                if not conditions:
                                    continue
                                cond_results = []
                                for cond in conditions:
                                    val = cond.get('value', '').strip().lower()
                                    if not val:
                                        continue
                                    op = cond.get('operator', 'contains')
                                    if op == 'contains':
                                        cond_results.append(val in title_lower)
                                    else:
                                        cond_results.append(val not in title_lower)
                                if not cond_results:
                                    continue
                                combine = rule.get('combine', 'AND')
                                matched = all(cond_results) if combine == 'AND' else any(cond_results)
                                if matched:
                                    asin_info['product_group'] = tag_val
                                    asins_updated += 1
                                    break  # Stop at first matching rule
                        if asins_updated > 0:
                            save_client_config(st.session_state.selected_client_name, st.session_state.client_config)
                            st.success(f"Updated {asins_updated} ASINs with Product Groups")
                            st.rerun()
                        else:
                            st.info("No ASINs matched the rules.")
            
                # -----------------------------------------------------------------------------
                # Bulk ASIN Product Group Tagger
                # -----------------------------------------------------------------------------
                with st.expander("Bulk Tag ASINs by List", expanded=False):
                    bulk_pg = st.text_input("Product Group", key="bulk_asin_product_group")
                    asin_text = st.text_area(
                        "Paste ASINs (one per line or comma-separated)",
                        height=150,
                        key="bulk_asin_list_input"
                    )
                    bulk_only_blank = st.checkbox(
                        "Only tag ASINs with blank Product Group",
                        key="bulk_asin_only_blank"
                    )
                    if st.button("Apply Bulk Tags", key="apply_bulk_asin_tags"):
                        import re  # local import permissible in Streamlit script
                        if not bulk_pg.strip():
                            st.warning("Please enter a Product Group to assign.")
                        else:
                            asin_list = [x.strip().upper() for x in re.split('[,\s]+', asin_text) if x.strip()]
                            if not asin_list:
                                st.warning("Please paste at least one ASIN.")
                            else:
                                updated_count = 0
                                for asin_key, asin_info in st.session_state.client_config['branded_asins_data'].items():
                                    if asin_key.upper() in asin_list:
                                        if bulk_only_blank and asin_info.get('product_group', '').strip():
                                            continue
                                        asin_info['product_group'] = bulk_pg.strip()
                                        updated_count += 1
                                if updated_count > 0:
                                    save_client_config(
                                        st.session_state.selected_client_name,
                                        st.session_state.client_config
                                    )
                                    st.success(f"Updated {updated_count} ASIN(s) with Product Group '{bulk_pg.strip()}'")
                                    st.rerun()
                                else:
                                    st.info("No matching ASINs found or nothing to update.")

                # -----------------------------------------------------------------------------
                # CSV Upload for Bulk Product Group Tagging
                # -----------------------------------------------------------------------------
                with st.expander("Bulk Update ASIN Tags via CSV Upload", expanded=False):
                    st.markdown("""
                    <div style='margin-bottom: 10px; padding: 10px; background-color: #1e1e1e; border-left: 3px solid #6CA8FF; border-radius: 3px;'>
                        <strong>CSV Format:</strong><br>
                        â€¢ Column A: <strong>ASIN</strong> <em>(required)</em><br>
                        â€¢ Column B: Product Group <em>(optional)</em><br>
                        â€¢ Column C: Tag 1 <em>(optional)</em><br>
                        â€¢ Column D: Tag 2 <em>(optional)</em><br>
                        â€¢ Column E: Tag 3 <em>(optional)</em><br>
                        â€¢ First row can be headers (auto-detected)
                    </div>
                    """, unsafe_allow_html=True)

                    # Provide a downloadable template
                    import pandas as pd  # local import permitted in Streamlit context
                    template_df = pd.DataFrame(
                        [["B0XXXXXXXXX", "", "", "", ""]],
                        columns=["ASIN", "Product Group", "Tag 1", "Tag 2", "Tag 3"]
                    )
                    st.download_button(
                        label="Download CSV Template",
                        data=template_df.to_csv(index=False).encode('utf-8'),
                        file_name="branded_asins_template.csv",
                        mime="text/csv",
                        help="Template includes ASIN (required) and Product Group/Tag 1/2/3 (optional) columns."
                    )
                    
                    # File upload
                    uploaded_csv = st.file_uploader(
                        "Choose CSV file",
                        type=["csv"],
                        key="product_group_csv_upload",
                        help="Upload a CSV with columns: ASIN (required), optional Product Group, Tag 1, Tag 2, Tag 3."
                    )
                    
                    # Checkbox for only tagging untagged ASINs
                    csv_only_untagged = st.checkbox(
                        "Only update Product Group when currently blank",
                        key="csv_only_untagged",
                        help="When checked, Product Group is set only for ASINs with no current Product Group. Tags will still update if provided."
                    )
                    
                    if uploaded_csv is not None:
                        try:
                            import pandas as pd
                            import io
                            
                            # Read the CSV file
                            csv_content = uploaded_csv.read()
                            csv_df = pd.read_csv(io.StringIO(csv_content.decode('utf-8')))
                            
                            # Auto-detect if first row contains headers
                            first_row_values = csv_df.iloc[0].astype(str).tolist()
                            has_headers = any(val.lower() in ['asin', 'product group', 'product_group', 'group', 'tag 1', 'tag1', 'tag_1'] for val in first_row_values)
                            
                            if not has_headers:
                                # If no headers detected, use the first two columns
                                if len(csv_df.columns) >= 2:
                                    csv_df.columns = ['ASIN', 'Product Group'] + list(csv_df.columns[2:])
                                else:
                                    st.error("CSV file must have at least 2 columns (ASIN and Product Group)")
                                    st.stop()
                            else:
                                # Use existing column names but ensure we can identify ASIN and Product Group columns
                                asin_col = None
                                pg_col = None
                                
                                # Find ASIN column
                                for col in csv_df.columns:
                                    if 'asin' in str(col).lower():
                                        asin_col = col
                                        break
                                if asin_col is None:
                                    asin_col = csv_df.columns[0]  # Default to first column
                                
                                # Find Product Group column
                                for col in csv_df.columns:
                                    if any(term in str(col).lower() for term in ['product group', 'product_group', 'group']):
                                        pg_col = col
                                        break
                                if pg_col is None:
                                    pg_col = csv_df.columns[1]  # Default to second column

                                # Find Tag columns (optional)
                                tag1_col = None
                                tag2_col = None
                                tag3_col = None
                                for col in csv_df.columns:
                                    low = str(col).lower()
                                    if tag1_col is None and any(term in low for term in ['tag 1', 'tag1', 'tag_1']):
                                        tag1_col = col
                                    if tag2_col is None and any(term in low for term in ['tag 2', 'tag2', 'tag_2']):
                                        tag2_col = col
                                    if tag3_col is None and any(term in low for term in ['tag 3', 'tag3', 'tag_3']):
                                        tag3_col = col
                                # Find SKU column (optional)
                                sku_col = None
                                for col in csv_df.columns:
                                    if 'sku' in str(col).lower():
                                        sku_col = col
                                        break
                                
                                # Rename columns for consistency
                                rename_map = {asin_col: 'ASIN'}
                                if pg_col is not None:
                                    rename_map[pg_col] = 'Product Group'
                                if tag1_col is not None:
                                    rename_map[tag1_col] = 'Tag 1'
                                if tag2_col is not None:
                                    rename_map[tag2_col] = 'Tag 2'
                                if tag3_col is not None:
                                    rename_map[tag3_col] = 'Tag 3'
                                if sku_col is not None:
                                    rename_map[sku_col] = 'SKU'
                                csv_df = csv_df.rename(columns=rename_map)
                            
                            # Clean and validate data
                            csv_df['ASIN'] = csv_df['ASIN'].astype(str).str.strip().str.upper()
                            if 'Product Group' in csv_df.columns:
                                csv_df['Product Group'] = csv_df['Product Group'].astype(str).str.strip()
                            for tcol in ['Tag 1','Tag 2','Tag 3']:
                                if tcol in csv_df.columns:
                                    csv_df[tcol] = csv_df[tcol].astype(str).fillna('').replace('nan','', regex=False).str.strip()
                            if 'SKU' in csv_df.columns:
                                csv_df['SKU'] = csv_df['SKU'].astype(str).fillna('').replace('nan','', regex=False).str.strip()
                            
                            # Remove rows with empty ASINs only (Product Group/Tags optional)
                            csv_df = csv_df[(csv_df['ASIN'] != '') & (csv_df['ASIN'] != 'NAN')]
                            
                            if len(csv_df) == 0:
                                st.warning("No valid ASIN and Product Group pairs found in the CSV file.")
                            else:
                                # Show preview of data
                                st.write(f"**Preview of {len(csv_df)} rows to be processed:**")
                                preview_cols = [c for c in ['ASIN','Product Group','Tag 1','Tag 2','Tag 3','SKU'] if c in csv_df.columns]
                                st.dataframe(csv_df.head(10)[preview_cols] if preview_cols else csv_df.head(10), use_container_width=True)
                                
                                if len(csv_df) > 10:
                                    st.info(f"... and {len(csv_df) - 10} more rows")
                                
                                # Process button
                                if st.button("Apply CSV Updates", key="apply_csv_product_groups"):
                                    updated_count_pg = 0
                                    updated_count_tags = 0
                                    skipped_count = 0
                                    not_found_count = 0
                                    not_found_rows = []
                                    
                                    for _, row in csv_df.iterrows():
                                        asin = row['ASIN']
                                        product_group = row.get('Product Group', '')
                                        sku_val = row.get('SKU', '')
                                        
                                        # Check if ASIN exists in branded_asins_data
                                        if asin in st.session_state.client_config['branded_asins_data']:
                                            asin_info = st.session_state.client_config['branded_asins_data'][asin]
                                            # Update Product Group if provided
                                            if 'Product Group' in csv_df.columns and str(product_group).strip() != '':
                                                current_pg = asin_info.get('product_group', '').strip()
                                                if csv_only_untagged and current_pg:
                                                    skipped_count += 1
                                                else:
                                                    asin_info['product_group'] = str(product_group).strip()
                                                    updated_count_pg += 1
                                            # Update Tag 1/2/3 if provided and non-empty
                                            for idx_tag, key in enumerate(['Tag 1','Tag 2','Tag 3'], start=1):
                                                if key in csv_df.columns:
                                                    val = str(row.get(key, '')).strip()
                                                    if val:
                                                        asin_info[f'tag{idx_tag}'] = val
                                                        updated_count_tags += 1
                                            # Update SKU if provided and non-empty
                                            if 'SKU' in csv_df.columns:
                                                if sku_val:
                                                    asin_info['sku'] = sku_val
                                        else:
                                            not_found_count += 1
                                            not_found_rows.append({
                                                'ASIN': asin,
                                                'Product Group': str(product_group).strip(),
                                                'Tag 1': str(row.get('Tag 1','')).strip(),
                                                'Tag 2': str(row.get('Tag 2','')).strip(),
                                                'Tag 3': str(row.get('Tag 3','')).strip(),
                                                'SKU': str(sku_val).strip()
                                            })
                                    
                                    # Save changes if any updates were made
                                    if (updated_count_pg + updated_count_tags) > 0:
                                        save_client_config(
                                            st.session_state.selected_client_name,
                                            st.session_state.client_config
                                        )
                                        msg_bits = []
                                        if updated_count_pg:
                                            msg_bits.append(f"{updated_count_pg} Product Group update(s)")
                                        if updated_count_tags:
                                            msg_bits.append(f"{updated_count_tags} Tag update(s)")
                                        st.success("âœ… " + ", ".join(msg_bits) + " applied from CSV")
                                        
                                        if skipped_count > 0:
                                            st.info(f"â„¹ï¸ Skipped {skipped_count} Product Group update(s) due to 'only update when blank'")
                                        # Store missing rows prompt state for next run
                                        if not_found_count > 0 and not_found_rows:
                                            unique_rows = {}
                                            for r in not_found_rows:
                                                asin_key = str(r.get('ASIN','')).strip().upper()
                                                if asin_key and asin_key not in unique_rows:
                                                    unique_rows[asin_key] = r
                                            st.session_state.asin_csv_missing_rows = list(unique_rows.values())
                                            st.session_state.asin_csv_missing_count = len(unique_rows)
                                            st.session_state.asin_csv_missing_sku_count = sum(1 for r in unique_rows.values() if str(r.get('SKU','')).strip())
                                            st.session_state.asin_csv_prompt_open = True
                                    else:
                                        if not_found_count > 0 and not_found_rows:
                                            unique_rows = {}
                                            for r in not_found_rows:
                                                asin_key = str(r.get('ASIN','')).strip().upper()
                                                if asin_key and asin_key not in unique_rows:
                                                    unique_rows[asin_key] = r
                                            st.session_state.asin_csv_missing_rows = list(unique_rows.values())
                                            st.session_state.asin_csv_missing_count = len(unique_rows)
                                            st.session_state.asin_csv_missing_sku_count = sum(1 for r in unique_rows.values() if str(r.get('SKU','')).strip())
                                            st.session_state.asin_csv_prompt_open = True
                                        elif skipped_count > 0:
                                            st.info(f"No Product Group updates made due to 'only update when blank'.")
                                        else:
                                            st.info("No updates were made. Ensure your CSV has at least one of Product Group or Tag 1/2/3 populated.")
                                
                        except Exception as e:
                            st.error(f"Error processing CSV file: {str(e)}")
                            st.error("Please ensure your CSV file has ASIN in column A and Product Group in column B.")

                # Global missing ASINs prompt (survives uploader resets)
                if st.session_state.get('asin_csv_prompt_open'):
                    cnt = int(st.session_state.get('asin_csv_missing_count', 0))
                    sku_cnt = int(st.session_state.get('asin_csv_missing_sku_count', 0))
                    if cnt > 0:
                        st.warning(f"âš ï¸ {cnt} unique ASIN(s) from CSV were not found in your Branded ASINs list.")
                        st.info(f"You can add them now. This will add {cnt} ASIN(s){' and ' + str(sku_cnt) + ' SKU(s)' if sku_cnt else ''}.")
                        cols_add_missing = st.columns([1,1])
                        with cols_add_missing[0]:
                            if st.button(f"Add {cnt} new ASIN(s)", key="add_missing_asins_btn_global"):
                                created = 0
                                rows_to_create = st.session_state.get('asin_csv_missing_rows', [])
                                for r in rows_to_create:
                                    asin_new = str(r.get('ASIN','')).strip().upper()
                                    if not asin_new:
                                        continue
                                    st.session_state.client_config['branded_asins_data'][asin_new] = {
                                        'product_title': 'Title not available',
                                        'product_group': r.get('Product Group','') or ''
                                    }
                                    sku_new = str(r.get('SKU','')).strip()
                                    if sku_new:
                                        st.session_state.client_config['branded_asins_data'][asin_new]['sku'] = sku_new
                                    for idx_tag, key in enumerate(['Tag 1','Tag 2','Tag 3'], start=1):
                                        val = str(r.get(key,'')).strip()
                                        if val:
                                            st.session_state.client_config['branded_asins_data'][asin_new][f'tag{idx_tag}'] = val
                                    created += 1
                                if created:
                                    save_client_config(
                                        st.session_state.selected_client_name,
                                        st.session_state.client_config
                                    )
                                    # Clear prompt state
                                    st.session_state.asin_csv_missing_rows = []
                                    st.session_state.asin_csv_missing_count = 0
                                    st.session_state.asin_csv_missing_sku_count = 0
                                    st.session_state.asin_csv_prompt_open = False
                                    st.success(f"Added {created} ASIN(s) to Branded ASINs")
                                    st.rerun()
                        with cols_add_missing[1]:
                            if st.button("Dismiss", key="dismiss_missing_asins_prompt"):
                                st.session_state.asin_csv_missing_rows = []
                                st.session_state.asin_csv_missing_count = 0
                                st.session_state.asin_csv_missing_sku_count = 0
                                st.session_state.asin_csv_prompt_open = False

                # Add filter options
                # Display current Product Groups across Campaign Tagging and Branded ASINs tabs
                display_pgs = set()
                # From campaign_tags_data
                for info in st.session_state.client_config.get('campaign_tags_data', {}).values():
                    pg_val = str(info.get('tag_1', '')).strip()
                    if pg_val and pg_val.lower() != 'none':
                        display_pgs.add(pg_val)
                # From branded_asins_data
                for asin_info in st.session_state.client_config.get('branded_asins_data', {}).values():
                    pg_val = str(asin_info.get('product_group', '')).strip()
                    if pg_val and pg_val.lower() != 'none':
                        display_pgs.add(pg_val)
                if display_pgs:
                    styled_groups = []
                    colors = ['#FFC107', '#FFF3C4']
                    for idx, pg in enumerate(sorted(display_pgs)):
                        color = colors[idx % 2]
                        styled_groups.append(f"<span style='color:{color};'>{pg}</span>")
                    groups_html = ", ".join(styled_groups)
                else:
                    groups_html = "No Product Groups yet"

                st.markdown(f"""<div style='margin-top: 10px; margin-bottom: 20px;'>
                    <span style='font-weight: 500; color: #FFFFFF;'>Current Product Groups: </span>{groups_html}
                </div>""", unsafe_allow_html=True)
                
                col1, col2, col3, col4 = st.columns([0.25, 0.25, 0.4, 0.1])
                
                # Get unique product groups for the filter
                product_groups = set()
                
                # First, get product groups from the client config
                if st.session_state.get('client_config'):
                    # Get from branded_asins_data
                    if 'branded_asins_data' in st.session_state.client_config:
                        for asin_info in st.session_state.client_config['branded_asins_data'].values():
                            product_group = asin_info.get('product_group', '')
                            if product_group and product_group.strip():
                                product_groups.add(product_group)
                    
                    # Get from campaign_tags_data
                    if 'campaign_tags_data' in st.session_state.client_config:
                        for campaign_info in st.session_state.client_config['campaign_tags_data'].values():
                            product_group = campaign_info.get('tag_1', '') or 'Untagged Group'
                            if product_group and product_group.strip():
                                product_groups.add(product_group)
                
                # Then get from the dataframe if available
                if not branded_asins_df.empty and 'Product Group' in branded_asins_df.columns:
                    groups = branded_asins_df['Product Group'].dropna().unique()
                    product_groups.update([g for g in groups if g and g.strip()])
                
                # Convert to sorted list
                product_groups = sorted(list(product_groups))
                
                # Initialize session state for product group filter if not exists
                if 'asin_product_group_filter' not in st.session_state:
                    st.session_state.asin_product_group_filter = []
                
                with col1:
                    # Display debug info if needed
                    if 'debug' in st.session_state and st.session_state.debug:
                        st.write(f"Available product groups: {product_groups}")
                    
                    # Use a different variable name to avoid conflicts
                    asin_selected_groups = st.multiselect(
                        "Filter by Product Group(s)",
                        options=product_groups,
                        key="asin_product_group_filter"
                    )
                    # Store filter state
                    st.session_state.asin_filter_active = len(asin_selected_groups) > 0
                
                with col2:
                    filter_asin = st.text_input("Filter by ASIN", key="filter_asin")
                    
                with col3:
                    filter_title = st.text_input("Filter by Product Title", key="filter_title")
                    
                with col4:
                    st.markdown("""<div style='height: 32px;'></div>""", unsafe_allow_html=True)
                    show_blank = st.checkbox("Show Blank", key="show_blank_asins", help="Show only ASINs with blank Product Group")
                
                # New row: Tag filters (Tag 1 / Tag 2 / Tag 3)
                tag_col1, tag_col2, tag_col3 = st.columns([1, 1, 1])
                with tag_col1:
                    filter_tag1 = st.text_input("Filter by Tag 1", key="filter_tag1")
                with tag_col2:
                    filter_tag2 = st.text_input("Filter by Tag 2", key="filter_tag2")
                with tag_col3:
                    filter_tag3 = st.text_input("Filter by Tag 3", key="filter_tag3")

                # Apply filters
                filtered_df = branded_asins_df
                
                # Apply product group filter if active
                if asin_selected_groups:
                    try:
                        if 'Product Group' in filtered_df.columns:
                            # Convert both sides to string to ensure consistent comparison
                            filtered_df['Product Group'] = filtered_df['Product Group'].astype(str)
                            filtered_df = filtered_df[filtered_df['Product Group'].isin([str(pg) for pg in asin_selected_groups])]
                            st.caption(f"Filtered by Product Group(s): {', '.join(asin_selected_groups)}")
                            
                            # Debug information
                            if 'debug' in st.session_state and st.session_state.debug:
                                st.write(f"Product groups in data: {filtered_df['Product Group'].unique()}")
                                st.write(f"Selected product groups: {asin_selected_groups}")
                                st.write(f"Filtered dataframe shape: {filtered_df.shape}")
                        else:
                            st.warning("Product Group column not found in data. Cannot apply filter.")
                            if 'debug_messages' in st.session_state:
                                st.session_state.debug_messages.append("[Performance by ASIN] Product Group column not found in data.")
                    except Exception as e:
                        st.error(f"Error applying product group filter: {e}")
                        if 'debug_messages' in st.session_state:
                            st.session_state.debug_messages.append(f"[Performance by ASIN] Error applying product group filter: {e}")
                            
                        # If there's an error, continue with the unfiltered dataframe
                        filtered_df = branded_asins_df
                if filter_asin:
                    filtered_df = filtered_df[filtered_df['ASIN'].str.contains(filter_asin, case=False)]
                if filter_title:
                    filtered_df = filtered_df[filtered_df['Product Title'].str.contains(filter_title, case=False)]
                # Apply tag filters if provided
                if st.session_state.get('filter_tag1'):
                    if 'Tag 1' in filtered_df.columns:
                        filtered_df = filtered_df[filtered_df['Tag 1'].astype(str).str.contains(st.session_state['filter_tag1'], case=False, na=False)]
                if st.session_state.get('filter_tag2'):
                    if 'Tag 2' in filtered_df.columns:
                        filtered_df = filtered_df[filtered_df['Tag 2'].astype(str).str.contains(st.session_state['filter_tag2'], case=False, na=False)]
                if st.session_state.get('filter_tag3'):
                    if 'Tag 3' in filtered_df.columns:
                        filtered_df = filtered_df[filtered_df['Tag 3'].astype(str).str.contains(st.session_state['filter_tag3'], case=False, na=False)]
                if st.session_state.get('show_blank_asins', False):
                    filtered_df = filtered_df[filtered_df['Product Group'] == '']
                
                # Get all existing product groups for data validation
                existing_product_groups = set()
                for asin_info in st.session_state.client_config['branded_asins_data'].values():
                    product_group = asin_info.get('product_group', '')
                    if product_group and product_group.strip():
                        existing_product_groups.add(product_group.strip())
                
                # Always use fresh filtered data for edits to prevent stale state bugs
                # Ensure desired column arrangement with SKU column positioned left of ASIN
                desired_cols = ['SKU', 'ASIN', 'Product Title', 'Product Group', 'Tag 1', 'Tag 2', 'Tag 3', 'Delete']
                existing_cols = [c for c in desired_cols if c in filtered_df.columns]
                remaining_cols = [c for c in filtered_df.columns if c not in existing_cols]
                filtered_df = filtered_df[existing_cols + remaining_cols]
                st.session_state.branded_asins_editor_temp = filtered_df.copy()
                # Apply select-all flag to mark all Delete checkboxes
                if st.session_state.get('asin_select_all', False):
                    if 'Delete' in st.session_state.branded_asins_editor_temp.columns:
                        st.session_state.branded_asins_editor_temp['Delete'] = True
                    st.session_state.asin_select_all = False

                # Display the editable table with the temp DataFrame
                edited_df = st.data_editor(
                    st.session_state.branded_asins_editor_temp,
                    key="branded_asins_editor",
                    use_container_width=True,
                    hide_index=True,
                    column_config={
                        'SKU': st.column_config.TextColumn('SKU', help="SKU for this ASIN (auto-populated from bulk files)"),
                        'ASIN': st.column_config.TextColumn('ASIN', disabled=True),
                        'Product Title': st.column_config.TextColumn('Product Title', disabled=True),
                        'Product Group': st.column_config.TextColumn('Product Group'),
                        'Tag 1': st.column_config.TextColumn('Tag 1', help="Optional Tag 1 for this ASIN"),
                        'Tag 2': st.column_config.TextColumn('Tag 2', help="Optional Tag 2 for this ASIN"),
                        'Tag 3': st.column_config.TextColumn('Tag 3', help="Optional Tag 3 for this ASIN"),
                        'Delete': st.column_config.CheckboxColumn('Delete Row')
                    }
                )
                st.session_state.branded_asins_editor_temp = edited_df.copy()

                # --- Action Buttons ---
                col_save, col_remove, col_delete_sel, col_select_all = st.columns([1,1,1,1])
                with col_save:
                    if st.button("Save Changes", key="save_branded_asins"):
                        # Write changes to config
                        for _, row in st.session_state.branded_asins_editor_temp.iterrows():
                            asin = row['ASIN']
                            product_group = row['Product Group']
                            sku = row.get('SKU', '')  # Get SKU value
                            if asin in st.session_state.client_config['branded_asins_data']:
                                st.session_state.client_config['branded_asins_data'][asin]['product_group'] = product_group
                                st.session_state.client_config['branded_asins_data'][asin]['sku'] = sku  # Save SKU
                                # Save Tag 1/2/3
                                st.session_state.client_config['branded_asins_data'][asin]['tag1'] = row.get('Tag 1', '')
                                st.session_state.client_config['branded_asins_data'][asin]['tag2'] = row.get('Tag 2', '')
                                st.session_state.client_config['branded_asins_data'][asin]['tag3'] = row.get('Tag 3', '')
                        save_client_config(st.session_state.selected_client_name, st.session_state.client_config)
                        st.success("Branded ASINs updated.")
                with col_delete_sel:
                    if st.button("Delete Selected Rows", key="delete_branded_asins_btn"):
                        rows_to_delete = st.session_state.branded_asins_editor_temp[st.session_state.branded_asins_editor_temp['Delete'] == True]
                        if not rows_to_delete.empty:
                            for _, row in rows_to_delete.iterrows():
                                asin = row['ASIN']
                                if asin in st.session_state.client_config['branded_asins_data']:
                                    del st.session_state.client_config['branded_asins_data'][asin]
                            save_client_config(st.session_state.selected_client_name, st.session_state.client_config)
                            st.success(f"{len(rows_to_delete)} ASINs deleted successfully.")
                            st.rerun()
                        else:
                            st.info("No rows selected for deletion.")
                with col_remove:
                    if st.button("Remove Rows (Custom)", key="remove_all_asins_btn", help="Open options to remove or clear ASIN rows"):
                        st.session_state.show_remove_asins_dialog = True
                with col_select_all:
                    if st.button("Select All Rows", key="select_all_asin_rows"):
                        st.session_state.asin_select_all = True
                        st.rerun()
                
                # Show confirmation dialog if triggered
                if st.session_state.get('show_remove_asins_dialog', False):
                    st.markdown("---")
                    st.markdown("""<div style='padding: 15px; background-color: rgba(255, 193, 7, 0.1); border: 2px solid #ffc107; border-radius: 8px; margin: 10px 0;'>
                        <h4 style='margin: 0 0 10px 0; color: #856404;'>âš ï¸ Remove All Branded ASIN Rows</h4>
                        <p style='margin: 0; color: #856404;'>This will remove all ASIN rows. Are you sure?</p>
                    </div>""", unsafe_allow_html=True)
                    confirm_col1, confirm_col2 = st.columns(2)
                    with confirm_col1:
                        if st.button("ðŸ—‘ï¸ Delete All Rows", key="confirm_delete_all_asins"):
                            st.session_state.client_config['branded_asins_data'] = {}
                            save_client_config(st.session_state.selected_client_name, st.session_state.client_config)
                            st.session_state.show_remove_asins_dialog = False
                            st.success("All branded ASIN rows removed.")
                            st.rerun()
                    with confirm_col2:
                        if st.button("Cancel", key="cancel_delete_all_asins"):
                            st.session_state.show_remove_asins_dialog = False
                            st.rerun()
                
                # Debug information for product groups
                st.markdown("---")
                if st.checkbox("Show Product Group Debug Info", key="debug_product_groups_branded_asins"):
                    st.write("**Product Groups from branded_asins_data:**")
                    if st.session_state.get('client_config') and 'branded_asins_data' in st.session_state.client_config:
                        asin_groups = set()
                        for asin_info in st.session_state.client_config['branded_asins_data'].values():
                            pg = asin_info.get('product_group', '')
                            if pg and pg.strip():
                                asin_groups.add(pg)
                        st.write(sorted(list(asin_groups)))
                    
                    st.write("**Product Groups from campaign_tags_data:**")
                    if st.session_state.get('client_config') and 'campaign_tags_data' in st.session_state.client_config:
                        campaign_groups = set()
                        for campaign_info in st.session_state.client_config['campaign_tags_data'].values():
                            pg = campaign_info.get('tag_1', '') or 'Untagged Group'
                            if pg and pg.strip():
                                campaign_groups.add(pg)
                        st.write(sorted(list(campaign_groups)))
                    
                    st.write("**Product Groups from dataframe:**")
                    if not branded_asins_df.empty and 'Product Group' in branded_asins_df.columns:
                        df_groups = branded_asins_df['Product Group'].dropna().unique()
                        st.write([g for g in df_groups if g and str(g).strip()])

            else:
                st.info("No branded ASINs found. Upload a Sales Report to automatically extract ASINs.")
        
        with settings_tab3:
            st.markdown("""
    <h4 style='font-family:'Inter','Roboto','Segoe UI',Arial,sans-serif; font-size:0.98rem; font-weight:600; color:#6c757d; margin-top:1.4rem; margin-bottom:0.7rem;'>
        Campaign Tagging
    </h4>""", unsafe_allow_html=True)
            
            # Initialize campaign_tags_data if not present in client config
            if 'campaign_tags_data' not in st.session_state.client_config:
                st.session_state.client_config['campaign_tags_data'] = {}
            
            # Initialize dialog state
            if 'show_remove_rows_dialog' not in st.session_state:
                st.session_state.show_remove_rows_dialog = False
                
            # Add automatic tagging functionality with improved UI inside an expander
            with st.expander("Auto-Tag Campaigns", expanded=False):
                st.markdown("""<div style='margin-bottom: 15px;'></div>""", unsafe_allow_html=True)
                
                # Checkbox for processing only un-tagged campaigns in auto-tagging
                only_blank_auto = st.checkbox("Only tag un-tagged campaigns", key="auto_only_blank")
                
                col1, col2 = st.columns([0.7, 0.3])
                with col1:
                    auto_tag_threshold = st.slider(
                        "Minimum percentage of ASINs from the same Product Group to auto-tag a campaign",
                        min_value=50, max_value=100, value=65, step=5,
                        help="If a campaign has at least this percentage of ASINs from the same Product Group, it will be automatically tagged with that Product Group name."
                    )
                with col2:
                    st.markdown("""<div style='height: 32px;'></div>""", unsafe_allow_html=True)
                if st.button("Auto-Tag Campaigns", help="Automatically tag campaigns based on the ASINs they target", use_container_width=True):
                    if 'bulk_data' in st.session_state and st.session_state.bulk_data and 'branded_asins_data' in st.session_state.client_config:
                        # Get product groups from branded_asins_data
                        product_groups = {}
                        for asin, info in st.session_state.client_config['branded_asins_data'].items():
                            group = info.get('product_group', '')
                            if group:  # Only include ASINs with a product group
                                product_groups[asin.upper()] = group
                        
                        if product_groups:
                            # Process each campaign in the bulk file
                            campaigns_tagged = 0
                            for sheet_name, df in st.session_state.bulk_data.items():
                                # Check if required columns exist (case-insensitive)
                                campaign_col = None
                                asin_col = None
                                state_col = None
                                
                                for col in df.columns:
                                    if col.lower() == 'campaign name' or col.lower() == 'campaign name (informational only)':
                                        campaign_col = col
                                    elif col.lower() == 'asin' or col.lower() == 'asin (informational only)':
                                        asin_col = col
                                    elif col.lower() == 'state':
                                        state_col = col
                                
                                if campaign_col and asin_col and state_col:
                                    # Remove state filtering - include all rows regardless of state
                                    all_rows = df
                                    
                                    campaign_asins = {}
                                    for _, row in all_rows.iterrows():
                                        campaign_name = row[campaign_col]
                                        product_type = str(row.get('Product', ''))
                                        
                                        # Enhanced logic for Sponsored Brands: use 'Creative ASINs' column
                                        if product_type.strip().lower() == 'sponsored brands' and 'creative asins' in df.columns:
                                            creative_asins = str(row.get('Creative ASINs', '')).strip()
                                            asin_list = [a.strip().upper() for a in creative_asins.split(',') if a.strip().upper().startswith('B0') and len(a.strip().upper()) == 10]
                                            for asin in asin_list:
                                                if campaign_name and asin and asin in product_groups:
                                                    if campaign_name not in campaign_asins:
                                                        campaign_asins[campaign_name] = []
                                                    campaign_asins[campaign_name].append((asin, product_groups[asin]))
                                        else:
                                            # Original logic for other campaign types
                                            asin = str(row[asin_col]).strip().upper() if not pd.isna(row[asin_col]) else None
                                            if campaign_name and asin and asin in product_groups:
                                                if campaign_name not in campaign_asins:
                                                    campaign_asins[campaign_name] = []
                                                campaign_asins[campaign_name].append((asin, product_groups[asin]))
                                    
                                    # Analyze each campaign and tag if threshold is met
                                    for campaign_name, asins_list in campaign_asins.items():
                                        if len(asins_list) > 0:
                                            # Count ASINs by product group
                                            group_counts = {}
                                            for _, group in asins_list:
                                                group_counts[group] = group_counts.get(group, 0) + 1
                                            
                                            # Find the most common product group
                                            total_asins = len(asins_list)
                                            max_group = max(group_counts.items(), key=lambda x: x[1])
                                            max_group_name, max_count = max_group
                                            percentage = (max_count / total_asins) * 100
                                            
                                            # Tag if threshold is met
                                            if percentage >= auto_tag_threshold:
                                                # Update campaign tag if it exists in campaign_tags_data
                                                if campaign_name in st.session_state.client_config['campaign_tags_data']:
                                                    # Check if we should only tag blank campaigns
                                                    if only_blank_auto and st.session_state.client_config['campaign_tags_data'][campaign_name]['tag_1']:
                                                        # Skip this campaign as it already has a tag
                                                        pass
                                                    else:
                                                        st.session_state.client_config['campaign_tags_data'][campaign_name]['tag_1'] = max_group_name
                                                        campaigns_tagged += 1
                            
                            # Save the updated config
                            save_client_config(st.session_state.selected_client_name, st.session_state.client_config)
                            
                            if campaigns_tagged > 0:
                                st.markdown(f"""<div class='success-box'>
                                    <span style='font-weight: 500;'>Successfully auto-tagged {campaigns_tagged} campaigns based on product groups</span>
                                </div>""", unsafe_allow_html=True)
                            else:
                                st.markdown("""<div class='info-box'>
                                    <span style='font-weight: 500;'>No campaigns met the threshold for auto-tagging</span>
                                </div>""", unsafe_allow_html=True)
                        else:
                            st.markdown("""<div class='warning-box'>
                                <span style='font-weight: 500;'>No product groups found. Please assign product groups to your ASINs in the Branded ASINs tab first.</span>
                            </div>""", unsafe_allow_html=True)
                    else:
                        st.markdown("""<div class='warning-box'>
                            <span style='font-weight: 500;'>Please upload a Bulk File and set up Product Groups in the Branded ASINs tab first.</span>
                        </div>""", unsafe_allow_html=True)
            
            # -----------------------------------------------------------------------------
            # Campaign Name â†’ Product Group Tagging Rules
            # -----------------------------------------------------------------------------
            
            # Initialize rule storage & modal helper state
            # Sync Campaign Name â†’ Product Group tagging rules from client config each run
            if st.session_state.client_config and not st.session_state.get('campaign_name_tag_rules'):
                cfg_rules = st.session_state.client_config.get('campaign_name_tag_rules', [])
                st.session_state.campaign_name_tag_rules = cfg_rules.copy()
            if 'rule_modal_open' not in st.session_state:
                st.session_state.rule_modal_open = False
            if 'temp_rule' not in st.session_state:
                st.session_state.temp_rule = None
            
            # Put the ruleset functionality in a dropdown
            with st.expander("Bulk Tag Campaigns by Rulesets", expanded=False):
                st.markdown("""<div style='margin-bottom: 15px;'></div>""", unsafe_allow_html=True)
                
                # Checkbox for processing only un-tagged campaigns
                only_blank_rule = st.checkbox("Only tag un-tagged campaigns", key="rule_only_blank")
                
                # -----------------------------------------------------------------------------
                # Helper to obtain a dialog-like container (fallback to container with header to avoid nested expanders)
                from contextlib import contextmanager
                
                @contextmanager
                def simple_container(title):
                    """Create a simple container with a header to avoid nested expanders"""
                    st.markdown(f"### {title}")
                    st.markdown("---")
                    yield
                    st.markdown("---")
                
                def get_dialog(title):
                    dialog_func = getattr(st, 'dialog', None)
                    if callable(dialog_func):
                        dlg = dialog_func(title)
                        # Verify the returned object implements context manager protocol
                        if hasattr(dlg, '__enter__') and hasattr(dlg, '__exit__'):
                            return dlg
                    # Fallback: use simple container to avoid nested expanders
                    return simple_container(title)
    
                # Modal dialog for adding / editing rules
                # -----------------------------------------------------------------------------
                def open_rule_modal(edit_idx=None):
                    """Open the rule creation / editing modal."""
                    # If edit, start with existing rule; else blank
                    if edit_idx is not None:
                        st.session_state.temp_rule = st.session_state.campaign_name_tag_rules[edit_idx].copy()
                        st.session_state.temp_rule['edit_idx'] = edit_idx
                    else:
                        st.session_state.temp_rule = {
                            'conditions': [{'operator': 'contains', 'value': ''}],
                            'combine': 'AND',
                            'tag': ''
                        }
                    st.session_state.rule_modal_open = True

                if st.session_state.rule_modal_open:
                    with get_dialog("Create / Edit Tagging Rule"):
                        rule = st.session_state.temp_rule

                        st.markdown("### Product Group to assign")
                        rule['tag'] = st.text_input("Product Group", value=rule.get('tag', ''), key="rule_modal_tag")

                        st.markdown("### Conditions")
                        rule['combine'] = st.selectbox("Combine all conditions with", ["AND", "OR"],
                                                      index=0 if rule.get('combine', 'AND') == 'AND' else 1,
                                                      key="rule_modal_combine")
                        # Render conditions
                        remove_cond_indices = []
                        for c_idx, cond in enumerate(rule['conditions']):
                            col_op, col_val, col_del = st.columns([0.25, 0.6, 0.15])
                            with col_op:
                                cond['operator'] = st.selectbox(
                                    "Operator",
                                    ["contains", "doesn't contain"],
                                    index=0 if cond.get('operator', 'contains') == 'contains' else 1,
                                    key=f"cond_operator_{c_idx}")
                            with col_val:
                                cond['value'] = st.text_input("Value", value=cond.get('value', ''), key=f"cond_value_{c_idx}")
                            with col_del:
                                if st.button("ðŸ—‘ï¸", key=f"cond_del_{c_idx}"):
                                    remove_cond_indices.append(c_idx)
                        for i in sorted(remove_cond_indices, reverse=True):
                            del rule['conditions'][i]

                        if st.button("Add Condition", key="add_condition_btn"):
                            rule['conditions'].append({'operator': 'contains', 'value': ''})

                        col_save, col_cancel = st.columns(2)
                        with col_save:
                            if st.button("Save", key="save_rule_btn"):
                                # Validate rule
                                if rule['tag'] and any(cond['value'] for cond in rule['conditions']):
                                    if 'edit_idx' in rule:
                                        st.session_state.campaign_name_tag_rules[rule['edit_idx']] = {k: rule[k] for k in ['conditions','combine','tag']}
                                    else:
                                        st.session_state.campaign_name_tag_rules.append({k: rule[k] for k in ['conditions','combine','tag']})
                                    # Persist to client config
                                    st.session_state.client_config['campaign_name_tag_rules'] = st.session_state.campaign_name_tag_rules
                                    save_client_config(st.session_state.selected_client_name, st.session_state.client_config)
                                    st.session_state.rule_modal_open = False
                                    st.session_state.temp_rule = None
                                    st.rerun()
                                else:
                                    st.warning("Please provide at least one condition value and a Product Group.")
                        with col_cancel:
                            if st.button("Cancel", key="cancel_rule_btn"):
                                st.session_state.rule_modal_open = False
                                st.session_state.temp_rule = None

                # -----------------------------------------------------------------------------
                # Display existing rules
                st.markdown("### Saved Rules")
                removal_indices = []
                for idx, rule in enumerate(st.session_state.campaign_name_tag_rules):
                    # Build human-readable summary
                    cond_strings = []
                    for cond in rule.get('conditions', []):
                        op = 'contains' if cond.get('operator') == 'contains' else "doesn't contain"
                        cond_strings.append(f"Campaign Name {op} \"{cond.get('value','')}\"")
                    summary = (f" {rule.get('combine','AND')} ").join(cond_strings)
                    summary += f"  â†’  **{rule.get('tag','')}**"
                    col_summary, col_edit, col_del = st.columns([0.8,0.1,0.1])
                    with col_summary:
                        st.markdown(summary)
                    with col_edit:
                        if st.button("âœï¸", key=f"rule_edit_{idx}"):
                            open_rule_modal(edit_idx=idx)
                    with col_del:
                        if st.button("ðŸ—‘ï¸", key=f"rule_del_{idx}"):
                            removal_indices.append(idx)
                for i in sorted(removal_indices, reverse=True):
                    del st.session_state.campaign_name_tag_rules[i]
                if removal_indices:
                    st.session_state.client_config['campaign_name_tag_rules'] = st.session_state.campaign_name_tag_rules
                    save_client_config(st.session_state.selected_client_name, st.session_state.client_config)

                # Button to add a new blank rule
                if st.button("Add Rule", key="add_rule"):
                    open_rule_modal()

                # Apply rules button
                if st.button("Apply Tagging Rules", key="apply_rules"):
                    campaigns_updated = 0
                    for campaign_name, info in st.session_state.client_config['campaign_tags_data'].items():
                        # Skip already-tagged campaigns when checkbox selected
                        if only_blank_rule and info.get('tag_1', ''):
                            continue

                        # Evaluate rules in order (top priority first)
                        for rule in st.session_state.campaign_name_tag_rules:
                            tag_val = rule.get('tag','').strip()
                            if not tag_val:
                                continue
                            conditions = rule.get('conditions',[])
                            if not conditions:
                                continue
                            # Evaluate each condition
                            condition_results = []
                            for cond in conditions:
                                val = cond.get('value','').strip().lower()
                                if not val:
                                    continue
                                op = cond.get('operator','contains')
                                if op == 'contains':
                                    condition_results.append(val in campaign_name.lower())
                                else:
                                    condition_results.append(val not in campaign_name.lower())
                            if not condition_results:
                                continue
                            if rule.get('combine','AND') == 'AND':
                                matches = all(condition_results)
                            else:
                                matches = any(condition_results)
                            if matches:
                                if info.get('tag_1', '') != tag_val:
                                    st.session_state.client_config['campaign_tags_data'][campaign_name]['tag_1'] = tag_val
                                    campaigns_updated += 1
                                break  # stop at first matching rule

                    # Persist changes
                    save_client_config(st.session_state.selected_client_name, st.session_state.client_config)

                    if campaigns_updated:
                        st.markdown(f"""<div class='success-box'>
                            <span style='font-weight: 500;'>Successfully tagged {campaigns_updated} campaign(s) based on rules</span>
                        </div>""", unsafe_allow_html=True)
                    else:
                        st.markdown("""<div class='info-box'>
                            <span style='font-weight: 500;'>No campaigns matched the rules or all were already tagged</span>
                        </div>""", unsafe_allow_html=True)
            
            # -----------------------------------------------------------------------------
            # Get campaign data from bulk file if available
            if 'bulk_data' in st.session_state and st.session_state.bulk_data:
                # Dictionary to store unique campaigns with their types
                campaign_data = {}
                
                # Campaign type mapping
                campaign_type_map = {
                    'Sponsored Products Campaigns': 'Sponsored Products',
                    'Sponsored Brands Campaigns': 'Sponsored Brands',
                    'Sponsored Display Campaigns': 'Sponsored Display'
                }
                
                # Process each sheet to extract campaigns with spend and state data
                for sheet_name, df in st.session_state.bulk_data.items():
                    if sheet_name in campaign_type_map:
                        campaign_type = campaign_type_map[sheet_name]
                        
                        # Check if required columns exist
                        if 'Campaign Name' in df.columns and 'State' in df.columns and 'Entity' in df.columns:
                            # Remove state filtering - include all campaigns regardless of state
                            all_campaigns = df[df['Entity'] == 'Campaign']
                            # Extract unique campaign names with spend and state data
                            for _, row in all_campaigns.drop_duplicates(subset=['Campaign Name']).iterrows():
                                campaign_name = row['Campaign Name']
                                campaign_state = row.get('State', '')
                                campaign_spend = row.get('Spend', 0)
                                
                                # Convert spend to numeric for calculations
                                try:
                                    if isinstance(campaign_spend, str):
                                        campaign_spend = float(campaign_spend.replace('$', '').replace(',', ''))
                                    campaign_spend = float(campaign_spend) if campaign_spend else 0
                                except (ValueError, TypeError):
                                    campaign_spend = 0
                                
                                # Only add if not already in the data
                                if campaign_name not in campaign_data:
                                    campaign_data[campaign_name] = {
                                        'campaign_type': campaign_type,
                                        'state': campaign_state,
                                        'spend': campaign_spend
                                    }
                
                # Update the campaign_tags_data with new campaigns from bulk file
                for campaign_name, campaign_info in campaign_data.items():
                    if campaign_name not in st.session_state.client_config['campaign_tags_data']:
                        st.session_state.client_config['campaign_tags_data'][campaign_name] = {
                            'campaign_type': campaign_info['campaign_type'],
                            'tag_1': '',
                            'state': campaign_info.get('state', ''),
                            'spend': campaign_info.get('spend', 0)
                        }
                    else:
                        # Update campaign type, state, and spend if they were previously not available
                        existing_data = st.session_state.client_config['campaign_tags_data'][campaign_name]
                        if existing_data.get('campaign_type', '') == '':
                            existing_data['campaign_type'] = campaign_info['campaign_type']
                        # Always update state and spend with latest data from bulk file
                        existing_data['state'] = campaign_info.get('state', '')
                        existing_data['spend'] = campaign_info.get('spend', 0)
                
                # Save the updated config
                save_client_config(st.session_state.selected_client_name, st.session_state.client_config)
            
            # Create DataFrame for display
            if st.session_state.client_config['campaign_tags_data']:
                data = []
                for campaign_name, info in st.session_state.client_config['campaign_tags_data'].items():
                    spend_value = info.get('spend', 0)
                    # Format spend as currency for display
                    spend_formatted = f"${spend_value:,.2f}" if spend_value > 0 else "$0.00"
                    
                    data.append({
                        'Campaign Name': campaign_name,
                        'Campaign Type': info.get('campaign_type', ''),
                        'Product Group': info.get('tag_1', ''),
                        'Spend': spend_formatted,
                        'State': info.get('state', ''),
                        'Delete': False
                    })
                
                campaigns_df = pd.DataFrame(data)
                
                # Add filter options
                # Show current Product Groups from Campaign Tagging & Branded ASINs
                product_groups = set()
                # From campaign_tags_data
                for info in st.session_state.client_config.get('campaign_tags_data', {}).values():
                    pg_val = str(info.get('tag_1', '')).strip()
                    if pg_val and pg_val.lower() != 'none':
                        product_groups.add(pg_val)
                # From branded_asins_data
                for asin_info in st.session_state.client_config.get('branded_asins_data', {}).values():
                    pg_val = str(asin_info.get('product_group', '')).strip()
                    if pg_val and pg_val.lower() != 'none':
                        product_groups.add(pg_val)

                if product_groups:
                    styled_groups = []
                    colors = ['#FFC107', '#FFF3C4']
                    for idx, pg in enumerate(sorted(product_groups)):
                        color = colors[idx % 2]
                        styled_groups.append(f"<span style='color:{color};'>{pg}</span>")
                    groups_html = ", ".join(styled_groups)
                else:
                    groups_html = "No Product Groups yet"

                st.markdown(f"""<div style='margin-top: 20px; margin-bottom: 20px;'>
                    <span style='font-weight: 500; color: #FFFFFF;'>Current Product Groups: </span>{groups_html}
                </div>""", unsafe_allow_html=True)
                
                # Filter controls - reorganized layout with state filter dropdown
                col1, col2, col3, col4, col5, col6 = st.columns([0.2, 0.2, 0.2, 0.15, 0.15, 0.1])
                
                with col1:
                    filter_name = st.text_input("Filter by Campaign Name", key="filter_campaign_name")
                
                with col2:
                    filter_group = st.text_input("Filter by Product Group", key="filter_campaign_group")
                    
                with col3:
                    # Get available campaign types from the data
                    available_types = ['Sponsored Products', 'Sponsored Brands', 'Sponsored Display']
                    filter_types = st.multiselect(
                        "Filter by Campaign Type", 
                        options=available_types,
                        default=[],
                        key="filter_campaign_types",
                        help="Select one or more campaign types to filter"
                    )
                    
                with col4:
                    min_spend = st.number_input("Min Spend ($)", min_value=0.0, value=0.0, step=1.0, key="filter_min_spend", help="Show campaigns with spend greater than this amount")
                    
                with col5:
                    # Get unique state values from the current data
                    available_states = sorted(campaigns_df['State'].dropna().unique().tolist())
                    filter_states = st.multiselect(
                        "Filter by State",
                        options=available_states,
                        default=[],
                        key="filter_campaign_states",
                        help="Select one or more campaign states to filter"
                    )
                    
                with col6:
                    st.markdown("""<div style='height: 32px;'></div>""", unsafe_allow_html=True)
                    show_blank = st.checkbox("Show Blank", key="show_blank_campaigns", help="Show only campaigns with blank Product Group")
                
                # Apply filters
                filtered_df = campaigns_df
                if filter_name:
                    filtered_df = filtered_df[filtered_df['Campaign Name'].str.contains(filter_name, case=False)]
                if filter_types:  # Changed from filter_type to filter_types (multiselect)
                    filtered_df = filtered_df[filtered_df['Campaign Type'].isin(filter_types)]
                if filter_group:
                    filtered_df = filtered_df[filtered_df['Product Group'].str.contains(filter_group, case=False)]
                if st.session_state.get('show_blank_campaigns', False):
                    filtered_df = filtered_df[filtered_df['Product Group'] == '']
                
                # Apply spend filter
                if min_spend > 0:
                    # Convert spend column back to numeric for filtering
                    filtered_df['Spend_numeric'] = filtered_df['Spend'].str.replace('$', '').str.replace(',', '').astype(float)
                    filtered_df = filtered_df[filtered_df['Spend_numeric'] > min_spend]
                    filtered_df = filtered_df.drop('Spend_numeric', axis=1)  # Remove temporary column
                
                # Apply state filter
                if filter_states:
                    filtered_df = filtered_df[filtered_df['State'].isin(filter_states)]
                
                # Always use fresh filtered data for edits to prevent stale state bugs
                st.session_state.campaign_tags_editor_temp = filtered_df.copy()

                # If a select-all flag is set from the previous click, mark all rows for deletion and reset the flag
                if st.session_state.get('campaign_select_all', False):
                    if 'Delete' in st.session_state.campaign_tags_editor_temp.columns:
                        st.session_state.campaign_tags_editor_temp['Delete'] = True
                    st.session_state.campaign_select_all = False

                
                # Display the editable table with the temp DataFrame
                edited_df = st.data_editor(
                    st.session_state.campaign_tags_editor_temp,
                    key="campaign_tags_editor",
                    use_container_width=True,
                    hide_index=True,
                    column_config={
                        'Campaign Name': st.column_config.TextColumn('Campaign Name', disabled=True),
                        'Campaign Type': st.column_config.TextColumn('Campaign Type', disabled=True),
                        'Product Group': st.column_config.TextColumn('Product Group'),
                        'Spend': st.column_config.TextColumn('Spend', disabled=True),
                        'State': st.column_config.TextColumn('State', disabled=True),
                        'Delete': st.column_config.CheckboxColumn('Delete Row')
                    }
                )
                st.session_state.campaign_tags_editor_temp = edited_df.copy()

                # --- Action Buttons ---
                col_save, col_remove, col_delete_sel, col_select_all = st.columns([1,1,1,1])
                with col_save:
                    if st.button("Save Changes", key="save_campaign_tags"):
                        # Write changes to config
                        for _, row in st.session_state.campaign_tags_editor_temp.iterrows():
                            campaign_name = row['Campaign Name']
                            product_group = row['Product Group']
                            if campaign_name in st.session_state.client_config['campaign_tags_data']:
                                st.session_state.client_config['campaign_tags_data'][campaign_name]['tag_1'] = product_group
                        save_client_config(st.session_state.selected_client_name, st.session_state.client_config)
                        st.success("Campaign tags updated.")
                
                with col_delete_sel:
                    if st.button("Delete Selected Rows", key="delete_selected_campaigns"):
                        rows_to_delete = st.session_state.campaign_tags_editor_temp[st.session_state.campaign_tags_editor_temp['Delete'] == True]
                        if not rows_to_delete.empty:
                            for _, row in rows_to_delete.iterrows():
                                cname = row['Campaign Name']
                                if cname in st.session_state.client_config['campaign_tags_data']:
                                    del st.session_state.client_config['campaign_tags_data'][cname]
                            save_client_config(st.session_state.selected_client_name, st.session_state.client_config)
                            st.success(f"Deleted {len(rows_to_delete)} campaign(s) successfully.")
                            st.rerun()
                        else:
                            st.info("No rows selected for deletion.")
                with col_remove:
                    if st.button("Remove Rows (Custom)", key="remove_all_rows_btn", help="Open options to remove rows in bulk or refresh from Bulk File"):
                        # Set dialog state to show confirmation
                        st.session_state.show_remove_rows_dialog = True
                with col_select_all:
                    if st.button("Select All Rows", key="select_all_campaign_rows"):
                        st.session_state.campaign_select_all = True
                        st.rerun()
                
                # Show confirmation dialog if triggered
                if st.session_state.get('show_remove_rows_dialog', False):
                    st.markdown("---")
                    st.markdown("""<div style='padding: 15px; background-color: rgba(255, 193, 7, 0.1); border: 2px solid #ffc107; border-radius: 8px; margin: 10px 0;'>
                        <h4 style='margin: 0 0 10px 0; color: #856404;'>âš ï¸ Remove All Campaign Rows</h4>
                        <p style='margin: 0; color: #856404;'>Choose what you want to do with all campaign data:</p>
                    </div>""", unsafe_allow_html=True)
                    
                    dialog_col1, dialog_col2, dialog_col3, dialog_col4 = st.columns(4)
                    
                    with dialog_col1:
                        if st.button("ðŸ”„ Refresh (Untagged)", key="option_1_btn", use_container_width=True, help="Remove all current rows and reload fresh, untagged campaign data from your uploaded Bulk File."):
                            # Option 1: Delete all rows and refresh from bulk file, clearing all tags.
                            st.session_state.client_config['campaign_tags_data'] = {}
                            
                            # Refresh campaigns from bulk file
                            if 'bulk_data' in st.session_state and st.session_state.bulk_data:
                                campaign_data = get_campaigns_from_bulk_file(st.session_state.bulk_data)
                                
                                # Update the campaign_tags_data with new campaigns from bulk file
                                for campaign_name, campaign_type in campaign_data.items():
                                    st.session_state.client_config['campaign_tags_data'][campaign_name] = {
                                        'campaign_type': campaign_type,
                                        'tag_1': ''
                                    }
                                
                                save_client_config(st.session_state.selected_client_name, st.session_state.client_config)
                                st.session_state.show_remove_rows_dialog = False
                                st.success(f"Removed all rows and refreshed {len(campaign_data)} campaigns from Bulk File.")
                                st.rerun()
                            else:
                                save_client_config(st.session_state.selected_client_name, st.session_state.client_config)
                                st.session_state.show_remove_rows_dialog = False
                                st.success("All campaign rows removed. No Bulk File available to refresh from.")
                                st.rerun()

                    with dialog_col2:
                        if st.button("ðŸ”„ Refresh & Keep Tags", key="option_3_btn", use_container_width=True, help="Reload campaigns from the Bulk File, keeping any existing tags."):
                            # Option 3: Refresh from bulk file but keep existing tags
                            existing_tags = {name: info.get('tag_1', '') for name, info in st.session_state.client_config.get('campaign_tags_data', {}).items()}
                            
                            st.session_state.client_config['campaign_tags_data'] = {}

                            if 'bulk_data' in st.session_state and st.session_state.bulk_data:
                                campaign_data = get_campaigns_from_bulk_file(st.session_state.bulk_data)
                                
                                for campaign_name, campaign_type in campaign_data.items():
                                    st.session_state.client_config['campaign_tags_data'][campaign_name] = {
                                        'campaign_type': campaign_type,
                                        'tag_1': existing_tags.get(campaign_name, '') # Keep old tag if it exists
                                    }
                                
                                save_client_config(st.session_state.selected_client_name, st.session_state.client_config)
                                st.session_state.show_remove_rows_dialog = False
                                st.success(f"Refreshed {len(campaign_data)} campaigns and preserved existing tags.")
                                st.rerun()
                            else:
                                save_client_config(st.session_state.selected_client_name, st.session_state.client_config)
                                st.session_state.show_remove_rows_dialog = False
                                st.warning("Campaign data cleared, but no Bulk File was available to refresh from.")
                                st.rerun()

                    with dialog_col3:
                        if st.button("ðŸ—‘ï¸ Delete All Rows Only", key="option_2_btn", use_container_width=True, help="Remove all campaign rows without refreshing from Bulk File"):
                            # Option 2: Delete all rows without refreshing
                            st.session_state.client_config['campaign_tags_data'] = {}
                            save_client_config(st.session_state.selected_client_name, st.session_state.client_config)
                            st.session_state.show_remove_rows_dialog = False
                            st.success("All campaign rows removed.")
                            st.rerun()
                            
                    with dialog_col4:
                        if st.button("Cancel", key="cancel_remove_rows_btn", use_container_width=True):
                            st.session_state.show_remove_rows_dialog = False
                            st.rerun()

                # Debug information for product groups
                st.markdown("---")
                if st.checkbox("Show Product Group Debug Info", key="debug_product_groups_campaign_tagging"):
                    st.write("**Product Groups from branded_asins_data:**")
                    if st.session_state.get('client_config') and 'branded_asins_data' in st.session_state.client_config:
                        asin_groups = set()
                        for asin_info in st.session_state.client_config['branded_asins_data'].values():
                            pg = asin_info.get('product_group', '')
                            if pg and pg.strip():
                                asin_groups.add(pg)
                        st.write(sorted(list(asin_groups)))
                    
                    st.write("**Product Groups from campaign_tags_data:**")
                    if st.session_state.get('client_config') and 'campaign_tags_data' in st.session_state.client_config:
                        campaign_groups = set()
                        for campaign_info in st.session_state.client_config['campaign_tags_data'].values():
                            pg = campaign_info.get('tag_1', '') or 'Untagged Group'
                            if pg and pg.strip():
                                campaign_groups.add(pg)
                        st.write(sorted(list(campaign_groups)))
                    
                    st.write("**Product Groups from dataframe:**")
                    if not campaigns_df.empty and 'Product Group' in campaigns_df.columns:
                        df_groups = campaigns_df['Product Group'].dropna().unique()
                        st.write([g for g in df_groups if g and str(g).strip()])



            else:
                st.markdown("""<div class='warning-box' style='margin-top: 20px;'>
                    <span style='font-weight: 500;'>Upload a Bulk File to see and tag campaigns.</span>
                </div>""", unsafe_allow_html=True)
        
        # ACoS Goals tab removed
        
        with settings_tab4:
            st.markdown("""
    <h4 style='font-family:'Inter','Roboto','Segoe UI',Arial,sans-serif; font-size:0.98rem; font-weight:600; color:#6c757d; margin-top:1.4rem; margin-bottom:0.7rem;'>
        Data Backup & Transfer
    </h4>""", unsafe_allow_html=True)
            
            if is_cloud_environment():
                st.info("ðŸŒ **Cloud Mode**: Your data is stored in your browser's localStorage. Use the export/import functions below to backup or transfer your data between browsers.")
            else:
                st.info("ðŸ’» **Local Mode**: Your data is stored on your computer's filesystem. You can still export/import for backup purposes.")
            
            st.markdown("---")
            
            # Export section
            st.subheader("ðŸ“¤ Export All Data")
            st.markdown("Download all your client configurations and saved sessions as a JSON file for backup or transfer to another browser.")
            
            if st.button("Export All Data", type="primary", use_container_width=True):
                try:
                    export_data = {
                        'version': '1.0',
                        'export_date': datetime.now().isoformat(),
                        'clients': {},
                        'sessions': {}
                    }
                    
                    # Export all client configs
                    clients = get_existing_clients()
                    for client_name in clients:
                        client_config = load_client_config(client_name)
                        if client_config:
                            export_data['clients'][client_name] = client_config
                            
                            # Export sessions for this client
                            sessions = get_saved_sessions(client_name)
                            export_data['sessions'][client_name] = []
                            
                            for session in sessions:
                                if is_cloud_environment():
                                    # Get session data from localStorage
                                    session_key = f'amazon_dashboard_session_data_{client_name}_{session["filename"]}'
                                    session_data = get_localStorage_value(session_key)
                                    if session_data:
                                        export_data['sessions'][client_name].append({
                                            'filename': session['filename'],
                                            'data': json.loads(session_data)
                                        })
                                else:
                                    # Get session data from filesystem
                                    filepath = os.path.join(CLIENT_SESSIONS_DIR, client_name, session['filename'])
                                    if os.path.exists(filepath):
                                        with open(filepath, 'r') as f:
                                            export_data['sessions'][client_name].append({
                                                'filename': session['filename'],
                                                'data': json.load(f)
                                            })
                    
                    # Create download button
                    export_json = json.dumps(export_data, indent=2)
                    st.download_button(
                        label="ðŸ’¾ Download Backup File",
                        data=export_json,
                        file_name=f"amazon_dashboard_backup_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json",
                        mime="application/json",
                        use_container_width=True
                    )
                    st.success("âœ… Export data prepared! Click the download button above.")
                    
                except Exception as e:
                    st.error(f"âŒ Error exporting data: {str(e)}")
            
            st.markdown("---")
            
            # Import section
            st.subheader("ðŸ“¥ Import Data")
            st.markdown("Upload a previously exported backup file to restore your data.")
            
            uploaded_backup = st.file_uploader("Choose backup file", type=['json'], key="backup_import")
            
            if uploaded_backup:
                try:
                    import_data = json.load(uploaded_backup)
                    
                    # Validate backup file
                    if 'version' not in import_data or 'clients' not in import_data:
                        st.error("âŒ Invalid backup file format.")
                    else:
                        st.success(f"âœ… Backup file loaded (exported on {import_data.get('export_date', 'unknown date')})")
                        
                        # Show what will be imported
                        st.markdown("**Contents:**")
                        num_clients = len(import_data['clients'])
                        total_sessions = sum(len(sessions) for sessions in import_data.get('sessions', {}).values())
                        st.write(f"- {num_clients} client(s)")
                        st.write(f"- {total_sessions} saved session(s)")
                        
                        # Show client list if not too many
                        if num_clients > 0:
                            with st.expander(f"ðŸ“‹ View Client List ({num_clients})", expanded=num_clients <= 5):
                                if num_clients <= 10:
                                    st.write(", ".join(sorted(import_data['clients'].keys())))
                                else:
                                    st.dataframe(
                                        pd.DataFrame({"Client Name": sorted(import_data['clients'].keys())}),
                                        use_container_width=True,
                                        hide_index=True,
                                        height=min(400, num_clients * 35 + 38)
                                    )
                        
                        col1, col2 = st.columns(2)
                        with col1:
                            import_mode = st.radio(
                                "Import mode:",
                                ["Merge (keep existing data)", "Replace (overwrite all data)"],
                                help="Merge will add to existing data. Replace will delete all current data first."
                            )
                        
                        if st.button("ðŸ”„ Import Data", type="primary", use_container_width=True):
                            try:
                                # Replace mode: clear existing data
                                if "Replace" in import_mode:
                                    if is_cloud_environment():
                                        # Clear localStorage
                                        for client_name in get_existing_clients():
                                            remove_localStorage_value(f'amazon_dashboard_client_{client_name}')
                                            # Clear sessions
                                            sessions = get_saved_sessions(client_name)
                                            for session in sessions:
                                                remove_localStorage_value(f'amazon_dashboard_session_data_{client_name}_{session["filename"]}')
                                            remove_localStorage_value(f'amazon_dashboard_sessions_{client_name}')
                                        remove_localStorage_value('amazon_dashboard_client_list')
                                    else:
                                        # Clear filesystem
                                        if os.path.exists(CLIENT_CONFIG_DIR):
                                            for file in os.listdir(CLIENT_CONFIG_DIR):
                                                if file.endswith('.json'):
                                                    os.remove(os.path.join(CLIENT_CONFIG_DIR, file))
                                        if os.path.exists(CLIENT_SESSIONS_DIR):
                                            import shutil
                                            shutil.rmtree(CLIENT_SESSIONS_DIR)
                                
                                # Import clients with progress
                                total_items = len(import_data['clients']) + sum(len(sessions) for sessions in import_data.get('sessions', {}).values())
                                progress_bar = st.progress(0)
                                status_text = st.empty()
                                current_item = 0
                                
                                for client_name, client_config in import_data['clients'].items():
                                    current_item += 1
                                    progress_bar.progress(current_item / total_items)
                                    status_text.text(f"Importing client {current_item}/{len(import_data['clients'])}: {client_name}")
                                    save_client_config(client_name, client_config)
                                
                                # Import sessions
                                session_counter = 0
                                for client_name, sessions_list in import_data.get('sessions', {}).items():
                                    for session_item in sessions_list:
                                        session_counter += 1
                                        current_item += 1
                                        progress_bar.progress(current_item / total_items)
                                        status_text.text(f"Importing session {session_counter}: {session_item['filename']}")
                                        
                                        filename = session_item['filename']
                                        session_data = session_item['data']
                                        
                                        if is_cloud_environment():
                                            # Save to localStorage
                                            session_key = f'amazon_dashboard_session_data_{client_name}_{filename}'
                                            set_localStorage_value(session_key, session_data)
                                            
                                            # Update sessions list
                                            sessions_key = f'amazon_dashboard_sessions_{client_name}'
                                            stored_sessions = get_localStorage_value(sessions_key)
                                            sessions_list_current = json.loads(stored_sessions) if stored_sessions else []
                                            
                                            session_metadata = {
                                                'filename': filename,
                                                'display_name': session_data.get('session_name', filename.replace('.json', '')),
                                                'timestamp': session_data.get('timestamp', 'Unknown'),
                                                'created_date': session_data.get('created_date', 'Unknown'),
                                                'description': session_data.get('description', 'No description'),
                                                'data_types': session_data.get('data_types', [])
                                            }
                                            
                                            # Remove old session with same filename if exists
                                            sessions_list_current = [s for s in sessions_list_current if s.get('filename') != filename]
                                            sessions_list_current.append(session_metadata)
                                            sessions_list_current.sort(key=lambda x: x.get('timestamp', ''), reverse=True)
                                            
                                            set_localStorage_value(sessions_key, sessions_list_current)
                                        else:
                                            # Save to filesystem
                                            client_session_dir = ensure_session_directory(client_name)
                                            filepath = os.path.join(client_session_dir, filename)
                                            with open(filepath, 'w') as f:
                                                json.dump(session_data, f, indent=2)
                                
                                # Clear progress indicators
                                progress_bar.empty()
                                status_text.empty()
                                
                                # Clear caches
                                clear_client_caches()
                                get_saved_sessions.clear()
                                
                                st.success("âœ… Data imported successfully! Please refresh the page.")
                                st.balloons()
                                
                            except Exception as e:
                                st.error(f"âŒ Error importing data: {str(e)}")
                                
                except json.JSONDecodeError:
                    st.error("âŒ Invalid JSON file. Please upload a valid backup file.")
                except Exception as e:
                    st.error(f"âŒ Error reading backup file: {str(e)}")
            
            st.markdown("---")
            st.caption("ðŸ’¡ **Tip**: Export your data regularly to prevent data loss, especially when using cloud mode where data is stored in your browser.")

        # Auto-save is now implemented for each individual setting
        st.divider() # Separator
        st.markdown("""<div style='padding: 10px 15px; background-color: rgba(38, 39, 48, 0.03); border-radius: 4px; margin-top: 10px;'>
            <span style='color: #555; font-size: 0.9rem;'>âœ“ Use the Save Changes buttons to save your edits</span>
        </div>""", unsafe_allow_html=True)

    elif st.session_state.current_page == "advertiser_actions":
        # Advertiser Actions Page
        st.title("Advertiser Actions")
        st.markdown("Take action on your advertising data with automated optimizations, pauses, and bulk file management.")
        
        # Check if data is available; allow page to render in limited mode
        has_bulk = bool(st.session_state.get('bulk_data'))
        has_client = bool(st.session_state.get('client_config'))
        if not (has_bulk and has_client):
            st.warning("Please upload advertising data and configure client settings for full functionality. Campaign Creation remains available; the Target/Search Term finder will be disabled without uploads.")
            st.info("Go to **File Uploads** to upload your bulk file, and **Client Settings Center** to configure your settings.")
            # Seed safe defaults so other sections don't error when referenced
            if 'bulk_data' not in st.session_state or st.session_state.get('bulk_data') is None:
                st.session_state.bulk_data = {}
            if 'client_config' not in st.session_state or st.session_state.get('client_config') is None:
                st.session_state.client_config = {}
        
        # Create tabs for different actions
        tab1, tab2, tab3, tab4, tab5 = st.tabs([
            "Negatives",
            "Pauses", 
            "Bid Optimization",
            "Campaign Creation",
            "Bulk File Export"
        ])
        
        with tab1:
            st.header("Negatives")
            st.markdown("Create negative keywords based on your branded terms and ASINs configured in Client Settings.")
            
            # Initialize session state for negative keywords
            if 'negative_keywords_to_add' not in st.session_state:
                st.session_state.negative_keywords_to_add = []
            
            # Based on Branded ASINs & Terms (only option)
            st.subheader("Based on Branded ASINs & Terms")
            st.markdown("Apply negative keywords from your Client Settings to campaigns containing 'Non' (non-branded campaigns).")
            
            # Get client config - fix branded ASINs source
            client_config = st.session_state.client_config
            
            # Get branded ASINs from the correct source (branded_asins_data)
            branded_asins_data = client_config.get('branded_asins_data', {})
            branded_asins = list(branded_asins_data.keys()) if branded_asins_data else []
            
            # Get branded terms
            branded_terms = client_config.get('branded_keywords', [])
            
            if branded_asins or branded_terms:
                # Show only totals
                col1, col2 = st.columns(2)
                with col1:
                    st.metric("Branded ASINs Configured", len(branded_asins))
                with col2:
                    st.metric("Branded Terms Configured", len(branded_terms))
                
                # Add level selection for negative keywords
                st.markdown("**Negative Keyword Application Level:**")
                neg_level = st.radio(
                    "Apply negative keywords at:",
                    ["Campaign Level (Default)", "Ad Group Level"],
                    key="neg_keyword_level",
                    help="Campaign level is recommended for most cases. Ad group level provides more granular control."
                )
                
                st.markdown("---")
                
                # Enhanced Campaign selection
                bulk_data = st.session_state.bulk_data
                all_campaign_options = []
                non_campaign_options = []
                
                # Get all campaigns from all campaign sheets with their IDs
                campaign_data = {}  # Store campaign info including IDs
                
                for sheet_name in ['Sponsored Products Campaigns', 'Sponsored Brands Campaigns', 'Sponsored Display Campaigns']:
                    if sheet_name in bulk_data:
                        df = bulk_data[sheet_name]
                        
                        # Check for required columns with flexible naming
                        campaign_name_col = None
                        state_col = None
                        campaign_id_col = None
                        
                        # Find campaign name column (try different variations)
                        for col in df.columns:
                            if col.lower() in ['campaign name', 'campaign', 'campaignname']:
                                campaign_name_col = col
                                break
                        
                        # Find state column
                        for col in df.columns:
                            if col.lower() in ['state', 'status', 'campaign state']:
                                state_col = col
                                break
                        
                        # Find campaign ID column
                        for col in df.columns:
                            if col.lower() in ['campaign id', 'campaign_id', 'campaignid', 'id']:
                                campaign_id_col = col
                                break
                        
                        if campaign_name_col and state_col:
                            enabled_campaigns = df[df[state_col].astype(str).str.lower() == 'enabled']
                            
                            # Store campaign data with IDs
                            for _, row in enabled_campaigns.iterrows():
                                campaign_name = row.get(campaign_name_col, '')
                                if isinstance(campaign_name, str) and campaign_name.strip():
                                    campaign_id = row.get(campaign_id_col, '') if campaign_id_col else ''
                                    campaign_key = f"{campaign_name} ({sheet_name})"
                                    campaign_data[campaign_key] = {
                                        'name': campaign_name,
                                        'id': campaign_id,
                                        'sheet': sheet_name,
                                        'type': sheet_name.replace(' Campaigns', '').replace('Sponsored ', '')
                                    }
                                    
                                    campaign_option = (campaign_name, sheet_name)
                                    all_campaign_options.append(campaign_option)
                                    if 'Non' in campaign_name:
                                        non_campaign_options.append(campaign_option)
                
                if all_campaign_options:
                    # Quick selection options
                    col1, col2 = st.columns(2)
                    with col1:
                        if st.button("Select All Enabled Non-Branded Campaigns", type="secondary"):
                            st.session_state.branded_neg_campaigns = [f"{camp[0]} ({camp[1]})" for camp in non_campaign_options]
                            st.rerun()
                    with col2:
                        if st.button("Clear Campaign Selection", type="secondary"):
                            st.session_state.branded_neg_campaigns = []
                            st.rerun()
                    
                    # Search and select campaigns
                    campaign_search = st.text_input(
                        "Search campaigns (leave blank to see all)",
                        key="campaign_search_branded"
                    )
                    
                    # Filter campaigns based on search
                    if campaign_search:
                        filtered_options = [
                            camp for camp in all_campaign_options 
                            if campaign_search.lower() in camp[0].lower()
                        ]
                    else:
                        filtered_options = all_campaign_options
                    
                    if filtered_options:
                        selected_campaigns = st.multiselect(
                            f"Select campaigns ({len(filtered_options)} available, {len(non_campaign_options)} contain 'Non')",
                            options=[f"{camp[0]} ({camp[1]})" for camp in filtered_options],
                            default=st.session_state.get('branded_neg_campaigns', []),
                            key="branded_neg_campaigns",
                            help="Tip: Use 'Select All Enabled Non-Branded Campaigns' button above for quick selection"
                        )
                    else:
                        st.info("No campaigns match your search criteria.")
                        selected_campaigns = []
                    
                    if selected_campaigns:
                        if st.button("Add Branded Negatives", type="primary"):
                            added_count = 0
                            is_ad_group_level = neg_level == "Ad Group Level"
                            
                            for campaign_display in selected_campaigns:
                                campaign_info = campaign_data.get(campaign_display, {})
                                campaign_name = campaign_info.get('name', '')
                                campaign_id = campaign_info.get('id', '')
                                sheet_name = campaign_info.get('sheet', '')
                                campaign_type = campaign_info.get('type', '')
                                
                                # Add branded terms as negative keywords
                                if campaign_type in ['Products', 'Brands'] and branded_terms:
                                    if is_ad_group_level:
                                        # For Sponsored Products and Sponsored Brands, get ad groups from the same bulk file where Entity = 'Ad Group'
                                        if campaign_type in ['Products', 'Brands'] and sheet_name in bulk_data:
                                            current_df = bulk_data[sheet_name]
                                            
                                            # Find Ad Group rows in the same sheet for this campaign
                                            if 'Entity' in current_df.columns:
                                                # For Ad Group entity rows, Campaign Name is often blank, so use Campaign Name (Informational Only)
                                                # Prioritize informational column for ad group lookups
                                                campaign_name_col = None
                                                
                                                # First try to find informational column
                                                for col in current_df.columns:
                                                    if 'informational only' in col.lower():
                                                        campaign_name_col = col
                                                        break
                                                
                                                # If not found, try regular campaign name columns
                                                if not campaign_name_col:
                                                    for col in current_df.columns:
                                                        if col.lower() in ['campaign name', 'campaign']:
                                                            campaign_name_col = col
                                                            break
                                                
                                                if campaign_name_col:
                                                    # Filter for ad group rows matching this campaign
                                                    ad_group_rows = current_df[
                                                        (current_df['Entity'].str.lower().fillna('') == 'ad group') &
                                                        (current_df[campaign_name_col].fillna('').str.strip() == campaign_name)
                                                    ]
                                                else:
                                                    ad_group_rows = pd.DataFrame()  # Empty if no campaign column found
                                                
                                                # Find column names with flexible naming
                                                ag_name_col = None
                                                ag_id_col = None
                                                
                                                for col in current_df.columns:
                                                    col_lower = col.lower()
                                                    # Prioritize informational columns first
                                                    if 'ad group name' in col_lower and 'informational only' in col_lower:
                                                        ag_name_col = col
                                                    elif col_lower in ['ad group name', 'ad group', 'adgroupname', 'adgroup name'] and not ag_name_col:
                                                        ag_name_col = col
                                                    elif col_lower in ['ad group id', 'ad group_id', 'adgroupid', 'adgroup id']:
                                                        ag_id_col = col
                                                
                                                for _, ag_row in ad_group_rows.iterrows():
                                                    ad_group_name = ag_row.get(ag_name_col, '') if ag_name_col else ''
                                                    ad_group_id = ag_row.get(ag_id_col, '') if ag_id_col else ''
                                                    
                                                    for term in branded_terms:
                                                        neg_keyword = {
                                                            'campaign_name': campaign_name,
                                                            'campaign_id': campaign_id,
                                                            'ad_group_name': ad_group_name,
                                                            'ad_group_id': ad_group_id,
                                                            'sheet_type': sheet_name,
                                                            'keyword': term,
                                                            'match_type': 'negativePhrase',
                                                            'entity': 'Negative Keyword',
                                                            'source': 'Branded Terms',
                                                            'level': 'Ad Group',
                                                            'operation': 'Create'
                                                        }
                                                        if neg_keyword not in st.session_state.negative_keywords_to_add:
                                                            st.session_state.negative_keywords_to_add.append(neg_keyword)
                                                            added_count += 1
                                        else:
                                            # For non-Sponsored Products campaigns, use the original sheet-based logic
                                            ad_group_sheet_map = {
                                                'Sponsored Brands Campaigns': 'SB Multi Ad Group Campaigns',
                                                'Sponsored Display Campaigns': 'Sponsored Display Ad Groups'
                                            }
                                            
                                            ad_group_sheet = ad_group_sheet_map.get(sheet_name, '')
                                            if ad_group_sheet not in bulk_data and sheet_name == 'Sponsored Brands Campaigns':
                                                # If the SB Multi sheet was normalized under 'Sponsored Brands Campaigns',
                                                # use the current sheet as the ad group source
                                                if sheet_name in bulk_data:
                                                    st.session_state.debug_messages.append(
                                                        "Using 'Sponsored Brands Campaigns' as ad group source (normalized from SB Multi)"
                                                    )
                                                    ad_group_sheet = sheet_name
                                                else:
                                                    alternative_sb_sheets = ['Sponsored Brands Ad Groups', 'SB Campaigns']
                                                    for alt_sheet in alternative_sb_sheets:
                                                        if alt_sheet in bulk_data:
                                                            ad_group_sheet = alt_sheet
                                                            break
                                            
                                            if ad_group_sheet in bulk_data:
                                                ad_group_df = bulk_data[ad_group_sheet]
                                                
                                                # Find column names with flexible naming
                                                ag_campaign_name_col = None
                                                ag_name_col = None
                                                ag_id_col = None
                                                
                                                for col in ad_group_df.columns:
                                                    col_lower = col.lower()
                                                    if col_lower in ['campaign name', 'campaign', 'campaignname']:
                                                        ag_campaign_name_col = col
                                                    elif col_lower in ['ad group name', 'ad group', 'adgroupname', 'adgroup name']:
                                                        ag_name_col = col
                                                    elif col_lower in ['ad group id', 'ad group_id', 'adgroupid', 'adgroup id']:
                                                        ag_id_col = col
                                                
                                                if ag_campaign_name_col:
                                                    campaign_ad_groups = ad_group_df[ad_group_df[ag_campaign_name_col] == campaign_name]
                                                    
                                                    for _, ag_row in campaign_ad_groups.iterrows():
                                                        ad_group_name = ag_row.get(ag_name_col, '') if ag_name_col else ''
                                                        ad_group_id = ag_row.get(ag_id_col, '') if ag_id_col else ''
                                                        
                                                        for term in branded_terms:
                                                            neg_keyword = {
                                                                'campaign_name': campaign_name,
                                                                'campaign_id': campaign_id,
                                                                'ad_group_name': ad_group_name,
                                                                'ad_group_id': ad_group_id,
                                                                'sheet_type': sheet_name,
                                                                'keyword': term,
                                                                'match_type': 'negativePhrase',
                                                                'entity': 'Negative Keyword',
                                                                'source': 'Branded Terms',
                                                                'level': 'Ad Group',
                                                                'operation': 'Create'
                                                            }
                                                            if neg_keyword not in st.session_state.negative_keywords_to_add:
                                                                st.session_state.negative_keywords_to_add.append(neg_keyword)
                                                                added_count += 1
                                    else:
                                        # Campaign level negatives (default)
                                        for term in branded_terms:
                                            neg_keyword = {
                                                'campaign_name': campaign_name,
                                                'campaign_id': campaign_id,
                                                'sheet_type': sheet_name,
                                                'keyword': term,
                                                'match_type': 'negativePhrase',
                                                'entity': 'Campaign Negative Keyword',
                                                'source': 'Branded Terms',
                                                'level': 'Campaign',
                                                'operation': 'Create'
                                            }
                                            if neg_keyword not in st.session_state.negative_keywords_to_add:
                                                st.session_state.negative_keywords_to_add.append(neg_keyword)
                                                added_count += 1
                                
                                # Add branded ASINs as Negative Product Targets (all campaign types, ad group level)
                                if branded_asins:
                                        # For Sponsored Products and Sponsored Brands, get ad groups from the same bulk file where Entity = 'Ad Group'
                                        if campaign_type in ['Products', 'Brands'] and sheet_name in bulk_data:
                                            current_df = bulk_data[sheet_name]
                                            
                                            # Find Ad Group rows in the same sheet for this campaign
                                            if 'Entity' in current_df.columns:
                                                # Debug: Check available columns and Entity values
                                                debug_info = []
                                                debug_info.append(f"Available columns: {list(current_df.columns)}")
                                                entity_values = current_df['Entity'].value_counts()
                                                debug_info.append(f"Entity values in sheet: {entity_values.to_dict()}")
                                                
                                                # For Ad Group entity rows, Campaign Name is usually blank, so use Campaign Name (Informational Only)
                                                # First try the informational column, then fall back to regular Campaign Name
                                                campaign_name_col = None
                                                
                                                # Debug: Check for informational column variations
                                                informational_cols = [col for col in current_df.columns if 'informational only' in col.lower()]
                                                debug_info.append(f"Informational columns found: {informational_cols}")
                                                
                                                if 'Campaign Name (Informational Only)' in current_df.columns:
                                                    campaign_name_col = 'Campaign Name (Informational Only)'
                                                    debug_info.append("Selected: Campaign Name (Informational Only)")
                                                elif informational_cols:
                                                    # Use the first informational column found
                                                    campaign_name_col = informational_cols[0]
                                                    debug_info.append(f"Selected first informational column: {campaign_name_col}")
                                                elif 'Campaign Name' in current_df.columns:
                                                    campaign_name_col = 'Campaign Name'
                                                    debug_info.append("Selected: Campaign Name")
                                                elif 'Campaign' in current_df.columns:
                                                    campaign_name_col = 'Campaign'
                                                    debug_info.append("Selected: Campaign")
                                                
                                                if campaign_name_col:
                                                    debug_info.append(f"Using campaign column: {campaign_name_col}")
                                                    debug_info.append(f"Looking for campaign: {campaign_name}")
                                                    
                                                    # Check what campaigns are available in this column for Ad Group entities
                                                    ad_group_campaigns = current_df[
                                                        current_df['Entity'] == 'Ad Group'
                                                    ][campaign_name_col].dropna().unique()
                                                    debug_info.append(f"Available campaigns for Ad Groups in {campaign_name_col}: {list(ad_group_campaigns)[:10]}...")  # Show first 10
                                                    
                                                    ad_group_rows = current_df[
                                                        (current_df['Entity'] == 'Ad Group') &
                                                        (current_df[campaign_name_col] == campaign_name)
                                                    ]
                                                    debug_info.append(f"Found {len(ad_group_rows)} ad group rows for campaign {campaign_name}")
                                                    
                                                    # Show debug info in error if no ad groups found
                                                    if len(ad_group_rows) == 0:
                                                        st.error(f"DEBUG - No ad groups found for branded ASINs. {' | '.join(debug_info)}")
                                                else:
                                                    debug_info.append(f"No campaign name column found")
                                                    st.error(f"DEBUG - {' | '.join(debug_info)}")
                                                    ad_group_rows = pd.DataFrame()  # Empty if no campaign column found
                                                
                                                # Find column names with flexible naming
                                                ag_name_col = None
                                                ag_id_col = None
                                                
                                                for col in current_df.columns:
                                                    col_lower = col.lower()
                                                    if col_lower in ['ad group name', 'ad group', 'adgroupname', 'adgroup name']:
                                                        ag_name_col = col
                                                    elif col_lower in ['ad group id', 'ad group_id', 'adgroupid', 'adgroup id']:
                                                        ag_id_col = col
                                                
                                                for _, ag_row in ad_group_rows.iterrows():
                                                    ad_group_name = ag_row.get(ag_name_col, '') if ag_name_col else ''
                                                    ad_group_id = ag_row.get(ag_id_col, '') if ag_id_col else ''
                                                    
                                                    # Ensure Ad Group ID is present (required for Negative Product Targets)
                                                    if not ad_group_id:
                                                        st.warning(f"Warning: Ad Group ID missing for {ad_group_name} in {campaign_name}. Negative Product Targets require Ad Group ID.")
                                                        continue
                                                    
                                                    for asin in branded_asins:
                                                        # Determine targeting expression column based on campaign type
                                                        targeting_column = 'Product Targeting Expression'
                                                        
                                                        neg_product_target = {
                                                            'campaign_name': campaign_name,
                                                            'campaign_id': campaign_id,
                                                            'ad_group_name': ad_group_name,
                                                            'ad_group_id': ad_group_id,
                                                            'sheet_type': sheet_name,
                                                            'asin': asin,
                                                            'targeting_expression': f'asin="{asin}"',
                                                            'targeting_column': targeting_column,
                                                            'entity': 'Negative Product Targeting',
                                                            'source': 'Branded ASINs',
                                                            'level': 'Ad Group',
                                                            'operation': 'Create'
                                                        }
                                                        
                                                        # Check for duplicates more carefully
                                                        is_duplicate = False
                                                        for existing in st.session_state.negative_keywords_to_add:
                                                            if (existing.get('entity') == 'Negative Product Targeting' and
                                                                existing.get('campaign_name') == campaign_name and
                                                                existing.get('ad_group_id') == ad_group_id and
                                                                existing.get('asin') == asin):
                                                                is_duplicate = True
                                                                break
                                                        
                                                        if not is_duplicate:
                                                            st.session_state.negative_keywords_to_add.append(neg_product_target)
                                                            added_count += 1
                                            else:
                                                st.error(f"Error: Entity column not found in bulk data. Cannot add Negative Product Targets for Sponsored Products.")
                                        else:
                                            # For non-Sponsored Products campaigns, use the original sheet-based logic
                                            ad_group_sheet_map = {
                                                'Sponsored Brands Campaigns': 'SB Multi Ad Group Campaigns',
                                                'Sponsored Display Campaigns': 'Sponsored Display Ad Groups'
                                            }
                                            
                                            ad_group_sheet = ad_group_sheet_map.get(sheet_name, '')
                                            # Try alternative sheet names for SB if primary not found
                                            if ad_group_sheet not in bulk_data and sheet_name == 'Sponsored Brands Campaigns':
                                                alternative_sb_sheets = ['Sponsored Brands Ad Groups', 'SB Campaigns']
                                                for alt_sheet in alternative_sb_sheets:
                                                    if alt_sheet in bulk_data:
                                                        ad_group_sheet = alt_sheet
                                                        break
                                            
                                            if ad_group_sheet in bulk_data:
                                                ad_group_df = bulk_data[ad_group_sheet]
                                                
                                                # Find column names with flexible naming
                                                ag_campaign_name_col = None
                                                ag_name_col = None
                                                ag_id_col = None
                                                
                                                for col in ad_group_df.columns:
                                                    col_lower = col.lower()
                                                    if col_lower in ['campaign name', 'campaign', 'campaignname']:
                                                        ag_campaign_name_col = col
                                                    elif col_lower in ['ad group name', 'ad group', 'adgroupname', 'adgroup name']:
                                                        ag_name_col = col
                                                    elif col_lower in ['ad group id', 'ad group_id', 'adgroupid', 'adgroup id']:
                                                        ag_id_col = col
                                                
                                                if ag_campaign_name_col:
                                                    campaign_ad_groups = ad_group_df[ad_group_df[ag_campaign_name_col] == campaign_name]
                                                    
                                                    for _, ag_row in campaign_ad_groups.iterrows():
                                                        ad_group_name = ag_row.get(ag_name_col, '') if ag_name_col else ''
                                                        ad_group_id = ag_row.get(ag_id_col, '') if ag_id_col else ''
                                                        
                                                        # Ensure Ad Group ID is present (required for Negative Product Targets)
                                                        if not ad_group_id:
                                                            st.warning(f"Warning: Ad Group ID missing for {ad_group_name} in {campaign_name}. Negative Product Targets require Ad Group ID.")
                                                            continue
                                                        
                                                        for asin in branded_asins:
                                                            # Determine targeting expression column based on campaign type
                                                            if campaign_type == 'Display':
                                                                targeting_column = 'Targeting Expression'
                                                            else:  # Brands
                                                                targeting_column = 'Product Targeting Expression'
                                                            
                                                            neg_product_target = {
                                                                'campaign_name': campaign_name,
                                                                'campaign_id': campaign_id,
                                                                'ad_group_name': ad_group_name,
                                                                'ad_group_id': ad_group_id,
                                                                'sheet_type': sheet_name,
                                                                'asin': asin,
                                                                'targeting_expression': f'asin="{asin}"',
                                                                'targeting_column': targeting_column,
                                                                'entity': 'Negative Product Targeting',
                                                                'source': 'Branded ASINs',
                                                                'level': 'Ad Group',
                                                                'operation': 'Create'
                                                            }
                                                            
                                                            # Check for duplicates more carefully
                                                            is_duplicate = False
                                                            for existing in st.session_state.negative_keywords_to_add:
                                                                if (existing.get('entity') == 'Negative Product Targeting' and
                                                                    existing.get('campaign_name') == campaign_name and
                                                                    existing.get('ad_group_id') == ad_group_id and
                                                                    existing.get('asin') == asin):
                                                                    is_duplicate = True
                                                                    break
                                                            
                                                            if not is_duplicate:
                                                                st.session_state.negative_keywords_to_add.append(neg_product_target)
                                                                added_count += 1
                                                else:
                                                    st.error(f"Error: Could not find campaign name column in {ad_group_sheet}")
                                            else:
                                                st.error(f"Error: Ad group sheet '{ad_group_sheet}' not found in bulk data. Cannot add Negative Product Targets.")
                            
                            st.success(f"Added {added_count} negative items to {len(selected_campaigns)} campaigns!")
                            st.rerun()

                else:
                    st.info("Configure branded ASINs and terms in Client Settings Center first.")
            
            # Shared table for negative keywords queue
            st.markdown("---")
            st.subheader("Negative Keywords Queue")
            
            if st.session_state.negative_keywords_to_add:
                # Convert to proper bulk file 2.0 format for preview
                preview_data = []
                
                for item in st.session_state.negative_keywords_to_add:
                    if item.get('entity') == 'Negative Keyword' and item.get('level') == 'Campaign':
                        preview_row = {
                            'Entity': 'Negative Keyword',
                            'Operation': item.get('operation', 'Create'),
                            'Campaign ID': item.get('campaign_id', ''),
                            'Campaign': item.get('campaign_name', ''),
                            'Keyword Text': item.get('keyword', ''),
                            'Match Type': item.get('match_type', 'negativePhrase'),
                            'State': 'enabled',
                            'Source': item.get('source', ''),
                            'Level': item.get('level', 'Campaign')
                        }
                    elif item.get('entity') == 'Negative Keyword' and item.get('level') == 'Ad Group':
                        preview_row = {
                            'Entity': 'Negative Keyword',
                            'Operation': item.get('operation', 'Create'),
                            'Campaign ID': item.get('campaign_id', ''),
                            'Campaign': item.get('campaign_name', ''),
                            'Ad Group ID': item.get('ad_group_id', ''),
                            'Ad Group': item.get('ad_group_name', ''),
                            'Keyword Text': item.get('keyword', ''),
                            'Match Type': item.get('match_type', 'negativePhrase'),
                            'State': 'enabled',
                            'Source': item.get('source', ''),
                            'Level': item.get('level', 'Ad Group')
                        }
                    elif item.get('entity') == 'Negative Product Targeting':
                        # Determine targeting column based on campaign type
                        targeting_col = item.get('targeting_column', 'Product Targeting Expression')
                        preview_row = {
                            'Entity': 'Negative Product Targeting',
                            'Operation': item.get('operation', 'Create'),
                            'Campaign ID': item.get('campaign_id', ''),
                            'Campaign': item.get('campaign_name', ''),
                            'Ad Group ID': item.get('ad_group_id', ''),
                            'Ad Group': item.get('ad_group_name', ''),
                            targeting_col: item.get('targeting_expression', ''),
                            'State': 'enabled',
                            'Source': item.get('source', ''),
                            'Level': item.get('level', 'Ad Group')
                        }
                    else:
                        # Fallback for other types
                        preview_row = item.copy()
                    
                    preview_data.append(preview_row)
                
                neg_df = pd.DataFrame(preview_data)
                
                # Display with proper formatting
                st.markdown(f"**{len(neg_df)} items queued for export**")
                st.dataframe(neg_df, use_container_width=True)
                
                col1, col2 = st.columns(2)
                with col1:
                    if st.button("Clear All", type="secondary"):
                        st.session_state.negative_keywords_to_add = []
                        st.rerun()
                
                with col2:
                    if st.button("Add to Bulk Export", type="primary"):
                        # Add to bulk export queue
                        if 'bulk_export_actions' not in st.session_state:
                            st.session_state.bulk_export_actions = []
                        
                        for neg_kw in st.session_state.negative_keywords_to_add:
                            action = {
                                'action_type': 'negative_keyword',
                                **neg_kw  # Pass through all fields from the negative keyword data
                            }
                            st.session_state.bulk_export_actions.append(action)
                        
                        st.success(f"Added {len(st.session_state.negative_keywords_to_add)} negative items to bulk export queue!")
                        st.session_state.negative_keywords_to_add = []
                        st.rerun()
            else:
                st.info("No negative keywords queued. Select a source above and add keywords to see them here.")
        
        with tab2:
            st.header("Pauses")
            st.markdown("Select and pause underperforming targets, products, or entire campaigns.")
            
            # Initialize session state for pauses
            if 'items_to_pause' not in st.session_state:
                st.session_state.items_to_pause = []
            
            # Helpers
            def _find_col(df, include_tokens, exclude_tokens=None):
                cols = list(df.columns)
                for c in cols:
                    name = str(c).lower()
                    if all(tok in name for tok in include_tokens) and (not exclude_tokens or all(tok not in name for tok in exclude_tokens)):
                        return c
                return None

            def _campaign_name_col(df):
                info = _find_col(df, ["campaign name", "informational"])  # Prefer informational
                if info and info in df.columns:
                    return info
                return 'Campaign Name' if 'Campaign Name' in df.columns else None

            def _to_numeric(series):
                # Strip $, %, and commas then coerce
                return pd.to_numeric(
                    series.astype(str).str.replace('$', '', regex=False).str.replace('%', '', regex=False).str.replace(',', '', regex=False),
                    errors='coerce'
                )

            def _gather_campaign_sheets(bulk_data):
                sheets = []
                for nm in ['Sponsored Products Campaigns', 'Sponsored Brands Campaigns', 'Sponsored Display Campaigns']:
                    if nm in bulk_data and isinstance(bulk_data[nm], pd.DataFrame) and not bulk_data[nm].empty:
                        df = bulk_data[nm].copy()
                        df['Sheet_Source'] = nm
                        # Normalize numeric metric columns if present
                        for col in ['Spend', 'Ad Sales', 'Sales', 'Orders', 'ACoS', 'ROAS']:
                            if col in df.columns:
                                df[col] = _to_numeric(df[col]).fillna(0)
                        # Ensure State column exists as string
                        if 'State' in df.columns:
                            df['State'] = df['State'].astype(str)
                        # Normalize an Entity column if possible
                        ent_col = 'Entity' if 'Entity' in df.columns else _find_col(df, ['record', 'type'])
                        if ent_col:
                            df['__EntityNorm__'] = df[ent_col].astype(str).str.strip().str.lower()
                        else:
                            df['__EntityNorm__'] = ''
                        # Normalize Bid as numeric
                        if 'Bid' in df.columns:
                            df['Bid'] = _to_numeric(df['Bid']).fillna(0)
                        sheets.append(df)
                return sheets

            def _infer_product_from_sheet(sheet_name):
                if 'Brands' in sheet_name:
                    return 'Sponsored Brands'
                if 'Display' in sheet_name:
                    return 'Sponsored Display'
                return 'Sponsored Products'

            bulk_data = st.session_state.bulk_data
            sheets = _gather_campaign_sheets(bulk_data)
            if not sheets:
                st.info("No campaign sheets found in bulk file. Please ensure 'Sponsored Products/Brands/Display Campaigns' sheets are present.")
            else:
                mode = st.radio("Pause mode", ["Based on performance", "Paste list"], horizontal=True, key="pauses_mode")

                if mode == "Based on performance":
                    st.subheader("Performance-based pauses")

                    entity_options = ["Targets", "Product Ads", "Campaigns"]
                    selected_entities = st.multiselect("Entities to evaluate (optional)", entity_options, default=entity_options, key="pauses_perf_entities")

                    # Initialize per-entity filter state with defaults: Spend > 100 and ROAS < 1
                    if 'pauses_perf_filters' not in st.session_state:
                        st.session_state.pauses_perf_filters = {}
                    for ent in selected_entities:
                        if ent not in st.session_state.pauses_perf_filters:
                            st.session_state.pauses_perf_filters[ent] = {
                                'groups': [
                                    {
                                        'join_with_prev': 'AND',  # only applies from the 2nd group onward
                                        'within': 'ALL',          # ALL (AND) or ANY (OR)
                                        'conditions': [
                                            {'field': 'Spend', 'op': '>', 'value': 100.0},
                                            {'field': 'ROAS', 'op': '<', 'value': 1.0}
                                        ]
                                    }
                                ]
                            }

                    def _render_condition(cond, key_ns):
                        # Side-by-side compact UI: Field | Operator | Value (labels collapsed)
                        fields = ["ROAS", "Spend", "Ad Sales", "Orders", "ACoS", "Campaign/Keyword"]
                        c1, c2, c3 = st.columns([2, 1, 3])
                        with c1:
                            field = st.selectbox(
                                "Field",
                                fields,
                                index=(fields.index(cond.get('field')) if cond.get('field') in fields else 0),
                                key=f"{key_ns}_field",
                                label_visibility="collapsed",
                                help="Field",
                            )
                            cond['field'] = field
                        # Choose operator set by field type
                        if field == "Campaign/Keyword":
                            ops = ["contains", "doesn't contain", "equals", "doesn't equal"]
                            default_op = "contains"
                        else:
                            ops = [">", ">=", "=", "<=", "<"]
                            default_op = "<"
                        with c2:
                            op = st.selectbox(
                                "Op",
                                ops,
                                index=(ops.index(cond.get('op')) if cond.get('op') in ops else ops.index(default_op)),
                                key=f"{key_ns}_op",
                                label_visibility="collapsed",
                                help="Operator",
                            )
                            cond['op'] = op
                        with c3:
                            if field == "Campaign/Keyword":
                                val = st.text_input(
                                    "Value",
                                    value=str(cond.get('value', '')),
                                    key=f"{key_ns}_val",
                                    label_visibility="collapsed",
                                    placeholder="text",
                                )
                            else:
                                # Safely coerce previous value to float in case user switched from text field
                                try:
                                    start_val = float(cond.get('value', 0.0))
                                except Exception:
                                    start_val = 0.0
                                val = st.number_input(
                                    "Value",
                                    value=start_val,
                                    key=f"{key_ns}_val",
                                    label_visibility="collapsed",
                                )
                            cond['value'] = val

                    def _apply_groups(df, groups):
                        # Build mask per group, then combine groups by join_with_prev
                        if not groups:
                            return pd.Series([True]*len(df), index=df.index)

                        def _cond_mask(df_, c):
                            if str(c.get('field','')).strip() == 'Campaign/Keyword':
                                # Apply to Campaign Name (Informational Only) and Keyword Text
                                cn_col = _campaign_name_col(df_)
                                kw_col = 'Keyword Text' if 'Keyword Text' in df_.columns else None
                                s1 = df_[cn_col].astype(str) if cn_col else pd.Series(['']*len(df_), index=df_.index)
                                s2 = df_[kw_col].astype(str) if kw_col else pd.Series(['']*len(df_), index=df_.index)
                                op = c.get('op','contains')
                                val = str(c.get('value',''))
                                if op == 'contains':
                                    m = s1.str.contains(val, case=False, na=False) | s2.str.contains(val, case=False, na=False)
                                elif op == "doesn't contain":
                                    m = ~s1.str.contains(val, case=False, na=False) & ~s2.str.contains(val, case=False, na=False)
                                elif op == 'equals':
                                    m = s1.str.strip().str.lower().eq(val.strip().lower()) | s2.str.strip().str.lower().eq(val.strip().lower())
                                else:  # doesn't equal
                                    m = ~s1.str.strip().str.lower().eq(val.strip().lower()) & ~s2.str.strip().str.lower().eq(val.strip().lower())
                                return m
                            # metric
                            field = c.get('field')
                            col = None
                            if field == 'Ad Sales':
                                col = 'Ad Sales' if 'Ad Sales' in df_.columns else ('Sales' if 'Sales' in df_.columns else None)
                            else:
                                col = field if field in df_.columns else None
                            if not col:
                                return pd.Series([True]*len(df_), index=df_.index)
                            s = _to_numeric(df_[col]).fillna(0)
                            op = c.get('op','<')
                            val = float(c.get('value', 0))
                            if op == '>':
                                return s > val
                            if op == '>=':
                                return s >= val
                            if op == '=':
                                return s == val
                            if op == '<=':
                                return s <= val
                            return s < val

                        # Evaluate groups
                        result_mask = None
                        for idx, g in enumerate(groups):
                            conds = g.get('conditions', [])
                            within = g.get('within','ALL')
                            if not conds:
                                g_mask = pd.Series([True]*len(df), index=df.index)
                            else:
                                masks = [_cond_mask(df, c) for c in conds]
                                g_mask = masks[0]
                                for m in masks[1:]:
                                    g_mask = (g_mask & m) if within == 'ALL' else (g_mask | m)
                            if result_mask is None:
                                result_mask = g_mask
                            else:
                                join = g.get('join_with_prev','AND')
                                result_mask = (result_mask & g_mask) if join == 'AND' else (result_mask | g_mask)
                        return result_mask if result_mask is not None else pd.Series([True]*len(df), index=df.index)

                    # Render per-entity criteria builders
                    for ent in selected_entities:
                        st.markdown(f"#### Criteria for {ent}")
                        data = st.session_state.pauses_perf_filters.get(ent, {'groups': []})
                        groups = data.get('groups', [])

                        # Safety: If no groups exist (first load or after all removed), seed defaults
                        if not groups:
                            groups = [{
                                'join_with_prev': 'AND',
                                'within': 'ALL',
                                'conditions': [
                                    {'field': 'Spend', 'op': '>', 'value': 100.0},
                                    {'field': 'ROAS', 'op': '<', 'value': 1.0}
                                ]
                            }]
                            st.session_state.pauses_perf_filters[ent]['groups'] = groups

                        # Buttons to add a group
                        if st.button(f"Add group (+)", key=f"{ent}_add_group"):
                            groups.append({'join_with_prev': 'AND', 'within': 'ALL', 'conditions': []})
                            st.session_state.pauses_perf_filters[ent]['groups'] = groups
                            st.rerun()

                        # Render each group
                        for gi, g in enumerate(groups):
                            with st.container():
                                st.markdown(f"Group {gi+1}")
                                cols = st.columns([1,1,1,1])
                                if gi > 0:
                                    g['join_with_prev'] = cols[0].selectbox("Combine with previous", ["AND", "OR"], index=["AND","OR"].index(g.get('join_with_prev','AND')), key=f"{ent}_g{gi}_join")
                                else:
                                    cols[0].markdown("&nbsp;")
                                g['within'] = cols[1].selectbox("Require", ["ALL", "ANY"], index=["ALL","ANY"].index(g.get('within','ALL')), help="ALL=AND within group, ANY=OR within group", key=f"{ent}_g{gi}_within")
                                # Add/remove condition buttons
                                if cols[2].button("Add condition", key=f"{ent}_g{gi}_addc"):
                                    g['conditions'].append({'field': 'ROAS', 'op': '<', 'value': 1.0})
                                    st.session_state.pauses_perf_filters[ent]['groups'] = groups
                                    st.rerun()
                                if cols[3].button("Remove group", key=f"{ent}_g{gi}_rm"):
                                    groups.pop(gi)
                                    st.session_state.pauses_perf_filters[ent]['groups'] = groups
                                    st.rerun()

                                # Render conditions
                                for ci, cond in enumerate(g.get('conditions', [])):
                                    with st.container():
                                        cc = st.columns([6,1])
                                        with cc[0]:
                                            _render_condition(cond, key_ns=f"{ent}_g{gi}_c{ci}")
                                        with cc[1]:
                                            if st.button("Remove", key=f"{ent}_g{gi}_c{ci}_rm"):
                                                g['conditions'].pop(ci)
                                                st.session_state.pauses_perf_filters[ent]['groups'] = groups
                                                st.rerun()

                        # Save back (in case of edits without rerun)
                        st.session_state.pauses_perf_filters[ent]['groups'] = groups

                    # Evaluate across sheets per entity
                    matches = []
                    for df in sheets:
                        base = df
                        # Build per-entity masks
                        ents_to_eval = selected_entities
                        for ent in ents_to_eval:
                            groups = st.session_state.pauses_perf_filters.get(ent, {}).get('groups', [])
                            if not groups:
                                continue
                            ent_mask = pd.Series([True]*len(base), index=base.index)
                            if ent == 'Targets':
                                # Bid > 0 and not campaign/product ad/ad group
                                bid_mask = _to_numeric(base['Bid']).fillna(0) > 0 if 'Bid' in base.columns else pd.Series([False]*len(base), index=base.index)
                                ent_mask = bid_mask & ~base['__EntityNorm__'].isin(['campaign', 'product ad', 'ad group'])
                            elif ent == 'Product Ads':
                                ent_mask = base['__EntityNorm__'].eq('product ad')
                            else:  # Campaigns
                                ent_mask = base['__EntityNorm__'].eq('campaign')

                            filt_mask = _apply_groups(base, groups)
                            m = ent_mask & filt_mask
                            sub = base[m].copy()
                            if not sub.empty:
                                sub['__PauseType__'] = 'target' if ent == 'Targets' else ('product_ad' if ent == 'Product Ads' else 'campaign')
                                matches.append(sub)

                    if matches:
                        result = pd.concat(matches, ignore_index=True)
                        st.markdown(f"**Found {len(result)} rows across selected entities.**")

                        # Simple preview
                        prev_cols = [c for c in [
                            _campaign_name_col(result), 'Ad Group Name', 'Keyword Text', 'Product Targeting Expression', 'ASIN', 'SKU', 'ROAS', 'Spend', 'Ad Sales', 'Orders', 'ACoS', 'Bid', '__PauseType__', 'Sheet_Source'
                        ] if c and c in result.columns]
                        st.dataframe(result[prev_cols].head(200), use_container_width=True)

                        if st.button("Add matches to Pause Queue", type="primary", key="pauses_perf_add"):
                            added = 0
                            for _, row in result.iterrows():
                                pause_item = {
                                    'type': row['__PauseType__'],
                                    'campaign_name': row.get(_campaign_name_col(result), row.get('Campaign Name', '')),
                                    'target': row.get('Keyword Text', row.get('Product Targeting Expression', '')),
                                    'asin': row.get('ASIN', ''),
                                    'sku': row.get('SKU', ''),
                                    'spend': row.get('Spend', None),
                                    'adsales': row.get('Ad Sales', row.get('Sales', None)),
                                    'orders': row.get('Orders', None),
                                    'acos': row.get('ACoS', None),
                                    'roas': row.get('ROAS', None),
                                    'sheet_source': row.get('Sheet_Source', ''),
                                    'row_data': row.to_dict(),
                                }
                                if pause_item not in st.session_state.items_to_pause:
                                    st.session_state.items_to_pause.append(pause_item)
                                    added += 1
                            st.success(f"Added {added} item(s) to pause queue")
                            st.rerun()
                    else:
                        st.info("No rows match the selected criteria.")

                else:
                    st.subheader("Paste list to pause")
                    entity_choice = st.selectbox("Entity", ["Targets", "Product Ads", "Campaigns"], index=0, key="pauses_list_entity")
                    id_choice = None
                    if entity_choice == "Product Ads":
                        id_choice = st.radio("Identifiers are", ["ASINs", "SKUs"], horizontal=True, key="pauses_productad_id")
                    st.caption("Enter one value per line. Searches are case-insensitive.")
                    raw = st.text_area("Paste values", height=200, key="pauses_list_values")
                    values = [v.strip() for v in raw.splitlines() if v.strip()]
                    exclude_mode = st.checkbox(
                        "Pause all enabled rows for this entity except the pasted values",
                        value=False,
                        key="pauses_list_exclude",
                        help="When checked, the pasted values act as an exclusion list. All ENABLED rows for the chosen entity will be queued, except any that match the pasted values."
                    )
                    if st.button("Find and queue pauses", type="primary", key="pauses_list_add") and values:
                        # Build matches across sheets
                        total_added = 0
                        for df in sheets:
                            cn_col = _campaign_name_col(df)
                            ent_norm = df['__EntityNorm__']
                            # Common masks
                            enabled_mask = df['State'].astype(str).str.lower().eq('enabled') if 'State' in df.columns else pd.Series([True]*len(df), index=df.index)
                            if entity_choice == "Campaigns":
                                base_ent_mask = (ent_norm == 'campaign')
                                # Exception mask by campaign name (prefer informational, fallback to 'Campaign Name')
                                name_col = cn_col if cn_col else ('Campaign Name' if 'Campaign Name' in df.columns else None)
                                if name_col:
                                    exc = pd.Series([False]*len(df), index=df.index)
                                    for v in values:
                                        exc = exc | df[name_col].astype(str).str.strip().str.lower().eq(v.lower())
                                else:
                                    exc = pd.Series([False]*len(df), index=df.index)
                                if exclude_mode:
                                    sub = df[base_ent_mask & enabled_mask & ~exc]
                                else:
                                    sub = df[base_ent_mask & exc]
                            elif entity_choice == "Targets":
                                # Eligible targets: not campaign/product ad/ad group, bid>0 when Bid exists
                                base_ent_mask = ~ent_norm.isin(['campaign', 'product ad', 'ad group'])
                                if 'Bid' in df.columns:
                                    base_ent_mask = base_ent_mask & (_to_numeric(df['Bid']).fillna(0) > 0)
                                # Exception mask by keyword/targeting
                                exc = pd.Series([False]*len(df), index=df.index)
                                if 'Keyword Text' in df.columns:
                                    for v in values:
                                        exc = exc | df['Keyword Text'].astype(str).str.strip().str.lower().eq(v.lower())
                                if 'Product Targeting Expression' in df.columns:
                                    for v in values:
                                        exc = exc | df['Product Targeting Expression'].astype(str).str.strip().str.lower().eq(v.lower())
                                if exclude_mode:
                                    sub = df[base_ent_mask & enabled_mask & ~exc]
                                else:
                                    sub = df[base_ent_mask & exc]
                            else:  # Product Ads
                                base_ent_mask = (ent_norm == 'product ad')
                                id_col = 'ASIN' if id_choice == 'ASINs' else 'SKU'
                                if id_col in df.columns:
                                    exc = pd.Series([False]*len(df), index=df.index)
                                    for v in values:
                                        exc = exc | df[id_col].astype(str).str.strip().str.lower().eq(v.lower())
                                    if exclude_mode:
                                        sub = df[base_ent_mask & enabled_mask & ~exc]
                                    else:
                                        sub = df[base_ent_mask & exc]
                                else:
                                    sub = df.iloc[0:0]

                            if not sub.empty:
                                for _, row in sub.iterrows():
                                    pause_item = {
                                        'type': 'campaign' if row['__EntityNorm__'] == 'campaign' else ('product_ad' if row['__EntityNorm__'] == 'product ad' else 'target'),
                                        'campaign_name': row.get(cn_col, row.get('Campaign Name', '')),
                                        'target': row.get('Keyword Text', row.get('Product Targeting Expression', '')),
                                        'asin': row.get('ASIN', ''),
                                        'sku': row.get('SKU', ''),
                                        'sheet_source': row.get('Sheet_Source', ''),
                                        'row_data': row.to_dict(),
                                    }
                                    if pause_item not in st.session_state.items_to_pause:
                                        st.session_state.items_to_pause.append(pause_item)
                                        total_added += 1
                        if total_added:
                            st.success(f"Queued {total_added} item(s) for pause")
                            st.rerun()
                        else:
                            st.info("No matches found for pasted values.")
            
            # Display pause queue
            if st.session_state.items_to_pause:
                st.markdown("---")
                st.subheader("â¸ï¸ Items to Pause")
                
                pause_df = pd.DataFrame(st.session_state.items_to_pause)
                st.dataframe(pause_df, use_container_width=True)
                
                col1, col2 = st.columns(2)
                with col1:
                    if st.button("Clear Pause Queue", type="secondary"):
                        st.session_state.items_to_pause = []
                        st.rerun()
                
                with col2:
                    if st.button("Add to Bulk Export", type="primary"):
                        # Add to bulk export queue
                        if 'bulk_export_actions' not in st.session_state:
                            st.session_state.bulk_export_actions = []
                        
                        for item in st.session_state.items_to_pause:
                            action = {
                                'action_type': 'pause',
                                'item_type': item['type'],
                                'data': item
                            }
                            st.session_state.bulk_export_actions.append(action)
                        
                        st.success(f"Added {len(st.session_state.items_to_pause)} pause actions to bulk export queue!")
                        st.session_state.items_to_pause = []
                        st.rerun()
        
        with tab3:
            st.header("Bid Optimization")
            st.markdown("Optimize bids based on performance data and target ACoS goals.")
            
            # Initialize bid optimization session state
            if 'bid_optimization_settings' not in st.session_state:
                st.session_state.bid_optimization_settings = {
                    'target_acos': 15.0,
                    'increase_percent': 5.0,
                    'decrease_percent': 3.0,
                    'adjust_based_on_increase': 'Orders',
                    'adjust_based_on_decrease': 'Clicks',
                    'min_spend_threshold': 10.0,
                    'filter_groups': [],
                    'priority_setting': 'Product Groups First'
                }
            
            if 'bid_optimization_results' not in st.session_state:
                st.session_state.bid_optimization_results = None
            
            # Complete bid optimization functions from bid_optimizer.py
            def classify_branding(campaign_name):
                """Classifies campaign branding based on keywords in Campaign Name."""
                if pd.isna(campaign_name):
                    return "Unknown"
                campaign_name_lower = str(campaign_name).lower()
                # Prioritize 'Non-Branded' if 'Non' is present, even if 'Brand' is also present
                if "non" in campaign_name_lower: # Catches 'non-brand', 'non brand', 'nonbranded'
                    return "Non-Branded"
                elif "brand" in campaign_name_lower: # Catches 'brand', 'branded'
                    return "Branded"
                else: # Default if neither specific keyword is found
                    return "Non-Branded"

            def create_filter_mask(df, col, filter_type, val):
                """Create a filter mask based on column, type and value"""
                # Special case for Campaign Name - look for Campaign Name (Informational Only) column
                actual_col = col
                if col == 'Campaign Name':
                    # Find the Campaign Name (Informational Only) column using case-insensitive search
                    info_col = next((c for c in df.columns if 'campaign name' in c.lower() and 'informational' in c.lower()), None)
                    if info_col:
                        actual_col = info_col
                
                # Check if the actual column exists in the dataframe
                if actual_col not in df.columns:
                    return pd.Series([True] * len(df))

                # Always treat 'Target' and 'Search Term' columns as text-only for filtering
                if actual_col in ('Target', 'Search Term'):
                    s = df[actual_col].astype(str)
                    if filter_type == "contains":
                        mask = s.str.contains(val, case=False, na=False)
                    elif filter_type == "doesn't contain":
                        mask = ~s.str.contains(val, case=False, na=False)
                    elif filter_type == "equals":
                        mask = s.str.strip().str.lower() == str(val).strip().lower()
                    elif filter_type == "doesn't equal":
                        mask = s.str.strip().str.lower() != str(val).strip().lower()
                    else:
                        # For numeric operators on 'Target', return all False to avoid misclassification
                        mask = pd.Series([False] * len(df))
                    return mask
                    
                if filter_type == "equals":
                    mask = df[actual_col] == val
                elif filter_type == "contains":
                    mask = df[actual_col].astype(str).str.contains(val, case=False, na=False)
                elif filter_type == "greater than":
                    try:
                        mask = df[actual_col] > float(val)
                    except:
                        mask = pd.Series([False] * len(df))
                elif filter_type == "less than":
                    try:
                        mask = df[actual_col] < float(val)
                    except:
                        mask = pd.Series([False] * len(df))
                elif filter_type == "doesn't contain":
                    mask = ~df[actual_col].astype(str).str.contains(val, case=False, na=False)
                elif filter_type == "doesn't equal":
                    mask = df[actual_col] != val
                elif filter_type == "less than or equal to":
                    try:
                        mask = df[actual_col] <= float(val)
                    except:
                        mask = pd.Series([False] * len(df))
                elif filter_type == "greater than or equal to":
                    try:
                        mask = df[actual_col] >= float(val)
                    except:
                        mask = pd.Series([False] * len(df))
                else:
                    # Unknown filter type
                    mask = pd.Series([True] * len(df))
                
                return mask

            def process_mixed_logic_filters(df, filters):
                """Process filters with mixed AND/OR logic"""
                if not filters:
                    return pd.Series([True] * len(df))
                
                # Start with the first condition
                result_mask = create_filter_mask(df, filters[0]["column"], filters[0]["type"], filters[0]["value"])
                
                # Process remaining conditions with their specified logic
                for i in range(1, len(filters)):
                    condition = filters[i]
                    current_mask = create_filter_mask(df, condition["column"], condition["type"], condition["value"])
                    
                    if condition["logic"] == "AND":
                        result_mask = result_mask & current_mask
                    elif condition["logic"] == "OR":
                        result_mask = result_mask | current_mask
                
                return result_mask

            def apply_filters(df, default_target_acos, filter_groups):
                """Apply filters exactly as in original bid_optimizer.py"""
                # Set default Target ACoS
                df['Target ACoS'] = default_target_acos
                
                # Apply each filter group
                for group_filters, group_acos in filter_groups:
                    # Determine which filter format is being used
                    if len(group_filters) > 0 and isinstance(group_filters[0], dict):  # New format with explicit logic
                        group_mask = process_mixed_logic_filters(df, group_filters)
                    else:  # Old format with implicit AND logic
                        # Start with all True for this group
                        group_mask = pd.Series([True] * len(df))
                        
                        # Apply each filter in the group (AND within group)
                        for col, filter_type, val in group_filters:
                            mask = create_filter_mask(df, col, filter_type, val)
                            # AND this filter with the current group mask
                            group_mask = group_mask & mask
                    
                    # Apply the group's target ACoS to matching rows
                    if group_mask.any():
                        df.loc[group_mask, 'Target ACoS'] = group_acos
                
                return df

            def apply_guardrails(df, increase_percent, decrease_percent, adjust_based_on_increase, adjust_based_on_decrease):
                """Apply guardrails exactly as in original bid_optimizer.py"""
                # Calculate recommended bid based on EQ Bid
                df['Recommended Bid'] = df['EQ Bid']
                # Use VCPM as the base bid if it's a VCPM campaign, otherwise use CPC
                is_vcpm_campaign = (df['Type'] == 'VCPM') | (pd.notna(df['VCPM']))
                df['Base Bid'] = np.where(is_vcpm_campaign, df['VCPM'], df['CPC'])

                if adjust_based_on_increase and adjust_based_on_increase in df.columns:
                    factor = {
                        "Clicks": df['Clicks'],
                        "Spend": df['Spend'] / 100,
                        "Orders": df['Orders']
                    }[adjust_based_on_increase]
                    df['Max Increase Limit'] = df['Base Bid'] * (1 + increase_percent * factor)
                else:
                    df['Max Increase Limit'] = df['Base Bid']

                if adjust_based_on_decrease and adjust_based_on_decrease in df.columns:
                    factor = {
                        "Clicks": df['Clicks'],
                        "Spend": df['Spend'],
                        "Orders": df['Orders']
                    }[adjust_based_on_decrease]
                    df['Min Decrease Limit'] = df['Base Bid'] * (1 - decrease_percent * factor)
                else:
                    df['Min Decrease Limit'] = df['Base Bid']

                df['Recommended Bid'] = np.where(
                    (df['EQ Bid'] == 0) & (df['Spend'] > 0),
                    np.maximum(df['Base Bid'] * (1 - decrease_percent * df['Spend']), df['Min Decrease Limit']),
                    np.where(
                        df['EQ Bid'] < df['Min Decrease Limit'],
                        df['Min Decrease Limit'],
                        np.where(
                            df['EQ Bid'] > df['Max Increase Limit'],
                            df['Max Increase Limit'],
                            df['EQ Bid']
                        )
                    )
                )
                
                # New condition: if both 'EQ Bid' and 'Old Bid' are lower than 'Recommended Bid',
                # cap 'Recommended Bid' at 'Old Bid'.
                condition_both_lower = (df['EQ Bid'] < df['Recommended Bid']) & (df['Old Bid'] < df['Recommended Bid'])
                df['Recommended Bid'] = np.where(
                    condition_both_lower,
                    df['Old Bid'],
                    df['Recommended Bid']
                )

                return df

            def process_sheet_complete(df, default_target_acos, increase_percent, decrease_percent, adjust_based_on_increase, adjust_based_on_decrease, filter_groups, min_spend_threshold=None, sd_vcpm_metric='Sales'):
                """Complete sheet processing exactly as in original bid_optimizer.py"""
                # Make a copy of the dataframe to avoid modifying the original
                df = df.copy()
                
                # Debug: Check input columns
                if hasattr(st.session_state, 'debug_messages'):
                    st.session_state.debug_messages.append(f"process_sheet_complete INPUT columns: {sorted(df.columns.tolist())}")
                    if 'Targeting ID' in df.columns:
                        st.session_state.debug_messages.append(f"Targeting ID found in INPUT")
                    if 'Targeting Expression' in df.columns:
                        st.session_state.debug_messages.append(f"Targeting Expression found in INPUT")
                
                # Strip whitespace from all column names
                df.columns = df.columns.str.strip()
                # Ensure 'State' column exists after cleaning
                if 'State' not in df.columns:
                    raise KeyError(f"'State' column not found in the uploaded file. Columns found: {list(df.columns)}. Please check for typos or extra spaces in column headers.")
                # Check if this is a Sponsored Brands sheet by looking for indicators in the columns or data
                is_sponsored_brands = False
                if 'Product' in df.columns:
                    is_sponsored_brands = df['Product'].astype(str).str.lower().eq('sponsored brands').any()
                # Filter active campaigns with valid bids
                df = df[df['State'].astype(str).str.lower() == "enabled"].dropna(subset=['Bid']).copy()
                # For Sponsored Brands, keep rows with either clicks > 0 or impressions > 1000
                if is_sponsored_brands:
                    df = df[(df['Clicks'] > 0) | (df.get('Impressions', 0) > 1000)].copy()
                else:
                    df = df[df['Clicks'] > 0].copy()
                # Apply minimum spend filter if set
                if min_spend_threshold is not None:
                    df = df[df['Spend'] >= min_spend_threshold]
                # Determine campaign type
                camp_col = next((col for col in df.columns if col.lower() == 'campaign name (informational only)'), None)
                if camp_col:
                    df['Type'] = np.where(df[camp_col].astype(str).str.contains("VCPM", case=False, na=False), "VCPM", "Other")
                if 'Product' in df.columns and 'Cost Type' in df.columns:
                    df['Type'] = np.where((df['Product'].astype(str).str.lower() == 'sponsored display') & (df['Cost Type'].astype(str).str.lower() == 'vcpm'), 'VCPM', df['Type'])
                # Ensure 'Bid' is numeric and store as 'Old Bid'
                df['Bid'] = pd.to_numeric(df['Bid'], errors='coerce').fillna(0)
                df['Old Bid'] = df['Bid']
                # Ensure 'VCPM' column exists and is numeric
                if 'VCPM' not in df.columns:
                    df['VCPM'] = np.nan
                df['VCPM'] = pd.to_numeric(df['VCPM'], errors='coerce')
                # Calculate VCPM if not provided
                if 'Type' in df.columns and 'Spend' in df.columns:
                    if 'Viewable Impressions' in df.columns and df['Product'].astype(str).str.lower().eq('sponsored display').any():
                        df['VCPM'] = np.where(
                            (df['Type'] == 'VCPM') & (df['Product'].astype(str).str.lower() == 'sponsored display') & (pd.isna(df['VCPM'])),
                            (df['Spend'] / (df['Viewable Impressions'] / 1000)).round(2),
                            df['VCPM']
                        )
                    elif 'Impressions' in df.columns:
                        df['VCPM'] = np.where(
                            (df['Type'] == 'VCPM') & (pd.isna(df['VCPM'])),
                            (df['Spend'] / (df['Impressions'] / 1000)).round(2),
                            df['VCPM']
                        )
                
                # If VCPM metric is present on the row, treat as VCPM campaign for all ad types
                if 'Type' in df.columns:
                    df['Type'] = np.where(pd.notna(df['VCPM']), 'VCPM', df['Type'])
                else:
                    df['Type'] = np.where(pd.notna(df['VCPM']), 'VCPM', 'Other')
                
                # Add CPC column if not present
                if 'CPC' not in df.columns:
                    df['CPC'] = np.where(df['Clicks'] > 0, df['Spend'] / df['Clicks'], df['Bid'])
                
                # Apply filters
                df = apply_filters(df, default_target_acos, filter_groups)
                # Calculate Current ACoS for VCPM campaigns
                df['Current ACoS'] = np.where(
                    df['Sales'] > 0,
                    df['Spend'] / df['Sales'],
                    np.inf  # Avoid division by zero; will handle in EQ Bid calculation
                )
                # Calculate EQ Bid
                sales_col = 'Sales' if sd_vcpm_metric == "Sales" else 'Sales (Views & Clicks)'
                is_vcpm_campaign = (df['Type'] == 'VCPM') | (pd.notna(df['VCPM']))
                df['EQ Bid'] = np.where(
                    is_vcpm_campaign,
                    np.where(
                        df['Current ACoS'] != np.inf,
                        df['VCPM'] * (df['Target ACoS'] / df['Current ACoS']),
                        0
                    ),
                    np.where(
                        (df['Clicks'] > 0) & (df['Sales'] == 0),
                        0,
                        np.where(
                            (df['Clicks'] > 0),
                            (df['Target ACoS'] * df['Sales']) / df['Clicks'],
                            df.get('CPC', np.nan)
                        )
                    )
                )
                # Apply guardrails
                df = apply_guardrails(df, increase_percent, decrease_percent, adjust_based_on_increase, adjust_based_on_decrease)
                
                # --- Branding Classification ---
                camp_col_name = next((col for col in df.columns if col.lower() == 'campaign name (informational only)'), None)
                if camp_col_name:
                    df['Branding_Category'] = df[camp_col_name].apply(classify_branding)
                else:
                    df['Branding_Category'] = 'Unknown' # Default if campaign name column is missing

                # Enforce minimum bids exactly as in original
                # Get the campaign name column
                camp_col = next((col for col in df.columns if col.lower() == 'campaign name (informational only)'), None)
                
                # Check for SBV or Video in campaign names if the column exists
                has_sbv_or_video = False
                if camp_col:
                    has_sbv_or_video = df[camp_col].astype(str).str.contains('SBV|Video', case=False, na=False)
                
                # Ensure Product column is properly handled for string operations
                if 'Product' in df.columns:
                    df['Product'] = df['Product'].astype(str).fillna('')
                else:
                    df['Product'] = ''
                
                # Define conditions for different campaign types
                conditions = [
                    # Sponsored Display conditions
                    (df['Product'].str.lower() == 'sponsored display') & (is_vcpm_campaign),
                    (df['Product'].str.lower() == 'sponsored display') & (~is_vcpm_campaign),
                    
                    # Sponsored Brand conditions with new criteria
                    # 1. SB with SBV/Video and VCPM
                    (df['Product'].str.lower() == 'sponsored brands') & (is_vcpm_campaign) & (has_sbv_or_video),
                    # 2. SB with SBV/Video and not VCPM
                    (df['Product'].str.lower() == 'sponsored brands') & (~is_vcpm_campaign) & (has_sbv_or_video),
                    # 3. SB without SBV/Video and VCPM
                    (df['Product'].str.lower() == 'sponsored brands') & (is_vcpm_campaign) & (~has_sbv_or_video),
                    # 4. SB without SBV/Video and not VCPM
                    (df['Product'].str.lower() == 'sponsored brands') & (~is_vcpm_campaign) & (~has_sbv_or_video),
                    
                    # Sponsored Products condition
                    (df['Product'].str.lower() == 'sponsored products')
                ]
                
                # Define minimum bids for each condition
                min_bids = [
                    1.00,  # SD with VCPM
                    0.10,  # SD without VCPM
                    12.00, # SB with SBV/Video and VCPM
                    0.25,  # SB with SBV/Video and not VCPM
                    8.00,  # SB without SBV/Video and VCPM
                    0.10,  # SB without SBV/Video and not VCPM
                    0.02   # SP
                ]
                
                # Apply minimum bids and round to 2 decimal places
                # First ensure Recommended Bid is numeric
                df['Recommended Bid'] = pd.to_numeric(df['Recommended Bid'], errors='coerce').fillna(0)
                df['Recommended Bid'] = df['Recommended Bid'].clip(lower=np.select(conditions, min_bids, default=0)).round(2)
                
                # Ensure 'Recommended Bid' and 'Old Bid' are numeric for analysis
                df['Recommended Bid'] = pd.to_numeric(df['Recommended Bid'], errors='coerce').fillna(0).round(2)
                df['Old Bid'] = pd.to_numeric(df['Old Bid'], errors='coerce').fillna(0).round(2)
                
                # Ensure 'ACoS' column exists
                if 'ACoS' not in df.columns:
                    df['ACoS'] = np.where(df['Sales'] > 0, df['Spend'] / df['Sales'], np.nan)
                
                # Add Operation column if it doesn't exist
                if 'Operation' not in df.columns:
                    df['Operation'] = ""
                
                # Update bids and operations
                df.loc[df['Recommended Bid'] != df['Old Bid'], 'Operation'] = "Update"
                df.loc[df['Operation'] == "Update", 'Bid'] = df['Recommended Bid']
                
                # Ensure final Bid column is properly rounded to 2 decimal places
                df['Bid'] = pd.to_numeric(df['Bid'], errors='coerce').fillna(0).round(2)
                
                # Calculate bid changes
                df['Bid Change'] = df['Recommended Bid'] - df['Old Bid']
                df['Bid Change %'] = np.where(
                    df['Old Bid'] > 0,
                    ((df['Recommended Bid'] / df['Old Bid']) - 1) * 100,
                    0
                )
                
                # Replace NaN with empty strings for cleaner output
                df = df.replace({np.nan: ''})
                
                # Debug: Check output columns
                if hasattr(st.session_state, 'debug_messages'):
                    st.session_state.debug_messages.append(f"process_sheet_complete OUTPUT columns: {sorted(df.columns.tolist())}")
                    if 'Targeting ID' in df.columns:
                        st.session_state.debug_messages.append(f"Targeting ID found in OUTPUT")
                    else:
                        st.session_state.debug_messages.append(f"Targeting ID NOT found in OUTPUT")
                    if 'Targeting Expression' in df.columns:
                        st.session_state.debug_messages.append(f"Targeting Expression found in OUTPUT")
                    else:
                        st.session_state.debug_messages.append(f"Targeting Expression NOT found in OUTPUT")
                
                return df
            
            # Settings section
            with st.expander("Optimization Settings", expanded=True):
                col1, col2, col3 = st.columns(3)
                
                with col1:
                    target_acos = st.text_input(
                        "Default Target ACoS (%)",
                        value=str(st.session_state.bid_optimization_settings['target_acos']),
                        key="bid_target_acos"
                    )
                    try:
                        st.session_state.bid_optimization_settings['target_acos'] = float(target_acos)
                    except:
                        st.session_state.bid_optimization_settings['target_acos'] = 15.0
                
                with col2:
                    increase_percent = st.text_input(
                        "Increase Limit (%)",
                        value=str(st.session_state.bid_optimization_settings['increase_percent']),
                        key="bid_increase_percent",
                        help="Maximum percentage to increase CPC from current bid"
                    )
                    try:
                        st.session_state.bid_optimization_settings['increase_percent'] = float(increase_percent)
                    except:
                        st.session_state.bid_optimization_settings['increase_percent'] = 5.0
                
                with col3:
                    decrease_percent = st.text_input(
                        "Decrease Limit (%)",
                        value=str(st.session_state.bid_optimization_settings['decrease_percent']),
                        key="bid_decrease_percent",
                        help="Maximum percentage to decrease CPC from current bid"
                    )
                    try:
                        st.session_state.bid_optimization_settings['decrease_percent'] = float(decrease_percent)
                    except:
                        st.session_state.bid_optimization_settings['decrease_percent'] = 3.0
                
                # Add descriptive text
                st.markdown("**Bid Adjustment Strategy**")
                st.markdown("Configure how aggressively bids should change based on performance metrics:")
                
                col4, col5 = st.columns(2)
                with col4:
                    adjust_based_on_increase = st.selectbox(
                        "Scale increases based on",
                        ["Orders", "Clicks", "Spend"],
                        index=["Orders", "Clicks", "Spend"].index(st.session_state.bid_optimization_settings['adjust_based_on_increase']),
                        key="bid_adjust_increase",
                        help=f"Increase bids up to {st.session_state.bid_optimization_settings['increase_percent']}% from CPC for every unit of this metric"
                    )
                    st.session_state.bid_optimization_settings['adjust_based_on_increase'] = adjust_based_on_increase
                    
                    # Show dynamic description
                    st.caption(f"Limit increases to **{st.session_state.bid_optimization_settings['increase_percent']}%** from CPC for every unit of **{adjust_based_on_increase}**")
                
                with col5:
                    adjust_based_on_decrease = st.selectbox(
                        "Scale decreases based on",
                        ["Clicks", "Spend", "Orders"],
                        index=["Clicks", "Spend", "Orders"].index(st.session_state.bid_optimization_settings['adjust_based_on_decrease']),
                        key="bid_adjust_decrease",
                        help=f"Decrease bids up to {st.session_state.bid_optimization_settings['decrease_percent']}% from CPC for every unit of this metric"
                    )
                    st.session_state.bid_optimization_settings['adjust_based_on_decrease'] = adjust_based_on_decrease
                    
                    # Show dynamic description
                    st.caption(f"Limit decreases to **{st.session_state.bid_optimization_settings['decrease_percent']}%** from CPC for every unit of **{adjust_based_on_decrease}**")
                
                st.markdown("---")
                st.markdown("**Filtering**")
                min_spend = st.text_input(
                    "Minimum Spend Threshold ($)",
                    value=str(st.session_state.bid_optimization_settings['min_spend_threshold']),
                    key="bid_min_spend",
                    help="Only optimize keywords/targets that have spent at least this amount"
                )
                try:
                    st.session_state.bid_optimization_settings['min_spend_threshold'] = float(min_spend) if min_spend else 0.0
                except:
                    st.session_state.bid_optimization_settings['min_spend_threshold'] = 10.0
            
            # Product Groups section
            st.subheader("Product Groups ACoS Targeting (Optional)")
            st.markdown("**Override the default target ACoS** for specific Product Groups defined in Campaign Tagging. Leave blank to use the default ACoS setting above.")
            
            # Get product groups from campaign tagging
            product_groups = set()
            if st.session_state.get('client_config') and 'campaign_tags_data' in st.session_state.client_config:
                for campaign_info in st.session_state.client_config['campaign_tags_data'].values():
                    product_group = campaign_info.get('tag_1', '') or 'Untagged Group'
                    if product_group and product_group.strip():
                        product_groups.add(product_group)
                
                # Add Untagged Group as default
                if st.session_state.client_config["campaign_tags_data"]:
                    product_groups.add("Untagged Group")
            
            # Initialize product group settings if not exists
            if 'product_group_acos' not in st.session_state.bid_optimization_settings:
                st.session_state.bid_optimization_settings['product_group_acos'] = {}
            
            if product_groups:
                with st.expander("Product Group ACoS Settings", expanded=False):
                    st.markdown("**Set custom target ACoS for each Product Group:**")
                    
                    # Quick fill option at the top
                    st.markdown("---")
                    st.markdown("**Quick Fill All Blank Fields:**")
                    col_fill1, col_fill2, col_fill3 = st.columns([2, 1, 1])
                    
                    with col_fill1:
                        quick_fill_acos = st.text_input(
                            "ACoS value to fill all blank fields (%)",
                            key="quick_fill_acos",
                            help="Enter an ACoS percentage to fill all blank Product Group fields. Leave empty to use default ACoS target."
                        )
                    
                    with col_fill2:
                        if st.button("Fill All Blank Fields", type="secondary"):
                            fill_value = None
                            if quick_fill_acos.strip():
                                try:
                                    fill_value = float(quick_fill_acos)
                                except:
                                    st.error("Please enter a valid numeric ACoS value.")
                            else:
                                # Use default ACoS target if no value provided
                                fill_value = st.session_state.bid_optimization_settings['target_acos']
                            
                            if fill_value is not None:
                                filled_count = 0
                                for pg in product_groups:
                                    if pg not in st.session_state.bid_optimization_settings['product_group_acos'] or not st.session_state.bid_optimization_settings['product_group_acos'].get(pg):
                                        st.session_state.bid_optimization_settings['product_group_acos'][pg] = fill_value
                                        filled_count += 1
                                
                                if filled_count > 0:
                                    st.success(f"Filled {filled_count} blank Product Group fields with {fill_value}% ACoS")
                                    st.rerun()
                                else:
                                    st.info("No blank fields to fill - all Product Groups already have ACoS values set.")
                    
                    with col_fill3:
                        if quick_fill_acos.strip():
                            st.caption(f"Will use: **{quick_fill_acos}%**")
                        else:
                            st.caption(f"Will use default: **{st.session_state.bid_optimization_settings['target_acos']}%**")
                    
                    st.markdown("---")
                    
                    for product_group in sorted(product_groups):
                        col1, col2 = st.columns([3, 1])
                        
                        with col1:
                            st.markdown(f"**{product_group}**")
                        
                        with col2:
                            # Get current value or use default
                            current_value = st.session_state.bid_optimization_settings['product_group_acos'].get(product_group, "")
                            
                            acos_value = st.text_input(
                                "Target ACoS (%)",
                                value=str(current_value) if current_value else "",
                                key=f"pg_acos_{product_group}",
                                help=f"Leave blank to use default ACoS for {product_group}"
                            )
                            
                            if acos_value.strip():
                                try:
                                    st.session_state.bid_optimization_settings['product_group_acos'][product_group] = float(acos_value)
                                except:
                                    if product_group in st.session_state.bid_optimization_settings['product_group_acos']:
                                        del st.session_state.bid_optimization_settings['product_group_acos'][product_group]
                            else:
                                if product_group in st.session_state.bid_optimization_settings['product_group_acos']:
                                    del st.session_state.bid_optimization_settings['product_group_acos'][product_group]
                    
                    # Save and Clear buttons
                    col_btn1, col_btn2 = st.columns(2)
                    
                    with col_btn1:
                        if st.button("Save Product Group Settings", type="primary"):
                            # Count how many product groups have settings
                            settings_count = len([pg for pg in product_groups 
                                                if pg in st.session_state.bid_optimization_settings['product_group_acos'] 
                                                and st.session_state.bid_optimization_settings['product_group_acos'][pg]])
                            
                            if settings_count > 0:
                                st.success(f"Saved ACoS settings for {settings_count} Product Groups!")
                                
                                # Show summary of saved settings
                                st.markdown("**Saved Settings:**")
                                for pg in sorted(product_groups):
                                    if pg in st.session_state.bid_optimization_settings['product_group_acos'] and st.session_state.bid_optimization_settings['product_group_acos'][pg]:
                                        acos_val = st.session_state.bid_optimization_settings['product_group_acos'][pg]
                                        st.write(f"â€¢ **{pg}**: {acos_val}% ACoS")
                            else:
                                st.info("No Product Group ACoS settings to save - all fields are blank.")
                    
                    with col_btn2:
                        if st.button("Clear All Product Group Settings"):
                            st.session_state.bid_optimization_settings['product_group_acos'] = {}
                            st.success("Cleared all Product Group ACoS settings")
                            st.rerun()
            else:
                st.info("No Product Groups found in Campaign Tagging. Configure Product Groups in Client Settings Center to use this feature.")
            
            # Filter groups section
            st.subheader("Advanced Filter Groups (Optional)")
            st.markdown("**Create custom rules** for campaigns that need special targeting beyond Product Groups. For example: different ACoS for high-performing campaigns, specific product types, etc.")
            
            with st.expander("Custom Filter Groups", expanded=False):
                # Display existing filter groups
                if st.session_state.bid_optimization_settings['filter_groups']:
                    st.markdown("**Current Filter Groups:**")
                    for i, (filters, acos) in enumerate(st.session_state.bid_optimization_settings['filter_groups']):
                        with st.container():
                            st.markdown(f"**Group {i+1}** - Target ACoS: {acos*100:.1f}%")
                            for j, (col, filter_type, val) in enumerate(filters):
                                st.markdown(f"  â€¢ {col} {filter_type} '{val}'")
                            if st.button(f"Remove Group {i+1}", key=f"remove_filter_group_{i}"):
                                st.session_state.bid_optimization_settings['filter_groups'].pop(i)
                                st.rerun()
                
                # Add new filter group
                st.markdown("**Add New Filter Group:**")
                with st.form("add_bid_filter_group"):
                    filter_col = st.selectbox(
                        "Column",
                        ["Campaign Name", "Product", "Product Group", "Bid", "Clicks", "Sales", "Spend", "ACoS"],
                        key="bid_filter_column"
                    )
                    filter_type = st.selectbox(
                        "Filter Type",
                        ["equals", "contains", "greater than", "less than", "doesn't contain", "doesn't equal", "less than or equal to", "greater than or equal to"],
                        key="bid_filter_type"
                    )
                    filter_value = st.text_input("Value", key="bid_filter_value")
                    group_acos = st.text_input(
                        "Target ACoS for this group (%)",
                        value="20",
                        key="bid_group_acos"
                    )
                    
                    if st.form_submit_button("Add Filter Group"):
                        if filter_value and group_acos:
                            try:
                                acos_val = float(group_acos) / 100
                                new_group = ([(filter_col, filter_type, filter_value)], acos_val)
                                st.session_state.bid_optimization_settings['filter_groups'].append(new_group)
                                st.success(f"Added filter group: {filter_col} {filter_type} '{filter_value}' with {group_acos}% target ACoS")
                                st.rerun()
                            except ValueError:
                                st.error("Please enter a valid numeric ACoS value.")
            
            # Helper function to add Product Groups to dataframe
            def add_product_groups_to_dataframe(df):
                """Add Product Group information to dataframe based on campaign tagging"""
                # Standardize campaign names for consistent matching
                def standardize_campaign_name(name):
                    return str(name).strip().lower()
                
                # Get campaign tags data with standardized keys
                campaign_tags_data = {}
                has_product_groups = False
                if st.session_state.get('client_config') and 'campaign_tags_data' in st.session_state.client_config:
                    campaign_tags_data = {standardize_campaign_name(k): v for k, v in st.session_state.client_config['campaign_tags_data'].items()}
                    # Check if there are any product groups defined
                    has_product_groups = any(v.get('tag_1', '') for v in st.session_state.client_config['campaign_tags_data'].values())
                
                # Find campaign name column
                campaign_col = None
                for col in ['Campaign Name (Informational Only)', 'Campaign Name']:
                    if col in df.columns:
                        campaign_col = col
                        break
                
                if campaign_col and has_product_groups:
                    # Apply product group mapping with standardized campaign names
                    df['Product Group'] = df[campaign_col].apply(
                        lambda x: campaign_tags_data.get(standardize_campaign_name(x), {}).get('tag_1', '') or 'Untagged Group'
                    )
                else:
                    df['Product Group'] = 'Untagged Group'  # Use 'Untagged Group' if no product groups defined
                
                return df
            
            # Priority Setting - only show if there are Advanced Filter Groups configured
            if st.session_state.bid_optimization_settings['filter_groups']:
                st.markdown("---")
                st.subheader("Priority Configuration")
                st.markdown("**When a campaign matches both Product Groups and Advanced Filter Groups, which should take priority?**")
                
                priority_setting = st.selectbox(
                    "Priority Setting",
                    ["Product Groups First", "Advanced Filter Groups First"],
                    index=["Product Groups First", "Advanced Filter Groups First"].index(st.session_state.bid_optimization_settings['priority_setting']),
                    key="priority_setting_select",
                    help="Choose which targeting rules have higher priority when there are conflicts"
                )
                st.session_state.bid_optimization_settings['priority_setting'] = priority_setting
                
                if priority_setting == "Product Groups First":
                    st.caption("Product Groups ACoS settings will override Advanced Filter Groups when both apply to the same campaign.")
                else:
                    st.caption("Advanced Filter Groups will override Product Groups ACoS settings when both apply to the same campaign.")
            
            # Save Settings as Preset button
            st.markdown("---")
            if st.button("Save Settings as Preset", help="Save current optimization settings for future use"):
                # This could be expanded to save/load presets
                st.success("Settings saved! (Feature can be expanded)")
            
            # Process optimization
            st.markdown("---")
            st.subheader("Run Optimization")
            st.markdown("**Ready to optimize?** Click below to process your bulk data with the settings configured above.")
            
            if st.button("Optimize Bids Now", type="primary", use_container_width=True):
                # Initialize debug messages for bid optimization
                if not hasattr(st.session_state, 'debug_messages'):
                    st.session_state.debug_messages = []
                st.session_state.debug_messages.clear()  # Clear previous messages
                # Get data from bulk file
                bulk_data = st.session_state.bulk_data
                
                # Process each sheet separately with complete logic
                processed_sheets = {}
                all_optimized_data = []
                
                try:
                    # Settings from session state
                    settings = st.session_state.bid_optimization_settings
                    default_target_acos = settings['target_acos'] / 100
                    increase_percent = settings['increase_percent'] / 100
                    decrease_percent = settings['decrease_percent'] / 100
                    adjust_based_on_increase = settings['adjust_based_on_increase']
                    adjust_based_on_decrease = settings['adjust_based_on_decrease']
                    min_spend_threshold = settings['min_spend_threshold'] if settings['min_spend_threshold'] > 0 else None
                    
                    # Create filter groups from Product Groups and custom filter groups based on priority
                    combined_filter_groups = []
                    product_group_filters = []
                    custom_filter_groups = []
                    
                    # Prepare Product Group filters
                    if 'product_group_acos' in settings and settings['product_group_acos']:
                        for product_group, acos_value in settings['product_group_acos'].items():
                            if acos_value and acos_value > 0:
                                # Create filter for this product group
                                product_group_filter = [('Product Group', 'equals', product_group)]
                                product_group_filters.append((product_group_filter, acos_value / 100))
                    
                    # Prepare custom filter groups
                    if settings['filter_groups']:
                        custom_filter_groups = settings['filter_groups']
                    
                    # Apply filters based on priority setting
                    priority_setting = settings.get('priority_setting', 'Product Groups First')
                    if priority_setting == 'Product Groups First':
                        # Apply custom filter groups first, then product groups (so product groups override)
                        combined_filter_groups.extend(custom_filter_groups)
                        combined_filter_groups.extend(product_group_filters)
                    else:
                        # Apply product groups first, then custom filter groups (so custom groups override)
                        combined_filter_groups.extend(product_group_filters)
                        combined_filter_groups.extend(custom_filter_groups)
                    
                    with st.spinner("Optimizing bids..."):
                        # Process each sheet type
                        sheet_names = ['Sponsored Products Campaigns', 'Sponsored Brands Campaigns', 'Sponsored Display Campaigns']
                        
                        # Handle both standard and multi ad group formats
                        if 'SB Multi Ad Group Campaigns' in bulk_data and 'Sponsored Brands Campaigns' in bulk_data:
                            # Use Multi Ad Group instead of standard
                            sheet_names = ['Sponsored Products Campaigns', 'SB Multi Ad Group Campaigns', 'Sponsored Display Campaigns']
                        
                        for sheet_name in sheet_names:
                            if sheet_name in bulk_data:
                                df = bulk_data[sheet_name].copy()
                                
                                if not df.empty and 'Campaign Name' in df.columns:
                                    # Add Product Group information
                                    df = add_product_groups_to_dataframe(df)
                                    
                                    # Process this sheet
                                    try:
                                        processed = process_sheet_complete(
                                            df,
                                            default_target_acos,
                                            increase_percent,
                                            decrease_percent,
                                            adjust_based_on_increase,
                                            adjust_based_on_decrease,
                                            combined_filter_groups,
                                            min_spend_threshold
                                        )
                                        
                                        if processed is not None and not processed.empty:
                                            processed['Sheet_Source'] = sheet_name
                                            processed_sheets[sheet_name] = processed
                                            all_optimized_data.append(processed)
                                            
                                    except Exception as e:
                                        st.error(f"Error processing {sheet_name}: {str(e)}")
                                        continue
                        
                        if all_optimized_data:
                            # Combine all processed data
                            combined_results = pd.concat(all_optimized_data, ignore_index=True)
                            
                            # Debug: Check what columns are in the combined results
                            if hasattr(st.session_state, 'debug_messages'):
                                st.session_state.debug_messages.append(f"Combined results columns: {sorted(combined_results.columns.tolist())}")
                                if 'Targeting ID' in combined_results.columns:
                                    st.session_state.debug_messages.append(f"Targeting ID found in combined results")
                                else:
                                    st.session_state.debug_messages.append(f"Targeting ID NOT found in combined results")
                                if 'Targeting Expression' in combined_results.columns:
                                    st.session_state.debug_messages.append(f"Targeting Expression found in combined results")
                                else:
                                    st.session_state.debug_messages.append(f"Targeting Expression NOT found in combined results")
                            
                            st.session_state.bid_optimization_results = combined_results
                            st.success(f"Optimization complete! Processed {len(combined_results)} items across {len(processed_sheets)} sheets.")
                            
                            # Debug: Show debug messages if available
                            if hasattr(st.session_state, 'debug_messages') and st.session_state.debug_messages:
                                with st.expander("Bid Optimization Debug Information", expanded=False):
                                    for msg in st.session_state.debug_messages:
                                        st.text(msg)
                        else:
                            st.error("No data was successfully processed. Please check your data and settings.")

                except Exception as e:
                    st.error(f"Error during optimization: {str(e)}")
            
            # Display results
            st.markdown("---")
            if st.session_state.bid_optimization_results is not None:
                results_df = st.session_state.bid_optimization_results
                
                st.subheader("Optimization Results")
                
                # Summary metrics
                total_items = len(results_df)
                items_increased = len(results_df[results_df['Bid Change'] > 0])
                items_decreased = len(results_df[results_df['Bid Change'] < 0])
                items_unchanged = len(results_df[results_df['Bid Change'] == 0])
                
                col1, col2, col3, col4 = st.columns(4)
                with col1:
                    st.metric("Total Items", total_items)
                with col2:
                    st.metric("Bids Increased", items_increased)
                with col3:
                    st.metric("Bids Decreased", items_decreased)
                with col4:
                    st.metric("Unchanged", items_unchanged)
                
                # Performance impact - with error handling for missing columns
                try:
                    current_spend = results_df['Spend'].sum() if 'Spend' in results_df.columns else 0
                    current_sales = results_df['Sales'].sum() if 'Sales' in results_df.columns else 0
                    current_acos = (current_spend / current_sales * 100) if current_sales > 0 else 0
                    
                    # Prefer model-based projections if available from bid_optimizer
                    if 'Estimated_New_Spend' in results_df.columns:
                        projected_spend = pd.to_numeric(results_df['Estimated_New_Spend'], errors='coerce').fillna(0).sum()
                    elif 'Clicks' in results_df.columns and 'Recommended Bid' in results_df.columns:
                        # Fallback: basic approximation (clicks * new CPC)
                        projected_spend = (
                            pd.to_numeric(results_df['Clicks'], errors='coerce').fillna(0)
                            * pd.to_numeric(results_df['Recommended Bid'], errors='coerce').fillna(0)
                        ).sum()
                    else:
                        projected_spend = current_spend

                    # Prefer model-based projected sales; otherwise keep current sales
                    if 'Estimated_New_Sales' in results_df.columns:
                        projected_sales = pd.to_numeric(results_df['Estimated_New_Sales'], errors='coerce').fillna(0).sum()
                    else:
                        projected_sales = current_sales
                    projected_acos = (projected_spend / projected_sales * 100) if projected_sales > 0 else 0
                except Exception as e:
                    st.error(f"Error calculating performance metrics: {str(e)}")
                    current_spend = projected_spend = current_sales = projected_sales = 0
                    current_acos = projected_acos = 0
                
                col1, col2, col3 = st.columns(3)
                with col1:
                    st.metric("Current ACoS", f"{current_acos:.2f}%")
                
                with col2:
                    try:
                        avg_old_bid = results_df['Old Bid'].mean() if 'Old Bid' in results_df.columns else 0
                        avg_new_bid = results_df['Recommended Bid'].mean() if 'Recommended Bid' in results_df.columns else 0
                        st.metric("Avg Current Bid", f"${avg_old_bid:.2f}")
                        delta_pct = ((avg_new_bid - avg_old_bid) / avg_old_bid * 100) if avg_old_bid else 0
                    except Exception as e:
                        st.error(f"Error calculating bid metrics: {str(e)}")
                        st.metric("Avg Current Bid", "$0.00")
                        avg_new_bid = 0
                        delta_pct = 0
                
                with col3:
                    try:
                        if 'Bid Change' in results_df.columns:
                            total_bid_increase = results_df[results_df['Bid Change'] > 0]['Bid Change'].sum()
                            total_bid_decrease = results_df[results_df['Bid Change'] < 0]['Bid Change'].abs().sum()
                        else:
                            total_bid_increase = total_bid_decrease = 0
                        st.metric("Total Bid Increases", f"${total_bid_increase:.2f}")
                    except Exception as e:
                        st.error(f"Error calculating bid changes: {str(e)}")
                        st.metric("Total Bid Increases", "$0.00")
                        total_bid_increase = 0
                        total_bid_decrease = 0

                # Second row of performance tiles (3 columns)
                row2_col1, row2_col2, row2_col3 = st.columns(3)
                with row2_col1:
                    try:
                        st.metric("Avg Recommended Bid", f"${avg_new_bid:.2f}", delta=f"{delta_pct:+.2f}%")
                    except Exception as e:
                        st.error(f"Error displaying Avg Recommended Bid: {str(e)}")
                        st.metric("Avg Recommended Bid", "$0.00", delta="+0.00%")

                with row2_col2:
                    try:
                        st.metric("Total Bid Decreases", f"${total_bid_decrease:.2f}")
                    except Exception as e:
                        st.error(f"Error displaying Total Bid Decreases: {str(e)}")
                        st.metric("Total Bid Decreases", "$0.00")

                with row2_col3:
                    try:
                        net_bid_change = (total_bid_increase - total_bid_decrease)
                        st.metric("Net Bid Change", f"${net_bid_change:.2f}", delta=f"{net_bid_change:+.2f}")
                    except Exception as e:
                        st.error(f"Error calculating Net Bid Change: {str(e)}")
                        st.metric("Net Bid Change", "$0.00", delta="+0.00")
                
                # Top changes
                st.subheader("Top Bid Changes")
                
                tab_increases, tab_decreases = st.tabs(["Top Increases", "Top Decreases"])
                
                with tab_increases:
                    try:
                        if 'Bid Change' in results_df.columns:
                            top_increases = results_df[results_df['Bid Change'] > 0].nlargest(10, 'Bid Change')
                            if not top_increases.empty:
                                # Create a copy for display modifications
                                display_df = top_increases.copy()
                                
                                # Use Campaign Name (Informational Only) if available, otherwise fall back to Campaign Name
                                if 'Campaign Name (Informational only)' in display_df.columns:
                                    display_df['Campaign Name'] = display_df['Campaign Name (Informational only)']
                                elif 'Campaign Name (Informational Only)' in display_df.columns:
                                    display_df['Campaign Name'] = display_df['Campaign Name (Informational Only)']
                                
                                # Create Keyword/Target column based on available data
                                def get_keyword_target(row):
                                    # Check for keyword data first
                                    if 'Keyword Text' in row and pd.notna(row['Keyword Text']) and row['Keyword Text'] != '':
                                        match_type = row.get('Match Type', '')
                                        if match_type:
                                            return f"{row['Keyword Text']} ({match_type})"
                                        else:
                                            return str(row['Keyword Text'])
                                    # Check for targeting expression
                                    elif 'Targeting Expression' in row and pd.notna(row['Targeting Expression']) and row['Targeting Expression'] != '':
                                        return str(row['Targeting Expression'])
                                    # Check for product targeting expression
                                    elif 'Product Targeting Expression' in row and pd.notna(row['Product Targeting Expression']) and row['Product Targeting Expression'] != '':
                                        return str(row['Product Targeting Expression'])
                                    # Check for entity type
                                    elif 'Entity' in row and pd.notna(row['Entity']):
                                        return str(row['Entity'])
                                    else:
                                        return 'N/A'
                                
                                display_df['Keyword/Target'] = display_df.apply(get_keyword_target, axis=1)
                                
                                display_cols = ['Campaign Name', 'Keyword/Target', 'Old Bid', 'Recommended Bid', 'Bid Change', 'Bid Change %', 'Current ACoS', 'Target ACoS']
                                available_cols = [col for col in display_cols if col in display_df.columns]
                                st.dataframe(display_df[available_cols], use_container_width=True)
                            else:
                                st.info("No bid increases found.")
                        else:
                            st.info("Bid change data not available.")
                    except Exception as e:
                        st.error(f"Error displaying top increases: {str(e)}")
                
                with tab_decreases:
                    try:
                        if 'Bid Change' in results_df.columns:
                            top_decreases = results_df[results_df['Bid Change'] < 0].nsmallest(10, 'Bid Change')
                            if not top_decreases.empty:
                                # Create a copy for display modifications
                                display_df = top_decreases.copy()
                                
                                # Use Campaign Name (Informational Only) if available, otherwise fall back to Campaign Name
                                if 'Campaign Name (Informational only)' in display_df.columns:
                                    display_df['Campaign Name'] = display_df['Campaign Name (Informational only)']
                                elif 'Campaign Name (Informational Only)' in display_df.columns:
                                    display_df['Campaign Name'] = display_df['Campaign Name (Informational Only)']
                                
                                # Create Keyword/Target column based on available data
                                def get_keyword_target(row):
                                    # Check for keyword data first
                                    if 'Keyword Text' in row and pd.notna(row['Keyword Text']) and row['Keyword Text'] != '':
                                        match_type = row.get('Match Type', '')
                                        if match_type:
                                            return f"{row['Keyword Text']} ({match_type})"
                                        else:
                                            return str(row['Keyword Text'])
                                    # Check for targeting expression
                                    elif 'Targeting Expression' in row and pd.notna(row['Targeting Expression']) and row['Targeting Expression'] != '':
                                        return str(row['Targeting Expression'])
                                    # Check for product targeting expression
                                    elif 'Product Targeting Expression' in row and pd.notna(row['Product Targeting Expression']) and row['Product Targeting Expression'] != '':
                                        return str(row['Product Targeting Expression'])
                                    # Check for entity type
                                    elif 'Entity' in row and pd.notna(row['Entity']):
                                        return str(row['Entity'])
                                    else:
                                        return 'N/A'
                                
                                display_df['Keyword/Target'] = display_df.apply(get_keyword_target, axis=1)
                                
                                display_cols = ['Campaign Name', 'Keyword/Target', 'Old Bid', 'Recommended Bid', 'Bid Change', 'Bid Change %', 'Current ACoS', 'Target ACoS']
                                available_cols = [col for col in display_cols if col in display_df.columns]
                                st.dataframe(display_df[available_cols], use_container_width=True)
                            else:
                                st.info("No bid decreases found.")
                        else:
                            st.info("Bid change data not available.")
                    except Exception as e:
                        st.error(f"Error displaying top decreases: {str(e)}")
                
                # Full results table
                with st.expander("Full Optimization Results", expanded=False):
                    st.dataframe(results_df, use_container_width=True)
                
                # Add to bulk export
                col1, col2 = st.columns(2)
                
                with col1:
                    if st.button("Add to Bulk Export Queue", type="primary"):
                        # Initialize debug messages
                        if not hasattr(st.session_state, 'debug_messages'):
                            st.session_state.debug_messages = []
                        st.session_state.debug_messages.clear()  # Clear previous messages
                        if 'bulk_export_actions' not in st.session_state:
                            st.session_state.bulk_export_actions = []
                        
                        # Add bid changes to export queue with only original bulk file columns
                        bid_changes = results_df[results_df['Bid Change'] != 0]
                        
                        # Get original column names from bulk data for filtering
                        original_columns = set()
                        if not bid_changes.empty:
                            sheet_source = bid_changes.iloc[0].get('Sheet_Source', 'Unknown')
                            
                            # Debug: Check what sheet sources we have
                            if hasattr(st.session_state, 'debug_messages'):
                                st.session_state.debug_messages.append(f"Sheet source: {sheet_source}")
                                st.session_state.debug_messages.append(f"Available bulk data sheets: {list(st.session_state.bulk_data.keys()) if hasattr(st.session_state, 'bulk_data') else 'No bulk data'}")
                            
                            if sheet_source != 'Unknown' and hasattr(st.session_state, 'bulk_data') and sheet_source in st.session_state.bulk_data:
                                original_columns = set(st.session_state.bulk_data[sheet_source].columns)
                                if hasattr(st.session_state, 'debug_messages'):
                                    st.session_state.debug_messages.append(f"Using original columns from {sheet_source}: {sorted(original_columns)}")
                            else:
                                # Fallback: try to determine from common bulk file columns
                                common_bulk_columns = {
                                    'Campaign Name', 'Campaign Name (Informational Only)', 
                                    'Ad Group Name', 'Ad Group Name (Informational Only)',
                                    'Keyword', 'Keyword Text', 'Keyword ID', 'Product', 'Cost Type',
                                    'Match Type', 'Bid', 'State', 'Portfolio Name', 'Entity',
                                    'Campaign ID', 'Ad Group ID', 'Product Targeting ID',
                                    'Product Targeting Expression', 'Targeting ID', 'Targeting Expression'
                                }
                                # Use intersection of common columns and available columns
                                available_columns = set(bid_changes.columns)
                                original_columns = common_bulk_columns.intersection(available_columns)
                                
                                if hasattr(st.session_state, 'debug_messages'):
                                    st.session_state.debug_messages.append(f"Using fallback columns. Available: {sorted(available_columns)}")
                                    st.session_state.debug_messages.append(f"Final original columns: {sorted(original_columns)}")
                        
                        for _, row in bid_changes.iterrows():
                            # Convert row to dictionary
                            row_dict = row.to_dict()
                            
                            # Debug: Check what columns are in the row data
                            if hasattr(st.session_state, 'debug_messages'):
                                st.session_state.debug_messages.append(f"Row columns: {sorted(row_dict.keys())}")
                                if 'Targeting ID' in row_dict:
                                    st.session_state.debug_messages.append(f"Targeting ID value: {row_dict['Targeting ID']}")
                                if 'Targeting Expression' in row_dict:
                                    st.session_state.debug_messages.append(f"Targeting Expression value: {row_dict['Targeting Expression']}")
                            
                            # Filter to only include original bulk file columns plus Operation
                            filtered_row_dict = {}
                            
                            # Always include Operation column
                            filtered_row_dict['Operation'] = 'Update'
                            
                            # Include only original columns from bulk file
                            for col in original_columns:
                                if col in row_dict:
                                    if col == 'Bid':
                                        # Update Bid with recommended value
                                        filtered_row_dict[col] = row.get('Recommended Bid', row_dict[col])
                                    else:
                                        # Keep original value
                                        filtered_row_dict[col] = row_dict[col]
                                        
                                        # Debug: Log when we preserve targeting columns
                                        if col in ['Targeting ID', 'Targeting Expression', 'Product Targeting ID', 'Product Targeting Expression'] and hasattr(st.session_state, 'debug_messages'):
                                            st.session_state.debug_messages.append(f"Preserved {col}: {row_dict[col]}")
                                
                                # Handle targeting column mapping for different campaign types
                                # Sponsored Display uses 'Targeting ID' and 'Targeting Expression'
                                # Sponsored Products/Brands use 'Product Targeting ID' and 'Product Targeting Expression'
                                elif col == 'Targeting ID' and 'Product Targeting ID' in row_dict:
                                    # Map Product Targeting ID to Targeting ID for consistency
                                    filtered_row_dict[col] = row_dict['Product Targeting ID']
                                    if hasattr(st.session_state, 'debug_messages'):
                                        st.session_state.debug_messages.append(f"Mapped Product Targeting ID to Targeting ID: {row_dict['Product Targeting ID']}")
                                elif col == 'Targeting Expression' and 'Product Targeting Expression' in row_dict:
                                    # Map Product Targeting Expression to Targeting Expression for consistency
                                    filtered_row_dict[col] = row_dict['Product Targeting Expression']
                                    if hasattr(st.session_state, 'debug_messages'):
                                        st.session_state.debug_messages.append(f"Mapped Product Targeting Expression to Targeting Expression: {row_dict['Product Targeting Expression']}")
                            
                            # Always ensure targeting columns are included if they have valid data
                            # This handles cases where targeting columns aren't in original_columns but exist in the data
                            if 'Targeting ID' in row_dict and pd.notna(row_dict['Targeting ID']) and row_dict['Targeting ID'] != '':
                                filtered_row_dict['Targeting ID'] = row_dict['Targeting ID']
                                if hasattr(st.session_state, 'debug_messages'):
                                    st.session_state.debug_messages.append(f"Force-included Targeting ID: {row_dict['Targeting ID']}")
                            
                            if 'Targeting Expression' in row_dict and pd.notna(row_dict['Targeting Expression']) and row_dict['Targeting Expression'] != '':
                                filtered_row_dict['Targeting Expression'] = row_dict['Targeting Expression']
                                if hasattr(st.session_state, 'debug_messages'):
                                    st.session_state.debug_messages.append(f"Force-included Targeting Expression: {row_dict['Targeting Expression']}")
                            
                            # Ensure name columns are present for export display (case-insensitive)
                            try:
                                # Campaign name: try any column that includes both 'campaign name' and 'informational'
                                campaign_info_val = None
                                for c in row.index:
                                    lc = str(c).lower()
                                    if 'campaign name' in lc and 'informational' in lc:
                                        campaign_info_val = row.get(c, None)
                                        break
                                if campaign_info_val is None:
                                    # Fallbacks: exact canonical name, or any 'campaign name' column
                                    campaign_info_val = row.get('Campaign Name', None)
                                    if campaign_info_val is None:
                                        for c in row.index:
                                            if 'campaign name' in str(c).lower():
                                                campaign_info_val = row.get(c, None)
                                                break
                                filtered_row_dict['Campaign Name'] = campaign_info_val if pd.notna(campaign_info_val) else ''

                                # Ad group name: try any column that includes both 'ad group name' and 'informational'
                                adgroup_info_val = None
                                for c in row.index:
                                    lc = str(c).lower()
                                    if 'ad group name' in lc and 'informational' in lc:
                                        adgroup_info_val = row.get(c, None)
                                        break
                                if adgroup_info_val is None:
                                    adgroup_info_val = row.get('Ad Group Name', None)
                                    if adgroup_info_val is None:
                                        for c in row.index:
                                            if 'ad group name' in str(c).lower():
                                                adgroup_info_val = row.get(c, None)
                                                break
                                filtered_row_dict['Ad Group Name'] = adgroup_info_val if pd.notna(adgroup_info_val) else ''
                            except Exception:
                                # Safe fallbacks
                                filtered_row_dict['Campaign Name'] = row.get('Campaign Name', '')
                                filtered_row_dict['Ad Group Name'] = row.get('Ad Group Name', '')
                            
                            # Compute and attach performance metrics for export
                            try:
                                spend_val = row.get('Spend', 0) or 0
                                # Use chosen SD attribution when available
                                sd_attr_choice = st.session_state.get('sd_attribution_choice', 'Sales')
                                sales_val = row.get('Sales', 0) or 0
                                if sd_attr_choice == "Sales (Views & Clicks)" and 'Sales (Views & Clicks)' in row.index:
                                    sales_val = row.get('Sales (Views & Clicks)', sales_val) or sales_val
                                clicks_val = row.get('Clicks', 0) or 0
                                orders_val = row.get('Orders', 0) or 0

                                # CPC: prefer explicit CPC, else Current_CPC, else Spend/Clicks
                                cpc_val = row.get('CPC', None)
                                if (cpc_val is None or cpc_val == '' or (isinstance(cpc_val, float) and cpc_val != cpc_val)) and 'Current_CPC' in row.index:
                                    cpc_val = row.get('Current_CPC', None)
                                if (cpc_val is None or cpc_val == '' or (isinstance(cpc_val, float) and cpc_val != cpc_val)) and clicks_val:
                                    cpc_val = spend_val / clicks_val if clicks_val else None

                                # ACoS: prefer Current ACoS if present, else Spend/Sales
                                acos_val = row.get('Current ACoS', None)
                                if (acos_val is None or acos_val == '' or (isinstance(acos_val, float) and acos_val != acos_val)):
                                    acos_val = (spend_val / sales_val) if sales_val else None

                                # ROAS and CVR
                                roas_val = (sales_val / spend_val) if spend_val else None
                                cvr_val = (orders_val / clicks_val) if clicks_val else None

                                metrics_payload = {
                                    'Spend': spend_val,
                                    'Ad Sales': sales_val,
                                    'ACoS': acos_val,
                                    'ROAS': roas_val,
                                    'CPC': cpc_val,
                                    'CVR': cvr_val,
                                    'Orders': orders_val,
                                }
                            except Exception:
                                metrics_payload = {
                                    'Spend': None,
                                    'Ad Sales': None,
                                    'ACoS': None,
                                    'ROAS': None,
                                    'CPC': None,
                                    'CVR': None,
                                    'Orders': None,
                                }

                            # Add metadata for tracking
                            action = {
                                'action_type': 'bid_optimization',
                                'row_data': filtered_row_dict,  # Store filtered row data
                                'campaign_name': row.get('Campaign Name', ''),
                                'old_bid': row.get('Old Bid', 0),
                                'new_bid': row.get('Recommended Bid', 0),
                                'bid_change': row.get('Bid Change', 0),
                                'sheet_source': row.get('Sheet_Source', 'Unknown'),
                                'metrics': metrics_payload,
                            }
                            st.session_state.bulk_export_actions.append(action)
                        
                        st.success(f"Added {len(bid_changes)} bid optimization actions to bulk export queue!")
                        
                        # Debug: Show debug messages if available
                        if hasattr(st.session_state, 'debug_messages') and st.session_state.debug_messages:
                            with st.expander("Debug Information", expanded=False):
                                for msg in st.session_state.debug_messages[-20:]:  # Show last 20 messages
                                    st.text(msg)
                
                with col2:
                    # Clear results option
                    if st.button("Clear Results", type="secondary"):
                        st.session_state.bid_optimization_results = None
                        st.rerun()
            else:
                st.info("**No optimization results yet.** Configure your settings above and click 'Optimize Bids Now' to see detailed analytics and bid recommendations.")
            
            # Help section
            with st.expander("How Bid Optimization Works", expanded=False):
                st.markdown("""
                **Bid Optimization Process:**
                
                1. **Data Filtering**: Only active campaigns with clicks and spend are processed
                2. **Target ACoS**: Apply default or custom target ACoS based on filter groups
                3. **Equilibrium Bid Calculation**: Calculate optimal bid to reach target ACoS
                4. **Guardrails**: Apply maximum increase/decrease limits to prevent dramatic changes
                5. **Minimum Bids**: Ensure bids meet platform minimums by campaign type
                
                **Key Metrics:**
                - **Current ACoS**: Spend Ã· Sales Ã— 100
                - **Target ACoS**: Your desired advertising cost of sale percentage
                - **EQ Bid**: (Target ACoS Ã— Sales) Ã· Clicks
                - **Bid Change**: Recommended Bid - Current Bid
                
                **Filter Groups**: Create custom rules to apply different target ACoS to specific campaigns, products, or keywords.
                """)
            

        
        with tab4:
            st.header("Campaign Creation")
            st.markdown("Set up and launch campaigns in bulk using your audit insights and client settings. This builder prepares bulk rows and adds them to the Bulk File Export queue.")

            # Initialize session containers
            # Always start this tab with empty Selected Products/Targets on first open of the session
            if 'cc_tab_first_open' not in st.session_state:
                st.session_state.cc_products = pd.DataFrame(columns=['ASIN','SKU','Product Title','Category','Tag 1','Tag 2','Tag 3','Product Group'])
                st.session_state.cc_targets = pd.DataFrame(columns=['Text','Kind','Branding','Source','Bid'])
                st.session_state.cc_tab_first_open = True
            if 'bulk_export_actions' not in st.session_state:
                st.session_state.bulk_export_actions = []

            st.markdown("---")
            # Section removed: Ad type selection now inferred from selected Tactics

            # Builder Mode selector
            st.subheader("Builder Mode")
            builder_mode = st.radio(
                "Choose how to build",
                ["Single Product Group", "Bulk Campaign Creation"],
                index=0,
                key="cc_builder_mode",
                help="Single Product Group: launch one group at a time with a Root Phrase. Bulk: upload IDs/CSV and auto-derive Product Group/Tags per ASIN/SKU."
            )

            st.subheader("Use ASINs or SKU")
            id_choice = st.selectbox("Identifier for Product Ads", ["ASIN", "SKU"], index=0)

            # Root Phrase only relevant in Single PG mode
            if builder_mode == 'Single Product Group':
                st.subheader("Campaign Root Phrase")
                root_phrase = st.text_input("Root phrase (e.g., 'Gels', 'Hand Soap', 'Body Wash')", value="")
            else:
                root_phrase = ""

            st.subheader("CSV Upload (optional)")
            st.caption("Upload a CSV to seed products for this build. Columns: 'ASIN' or 'SKU' (required), 'Product Title' (optional). Optional: 'Category' (Single mode only), 'Tag 1-3'. In Bulk mode, Product Group and Tags can be auto-filled from Client Settings. The ASIN Selection section will be pre-populated from this file; use Product Finder there to add more if needed.")
            # Provide a prominent template CSV download button
            tpl_cols = ['ASIN','SKU','Product Title','Category','Tag 1','Tag 2','Tag 3']
            _tpl_df = pd.DataFrame(columns=tpl_cols)
            _tpl_bytes = _tpl_df.to_csv(index=False).encode('utf-8')
            st.markdown('<div id="csv-template-cta">', unsafe_allow_html=True)
            st.download_button(
                label="â¬‡ï¸ Download CSV Template",
                data=_tpl_bytes,
                file_name="Campaign_Creation_Template.csv",
                mime="text/csv",
                type="primary",
                key="cc_csv_template_btn"
            )
            st.markdown('</div>', unsafe_allow_html=True)
            # High-contrast styling for visibility across themes
            st.markdown(
                """
                <style>
                #csv-template-cta .stDownloadButton > button {
                    background: linear-gradient(90deg, #FFCC00, #FFA000) !important;
                    color: #000 !important;
                    border: 2px solid #000 !important;
                    font-weight: 800 !important;
                    padding: 0.6rem 1.1rem !important;
                    border-radius: 6px !important;
                    box-shadow: 0 0 0 3px rgba(255, 193, 7, 0.25) !important;
                }
                #csv-template-cta .stDownloadButton > button:hover { filter: brightness(0.95); }
                </style>
                """,
                unsafe_allow_html=True,
            )
            st.caption("At least one of ASIN or SKU is required. Optional columns: Product Title, Tag 1, Tag 2, Tag 3.")
            uploaded_cc_file = st.file_uploader("Upload Campaign Creation CSV", type=["csv"], key="cc_csv_upload")
            uploaded_cc_df = None
            csv_id_col = None
            csv_meta_by_id = {}
            if uploaded_cc_file is not None:
                try:
                    uploaded_cc_df = pd.read_csv(uploaded_cc_file)
                    # Normalize columns
                    cols = {c.lower(): c for c in uploaded_cc_df.columns}
                    has_asin = 'asin' in cols
                    has_sku = 'sku' in cols
                    if not has_asin and not has_sku:
                        st.error("CSV must contain either an 'ASIN' or 'SKU' column.")
                    else:
                        csv_id_col = cols.get('asin') or cols.get('sku')
                        # Show preview
                        st.dataframe(uploaded_cc_df.head(20), use_container_width=True)
                        # Build meta lookup per id
                        client_cfg_for_title = st.session_state.get('client_config', {}) or {}
                        branded_asins_data_for_title = client_cfg_for_title.get('branded_asins_data', {})
                        for _, row in uploaded_cc_df.iterrows():
                            pid = str(row.get(csv_id_col, '')).strip()
                            if not pid:
                                continue
                            title = row.get(cols.get('product title','Product Title'), '')
                            if not title:
                                # Try branded_asins_data
                                if csv_id_col.lower() == 'asin' and pid in branded_asins_data_for_title:
                                    title = branded_asins_data_for_title[pid].get('product_title','')
                            # Lookups from Client Settings (Product Group + Tags)
                            pg_lookup = ''
                            t1_lookup = t2_lookup = t3_lookup = ''
                            if csv_id_col.lower() == 'asin' and pid in branded_asins_data_for_title:
                                meta_cfg = branded_asins_data_for_title.get(pid, {})
                                pg_lookup = (meta_cfg.get('product_group') or '').strip()
                                t1_lookup = (meta_cfg.get('tag1') or '').strip()
                                t2_lookup = (meta_cfg.get('tag2') or '').strip()
                                t3_lookup = (meta_cfg.get('tag3') or '').strip()
                            csv_meta_by_id[pid] = {
                                'title': title,
                                # In Bulk mode, prefer Product Group from lookup when Category is blank
                                'category': (
                                    str(row.get(cols.get('category','Category'), '')).strip()
                                    if builder_mode == 'Single Product Group' else
                                    (str(row.get(cols.get('category','Category'), '')).strip() or pg_lookup)
                                ) or (root_phrase or 'General'),
                                'product_group': pg_lookup,
                                'tags': [
                                    (str(row.get(cols.get('tag 1','Tag 1'), '')).strip() or t1_lookup),
                                    (str(row.get(cols.get('tag 2','Tag 2'), '')).strip() or t2_lookup),
                                    (str(row.get(cols.get('tag 3','Tag 3'), '')).strip() or t3_lookup),
                                ]
                            }
                        st.info(f"CSV loaded with identifier column: {csv_id_col}")
                except Exception as e:
                    st.error(f"Failed to read CSV: {e}")

            st.markdown("---")
            st.subheader("Campaign/Ad Group Structure")
            # Only Custom Dimensions builder (fully replaces legacy structuring options)
            ag_mode = 'custom'
            st.session_state['cc_ag_mode_key'] = ag_mode
            col_c1, col_c2 = st.columns([0.6, 0.4])
            with col_c1:
                campaign_dims_opts = ["Product Group", "Tag 1", "Tag 2", "Tag 3", "Tactic"]
                default_dims = st.session_state.get('cc_custom_campaign_dims', ["Product Group", "Tag 1", "Tag 2", "Tactic"])
                campaign_dims = st.multiselect(
                    "Campaign key dimensions",
                    options=campaign_dims_opts,
                    default=[d for d in default_dims if d in campaign_dims_opts],
                    help="A unique campaign will be created for each distinct combination of these fields. Include 'Tactic' to create separate campaigns per tactic."
                )
                st.session_state['cc_custom_campaign_dims'] = campaign_dims
            with col_c2:
                ag_split_opts = [
                    "Single (one ad group)",
                    "Product (ASIN/SKU)",
                    "Tactic",
                    "Tag 1",
                    "Tag 2",
                    "Tag 3",
                ]
                default_ag_split = st.session_state.get('cc_custom_adgroup_dim', 'Tag 3')
                ag_split = st.selectbox(
                    "Ad group split by",
                    options=ag_split_opts,
                    index=ag_split_opts.index(default_ag_split) if default_ag_split in ag_split_opts else 5,
                    help="Choose how ad groups are split inside each campaign."
                )
                st.session_state['cc_custom_adgroup_dim'] = ag_split
            # Quick tips for mapping common setups
            st.caption("Tips: Single ad group per tactic = dims [Tactic], split Single. Per tactic per product group = dims [Product Group, Tactic], split Single. Per product per tactic = dims [Product Group, Tactic], split Product.")

            st.markdown("---")
            st.subheader("ASIN Selection")
            # Context note when CSV is used to seed selection
            if uploaded_cc_df is not None and csv_id_col is not None:
                st.caption("Products from your CSV are pre-loaded below. Use Product Finder to add more items if needed.")
            # Initialize Selected Products table in session state (empty by default)
            if 'cc_products' not in st.session_state:
                st.session_state.cc_products = pd.DataFrame(columns=['ASIN','SKU','Product Title','Category','Tag 1','Tag 2','Tag 3','Product Group'])

            # Finder panel to discover products from CSV or Client Settings
            with st.expander("Product Finder (Filters)", expanded=False):
                client_cfg = st.session_state.get('client_config', {})
                branded_asins_data = client_cfg.get('branded_asins_data', {}) if client_cfg else {}
                product_groups = sorted({(v.get('product_group') or '').strip() for v in branded_asins_data.values() if isinstance(v, dict)})
                product_groups = [p for p in product_groups if p]

                # Build Tag options from Client Settings and uploaded CSV (if any)
                all_tags = set()
                for meta in branded_asins_data.values():
                    if not isinstance(meta, dict):
                        continue
                    for t in [meta.get('tag1',''), meta.get('tag2',''), meta.get('tag3','')]:
                        t = (t or '').strip()
                        if t:
                            all_tags.add(t)
                # If CSV uploaded, include its tag values in options BEFORE rendering multiselect
                if uploaded_cc_df is not None and csv_id_col is not None:
                    for _pid, _meta in csv_meta_by_id.items():
                        for t in (_meta.get('tags') or []):
                            t = (t or '').strip()
                            if t:
                                all_tags.add(t)

                colpf1, colpf2 = st.columns(2)
                with colpf1:
                    pf_contains_id = st.text_input("ASIN/SKU Contains", key="cc_pf_contains_id")
                    pf_contains_title = st.text_input("Product Title Contains", key="cc_pf_contains_title")
                with colpf2:
                    st.markdown('<div id="pf-right">', unsafe_allow_html=True)
                    pg_selected = st.multiselect("Product Group(s)", product_groups, key="cc_pf_groups")
                    # Tag options may be augmented later by CSV; for now use known set
                    tag_options = sorted(list(all_tags))
                    pf_selected_tags = st.multiselect("Filter by Tag(s)", tag_options, key="cc_pf_tags")
                    st.markdown('</div>', unsafe_allow_html=True)
                    # Increase multi-select heights
                    st.markdown(
                        """
                        <style>
                        #pf-right div[data-baseweb="select"] > div { min-height: 64px; }
                        </style>
                        """,
                        unsafe_allow_html=True,
                    )

                candidate_rows = []
                # Prefer CSV if uploaded; otherwise use Client Settings
                if uploaded_cc_df is not None and csv_id_col is not None:
                    # Normalize: derive group/category/tags from CSV metadata map
                    for pid, meta in csv_meta_by_id.items():
                        row = {
                            'ASIN': pid if csv_id_col.lower() == 'asin' else '',
                            'SKU': pid if csv_id_col.lower() == 'sku' else '',
                            'Product Title': meta.get('title',''),
                            'Product Group': meta.get('product_group','') or meta.get('category','') or '',
                            'Category': (
                                meta.get('category','') or
                                meta.get('product_group','') if builder_mode == 'Bulk Campaign Creation' else meta.get('category','')
                            ) or '',
                            'Tag 1': (meta.get('tags') or [None,None,None])[0],
                            'Tag 2': (meta.get('tags') or [None,None,None])[1],
                            'Tag 3': (meta.get('tags') or [None,None,None])[2],
                        }
                        candidate_rows.append(row)
                    st.caption("Using products from uploaded CSV. Category and Tags will drive campaign naming.")
                    # Auto-seed Selected Products exactly once per uploaded file
                    try:
                        seeded_key = 'cc_csv_seeded_for'
                        current_file_name = uploaded_cc_file.name if uploaded_cc_file is not None else None
                        # Ensure session container exists
                        if 'cc_products' not in st.session_state:
                            st.session_state.cc_products = pd.DataFrame(columns=['ASIN','SKU','Product Title','Category','Tag 1','Tag 2','Tag 3','Product Group'])
                        if current_file_name and st.session_state.get(seeded_key) != current_file_name:
                            seed_df = pd.DataFrame(candidate_rows)
                            # Keep consistent column order
                            cols = ['ASIN','SKU','Product Title','Category','Tag 1','Tag 2','Tag 3','Product Group']
                            for c in cols:
                                if c not in seed_df.columns:
                                    seed_df[c] = ''
                            seed_df = seed_df[cols]
                            # Merge and dedupe by chosen identifier
                            tmp = pd.concat([st.session_state.cc_products, seed_df], ignore_index=True)
                            id_col_for_dedupe = 'ASIN' if (csv_id_col and csv_id_col.lower() == 'asin') else 'SKU'
                            if id_col_for_dedupe in tmp.columns:
                                tmp[id_col_for_dedupe] = tmp[id_col_for_dedupe].astype(str).str.strip()
                                tmp = tmp[tmp[id_col_for_dedupe] != '']
                                tmp.drop_duplicates(subset=[id_col_for_dedupe], inplace=True)
                            st.session_state.cc_products = tmp
                            st.session_state[seeded_key] = current_file_name
                            st.success(f"Added {len(seed_df)} product(s) from CSV to Selected Products")
                    except Exception as e:
                        try:
                            st.session_state.debug_messages.append(f"[CC CSV Seed] Failed to auto-seed Selected Products: {e}")
                        except Exception:
                            pass
                    # Force ID choice to CSV identifier for downstream steps
                    id_choice = 'ASIN' if csv_id_col and csv_id_col.lower() == 'asin' else 'SKU'
                else:
                    for asin, meta in branded_asins_data.items():
                        if not isinstance(meta, dict):
                            continue
                        grp = (meta.get('product_group') or '').strip()
                        row = {
                            'ASIN': asin,
                            'SKU': meta.get('sku', ''),
                            'Product Title': meta.get('product_title', ''),
                            'Product Group': grp,
                            'Category': grp,
                            'Tag 1': meta.get('tag1',''),
                            'Tag 2': meta.get('tag2',''),
                            'Tag 3': meta.get('tag3',''),
                        }
                        candidate_rows.append(row)

                cand_df = pd.DataFrame(candidate_rows)
                if not cand_df.empty:
                    # Basic filters (product groups + split contains)
                    if pg_selected:
                        cand_df = cand_df[cand_df['Product Group'].isin(pg_selected)]
                    if pf_contains_id:
                        q1 = pf_contains_id.lower()
                        mask_id = (
                            cand_df['ASIN'].astype(str).str.lower().str.contains(q1, na=False) |
                            cand_df['SKU'].astype(str).str.lower().str.contains(q1, na=False)
                        )
                        cand_df = cand_df[mask_id]
                    if pf_contains_title:
                        q2 = pf_contains_title.lower()
                        mask_title = cand_df['Product Title'].astype(str).str.lower().str.contains(q2, na=False)
                        cand_df = cand_df[mask_title]
                    # Filter by Tag(s): include rows where any Tag 1/2/3 matches one of the selected tags
                    if 'pf_selected_tags' in locals() and pf_selected_tags:
                        tag_cols = [c for c in ['Tag 1','Tag 2','Tag 3'] if c in cand_df.columns]
                        if tag_cols:
                            mask_tags = cand_df[tag_cols].isin(pf_selected_tags).any(axis=1)
                            cand_df = cand_df[mask_tags]

                    # Advanced Filters (AND within group, OR across groups)
                    def _classify_columns(df_in):
                        # Always treat identifier-like columns as text
                        text_overrides = {'ASIN','SKU','Product Title','Product Group','Category','Tag 1','Tag 2','Tag 3'}
                        numeric_cols = []
                        text_cols = []
                        for c in df_in.columns:
                            if c in text_overrides:
                                text_cols.append(c)
                                continue
                            s = df_in[c]
                            # Try numeric detection: strip $, %, commas then numeric
                            s_clean = s.astype(str).str.replace('[,$%]','', regex=True).str.replace(',','', regex=False)
                            num = pd.to_numeric(s_clean, errors='coerce')
                            if num.notna().mean() >= 0.7:
                                numeric_cols.append(c)
                            else:
                                text_cols.append(c)
                        return numeric_cols, text_cols

                    def _mask_for_condition(df_in, col, op, val):
                        col_actual = col
                        s = df_in[col_actual]
                        # Decide operator set based on classification
                        num_cols, text_cols = _classify_columns(df_in)
                        if col_actual in text_cols:
                            s_text = s.astype(str)
                            v = str(val)
                            if op == 'contains':
                                return s_text.str.contains(v, case=False, na=False)
                            if op == "doesn't contain":
                                return ~s_text.str.contains(v, case=False, na=False)
                            if op == 'equals':
                                return s_text.str.strip().str.lower() == v.strip().lower()
                            if op == "doesn't equal":
                                return s_text.str.strip().str.lower() != v.strip().lower()
                            return pd.Series([True]*len(df_in))
                        else:
                            s_num = pd.to_numeric(s.astype(str).str.replace('[,$%]','', regex=True).str.replace(',','', regex=False), errors='coerce')
                            try:
                                vnum = float(val)
                            except:
                                return pd.Series([False]*len(df_in))
                            if op == '>':
                                return s_num > vnum
                            if op == '>=':
                                return s_num >= vnum
                            if op == '=':
                                return s_num == vnum
                            if op == '<=':
                                return s_num <= vnum
                            if op == '<':
                                return s_num < vnum
                            return pd.Series([True]*len(df_in))

                    # Build/Render Advanced Filter UI for Product Finder (no nested expanders)
                    if 'cc_pf_filter_groups' not in st.session_state:
                        st.session_state.cc_pf_filter_groups = []  # list of groups, group = list of conditions
                    st.markdown("#### Advanced Filters (Product Finder)")
                    with st.container():
                        num_cols, txt_cols = _classify_columns(cand_df)
                        # Remove ID-like fields from dropdowns (ASIN, SKU, any column containing 'ID')
                        def _pf_is_id_like(name: str) -> bool:
                            n = str(name).strip().lower()
                            return 'id' in n or n in ('asin','sku')
                        display_cols_pf = [c for c in list(cand_df.columns) if not _pf_is_id_like(c)] or list(cand_df.columns)
                        st.caption("Add one or more filter groups. ALL conditions in a group must match (AND). Rows matching ANY group are included (OR).")
                        # Buttons to manage groups
                        cols_mg = st.columns([1,1,1])
                        if cols_mg[0].button("Add Group", key="cc_pf_add_group"):
                            st.session_state.cc_pf_filter_groups.append([])
                        if cols_mg[1].button("Clear All", key="cc_pf_clear_groups"):
                            st.session_state.cc_pf_filter_groups = []

                        # Render each group
                        for gidx, group in enumerate(st.session_state.cc_pf_filter_groups):
                            st.markdown(f"Filter Group {gidx+1} (AND)")
                            # Controls for each condition
                            if len(group) == 0:
                                # Default to first available non-ID column
                                default_col = display_cols_pf[0] if display_cols_pf else ((txt_cols+num_cols)[0] if (txt_cols or num_cols) else '')
                                group.append({'column': default_col, 'op': 'contains', 'value': ''})
                            rm_idx = None
                            for cidx, cond in enumerate(group):
                                cc1, cc2, cc3, cc4 = st.columns([2,1,2,0.5])
                                with cc1:
                                    current = cond.get('column', display_cols_pf[0] if display_cols_pf else (list(cand_df.columns)[0] if len(cand_df.columns)>0 else ''))
                                    idx = display_cols_pf.index(current) if current in display_cols_pf else 0
                                    cond['column'] = st.selectbox("Column", options=display_cols_pf, index=idx, key=f"cc_pf_col_{gidx}_{cidx}")
                                # Determine operator choices
                                is_text = cond['column'] in txt_cols
                                ops = ['contains', "doesn't contain", 'equals', "doesn't equal"] if is_text else ['>','>=','=','<=','<']
                                with cc2:
                                    cond['op'] = st.selectbox("Op", options=ops, index=ops.index(cond.get('op', ops[0])) if cond.get('op') in ops else 0, key=f"cc_pf_op_{gidx}_{cidx}")
                                with cc3:
                                    cond['value'] = st.text_input("Value", value=str(cond.get('value','')), key=f"cc_pf_val_{gidx}_{cidx}")
                                with cc4:
                                    if st.button("âœ•", key=f"cc_pf_rm_{gidx}_{cidx}"):
                                        rm_idx = cidx
                            if rm_idx is not None:
                                group.pop(rm_idx)
                            st.button("Add Condition", key=f"cc_pf_add_cond_{gidx}", on_click=lambda g=gidx, dcols=list(display_cols_pf): st.session_state.cc_pf_filter_groups[g].append({'column': dcols[0] if dcols else '', 'op': 'contains', 'value': ''}))
                            st.divider()

                    # Apply advanced filters
                    if st.session_state.cc_pf_filter_groups:
                        group_masks = []
                        for group in st.session_state.cc_pf_filter_groups:
                            if not group:
                                continue
                            m = pd.Series([True]*len(cand_df))
                            for cond in group:
                                m = m & _mask_for_condition(cand_df, cond.get('column',''), cond.get('op','contains'), cond.get('value',''))
                            group_masks.append(m)
                        if group_masks:
                            final_mask = group_masks[0]
                            for gm in group_masks[1:]:
                                final_mask = final_mask | gm
                            cand_df = cand_df[final_mask]

                    st.dataframe(cand_df, use_container_width=True, height=260)
                    id_col_for_picker = 'ASIN' if id_choice == 'ASIN' else 'SKU'
                    cols_add = st.columns([1,1])
                    with cols_add[0]:
                        if st.button("Add All Filtered Products", key="cc_pf_add_all"):
                            to_add = cand_df.copy()
                            if not to_add.empty:
                                if st.session_state.cc_products.empty:
                                    st.session_state.cc_products = to_add.reset_index(drop=True)
                                else:
                                    st.session_state.cc_products = pd.concat([st.session_state.cc_products, to_add], ignore_index=True)
                                st.session_state.cc_products.drop_duplicates(subset=[id_col_for_picker], inplace=True)
                                st.success(f"Added {len(to_add)} product(s) to Selected Products")
                    with cols_add[1]:
                        st.empty()
                else:
                    st.info("No product candidates available. Upload a CSV or configure Client Settings > Branded ASINs.")

            # Manual add products
            with st.expander("Add Products Manually", expanded=False):
                man_txt = st.text_area(f"Enter {id_choice}s (one per line)", key="cc_pf_manual")
                # Fallback metadata (optional) used when Client Settings has no match
                fb_col1, fb_col2, fb_col3, fb_col4 = st.columns([1,1,1,1])
                with fb_col1:
                    fallback_pg = st.text_input("Fallback Product Group (optional)", value=st.session_state.get('cc_pf_fallback_pg',''), key="cc_pf_fallback_pg")
                with fb_col2:
                    fallback_t1 = st.text_input("Fallback Tag 1 (optional)", value=st.session_state.get('cc_pf_fallback_t1',''), key="cc_pf_fallback_t1")
                with fb_col3:
                    fallback_t2 = st.text_input("Fallback Tag 2 (optional)", value=st.session_state.get('cc_pf_fallback_t2',''), key="cc_pf_fallback_t2")
                with fb_col4:
                    fallback_t3 = st.text_input("Fallback Tag 3 (optional)", value=st.session_state.get('cc_pf_fallback_t3',''), key="cc_pf_fallback_t3")
                if st.button("Add Manual Products", key="cc_pf_manual_btn"):
                    rows = []
                    client_cfg = st.session_state.get('client_config', {}) or {}
                    branded_asins_data = client_cfg.get('branded_asins_data', {}) if isinstance(client_cfg, dict) else {}
                    matched = 0
                    for line in (man_txt or '').splitlines():
                        pid = line.strip()
                        if not pid:
                            continue
                        meta = None
                        if isinstance(branded_asins_data, dict):
                            if id_choice == 'ASIN':
                                meta = branded_asins_data.get(pid)
                            else:  # SKU entered: find matching sku in branded_asins_data values
                                try:
                                    meta = next((m for m in branded_asins_data.values() if isinstance(m, dict) and str(m.get('sku','')).strip() == pid), None)
                                except Exception:
                                    meta = None
                        # Defaults
                        title = ''
                        pg = ''
                        t1 = t2 = t3 = ''
                        if isinstance(meta, dict):
                            matched += 1
                            title = str(meta.get('product_title','') or '').strip()
                            pg = str(meta.get('product_group','') or '').strip()
                            t1 = str(meta.get('tag1','') or '').strip()
                            t2 = str(meta.get('tag2','') or '').strip()
                            t3 = str(meta.get('tag3','') or '').strip()
                        else:
                            # Use fallbacks when no metadata found
                            pg = str(st.session_state.get('cc_pf_fallback_pg','') or fallback_pg or '').strip()
                            t1 = str(st.session_state.get('cc_pf_fallback_t1','') or fallback_t1 or '').strip()
                            t2 = str(st.session_state.get('cc_pf_fallback_t2','') or fallback_t2 or '').strip()
                            t3 = str(st.session_state.get('cc_pf_fallback_t3','') or fallback_t3 or '').strip()
                        # Category: in Bulk mode prefer Product Group; otherwise use provided default/root phrase
                        if builder_mode == 'Bulk Campaign Creation':
                            category_val = pg or (root_phrase or 'General')
                        else:
                            category_val = (pg or (root_phrase or 'General'))
                        rows.append({
                            'ASIN': pid if id_choice=='ASIN' else '',
                            'SKU': pid if id_choice=='SKU' else '',
                            'Product Title': title,
                            'Product Group': pg or (root_phrase or 'General'),
                            'Category': category_val,
                            'Tag 1': t1, 'Tag 2': t2, 'Tag 3': t3
                        })
                    if rows:
                        add_df = pd.DataFrame(rows)
                        if st.session_state.cc_products.empty:
                            st.session_state.cc_products = add_df
                        else:
                            st.session_state.cc_products = pd.concat([st.session_state.cc_products, add_df], ignore_index=True)
                        id_col_for_picker = 'ASIN' if id_choice == 'ASIN' else 'SKU'
                        st.session_state.cc_products.drop_duplicates(subset=[id_col_for_picker], inplace=True)
                        msg = f"Added {len(rows)} manual product(s)."
                        if matched:
                            msg += f" Matched {matched} to Client Settings for Product Group/Tags."
                        st.success(msg)

            # Selected Products table (starts empty)
            st.markdown("**Selected Products**")
            if st.session_state.cc_products.empty:
                st.info("No products selected yet. Use the Product Finder or add manually.")
            else:
                st.session_state.cc_products = st.data_editor(
                    st.session_state.cc_products,
                    num_rows="dynamic",
                    use_container_width=True,
                    key="cc_products_editor"
                )
            # Clear Selected Products button
            cp_cols = st.columns([1,3])
            with cp_cols[0]:
                if st.button("Clear Selected Products", key="cc_clear_products"):
                    st.session_state.cc_products = pd.DataFrame(columns=['ASIN','SKU','Product Title','Category','Tag 1','Tag 2','Tag 3','Product Group'])
                    st.success("Cleared Selected Products")
                    # Reset editor widget state and force immediate rerun so the cleared state persists
                    try:
                        st.session_state.pop('cc_products_editor', None)
                    except Exception:
                        pass
                    st.rerun()

            st.markdown("---")
            # Section removed: Targeting Type is determined by each selected tactic

            # Initialize selected targets storage
            if 'cc_targets' not in st.session_state:
                st.session_state.cc_targets = pd.DataFrame(columns=['Text','Kind','Branding','Source','Bid'])

            st.markdown("---")
            st.subheader("Targets")
            st.caption("Build complex finder rules. Each group has AND logic; results across groups are OR'ed. Add results to Selected Targets.")

            # Determine data availability to conditionally show quick contains bars
            has_search_term = False
            has_target_perf = False
            try:
                st_df_chk = get_search_term_data(st.session_state.bulk_data, st.session_state.client_config)
                has_search_term = st_df_chk is not None and not st_df_chk.empty
            except Exception:
                has_search_term = False
            try:
                tgt_b_chk, tgt_nb_chk = get_targeting_performance_data(st.session_state.bulk_data, st.session_state.client_config)
                has_target_perf = (
                    (isinstance(tgt_b_chk, pd.DataFrame) and not tgt_b_chk.empty) or
                    (isinstance(tgt_nb_chk, pd.DataFrame) and not tgt_nb_chk.empty)
                )
            except Exception:
                has_target_perf = False
            # If no bulk data has been uploaded, we should disable the Advanced Finder
            has_bulk_sources = bool(has_search_term or has_target_perf)

            # Build quick filter UI dynamically only when bulk sources exist
            ui_cols = []
            if has_bulk_sources:
                if has_target_perf and has_search_term:
                    ui_cols = st.columns(4)
                    c0, c1, c2, c3 = ui_cols
                    with c0:
                        txt_contains_target = st.text_input("Target Contains", key="cc_txt_contains_target")
                    with c1:
                        txt_contains_search = st.text_input("Search Term Contains", key="cc_txt_contains_search")
                    with c2:
                        min_roas = st.number_input("Min ROAS", min_value=0.0, value=0.0, step=0.1, key="cc_filter_min_roas")
                    with c3:
                        min_orders = st.number_input("Min Orders", min_value=0, value=0, step=1, key="cc_filter_min_orders")
                elif has_target_perf:
                    ui_cols = st.columns(3)
                    c0, c1, c2 = ui_cols
                    with c0:
                        txt_contains_target = st.text_input("Target Contains", key="cc_txt_contains_target")
                    with c1:
                        min_roas = st.number_input("Min ROAS", min_value=0.0, value=0.0, step=0.1, key="cc_filter_min_roas")
                    with c2:
                        min_orders = st.number_input("Min Orders", min_value=0, value=0, step=1, key="cc_filter_min_orders")
                    txt_contains_search = ''
                elif has_search_term:
                    ui_cols = st.columns(3)
                    c0, c1, c2 = ui_cols
                    with c0:
                        txt_contains_search = st.text_input("Search Term Contains", key="cc_txt_contains_search")
                    with c1:
                        min_roas = st.number_input("Min ROAS", min_value=0.0, value=0.0, step=0.1, key="cc_filter_min_roas")
                    with c2:
                        min_orders = st.number_input("Min Orders", min_value=0, value=0, step=1, key="cc_filter_min_orders")
                    txt_contains_target = ''
            else:
                st.info("Target/Search Term finder is disabled until you upload files in the File Upload tab. You can still add targets manually below.")
                # Provide safe default values so downstream references remain valid if needed
                txt_contains_target = ''
                txt_contains_search = ''
                min_roas = 0.0
                min_orders = 0

            # Aggregation and scope options for candidate metrics
            st.markdown("**Aggregation**")
            agg_cols = st.columns(2)
            with agg_cols[0]:
                st.checkbox("Aggregate by Product Group", value=True, key="cc_agg_pg")
            with agg_cols[1]:
                st.checkbox("Restrict to selected products' ad groups", value=False, key="cc_agg_restrict_adg")

            # Debug panel to show what's happening
            if st.checkbox("Show Debug Info", value=False, key="cc_show_debug"):
                debug_msgs = st.session_state.get('debug_messages', [])
                recent_msgs = [msg for msg in debug_msgs if '[Advanced Finder]' in msg or '[Targets Advanced Finder]' in msg][-20:]
                if recent_msgs:
                    st.text_area("Recent Debug Messages", value='\n'.join(recent_msgs), height=200)
                else:
                    st.info("No debug messages yet. Try running the Advanced Finder.")

            # Helpers for classification and masks
            def _classify_cols_generic(df_in):
                # Prioritize known numeric metrics even if many blanks
                numeric_tokens = [
                    'spend','sales','orders','clicks','impressions','cpc','cpa','acos','roas','ctr','cvr',
                    'units','units ordered','unit sales','unit_sessions','conversion rate'
                ]
                num_cols = []
                txt_cols = []
                for c in df_in.columns:
                    cl = str(c).strip().lower()
                    if c in ('Target','Search Term'):
                        txt_cols.append(c)
                        continue
                    # If any numeric token appears in the column name, treat as numeric
                    if any(tok in cl for tok in numeric_tokens):
                        num_cols.append(c)
                        continue
                    s_clean = df_in[c].astype(str).str.replace('[,$%]','', regex=True).str.replace(',','', regex=False)
                    num = pd.to_numeric(s_clean, errors='coerce')
                    if num.notna().mean() >= 0.7:
                        num_cols.append(c)
                    else:
                        txt_cols.append(c)
                return num_cols, txt_cols

            def _mask_cond_generic(df_in, col, op, val):
                s = df_in[col]
                num_cols, txt_cols = _classify_cols_generic(df_in)
                if col in ('Target','Search Term') or col in txt_cols:
                    s_text = s.astype(str)
                    v = str(val)
                    if op == 'contains':
                        return s_text.str.contains(v, case=False, na=False)
                    if op == "doesn't contain":
                        return ~s_text.str.contains(v, case=False, na=False)
                    if op == 'equals':
                        return s_text.str.strip().str.lower() == v.strip().lower()
                    if op == "doesn't equal":
                        return s_text.str.strip().str.lower() != v.strip().lower()
                    return pd.Series([True]*len(df_in))
                else:
                    s_num = pd.to_numeric(s.astype(str).str.replace('[,$%]','', regex=True).str.replace(',','', regex=False), errors='coerce')
                    try:
                        vnum = float(val)
                    except:
                        return pd.Series([False]*len(df_in))
                    if op == '>':
                        return s_num > vnum
                    if op == '>=':
                        return s_num >= vnum
                    if op == '=':
                        return s_num == vnum
                    if op == '<=':
                        return s_num <= vnum
                    if op == '<':
                        return s_num < vnum
                    return pd.Series([True]*len(df_in))

            # Build set of allowed Campaign/Ad Group pairs that have enabled Product Ads for selected ASIN/SKU
            def _allowed_adgroups_for_selected_products():
                try:
                    if 'cc_products' not in st.session_state or st.session_state.cc_products.empty:
                        return set(), set()
                    prod_df = st.session_state.cc_products.copy()
                    asin_set = set()
                    sku_set = set()
                    if 'ASIN' in prod_df.columns:
                        asin_set = {str(x).strip().upper() for x in prod_df['ASIN'].dropna() if str(x).strip()}
                    if 'SKU' in prod_df.columns:
                        sku_set = {str(x).strip().upper() for x in prod_df['SKU'].dropna() if str(x).strip()}
                    if not asin_set and not sku_set:
                        return set(), set()
                    allowed_campaigns = set()
                    allowed_pairs = set()
                    bulk_data = st.session_state.get('bulk_data', {}) or {}
                    for sheet_name, df in bulk_data.items():
                        if not isinstance(df, pd.DataFrame) or df.empty:
                            continue
                        # Prefer Sponsored Products where Product Ads exist
                        if not any(tok in sheet_name.lower() for tok in ['sponsored products', 'sp', 'campaign']):
                            # Skip obvious non-campaign sheets
                            continue
                        lower_map = {c.lower(): c for c in df.columns}
                        # Identify columns
                        rec_col = next((lower_map[c] for c in ['record type'] if c in lower_map), None)
                        ent_col = next((lower_map[c] for c in ['entity'] if c in lower_map), None)
                        state_col = next((lower_map[c] for c in ['state', 'status'] if c in lower_map), None)
                        camp_col = next((lower_map[c] for c in ['campaign name (informational only)', 'campaign name', 'campaign'] if c in lower_map), None)
                        ag_col = next((lower_map[c] for c in ['ad group name', 'ad group'] if c in lower_map), None)
                        asin_col = next((lower_map[c] for c in ['asin', 'advertised asin'] if c in lower_map), None)
                        sku_col = next((lower_map[c] for c in ['sku', 'advertised sku'] if c in lower_map), None)
                        # Require at least a product identifier column and campaign
                        if camp_col is None or (asin_col is None and sku_col is None):
                            continue
                        df2 = df
                        # Robust filter for Product Ads: prefer Entity=='Product Ad'; fallback to Record Type 'Ad' or contains 'Product Ad'
                        try:
                            mask_entity = pd.Series([False]*len(df2))
                            if ent_col is not None:
                                col = df2[ent_col].astype(str).str.strip().str.lower()
                                mask_entity = (col == 'product ad') | (col == 'ad')
                            mask_record = pd.Series([False]*len(df2))
                            if rec_col is not None:
                                colr = df2[rec_col].astype(str).str.strip().str.lower()
                                mask_record = (colr == 'ad') | (colr.str.contains('product ad'))
                            mask_prod_ad = mask_entity | mask_record
                            if mask_prod_ad.any():
                                df2 = df2[mask_prod_ad]
                        except Exception:
                            pass
                        if state_col is not None:
                            try:
                                df2 = df2[df2[state_col].astype(str).str.strip().str.lower() == 'enabled']
                            except Exception:
                                pass
                        if df2.empty:
                            continue
                        # Product match
                        mask = pd.Series([False]*len(df2))
                        if asin_col and asin_set:
                            try:
                                mask = mask | df2[asin_col].astype(str).str.strip().str.upper().isin(asin_set)
                            except Exception:
                                pass
                        if sku_col and sku_set:
                            try:
                                mask = mask | df2[sku_col].astype(str).str.strip().str.upper().isin(sku_set)
                            except Exception:
                                pass
                        df3 = df2[mask]
                        if df3.empty:
                            continue
                        # Collect allowed campaign/ad group pairs
                        for _, r in df3.iterrows():
                            camp = str(r.get(camp_col, '')).strip()
                            if not camp:
                                continue
                            allowed_campaigns.add(camp)
                            if ag_col and pd.notna(r.get(ag_col, None)):
                                adg = str(r.get(ag_col, '')).strip()
                                if adg:
                                    allowed_pairs.add((camp, adg))
                    return allowed_campaigns, allowed_pairs
                except Exception as e:
                    st.session_state.debug_messages.append(f"[Agg AdGroup Filter] Error building allowed pairs: {e}")
                    return set(), set()

            # Multi-group advanced finder (only when bulk sources are available)
            if has_bulk_sources:
                if 'cc_cand_query_groups' not in st.session_state:
                    # list of groups; group = {'source': 'All'|'Search Terms'|'Targets'|'ASIN Targets', 'conds': []}
                    st.session_state.cc_cand_query_groups = []
                # Ensure at least one default group exists (once per session)
                if not st.session_state.cc_cand_query_groups and not st.session_state.get('ccq_default_group_created', False):
                    st.session_state.cc_cand_query_groups.append({'source': 'All', 'conds': []})
                    st.session_state['ccq_default_group_created'] = True

                with st.expander("Advanced Finder (Targets / Search Terms)", expanded=True):
                    mg1, mg2, mg3 = st.columns([1,1,1])
                    if mg1.button("Add Group", key="ccq_add_group"):
                        st.session_state.cc_cand_query_groups.append({'source': 'All', 'conds': []})
                    if mg2.button("Clear All", key="ccq_clear_all"):
                        st.session_state.cc_cand_query_groups = []

                # Render groups
                def _is_id_like(name: str) -> bool:
                    n = str(name).strip().lower()
                    return 'id' in n or n in ('asin','sku')

                for gidx, group in enumerate(st.session_state.cc_cand_query_groups):
                    st.markdown(f"Group {gidx+1} (AND)")
                    # Source selection for this group
                    src_options = ["All","Search Terms","Targets","ASIN Targets"]
                    default_src = group.get('source','All')
                    if default_src not in src_options:
                        default_src = "All"
                    group['source'] = st.selectbox("Source", options=src_options, index=src_options.index(default_src), key=f"ccq_src_{gidx}")
                    # Load source df for columns/options
                    df_src = pd.DataFrame()
                    if group['source'] == 'Search Terms':
                        try:
                            st_df = get_search_term_data(st.session_state.bulk_data, st.session_state.client_config)
                            if not st_df.empty:
                                df_src = st_df.copy()
                        except Exception:
                            df_src = pd.DataFrame()
                    elif group['source'] in ('Targets','ASIN Targets'):
                        try:
                            tgt_b, tgt_nb = get_targeting_performance_data(st.session_state.bulk_data, st.session_state.client_config)
                            parts = []
                            if isinstance(tgt_b, pd.DataFrame) and not tgt_b.empty:
                                parts.append(tgt_b)
                            if isinstance(tgt_nb, pd.DataFrame) and not tgt_nb.empty:
                                parts.append(tgt_nb)
                            if parts:
                                df_src = pd.concat(parts, ignore_index=True, sort=False)
                        except Exception:
                            df_src = pd.DataFrame()
                    else:
                        # All: combine Search Terms and Targets sources
                        try:
                            st_df = get_search_term_data(st.session_state.bulk_data, st.session_state.client_config)
                        except Exception:
                            st_df = pd.DataFrame()
                        try:
                            tgt_b, tgt_nb = get_targeting_performance_data(st.session_state.bulk_data, st.session_state.client_config)
                            tgt_df = pd.concat([tgt_b, tgt_nb], ignore_index=True, sort=False) if (isinstance(tgt_b, pd.DataFrame) and isinstance(tgt_nb, pd.DataFrame)) else pd.DataFrame()
                        except Exception:
                            tgt_df = pd.DataFrame()
                        parts = []
                        if isinstance(st_df, pd.DataFrame) and not st_df.empty:
                            parts.append(st_df.copy())
                        if isinstance(tgt_df, pd.DataFrame) and not tgt_df.empty:
                            parts.append(tgt_df)
                        if not parts:
                            continue
                        df_src = pd.concat(parts, ignore_index=True, sort=False)

                    # Normalize common metric column names
                    if not df_src.empty:
                        # Map Ad Sales -> Sales (if Sales missing)
                        if 'Sales' not in df_src.columns and 'Ad Sales' in df_src.columns:
                            df_src['Sales'] = pd.to_numeric(df_src['Ad Sales'], errors='coerce')
                        # Map Cost -> Spend (if Spend missing)
                        if 'Spend' not in df_src.columns and 'Cost' in df_src.columns:
                            df_src['Spend'] = pd.to_numeric(df_src['Cost'], errors='coerce')
                        # Map alternate Orders columns -> Orders (first one found)
                        if 'Orders' not in df_src.columns:
                            for alt in ['Total Orders', '14 Day Total Orders (#)', '7 Day Total Orders (#)', 'Orders (Views & Clicks)']:
                                if alt in df_src.columns:
                                    df_src['Orders'] = pd.to_numeric(df_src[alt], errors='coerce')
                                    break
                    # Add ROAS if missing and Sales/Spend exist
                    if not df_src.empty and 'ROAS' not in df_src.columns and {'Sales','Spend'}.issubset(set(df_src.columns)):
                        spend_nonzero = pd.to_numeric(df_src['Spend'], errors='coerce').replace(0, np.nan)
                        sales_num = pd.to_numeric(df_src['Sales'], errors='coerce')
                        df_src['ROAS'] = (sales_num / spend_nonzero).fillna(0)
                    # Apply quick numeric filters across all sources prior to per-group processing
                    try:
                        if min_roas and 'ROAS' in df_src.columns:
                            before = len(df_src)
                            df_src = df_src[pd.to_numeric(df_src['ROAS'], errors='coerce').fillna(0) >= float(min_roas)]
                            st.session_state.debug_messages.append(f"[Advanced Finder] Min ROAS filter kept {len(df_src)}/{before} rows")
                    except Exception:
                        pass
                    try:
                        if min_orders and 'Orders' in df_src.columns:
                            before = len(df_src)
                            df_src = df_src[pd.to_numeric(df_src['Orders'], errors='coerce').fillna(0) >= float(min_orders)]
                            st.session_state.debug_messages.append(f"[Advanced Finder] Min Orders filter kept {len(df_src)}/{before} rows")
                    except Exception:
                        pass

                    if df_src.empty:
                        st.info("No data for this source.")
                        continue

                    # Apply quick and product-group filters pre-emptively for preview & column discovery
                    # If the group source is ASIN Targets, restrict to ASIN-like Targets only
                    if group.get('source') == 'ASIN Targets' and 'Target' in df_src.columns:
                        df_src = df_src[df_src['Target'].astype(str).str.contains(r'B0[A-Za-z0-9]{8}', case=False, na=False)]
                        if df_src.empty:
                            st.info("No ASIN targets found for current filters.")
                            continue
                    # Contains bars depend on source
                    if group.get('source','All') in ('Search Terms','All') and txt_contains_search:
                        q = txt_contains_search.lower()
                        name_cols = [c for c in df_src.columns if c.lower() in ['search term','campaign name','ad group name','campaign']]
                        if name_cols:
                            mask_q = False
                            for c in name_cols:
                                mask_q = mask_q | df_src[c].astype(str).str.lower().str.contains(q, na=False)
                            df_src = df_src[mask_q]
                    if group.get('source','Targets') in ('Targets','ASIN Targets','All') and txt_contains_target:
                        q = txt_contains_target.lower()
                        name_cols = [c for c in df_src.columns if c.lower() in ['target','campaign name','ad group name','campaign']]
                        if name_cols:
                            mask_q = False
                            for c in name_cols:
                                mask_q = mask_q | df_src[c].astype(str).str.lower().str.contains(q, na=False)
                            df_src = df_src[mask_q]
                    sel_groups = st.session_state.get('cc_pf_groups', [])
                    # Only apply Product Group filter when PG aggregation is enabled
                    if st.session_state.get('cc_agg_pg', True) and sel_groups and 'Product Group' in df_src.columns:
                        df_src = df_src[df_src['Product Group'].isin(sel_groups)]

                    # Conditions UI
                    num_cols, txt_cols = _classify_cols_generic(df_src)
                    # Filter out ID-like columns from dropdown
                    display_cols = [c for c in list(df_src.columns) if not _is_id_like(c)] or list(df_src.columns)
                    # Only auto-add a default condition when the group is newly initialized
                    if 'initialized' not in group:
                        group['initialized'] = True
                        if not group.get('conds'):
                            # Choose a sensible default from filtered columns
                            if group.get('source')=='Search Terms' and 'Search Term' in display_cols:
                                default_col = 'Search Term'
                            elif 'Target' in display_cols:
                                default_col = 'Target'
                            else:
                                default_col = display_cols[0]
                            group['conds'].append({'column': default_col, 'op': 'contains', 'value': ''})
                    rm_idx = None
                    for cidx, cond in enumerate(group['conds']):
                        cc1, cc2, cc3, cc4 = st.columns([2,1,2,0.5])
                        with cc1:
                            # Use filtered display columns for selection
                            current = cond.get('column', display_cols[0])
                            idx = display_cols.index(current) if current in display_cols else 0
                            cond['column'] = st.selectbox("Column", options=display_cols, index=idx, key=f"ccq_col_{gidx}_{cidx}")
                        is_text = cond['column'] in txt_cols or cond['column'] in ('Target','Search Term')
                        ops = ['contains', "doesn't contain", 'equals', "doesn't equal"] if is_text else ['>','>=','=','<=','<']
                        with cc2:
                            cond['op'] = st.selectbox("Op", options=ops, index=ops.index(cond.get('op', ops[0])) if cond.get('op') in ops else 0, key=f"ccq_op_{gidx}_{cidx}")
                        with cc3:
                            cond['value'] = st.text_input("Value", value=str(cond.get('value','')), key=f"ccq_val_{gidx}_{cidx}")
                        with cc4:
                            if st.button("âœ•", key=f"ccq_rm_{gidx}_{cidx}"):
                                rm_idx = cidx
                    if rm_idx is not None:
                        group['conds'].pop(rm_idx)
                    st.button("Add Condition", key=f"ccq_add_cond_{gidx}", on_click=lambda g=gidx, dcols=list(display_cols): st.session_state.cc_cand_query_groups[g]['conds'].append({'column': dcols[0] if dcols else '', 'op': 'contains', 'value': ''}))
                    st.divider()

                # Build candidates by applying each group's conditions and OR'ing results
                candidates = []
                diag_msgs = []
                try:
                    do_pg_agg = st.session_state.get('cc_agg_pg', True)
                    restrict_adg = st.session_state.get('cc_agg_restrict_adg', False)
                    for group in st.session_state.cc_cand_query_groups:
                        source = group.get('source','All')
                        df_src = pd.DataFrame()
                        if source == 'Search Terms':
                            st_df = get_search_term_data(st.session_state.bulk_data, st.session_state.client_config)
                            if st_df is None or st_df.empty:
                                diag_msgs.append(f"[Diag] Source=Search Terms: 0 rows from get_search_term_data")
                                continue
                            df_src = st_df.copy()
                            diag_msgs.append(f"[Diag] Source=Search Terms: {len(df_src)} base rows")
                        elif source in ('Targets','ASIN Targets'):
                            tgt_b, tgt_nb = get_targeting_performance_data(st.session_state.bulk_data, st.session_state.client_config)
                            parts = []
                            if isinstance(tgt_b, pd.DataFrame) and not tgt_b.empty:
                                parts.append(tgt_b)
                            if isinstance(tgt_nb, pd.DataFrame) and not tgt_nb.empty:
                                parts.append(tgt_nb)
                            if not parts:
                                diag_msgs.append(f"[Diag] Source={source}: 0 rows from get_targeting_performance_data")
                                continue
                            df_src = pd.concat(parts, ignore_index=True, sort=False)
                            diag_msgs.append(f"[Diag] Source={source}: {len(df_src)} base rows (targets)")
                        else:
                            # All: combine search terms and targeting
                            st_df = get_search_term_data(st.session_state.bulk_data, st.session_state.client_config)
                            tgt_b, tgt_nb = get_targeting_performance_data(st.session_state.bulk_data, st.session_state.client_config)
                            parts = []
                            if isinstance(st_df, pd.DataFrame) and not st_df.empty:
                                parts.append(st_df.copy())
                            if isinstance(tgt_b, pd.DataFrame) and not tgt_b.empty:
                                parts.append(tgt_b)
                            if isinstance(tgt_nb, pd.DataFrame) and not tgt_nb.empty:
                                parts.append(tgt_nb)
                            if not parts:
                                diag_msgs.append("[Diag] Source=All: 0 rows (no Search Terms or Targets available)")
                                continue
                            df_src = pd.concat(parts, ignore_index=True, sort=False)
                            diag_msgs.append(f"[Diag] Source=All: {len(df_src)} base rows (combined)")

                        # Normalize common metric column names prior to computing ROAS
                        if not df_src.empty:
                            if 'Sales' not in df_src.columns and 'Ad Sales' in df_src.columns:
                                df_src['Sales'] = pd.to_numeric(df_src['Ad Sales'], errors='coerce')
                            if 'Spend' not in df_src.columns and 'Cost' in df_src.columns:
                                df_src['Spend'] = pd.to_numeric(df_src['Cost'], errors='coerce')
                            if 'Orders' not in df_src.columns:
                                for alt in ['Total Orders', '14 Day Total Orders (#)', '7 Day Total Orders (#)', 'Orders (Views & Clicks)']:
                                    if alt in df_src.columns:
                                        df_src['Orders'] = pd.to_numeric(df_src[alt], errors='coerce')
                                        break
                        # Add ROAS if missing
                        if 'ROAS' not in df_src.columns and {'Sales','Spend'}.issubset(set(df_src.columns)):
                            spend_nonzero = pd.to_numeric(df_src['Spend'], errors='coerce').replace(0, np.nan)
                            sales_num = pd.to_numeric(df_src['Sales'], errors='coerce')
                            df_src['ROAS'] = (sales_num / spend_nonzero).fillna(0)

                        # Apply quick filters and product group restrictions
                        before_quick = len(df_src)
                        if source in ('Search Terms','All') and txt_contains_search:
                            q = txt_contains_search.lower()
                            name_cols = [c for c in df_src.columns if c.lower() in (['search term','campaign name','ad group name','campaign'])]
                            if name_cols:
                                mask_q = False
                                for c in name_cols:
                                    mask_q = mask_q | df_src[c].astype(str).str.lower().str.contains(q, na=False)
                                df_src = df_src[mask_q]
                        if source in ('Targets','ASIN Targets','All') and txt_contains_target:
                            q = txt_contains_target.lower()
                            name_cols = [c for c in df_src.columns if c.lower() in (['target','campaign name','ad group name','campaign'])]
                            if name_cols:
                                mask_q = False
                                for c in name_cols:
                                    mask_q = mask_q | df_src[c].astype(str).str.lower().str.contains(q, na=False)
                                df_src = df_src[mask_q]
                        sel_groups = st.session_state.get('cc_pf_groups', [])
                        if do_pg_agg and sel_groups and 'Product Group' in df_src.columns:
                            df_src = df_src[df_src['Product Group'].isin(sel_groups)]
                        try:
                            st.session_state.debug_messages.append(f"[Advanced Finder] After text+PG filters: {len(df_src)} rows")
                            diag_msgs.append(f"[Diag] After text/PG filters: {len(df_src)} (was {before_quick})")
                        except Exception:
                            pass

                        # Normalize naming for restriction step
                        if 'Campaign' not in df_src.columns and 'Campaign Name' in df_src.columns:
                            df_src['Campaign'] = df_src['Campaign Name']
                        if 'Campaign' not in df_src.columns and 'Campaign Name (Informational Only)' in df_src.columns:
                            df_src['Campaign'] = df_src['Campaign Name (Informational Only)']
                        if 'Ad Group' not in df_src.columns and 'Ad Group Name' in df_src.columns:
                            df_src['Ad Group'] = df_src['Ad Group Name']

                        # Optional: restrict to ad groups that have enabled Product Ads for selected products
                        if restrict_adg:
                            # If no selected products, skip restriction (avoid wiping dataset)
                            prod_df_chk = st.session_state.get('cc_products', None)
                            if not isinstance(prod_df_chk, pd.DataFrame) or prod_df_chk.empty:
                                try:
                                    st.session_state.debug_messages.append("[Advanced Finder] restrict_adg=True but no selected products in cc_products; skipping ad group restriction")
                                except Exception:
                                    pass
                            else:
                                allow_camps, allow_pairs = _allowed_adgroups_for_selected_products()
                                if not allow_camps and not allow_pairs:
                                    try:
                                        st.session_state.debug_messages.append("[Advanced Finder] No allowed campaigns/ad groups found for selected products; skipping ad group restriction (leaving dataset unchanged)")
                                    except Exception:
                                        pass
                                else:
                                    ag_colname = 'Ad Group' if 'Ad Group' in df_src.columns else ('Ad Group Name' if 'Ad Group Name' in df_src.columns else None)
                                    if ag_colname:
                                        pair_keys = set([f"{c}||{a}" for (c,a) in allow_pairs])
                                        series_pairs = df_src['Campaign'].astype(str).str.strip() + '||' + df_src[ag_colname].astype(str).str.strip()
                                        mask_pairs = series_pairs.isin(pair_keys)
                                        # Fallback to campaign-only for rows where Ad Group is missing from allowed set
                                        mask_camps = df_src['Campaign'].astype(str).str.strip().isin(allow_camps)
                                        df_src = df_src[mask_pairs | mask_camps]
                                    elif 'Campaign' in df_src.columns:
                                        df_src = df_src[df_src['Campaign'].astype(str).str.strip().isin(allow_camps)]
                        try:
                            st.session_state.debug_messages.append(f"[Advanced Finder] After ad group restriction: {len(df_src)} rows")
                            diag_msgs.append(f"[Diag] After ad group restriction: {len(df_src)}")
                        except Exception:
                            pass

                        # Apply group conditions (AND)
                        if group.get('conds'):
                            m = pd.Series([True]*len(df_src))
                            for cond in group['conds']:
                                m = m & _mask_cond_generic(df_src, cond.get('column',''), cond.get('op','contains'), cond.get('value',''))
                            df_src = df_src[m]
                            try:
                                st.session_state.debug_messages.append(f"[Advanced Finder] After conditions: {len(df_src)} rows")
                                diag_msgs.append(f"[Diag] After conditions: {len(df_src)}")
                            except Exception:
                                pass

                        # Normalize to a single text column for extraction
                        df_loc = df_src.copy()
                        df_loc['_text'] = ''
                        if 'Search Term' in df_loc.columns:
                            df_loc['_text'] = df_loc['Search Term'].astype(str)
                        if 'Target' in df_loc.columns:
                            df_loc['_text'] = df_loc['_text'].mask(df_loc['_text'].isin(['', 'nan', 'None']), df_loc['Target'].astype(str))

                        # Drop SD Remarketing and Category targets (handle both raw expressions and cleaned labels)
                        df_loc = df_loc[
                            ~df_loc['_text'].str.match(r'(?i)^(views|purchases)=', na=False) &
                            ~df_loc['_text'].str.contains(r'(?i)remarketing', na=False) &
                            ~df_loc['_text'].str.contains(r'(?i)category target', na=False) &
                            ~df_loc['_text'].str.match(r'(?i)^category=', na=False)
                        ]

                        # Debug source and data before classification
                        try:
                            st.session_state.debug_messages.append(f"[Advanced Finder] Processing source '{source}' with {len(df_loc)} rows")
                        except Exception:
                            pass

                        # Classify kind: ASIN if contains ASIN anywhere; Category if starts with category=; else Keyword
                        is_asin = df_loc['_text'].str.contains(r'B0[A-Za-z0-9]{8}', case=False, na=False)
                        is_cat = df_loc['_text'].str.match(r'(?i)^category=', na=False)
                        df_loc['_kind'] = np.where(is_asin, 'ASIN', np.where(is_cat, 'Category', 'Keyword'))

                        # Debug kind distribution
                        try:
                            kind_counts = df_loc['_kind'].value_counts().to_dict()
                            st.session_state.debug_messages.append(f"[Advanced Finder] Kind distribution: {kind_counts}")
                        except Exception:
                            pass

                        # If source explicitly ASIN Targets, keep only ASIN
                        if source == 'ASIN Targets':
                            before_asin_filter = len(df_loc)
                            df_loc = df_loc[df_loc['_kind'] == 'ASIN']
                            try:
                                st.session_state.debug_messages.append(f"[Advanced Finder] ASIN filter kept {len(df_loc)}/{before_asin_filter} rows")
                            except Exception:
                                pass

                        # Normalize Sales for aggregation (use Ad Sales if available)
                        if 'Ad Sales' in df_loc.columns:
                            df_loc['Sales'] = df_loc['Ad Sales']

                        # Aggregation across selected Product Groups (optional)
                        if do_pg_agg:
                            agg_dict = {}
                            for col in ['Sales','Spend','Orders']:
                                if col in df_loc.columns:
                                    agg_dict[col] = 'sum'
                            try:
                                # Debug aggregation input
                                st.session_state.debug_messages.append(f"[Advanced Finder] Pre-agg: {len(df_loc)} rows, agg_dict: {agg_dict}")
                                if '_text' in df_loc.columns:
                                    st.session_state.debug_messages.append(f"[Advanced Finder] Sample _text values: {df_loc['_text'].head(3).tolist()}")
                                else:
                                    st.session_state.debug_messages.append(f"[Advanced Finder] Missing _text column, available: {list(df_loc.columns)}")
                                if '_kind' in df_loc.columns:
                                    st.session_state.debug_messages.append(f"[Advanced Finder] Sample _kind values: {df_loc['_kind'].head(3).tolist()}")
                                else:
                                    st.session_state.debug_messages.append(f"[Advanced Finder] Missing _kind column")
                            except Exception:
                                pass
                            
                            if agg_dict and not df_loc.empty:
                                try:
                                    # Debug: Check available columns before aggregation
                                    st.session_state.debug_messages.append(f"[Advanced Finder] Pre-agg columns: {list(df_loc.columns)}")
                                    # Check for Ad Group column with case-insensitive search
                                    if restrict_adg:
                                        if 'Campaign' in df_loc.columns:
                                            groupby_cols.append('Campaign')
                                            st.session_state.debug_messages.append("[Advanced Finder] Added Campaign to groupby")
                                        
                                        # Look for Ad Group column with case-insensitive search
                                        adg_col = None
                                        for col in df_loc.columns:
                                            if 'ad group' in str(col).lower():
                                                adg_col = col
                                                break
                                        
                                        if adg_col:
                                            groupby_cols.append(adg_col)
                                            st.session_state.debug_messages.append(f"[Advanced Finder] Added {adg_col} to groupby")
                                        else:
                                            st.session_state.debug_messages.append("[Advanced Finder] No Ad Group column found")
                                    
                                    grouped = df_loc.groupby(groupby_cols, as_index=False).agg(agg_dict)
                                    st.session_state.debug_messages.append(f"[Advanced Finder] Post-groupby: {len(grouped)} rows, groupby_cols: {groupby_cols}")
                                    
                                    if 'Sales' in grouped.columns and 'Spend' in grouped.columns:
                                        spend_nz = grouped['Spend'].replace(0, np.nan)
                                        grouped['ROAS'] = (grouped['Sales'] / spend_nz).fillna(0)
                                    df_loc = grouped
                                except Exception as e:
                                    st.session_state.debug_messages.append(f"[Advanced Finder] Aggregation error: {str(e)}")
                            elif not agg_dict:
                                st.session_state.debug_messages.append("[Advanced Finder] No aggregatable columns found")
                            elif df_loc.empty:
                                st.session_state.debug_messages.append("[Advanced Finder] df_loc is empty before aggregation")
                        try:
                            st.session_state.debug_messages.append(f"[Advanced Finder] After aggregation: {len(df_loc)} rows")
                        except Exception:
                            pass

                        # Apply Min ROAS/Orders after aggregation
                        if 'ROAS' in df_loc.columns and min_roas:
                            before = len(df_loc)
                            df_loc = df_loc[pd.to_numeric(df_loc['ROAS'], errors='coerce').fillna(0) >= float(min_roas)]
                            try:
                                st.session_state.debug_messages.append(f"[Advanced Finder] Post-agg Min ROAS kept {len(df_loc)}/{before} rows")
                            except Exception:
                                pass
                        if 'Orders' in df_loc.columns and min_orders:
                            before = len(df_loc)
                            df_loc = df_loc[pd.to_numeric(df_loc['Orders'], errors='coerce').fillna(0) >= float(min_orders)]
                            try:
                                st.session_state.debug_messages.append(f"[Advanced Finder] Post-agg Min Orders kept {len(df_loc)}/{before} rows")
                            except Exception:
                                pass

                        # Append candidates from this group
                        for _, rr in df_loc.head(2000).iterrows():
                            t = str(rr['_text']).strip()
                            if not t:
                                continue
                            kind = rr.get('_kind', 'Keyword')
                            spend_v = float(rr['Spend']) if 'Spend' in rr and pd.notna(rr['Spend']) else 0.0
                            sales_v = float(rr['Sales']) if 'Sales' in rr and pd.notna(rr['Sales']) else 0.0
                            roas_v = float(rr['ROAS']) if 'ROAS' in rr and pd.notna(rr['ROAS']) else (sales_v / spend_v if spend_v else 0.0)
                            
                            # Determine discovery label (Found Via) per candidate based on aggregation/restriction options
                            found_via = ''
                            try:
                                if restrict_adg:
                                    # For ad group restriction, determine which specific ASINs this candidate relates to
                                    # by checking the campaign/ad group context from the original row
                                    campaign = str(rr.get('Campaign', '')).strip()
                                    
                                    # Look for Ad Group column with case-insensitive search, prioritizing Name over ID
                                    ad_group = ''
                                    selected_col = None
                                    # First try to find Ad Group Name columns
                                    for col in rr.index:
                                        col_lower = str(col).lower()
                                        if 'ad group' in col_lower and ('name' in col_lower or col_lower == 'ad group'):
                                            ad_group = str(rr.get(col, '')).strip()
                                            selected_col = col
                                            break
                                    # If no name column found, fall back to any ad group column (including ID)
                                    if not ad_group:
                                        for col in rr.index:
                                            if 'ad group' in str(col).lower():
                                                ad_group = str(rr.get(col, '')).strip()
                                                selected_col = col
                                                break
                                    
                                    # Debug: Check if campaign/ad group info is available after aggregation
                                    st.session_state.debug_messages.append(f"[Advanced Finder] Candidate '{t[:20]}...' campaign: '{campaign}', ad_group: '{ad_group}' (from column: '{selected_col}')")
                                    
                                    # Find matching ASINs from selected products
                                    sel_df = st.session_state.get('cc_products', pd.DataFrame())
                                    matching_asins = []
                                    
                                    # Debug: Show selected products structure once
                                    debug_key = 'cc_products_debug_shown'
                                    if not st.session_state.get(debug_key, False):
                                        st.session_state[debug_key] = True
                                        st.session_state.debug_messages.append(f"[Advanced Finder] Selected products columns: {list(sel_df.columns) if isinstance(sel_df, pd.DataFrame) else 'Not a DataFrame'}")
                                        # Check for campaign/ad group columns
                                        has_campaign_data = isinstance(sel_df, pd.DataFrame) and any('campaign' in str(col).lower() for col in sel_df.columns)
                                        has_adgroup_data = isinstance(sel_df, pd.DataFrame) and any('ad group' in str(col).lower() for col in sel_df.columns)
                                        st.session_state.debug_messages.append(f"[Advanced Finder] Selected products has campaign data: {has_campaign_data}, ad group data: {has_adgroup_data}")
                                    
                                    if isinstance(sel_df, pd.DataFrame) and not sel_df.empty:
                                        # Check if selected products have campaign/ad group data for matching
                                        has_campaign_col = 'Campaign' in sel_df.columns
                                        adg_col_sel = None
                                        for col in sel_df.columns:
                                            if 'ad group' in str(col).lower():
                                                adg_col_sel = col
                                                break
                                        
                                        if has_campaign_col and adg_col_sel:
                                            # Match by campaign and ad group (specific matching)
                                            mask_campaign = sel_df['Campaign'].astype(str).str.strip() == campaign
                                            mask_adgroup = sel_df[adg_col_sel].astype(str).str.strip() == ad_group
                                            mask_combined = mask_campaign & mask_adgroup
                                            
                                            if mask_combined.any():
                                                matching_rows = sel_df[mask_combined]
                                                for _, row in matching_rows.iterrows():
                                                    if 'ASIN' in row and pd.notna(row['ASIN']):
                                                        matching_asins.append(str(row['ASIN']).strip())
                                                    if 'SKU' in row and pd.notna(row['SKU']):
                                                        matching_asins.append(str(row['SKU']).strip())
                                        else:
                                            # Selected products don't have campaign/ad group data
                                            # Use all selected ASINs/SKUs for matching (fallback approach)
                                            for _, row in sel_df.iterrows():
                                                if 'ASIN' in row and pd.notna(row['ASIN']):
                                                    matching_asins.append(str(row['ASIN']).strip())
                                                if 'SKU' in row and pd.notna(row['SKU']):
                                                    matching_asins.append(str(row['SKU']).strip())
                                    
                                    # Remove duplicates and empty values, create Found Via label
                                    matching_asins = [asin for asin in list(set(matching_asins)) if asin.strip()]
                                    
                                    # Debug: Show what ASINs matched
                                    if len(st.session_state.debug_messages) < 60:
                                        if matching_asins:
                                            st.session_state.debug_messages.append(f"[Advanced Finder] Found {len(matching_asins)} matching ASINs: {matching_asins[:5]}{'...' if len(matching_asins) > 5 else ''}")
                                        else:
                                            st.session_state.debug_messages.append(f"[Advanced Finder] No matching ASINs found for campaign: '{campaign}', ad_group: '{ad_group}'")
                                    
                                    if matching_asins:
                                        lbl = ", ".join(matching_asins[:3]) + (f" (+{len(matching_asins)-3} more)" if len(matching_asins) > 3 else '')
                                        found_via = f"ASIN Match - {lbl}"
                                        try:
                                            st.session_state.debug_messages.append(f"[Advanced Finder] Found Via for '{t[:20]}...': {found_via}")
                                        except Exception:
                                            pass
                                    else:
                                        found_via = "ASIN Match"
                                        try:
                                            st.session_state.debug_messages.append(f"[Advanced Finder] No matching ASINs for '{t[:20]}...', using default")
                                        except Exception:
                                            pass
                                elif do_pg_agg:
                                    sel_groups2 = st.session_state.get('cc_pf_groups', []) or []
                                    vals2 = [str(x).strip() for x in sel_groups2 if str(x).strip()]
                                    if vals2:
                                        lbl2 = ", ".join(vals2[:3]) + (f" (+{len(vals2)-3} more)" if len(vals2) > 3 else '')
                                        found_via = f"Product Group Match - {lbl2}"
                                    else:
                                        found_via = "Product Group Match"
                            except Exception:
                                pass
                            
                            row = {'Text': t, 'Kind': kind, 'Spend': spend_v, 'Sales': sales_v, 'ROAS': roas_v}
                            if found_via:
                                row['Found Via'] = found_via
                            candidates.append(row)

                    # Add manual targets to candidates (outside per-group loop)
                    manual_targets = st.session_state.get('cc_manual_targets', '') or ''
                    if manual_targets.strip():
                        for line in manual_targets.strip().splitlines():
                            t = line.strip()
                            if not t:
                                continue
                            # Skip SD remarketing
                            tl = t.lower()
                            if (tl.startswith('views=') or tl.startswith('purchases=') or
                                'remarketing' in tl or 'category target' in tl or
                                tl.startswith('category=')):
                                continue
                            # Classify
                            if re.search(r'B0[A-Za-z0-9]{8}', t, flags=re.I):
                                kind = 'ASIN'
                            elif tl.startswith('category='):
                                kind = 'Category'
                            else:
                                kind = 'Keyword'
                            # Determine Found Via for manual adds using current aggregation selections
                            found_via_manual = ''
                            if st.session_state.get('cc_agg_pg', True):
                                sel_groups_m = st.session_state.get('cc_pf_groups', []) or []
                                vals_m = [str(x).strip() for x in sel_groups_m if str(x).strip()]
                                if vals_m:
                                    lbl_m = ", ".join(vals_m[:3]) + (f" (+{len(vals_m)-3} more)" if len(vals_m) > 3 else '')
                                    found_via_manual = f"Product Group Match - {lbl_m}"
                                else:
                                    found_via_manual = "Product Group Match"
                            elif st.session_state.get('cc_agg_restrict_adg', False):
                                sel_df_m = st.session_state.get('cc_products', pd.DataFrame())
                                vals_m = []
                                if isinstance(sel_df_m, pd.DataFrame) and not sel_df_m.empty:
                                    id_col_candidates_m = [c for c in ['ASIN','SKU'] if c in sel_df_m.columns]
                                    if id_col_candidates_m:
                                        id_col_use_m = 'ASIN' if 'ASIN' in id_col_candidates_m else id_col_candidates_m[0]
                                        vals_m = [str(x).strip() for x in sel_df_m[id_col_use_m].dropna().astype(str).tolist() if str(x).strip()]
                                if vals_m:
                                    lbl_m = ", ".join(vals_m[:3]) + (f" (+{len(vals_m)-3} more)" if len(vals_m) > 3 else '')
                                    found_via_manual = f"ASIN Match - {lbl_m}"
                                else:
                                    found_via_manual = "ASIN Match"
                            row_m = {'Text': t, 'Kind': kind}
                            if found_via_manual:
                                row_m['Found Via'] = found_via_manual
                            candidates.append(row_m)

                    # Store candidates and diagnostics for rendering outside the expander
                    st.session_state.cc_candidates = candidates
                    st.session_state.cc_cand_diag = diag_msgs

                except Exception as e:
                    try:
                        st.session_state.debug_messages.append(f"[Targets Advanced Finder] Candidate build error: {e}")
                    except Exception:
                        pass

            # Candidate Results rendered below the Advanced Finder expander
            st.markdown("**Candidate Results**")
            if not has_bulk_sources:
                # Clear any stale candidates when finder is disabled
                st.session_state.cc_candidates = []
            candidates = st.session_state.get('cc_candidates', []) or []
            cand_df = pd.DataFrame(candidates).drop_duplicates() if candidates else pd.DataFrame(columns=['Text','Kind','Spend','Sales','ROAS'])
            # Final safeguard: filter out Remarketing and Category Target items (handles both raw and cleaned forms)
            if not cand_df.empty and 'Text' in cand_df.columns:
                text_series = cand_df['Text'].astype(str)
                mask_exclude = (
                    text_series.str.match(r'(?i)^(views|purchases)=', na=False) |
                    text_series.str.contains(r'(?i)remarketing', na=False) |
                    text_series.str.contains(r'(?i)category target', na=False) |
                    text_series.str.match(r'(?i)^category=', na=False)
                )
                cand_df = cand_df[~mask_exclude]
            # Add Classification column (Branded/Non-Branded)
            if not cand_df.empty:
                client_cfg = st.session_state.get('client_config', {}) or {}
                branded_terms = [str(term).strip().lower() for term in client_cfg.get('branded_keywords', []) if str(term).strip()]
                branded_asins = {str(a).strip().upper() for a in (client_cfg.get('branded_asins_data', {}) or {}).keys()}
                def _classify_row(r):
                    text = str(r.get('Text','')).strip()
                    kind = r.get('Kind','Keyword')
                    if kind == 'ASIN':
                        m = re.search(r'B0[A-Za-z0-9]{8}', text, flags=re.I)
                        if m and m.group(0).upper() in branded_asins:
                            return 'Branded'
                        return 'Non-Branded'
                    if kind == 'Keyword':
                        low = text.lower()
                        if any(t and t in low for t in branded_terms):
                            return 'Branded'
                        return 'Non-Branded'
                    if kind == 'Category':
                        return 'Non-Branded'
                    return 'Non-Branded'
                cand_df['Classification'] = cand_df.apply(_classify_row, axis=1)
                # Present Ad Sales alias
                cand_df['Ad Sales'] = cand_df.get('Sales', 0.0)
                # Ensure Found Via column exists for display
                if 'Found Via' not in cand_df.columns:
                    cand_df['Found Via'] = ''
                # Normalize candidate keys for joining to scope maps
                def _norm_kw(txt: str) -> str:
                    t = re.sub(r"[+]+", " ", str(txt))
                    t = re.sub(r"\s+", " ", t)
                    return t.strip().lower()
                def _to_key_cand(r):
                    txt = str(r.get('Text','')).strip()
                    kind = r.get('Kind','Keyword')
                    if kind == 'ASIN':
                        m = re.search(r'B0[A-Za-z0-9]{8}', txt, flags=re.I)
                        return m.group(0).upper() if m else txt.lower()
                    return _norm_kw(txt)
                cand_df['_key'] = cand_df.apply(_to_key_cand, axis=1)
                # Build per-key product group and tag maps from underlying data for better Found Via labels
                try:
                    do_pg_agg_fv = st.session_state.get('cc_agg_pg', True)
                    restrict_adg_fv = st.session_state.get('cc_agg_restrict_adg', False)
                    st_df_fv = get_search_term_data(st.session_state.bulk_data, st.session_state.client_config)
                    tgt_b_fv, tgt_nb_fv = get_targeting_performance_data(st.session_state.bulk_data, st.session_state.client_config)
                    parts_fv = []
                    if isinstance(st_df_fv, pd.DataFrame) and not st_df_fv.empty:
                        parts_fv.append(st_df_fv.copy())
                    if isinstance(tgt_b_fv, pd.DataFrame) and not tgt_b_fv.empty:
                        parts_fv.append(tgt_b_fv)
                    if isinstance(tgt_nb_fv, pd.DataFrame) and not tgt_nb_fv.empty:
                        parts_fv.append(tgt_nb_fv)
                    scope_pg_map = {}
                    if parts_fv:
                        base_fv = pd.concat(parts_fv, ignore_index=True, sort=False)
                        # Normalize naming for restriction step
                        if 'Campaign' not in base_fv.columns and 'Campaign Name' in base_fv.columns:
                            base_fv['Campaign'] = base_fv['Campaign Name']
                        if 'Campaign' not in base_fv.columns and 'Campaign Name (Informational Only)' in base_fv.columns:
                            base_fv['Campaign'] = base_fv['Campaign Name (Informational Only)']
                        if 'Ad Group' not in base_fv.columns and 'Ad Group Name' in base_fv.columns:
                            base_fv['Ad Group'] = base_fv['Ad Group Name']
                        # Optional: restrict to selected ad groups
                        if restrict_adg_fv:
                            allow_camps, allow_pairs = _allowed_adgroups_for_selected_products()
                            if not allow_camps and not allow_pairs:
                                try:
                                    st.session_state.debug_messages.append("[Advanced Finder][Found Via] No allowed campaigns/ad groups for selected products; skipping ad group restriction for Found Via mapping")
                                except Exception:
                                    pass
                            else:
                                ag_colname2 = 'Ad Group' if 'Ad Group' in base_fv.columns else ('Ad Group Name' if 'Ad Group Name' in base_fv.columns else None)
                                if ag_colname2:
                                    pair_keys = set([f"{c}||{a}" for (c,a) in allow_pairs])
                                    series_pairs = base_fv['Campaign'].astype(str).str.strip() + '||' + base_fv[ag_colname2].astype(str).str.strip()
                                    mask_pairs = series_pairs.isin(pair_keys)
                                    mask_camps = base_fv['Campaign'].astype(str).str.strip().isin(allow_camps)
                                    base_fv = base_fv[mask_pairs | mask_camps]
                                elif 'Campaign' in base_fv.columns:
                                    base_fv = base_fv[base_fv['Campaign'].astype(str).str.strip().isin(allow_camps)]
                        # Normalize
                        base_fv = base_fv.copy()
                        base_fv['_text'] = ''
                        if 'Search Term' in base_fv.columns:
                            base_fv['_text'] = base_fv['Search Term'].astype(str)
                        if 'Target' in base_fv.columns:
                            base_fv['_text'] = base_fv['_text'].mask(base_fv['_text'].isin(['', 'nan', 'None']), base_fv['Target'].astype(str))
                        # Exclude remarketing and category targets from Found Via mapping as well
                        base_fv = base_fv[
                            ~base_fv['_text'].str.match(r'(?i)^(views|purchases)=', na=False) &
                            ~base_fv['_text'].str.contains(r'(?i)remarketing', na=False) &
                            ~base_fv['_text'].str.contains(r'(?i)category target', na=False) &
                            ~base_fv['_text'].str.match(r'(?i)^category=', na=False)
                        ]
                        is_asin_fv = base_fv['_text'].str.contains(r'B0[A-Za-z0-9]{8}', case=False, na=False)
                        is_cat_fv = base_fv['_text'].str.match(r'(?i)^category=', na=False)
                        base_fv['_kind'] = np.where(is_asin_fv, 'ASIN', np.where(is_cat_fv, 'Category', 'Keyword'))
                        base_fv['_key'] = base_fv.apply(lambda rr: _to_key_cand({'Text': rr['_text'], 'Kind': rr['_kind']}), axis=1)
                        # Build scope map from actual rows
                        def _label_list(vals: list) -> str:
                            vals = [str(v).strip() for v in vals if str(v).strip() and str(v).strip().lower() not in ('none','nan')]
                            if not vals:
                                return ''
                            vals = sorted(list(set(vals)))
                            return ", ".join(vals[:3]) + (f" (+{len(vals)-3} more)" if len(vals) > 3 else '')
                        if 'Product Group' in base_fv.columns:
                            tmp_pg_fv = base_fv[['Product Group','_key']].dropna()
                            if not tmp_pg_fv.empty:
                                pg_lists_fv = tmp_pg_fv.groupby('_key')['Product Group'].apply(list)
                                scope_pg_map = {k: _label_list(v) for k, v in pg_lists_fv.to_dict().items()}
                    if do_pg_agg_fv and scope_pg_map:
                        per_pg = cand_df['_key'].map(scope_pg_map)
                        mask_has_pg = per_pg.notna() & (per_pg.astype(str).str.strip() != '')
                        cand_df.loc[mask_has_pg, 'Found Via'] = 'Product Group Match - ' + per_pg[mask_has_pg].astype(str)
                except Exception:
                    pass
            if not cand_df.empty:
                # Round ROAS to 2 decimals for display
                if 'ROAS' in cand_df.columns:
                    cand_df['ROAS'] = pd.to_numeric(cand_df['ROAS'], errors='coerce').fillna(0).round(2)
                # Backfill Found Via if empty using current aggregation settings
                do_pg_agg = st.session_state.get('cc_agg_pg', True)
                restrict_adg = st.session_state.get('cc_agg_restrict_adg', False)
                if 'Found Via' not in cand_df.columns:
                    cand_df['Found Via'] = ''
                cand_df['Found Via'] = cand_df['Found Via'].astype(str).fillna('')
                if do_pg_agg:
                    sel_groups_fx = st.session_state.get('cc_pf_groups', []) or []
                    vals_fx2 = [str(x).strip() for x in sel_groups_fx if str(x).strip()]
                    if vals_fx2:
                        lbl_fx2 = ", ".join(vals_fx2[:3]) + (f" (+{len(vals_fx2)-3} more)" if len(vals_fx2) > 3 else '')
                        default_found_via = f"Product Group Match - {lbl_fx2}"
                    else:
                        default_found_via = "Product Group Match"
                elif restrict_adg:
                    sel_df_fx = st.session_state.get('cc_products', pd.DataFrame())
                    vals_fx = []
                    if isinstance(sel_df_fx, pd.DataFrame) and not sel_df_fx.empty:
                        id_col_candidates_fx = [c for c in ['ASIN','SKU'] if c in sel_df_fx.columns]
                        if id_col_candidates_fx:
                            id_col_use_fx = 'ASIN' if 'ASIN' in id_col_candidates_fx else id_col_candidates_fx[0]
                            vals_fx = [str(x).strip() for x in sel_df_fx[id_col_use_fx].dropna().astype(str).tolist() if str(x).strip()]
                    if vals_fx:
                        lbl_fx = ", ".join(vals_fx[:3]) + (f" (+{len(vals_fx)-3} more)" if len(vals_fx) > 3 else '')
                        default_found_via = f"ASIN Match - {lbl_fx}"
                    else:
                        default_found_via = "ASIN Match"
                else:
                    default_found_via = ''
                # Debug: Log backfill behavior
                try:
                    before_backfill = cand_df['Found Via'].value_counts().to_dict()
                    st.session_state.debug_messages.append(f"[Advanced Finder] Before backfill Found Via counts: {before_backfill}")
                except Exception:
                    pass
                
                # Only backfill truly empty Found Via values, not ones that were specifically set
                mask_empty_fv = cand_df['Found Via'].str.strip().eq('') | cand_df['Found Via'].isna()
                cand_df.loc[mask_empty_fv, 'Found Via'] = default_found_via
                
                # Debug: Log after backfill
                try:
                    after_backfill = cand_df['Found Via'].value_counts().to_dict()
                    st.session_state.debug_messages.append(f"[Advanced Finder] After backfill Found Via counts: {after_backfill}")
                    st.session_state.debug_messages.append(f"[Advanced Finder] Default Found Via used: {default_found_via}")
                except Exception:
                    pass
                # Order columns for display (omit raw 'Sales')
                disp_cols = [c for c in ['Text','Kind','Found Via','Classification','Spend','Ad Sales','ROAS'] if c in cand_df.columns]
                # Column formatting
                col_cfg = {
                    'Spend': st.column_config.NumberColumn('Spend', format='$%.2f'),
                    'Ad Sales': st.column_config.NumberColumn('Ad Sales', format='$%.2f'),
                    'ROAS': st.column_config.NumberColumn('ROAS', format='%.2f')
                }
                st.dataframe(cand_df[disp_cols], use_container_width=True, height=220, column_config=col_cfg)
                if st.button("Add Candidates", key="cc_add_cands"):
                    # Classify branding using client settings
                    client_cfg = st.session_state.get('client_config', {}) or {}
                    branded_terms = [str(term).strip().lower() for term in client_cfg.get('branded_keywords', []) if str(term).strip()]
                    branded_asins = {str(a).strip().upper() for a in client_cfg.get('branded_asins_data', {}).keys()}
                    # Only add candidates that have nonzero Spend or Ad Sales
                    cand_df_add = cand_df.copy()
                    if 'Spend' in cand_df_add.columns or 'Ad Sales' in cand_df_add.columns:
                        spend_series = pd.to_numeric(cand_df_add.get('Spend', 0), errors='coerce').fillna(0)
                        sales_series = pd.to_numeric(cand_df_add.get('Ad Sales', 0), errors='coerce').fillna(0)
                        cand_df_add = cand_df_add[(spend_series > 0) | (sales_series > 0)]
                    # Build normalized key for deduplication at add-time
                    def _norm_kw(txt: str) -> str:
                        t = re.sub(r"[+]+", " ", str(txt))
                        t = re.sub(r"\s+", " ", t)
                        return t.strip().lower()
                    def _make_key(text: str, kind: str) -> str:
                        if kind == 'ASIN':
                            m = re.search(r'B0[A-Za-z0-9]{8}', str(text), flags=re.I)
                            return m.group(0).upper() if m else str(text).strip().lower()
                        return _norm_kw(str(text))

                    # Ensure Ad Sales column exists for metric preference
                    if 'Ad Sales' not in cand_df_add.columns and 'Sales' in cand_df_add.columns:
                        cand_df_add['Ad Sales'] = cand_df_add['Sales']
                    cand_df_add['_key'] = cand_df_add.apply(lambda rr: _make_key(rr['Text'], rr['Kind']), axis=1)
                    # Prefer variants with higher metrics when duplicates exist
                    cand_df_add['__metric_pref'] = pd.to_numeric(cand_df_add.get('Spend', 0), errors='coerce').fillna(0) \
                                                  + pd.to_numeric(cand_df_add.get('Ad Sales', 0), errors='coerce').fillna(0)
                    cand_df_add = cand_df_add.sort_values(['__metric_pref'], ascending=False)
                    cand_df_add = cand_df_add.drop_duplicates(subset=['_key','Kind'], keep='first')
                    cand_df_add = cand_df_add.drop(columns=['__metric_pref'], errors='ignore')

                    # Persist scope from current Product Finder filters
                    scope_groups_list = st.session_state.get('cc_pf_groups', []) or []
                    scope_tags_list = st.session_state.get('cc_pf_tags', []) or []
                
                    # Process candidates for adding to Selected Targets
                    scope_groups_list = st.session_state.get('cc_pf_groups', []) or []
                    scope_tags_list = st.session_state.get('cc_pf_tags', []) or []
                    do_pg_agg_add = st.session_state.get('cc_agg_pg', True)
                    scope_groups = ",".join([str(x).strip() for x in scope_groups_list if str(x).strip()]) or ("All" if do_pg_agg_add else "")
                    scope_tags = ",".join([str(x).strip() for x in scope_tags_list if str(x).strip()]) or "All"
                    new_rows = []
                    for _, r in cand_df_add.iterrows():
                        text = str(r['Text']).strip()
                        kind = r['Kind']
                        branding = 'Non'
                        if kind == 'ASIN':
                            m = re.search(r'B0[A-Za-z0-9]{8}', text, flags=re.I)
                            if m and m.group(0).upper() in branded_asins:
                                branding = 'Brand'
                        elif kind == 'Keyword':
                            low = text.lower()
                            if any(t and t in low for t in branded_terms):
                                branding = 'Brand'
                        elif kind == 'Category':
                            branding = 'Non'
                        # Determine Found Via to persist: prefer row value, else compute default based on toggles
                        fv_val = str(r.get('Found Via', '') or '').strip()
                        if not fv_val:
                            if st.session_state.get('cc_agg_pg', True):
                                sel_groups_m2 = st.session_state.get('cc_pf_groups', []) or []
                                vals_m2 = [str(x).strip() for x in sel_groups_m2 if str(x).strip()]
                                if vals_m2:
                                    lbl_m2 = ", ".join(vals_m2[:3]) + (f" (+{len(vals_m2)-3} more)" if len(vals_m2) > 3 else '')
                                    fv_val = f"Product Group Match - {lbl_m2}"
                                else:
                                    fv_val = "Product Group Match"
                            elif st.session_state.get('cc_agg_restrict_adg', False):
                                sel_df_m2 = st.session_state.get('cc_products', pd.DataFrame())
                                vals_m2 = []
                                if isinstance(sel_df_m2, pd.DataFrame) and not sel_df_m2.empty:
                                    id_col_candidates_m2 = [c for c in ['ASIN','SKU'] if c in sel_df_m2.columns]
                                    if id_col_candidates_m2:
                                        id_col_use_m2 = 'ASIN' if 'ASIN' in id_col_candidates_m2 else id_col_candidates_m2[0]
                                        vals_m2 = [str(x).strip() for x in sel_df_m2[id_col_use_m2].dropna().astype(str).tolist() if str(x).strip()]
                                if vals_m2:
                                    lbl_m2 = ", ".join(vals_m2[:3]) + (f" (+{len(vals_m2)-3} more)" if len(vals_m2) > 3 else '')
                                    fv_val = f"ASIN Match - {lbl_m2}"
                                else:
                                    fv_val = "ASIN Match"
                        new_rows.append({
                            'Text': text,
                            'Kind': kind,
                            'Branding': branding,
                            'Source': 'Audit/Manual',
                            'Bid': None,
                            # Carry metrics from Candidate Results so they are consistent with filters at add time
                            'Spend': float(r.get('Spend', 0) or 0),
                            'Ad Sales': float(r.get('Ad Sales', r.get('Sales', 0)) or 0),
                            'ROAS': float(r.get('ROAS', 0) or 0),
                            'Found Via': fv_val,
                            'Scope Product Groups': scope_groups,
                            'Scope Tags': scope_tags,
                        })
                    add_df = pd.DataFrame(new_rows)
                    if st.session_state.cc_targets.empty:
                        # Compute keys to count unique additions
                        add_df['_key'] = add_df.apply(lambda rr: _make_key(rr['Text'], rr['Kind']), axis=1)
                        st.session_state.cc_targets = add_df.drop_duplicates(subset=['_key','Kind'], keep='first').drop(columns=['_key'], errors='ignore')
                        added_ct = len(st.session_state.cc_targets)
                    else:
                        # Merge with existing, dedupe by normalized key preserving latest (new rows)
                        existing = st.session_state.cc_targets.copy()
                        existing['_key'] = existing.apply(lambda rr: _make_key(rr['Text'], rr['Kind']), axis=1)
                        add_df['_key'] = add_df.apply(lambda rr: _make_key(rr['Text'], rr['Kind']), axis=1)
                        before_keys = set(existing['_key'] + '|' + existing['Kind'].astype(str))
                        combined = pd.concat([existing, add_df], ignore_index=True)
                        combined = combined.drop_duplicates(subset=['_key','Kind'], keep='last')
                        after_keys = set(combined['_key'] + '|' + combined['Kind'].astype(str))
                        added_ct = max(0, len(after_keys - before_keys))
                        st.session_state.cc_targets = combined.drop(columns=['_key'], errors='ignore')
                    st.success(f"Added {int(added_ct)} targets")
            else:
                # Display diagnostics if no candidates found
                if cand_df.empty:
                    st.info("No candidates found matching the current filters and settings.")
                    # Show diagnostics from session state if available
                    diag_msgs = st.session_state.get('cc_cand_diag', [])
                    if diag_msgs:
                        with st.expander("ðŸ” Filtering Diagnostics", expanded=False):
                            for msg in diag_msgs:
                                st.text(msg)
                else:
                    st.info("No candidates from the selected source and filters.")
            
            # Debug messages window for troubleshooting
            debug_msgs = st.session_state.get('debug_messages', [])
            if debug_msgs:
                with st.expander("ðŸ› Debug Messages", expanded=False):
                    debug_text = "\n".join(debug_msgs[-50:])  # Show last 50 messages
                    st.text_area("Debug Output (copy/paste friendly)", debug_text, height=300)
                    if st.button("Clear Debug Messages"):
                        st.session_state.debug_messages = []
                        st.rerun()

            # Manual Targets Add - allow user to paste targets directly
            with st.expander("Add Targets Manually", expanded=False):
                # Allow tagging the manual targets with Product Group scope from Selected Products
                try:
                    sel_df_pg = st.session_state.get('cc_products', pd.DataFrame())
                    pg_options = []
                    if isinstance(sel_df_pg, pd.DataFrame) and not sel_df_pg.empty and 'Product Group' in sel_df_pg.columns:
                        vals = sel_df_pg['Product Group'].astype(str).tolist()
                        pg_options = sorted({v.strip()[:200] for v in vals if v and str(v).strip() and str(v).strip().lower() not in ['nan','none']})
                    selected_scope_pgs = st.multiselect(
                        "Tag these targets to Product Group(s) (from Selected Products)",
                        options=pg_options,
                        default=st.session_state.get('cc_manual_targets_scope_pgs', []),
                        key='cc_manual_targets_scope_pgs'
                    )
                except Exception:
                    selected_scope_pgs = st.session_state.get('cc_manual_targets_scope_pgs', [])
                manual_tgt_txt = st.text_area("Enter targets (one per line)", key="cc_manual_targets_txt")
                # Optional default bid for immediate prefill (can still be edited later)
                default_bid_prefill = st.number_input(
                    "Optional default bid for these targets",
                    min_value=0.0,
                    value=float(st.session_state.get('cc_fallback_bid', 0.5)),
                    step=0.05,
                    key="cc_manual_targets_default_bid"
                )
                if st.button("Add Manual Targets", key="cc_add_manual_targets_btn"):
                    lines = [str(x).strip() for x in (manual_tgt_txt or '').splitlines()]
                    lines = [x for x in lines if x]
                    if not lines:
                        st.info("No targets entered.")
                    else:
                        # Gather client settings
                        client_cfg = st.session_state.get('client_config', {}) or {}
                        branded_asins_map = client_cfg.get('branded_asins_data', {}) or {}
                        branded_terms_list = client_cfg.get('branded_terms', []) or client_cfg.get('branded_keywords', []) or []
                        branded_terms_list = [str(t).strip().lower() for t in branded_terms_list if str(t).strip()]

                        new_rows = []
                        for raw in lines:
                            text = raw.strip()
                            upper = text.upper()
                            # Determine kind (ASIN vs Keyword)
                            if 'B0' in upper:
                                kind = 'ASIN'
                                # Try to extract canonical ASIN token for branding check
                                asin_token = None
                                for tok in upper.replace('"',' ').replace("'", ' ').split():
                                    if tok.startswith('B0') and len(tok) >= 10:
                                        asin_token = tok[:10]
                                        break
                                branding = 'Brand' if (asin_token and asin_token in set(map(str.upper, branded_asins_map.keys()))) else 'Non'
                            else:
                                kind = 'Keyword'
                                low = text.lower()
                                branding = 'Brand' if any(t and t in low for t in branded_terms_list) else 'Non'

                            new_rows.append({
                                'Text': text,
                                'Kind': kind,
                                'Branding': branding,
                                'Source': 'Manual',
                                'Bid': default_bid_prefill if default_bid_prefill > 0 else None,
                                'Spend': 0.0,
                                'Ad Sales': 0.0,
                                'ROAS': 0.0,
                                'Found Via': 'Manual Entry',
                                # Attach scope based on multiselect; empty list becomes '' and will be defaulted later
                                'Scope Product Groups': ",".join([str(x).strip() for x in (st.session_state.get('cc_manual_targets_scope_pgs', selected_scope_pgs) or []) if str(x).strip()]),
                                'Scope Tags': '',
                            })

                        # Deduplicate with existing using normalized key
                        def _make_key(text: str, kind: str) -> str:
                            if kind == 'ASIN':
                                m = re.search(r'B0[A-Za-z0-9]{8}', str(text), flags=re.I) if 're' in globals() else None
                                return m.group(0).upper() if m else str(text).strip().lower()
                            # Normalize keyword by removing + and collapsing whitespace
                            t = re.sub(r"[+]+", " ", str(text)) if 're' in globals() else str(text)
                            t = re.sub(r"\s+", " ", t) if 're' in globals() else t
                            return t.strip().lower()

                        add_df = pd.DataFrame(new_rows)
                        add_df['_key'] = add_df.apply(lambda rr: _make_key(rr['Text'], rr['Kind']), axis=1)
                        if st.session_state.cc_targets.empty:
                            st.session_state.cc_targets = add_df.drop_duplicates(subset=['_key','Kind'], keep='first').drop(columns=['_key'], errors='ignore')
                            added_ct = len(st.session_state.cc_targets)
                        else:
                            existing = st.session_state.cc_targets.copy()
                            existing['_key'] = existing.apply(lambda rr: _make_key(rr['Text'], rr['Kind']), axis=1)
                            before_keys = set(existing['_key'] + '|' + existing['Kind'].astype(str))
                            combined = pd.concat([existing, add_df], ignore_index=True)
                            combined = combined.drop_duplicates(subset=['_key','Kind'], keep='last')
                            after_keys = set(combined['_key'] + '|' + combined['Kind'].astype(str))
                            added_ct = max(0, len(after_keys - before_keys))
                            st.session_state.cc_targets = combined.drop(columns=['_key'], errors='ignore')
                        st.success(f"Added {int(added_ct)} manual target(s)")

            st.markdown("**Selected Targets**")
            if st.session_state.cc_targets.empty:
                st.info("No targets selected yet.")
            else:
                # Build metrics map for Selected Targets according to current aggregation and product group filters
                try:
                    do_pg_agg = st.session_state.get('cc_agg_pg', True)
                    restrict_adg = st.session_state.get('cc_agg_restrict_adg', False)
                    # Build combined source similar to candidate builder
                    st_df = get_search_term_data(st.session_state.bulk_data, st.session_state.client_config)
                    tgt_b, tgt_nb = get_targeting_performance_data(st.session_state.bulk_data, st.session_state.client_config)
                    parts = []
                    if isinstance(st_df, pd.DataFrame) and not st_df.empty:
                        parts.append(st_df.copy())
                    if isinstance(tgt_b, pd.DataFrame) and not tgt_b.empty:
                        parts.append(tgt_b)
                    if isinstance(tgt_nb, pd.DataFrame) and not tgt_nb.empty:
                        parts.append(tgt_nb)
                    metrics_map = {}
                    if parts:
                        base = pd.concat(parts, ignore_index=True, sort=False)
                        # Normalize naming for restriction step
                        if 'Campaign' not in base.columns and 'Campaign Name' in base.columns:
                            base['Campaign'] = base['Campaign Name']
                        if 'Campaign' not in base.columns and 'Campaign Name (Informational Only)' in base.columns:
                            base['Campaign'] = base['Campaign Name (Informational Only)']
                        if 'Ad Group' not in base.columns and 'Ad Group Name' in base.columns:
                            base['Ad Group'] = base['Ad Group Name']
                        # Quick filter to selected product groups
                        sel_groups = st.session_state.get('cc_pf_groups', [])
                        if do_pg_agg and sel_groups and 'Product Group' in base.columns:
                            base = base[base['Product Group'].isin(sel_groups)]
                        # Optional: restrict to ad groups that have enabled Product Ads for selected products
                        if restrict_adg:
                            allow_camps, allow_pairs = _allowed_adgroups_for_selected_products()
                            if not allow_camps and not allow_pairs:
                                try:
                                    st.session_state.debug_messages.append("[Selected Targets][Metrics Map] No allowed campaigns/ad groups for selected products; skipping ad group restriction for metrics backfill")
                                except Exception:
                                    pass
                            else:
                                ag_colname2 = 'Ad Group' if 'Ad Group' in base.columns else ('Ad Group Name' if 'Ad Group Name' in base.columns else None)
                                if ag_colname2:
                                    pair_keys = set([f"{c}||{a}" for (c,a) in allow_pairs])
                                    series_pairs = base['Campaign'].astype(str).str.strip() + '||' + base[ag_colname2].astype(str).str.strip()
                                    mask_pairs = series_pairs.isin(pair_keys)
                                    mask_camps = base['Campaign'].astype(str).str.strip().isin(allow_camps)
                                    base = base[mask_pairs | mask_camps]
                                elif 'Campaign' in base.columns:
                                    base = base[base['Campaign'].astype(str).str.strip().isin(allow_camps)]

                        # Normalize text and kind and drop SD remarketing
                        base = base.copy()
                        base['_text'] = ''
                        if 'Search Term' in base.columns:
                            base['_text'] = base['Search Term'].astype(str)
                        if 'Target' in base.columns:
                            base['_text'] = base['_text'].mask(base['_text'].isin(['', 'nan', 'None']), base['Target'].astype(str))
                        base = base[~base['_text'].str.match(r'(?i)^(views|purchases)=', na=False)]
                        is_asin_b = base['_text'].str.contains(r'B0[A-Za-z0-9]{8}', case=False, na=False)
                        is_cat_b = base['_text'].str.match(r'(?i)^category=', na=False)
                        base['_kind'] = np.where(is_asin_b, 'ASIN', np.where(is_cat_b, 'Category', 'Keyword'))
                        # Aggregate if needed
                        # Normalize Sales for aggregation (use Ad Sales if available)
                        if 'Ad Sales' in base.columns and 'Sales' not in base.columns:
                            base['Sales'] = base['Ad Sales']
                        elif 'Ad Sales' in base.columns:
                            base['Sales'] = base['Ad Sales']

                        if do_pg_agg:
                            agg_dict = {}
                            for col in ['Sales','Spend','Orders']:
                                if col in base.columns:
                                    agg_dict[col] = 'sum'
                            if agg_dict:
                                base = base.groupby(['_text','_kind'], as_index=False).agg(agg_dict)
                                if 'Sales' in base.columns and 'Spend' in base.columns:
                                    spend_nz = base['Spend'].replace(0, np.nan)
                                    base['ROAS'] = (base['Sales'] / spend_nz).fillna(0)
                        # Build normalized key for merging
                        def _norm_kw(txt: str) -> str:
                            # Remove '+' tokens and collapse whitespace
                            t = re.sub(r"[+]+", " ", str(txt))
                            t = re.sub(r"\s+", " ", t)
                            return t.strip().lower()
                        def _to_key_row(r):
                            txt = str(r['_text']).strip()
                            kind = r.get('_kind','Keyword')
                            if kind == 'ASIN':
                                m = re.search(r'B0[A-Za-z0-9]{8}', txt, flags=re.I)
                                return m.group(0).upper() if m else txt.lower()
                            return _norm_kw(txt)
                        base['_key'] = base.apply(_to_key_row, axis=1)
                        # Build per-key scope labels from actual contributing rows
                        def _label_list(vals: list) -> str:
                            vals = [str(v).strip() for v in vals if str(v).strip() and str(v).strip().lower() not in ('none','nan')]
                            if not vals:
                                return ''
                            vals = sorted(list(set(vals)))
                            return ", ".join(vals[:3]) + (f" (+{len(vals)-3} more)" if len(vals) > 3 else '')
                        scope_pg_map = {}
                        if 'Product Group' in base.columns:
                            tmp_pg = base[['Product Group','_key']].dropna()
                            if not tmp_pg.empty:
                                pg_lists = tmp_pg.groupby('_key')['Product Group'].apply(list)
                                scope_pg_map = {k: _label_list(v) for k, v in pg_lists.to_dict().items()}
                        tag_cols = [c for c in ['Tag 1','Tag 2','Tag 3'] if c in base.columns]
                        scope_tag_map = {}
                        if tag_cols:
                            tmp_tag = base[tag_cols + ['_key']].melt(id_vars=['_key'], value_vars=tag_cols, value_name='tag')
                            tmp_tag = tmp_tag.dropna(subset=['tag'])
                            if not tmp_tag.empty:
                                tag_lists = tmp_tag.groupby('_key')['tag'].apply(list)
                                scope_tag_map = {k: _label_list(v) for k, v in tag_lists.to_dict().items()}
                        # Final aggregation by key
                        agg_cols = [c for c in ['Sales','Spend','ROAS'] if c in base.columns]
                        base_key = base.groupby('_key', as_index=False)[agg_cols].sum(min_count=1)
                        # If ROAS was summed, recompute from sums
                        if set(['Sales','Spend']).issubset(base_key.columns):
                            spend_nz = base_key['Spend'].replace(0, np.nan)
                            base_key['ROAS'] = (base_key['Sales'] / spend_nz).fillna(0)
                        metrics_map = {k: {'Spend': float(row.get('Spend', 0) or 0), 'Sales': float(row.get('Sales', 0) or 0), 'ROAS': float(row.get('ROAS', 0) or 0)} for k, row in base_key.set_index('_key').to_dict(orient='index').items()}
                    # Build Selected Targets display df
                    disp = st.session_state.cc_targets.copy()
                    # Derive metrics
                    def _to_key_sel(r):
                        def _norm_kw(txt: str) -> str:
                            t = re.sub(r"[+]+", " ", str(txt))
                            t = re.sub(r"\s+", " ", t)
                            return t.strip().lower()
                        txt = str(r.get('Text','')).strip()
                        kind = r.get('Kind','Keyword')
                        if kind == 'ASIN':
                            m = re.search(r'B0[A-Za-z0-9]{8}', txt, flags=re.I)
                            return m.group(0).upper() if m else txt.lower()
                        return _norm_kw(txt)
                    disp['_key'] = disp.apply(_to_key_sel, axis=1)
                    # Preserve existing metrics if present; backfill from metrics_map only where missing or zero
                    if 'Spend' not in disp.columns:
                        disp['Spend'] = np.nan
                    if 'Ad Sales' not in disp.columns:
                        disp['Ad Sales'] = np.nan
                    disp['Spend'] = pd.to_numeric(disp['Spend'], errors='coerce')
                    disp['Ad Sales'] = pd.to_numeric(disp['Ad Sales'], errors='coerce')
                    spend_backfill = disp['_key'].map(lambda k: metrics_map.get(k, {}).get('Spend', 0.0))
                    sales_backfill = disp['_key'].map(lambda k: metrics_map.get(k, {}).get('Sales', 0.0))
                    roas_backfill = disp['_key'].map(lambda k: metrics_map.get(k, {}).get('ROAS', 0.0))
                    disp['Spend'] = disp['Spend'].where(disp['Spend'].notna() & (disp['Spend'] > 0), spend_backfill)
                    disp['Ad Sales'] = disp['Ad Sales'].where(disp['Ad Sales'].notna() & (disp['Ad Sales'] > 0), sales_backfill)
                    # Compute ROAS from Spend/Ad Sales if not carried; else backfill
                    if 'ROAS' not in disp.columns:
                        disp['ROAS'] = np.nan
                    disp['ROAS'] = pd.to_numeric(disp['ROAS'], errors='coerce')
                    roas_calc = (disp['Ad Sales'] / disp['Spend'].replace(0, np.nan)).fillna(0)
                    disp['ROAS'] = disp['ROAS'].where(disp['ROAS'].notna() & (disp['ROAS'] > 0), roas_calc)
                    disp['ROAS'] = disp['ROAS'].where(disp['ROAS'] > 0, roas_backfill)
                    # Backfill scope columns based on current Product Finder filters if empty; use 'All' when aggregation is on but no selection
                    scope_groups_list = st.session_state.get('cc_pf_groups', []) or []
                    scope_tags_list = st.session_state.get('cc_pf_tags', []) or []
                    do_pg_agg2 = st.session_state.get('cc_agg_pg', True)
                    scope_groups = ",".join([str(x).strip() for x in scope_groups_list if str(x).strip()]) or ("All" if do_pg_agg2 else "")
                    scope_tags = ",".join([str(x).strip() for x in scope_tags_list if str(x).strip()]) or "All"
                    # Use per-target scopes from base where available; otherwise use session-level defaults
                    # Product Groups
                    disp['Scope Product Groups'] = disp.get('Scope Product Groups', scope_groups)
                    disp['Scope Product Groups'] = disp['Scope Product Groups'].astype(str)
                    per_key_pg = disp['_key'].map(scope_pg_map)
                    mask_pg_has = per_key_pg.notna() & (per_key_pg.astype(str).str.strip() != '')
                    disp.loc[mask_pg_has, 'Scope Product Groups'] = per_key_pg[mask_pg_has]
                    # If still empty or placeholder, apply session default
                    col = disp['Scope Product Groups'].astype(str)
                    mask_pg_empty = col.str.strip().eq('') | col.isna() | col.str.strip().str.lower().isin(['none','nan','all'])
                    disp.loc[mask_pg_empty, 'Scope Product Groups'] = scope_groups
                    # Tags
                    disp['Scope Tags'] = disp.get('Scope Tags', scope_tags)
                    disp['Scope Tags'] = disp['Scope Tags'].astype(str)
                    per_key_tag = disp['_key'].map(scope_tag_map)
                    mask_tag_has = per_key_tag.notna() & (per_key_tag.astype(str).str.strip() != '')
                    disp.loc[mask_tag_has, 'Scope Tags'] = per_key_tag[mask_tag_has]
                    colt = disp['Scope Tags'].astype(str)
                    maskt = colt.str.strip().eq('') | colt.isna() | colt.str.strip().str.lower().isin(['none','nan','all'])
                    disp.loc[maskt, 'Scope Tags'] = scope_tags
                    # Ensure Found Via is present in Selected Targets; backfill if empty based on current toggles
                    if 'Found Via' not in disp.columns:
                        disp['Found Via'] = ''
                    restrict_adg2 = st.session_state.get('cc_agg_restrict_adg', False)
                    disp['Found Via'] = disp['Found Via'].astype(str)
                    if do_pg_agg2:
                        vals_g = [str(x).strip() for x in scope_groups_list if str(x).strip()]
                        if vals_g:
                            lbl_g = ", ".join(vals_g[:3]) + (f" (+{len(vals_g)-3} more)" if len(vals_g) > 3 else '')
                            default_fv2 = f"Product Group Match - {lbl_g}"
                        else:
                            default_fv2 = "Product Group Match"
                    elif restrict_adg2:
                        sel_df_s = st.session_state.get('cc_products', pd.DataFrame())
                        vals_s = []
                        if isinstance(sel_df_s, pd.DataFrame) and not sel_df_s.empty:
                            id_col_candidates_s = [c for c in ['ASIN','SKU'] if c in sel_df_s.columns]
                            if id_col_candidates_s:
                                id_col_use_s = 'ASIN' if 'ASIN' in id_col_candidates_s else id_col_candidates_s[0]
                                vals_s = [str(x).strip() for x in sel_df_s[id_col_use_s].dropna().astype(str).tolist() if str(x).strip()]
                        if vals_s:
                            lbl_s = ", ".join(vals_s[:3]) + (f" (+{len(vals_s)-3} more)" if len(vals_s) > 3 else '')
                            default_fv2 = f"ASIN Match - {lbl_s}"
                        else:
                            default_fv2 = "ASIN Match"
                    else:
                        default_fv2 = ''
                    mask_fv = disp['Found Via'].str.strip().eq('') | disp['Found Via'].isna() | disp['Found Via'].str.strip().str.lower().isin(['none','nan'])
                    disp.loc[mask_fv, 'Found Via'] = default_fv2
                    # Reorder for readability
                    order = ['Text','Kind','Branding','Source','Bid','Spend','Ad Sales','ROAS','Found Via','Scope Product Groups','Scope Tags']
                    disp = disp[[c for c in order if c in disp.columns] + [c for c in disp.columns if c not in order]]
                    # Round ROAS to 2 decimals
                    disp['ROAS'] = pd.to_numeric(disp['ROAS'], errors='coerce').fillna(0).round(2)
                    # Configure currency formatting
                    col_cfg2 = {
                        'Spend': st.column_config.NumberColumn('Spend', format='$%.2f'),
                        'Ad Sales': st.column_config.NumberColumn('Ad Sales', format='$%.2f'),
                        'ROAS': st.column_config.NumberColumn('ROAS', format='%.2f')
                    }
                    edited = st.data_editor(
                        disp.drop(columns=['_key'], errors='ignore'),
                        num_rows="dynamic",
                        use_container_width=True,
                        key="cc_targets_editor",
                        column_config=col_cfg2
                    )
                    # Persist edits back to core df, keep metrics and scope columns
                    if isinstance(edited, pd.DataFrame):
                        st.session_state.cc_targets = edited
                except Exception:
                    # Fallback to original behavior if any error
                    st.session_state.cc_targets = st.data_editor(
                        st.session_state.cc_targets,
                        num_rows="dynamic",
                        use_container_width=True,
                        key="cc_targets_editor"
                    )
            # Clear Selected Targets button
            ct_cols = st.columns([1,3])
            with ct_cols[0]:
                if st.button("Clear Selected Targets", key="cc_clear_targets"):
                    st.session_state.cc_targets = pd.DataFrame(columns=['Text','Kind','Branding','Source','Bid'])
                    st.success("Cleared Selected Targets")

            st.markdown("---")
            st.subheader("Tactics")
            tactic_options = [
                "SP Branded Research",
                "SP Branded Performance",
                "SP Branded ASIN Targeting",
                "SD Branded ASIN Targeting",
                "SP Non-Branded Auto",
                "SP Non-Branded Research",
                "SP Non-Branded Performance",
                "SP Non-Branded ASIN Targeting",
                "SD Non-Branded ASIN Targeting",
            ]
            # Tactics multiselect with scoped CSS to increase height and show full labels
            with st.container():
                st.markdown('<div id="tactics-multi">', unsafe_allow_html=True)
                selected_tactics = st.multiselect("Select tactics", tactic_options, default=[], key="cc_selected_tactics")
                st.markdown('</div>', unsafe_allow_html=True)
                st.markdown(
                    """
                    <style>
                    /* Scope to tactics multiselect container */
                    #tactics-multi div[data-baseweb="select"] > div {
                        min-height: 88px; /* taller control to display more selected items */
                    }
                    /* Allow tag labels to wrap and show full names */
                    #tactics-multi div[data-baseweb="tag"] span {
                        white-space: normal !important;
                        max-width: none !important;
                    }
                    </style>
                    """,
                    unsafe_allow_html=True,
                )
            # Live text below showing fully selected tactics (ensures visibility even if UI truncates)
            if st.session_state.get('cc_selected_tactics'):
                st.caption("Selected: " + ", ".join(st.session_state.cc_selected_tactics))

            # ---- Section 7. Bids ----
            st.markdown("---")
            st.subheader("Bids")
            colb1, colb2 = st.columns(2)
            with colb1:
                fallback_bid = st.number_input("Fallback Target Bid", min_value=0.02, value=0.5, step=0.01, key="cc_fallback_bid")
            # Ad Group Default Bid matches Fallback Bid and is not editable
            ag_default_bid = fallback_bid
            st.session_state['cc_ag_default_bid'] = ag_default_bid

            # (Presets moved below Live Preview)

            # ---- Section 8. Campaign Options ----
            st.markdown("---")
            st.subheader("Campaign Options")
            bcol1, bcol2 = st.columns([0.35, 1.65])
            with bcol1:
                daily_budget = st.number_input("Daily Budget (USD)", min_value=1.0, value=10.0, step=1.0, key="cc_daily_budget")
            with bcol2:
                st.caption("This is the default budget. You can override it per campaign in the Live Preview below.")
            start_date = st.date_input("Campaign Start Date", value=datetime.now().date(), key="cc_start_date")
            start_date_str = start_date.strftime('%Y%m%d')
            # Ensure per-campaign budget mapping exists
            if 'cc_campaign_budgets' not in st.session_state:
                st.session_state['cc_campaign_budgets'] = {}

            # Sponsored Products-only options
            st.caption("Sponsored Products options")
            st.session_state['cc_sp_bidding_strategy'] = st.selectbox(
                "Bidding Strategy (SP only)",
                [
                    'Dynamic Bids - Up and Down',
                    'Dynamic Bids - Down Only',
                    'Fixed Bids'
                ],
                index=[
                    'Dynamic Bids - Up and Down',
                    'Dynamic Bids - Down Only',
                    'Fixed Bids'
                ].index(st.session_state.get('cc_sp_bidding_strategy','Dynamic Bids - Up and Down')),
                key="cc_sp_bidding_strategy_select"
            )

            st.markdown("Placement Modifiers (SP only, %)")
            cpm1, cpm2 = st.columns(2)
            with cpm1:
                st.session_state['cc_sp_place_top'] = st.number_input("Placement Top", min_value=0, max_value=900, value=int(st.session_state.get('cc_sp_place_top', 0)), step=1)
                st.session_state['cc_sp_place_ros'] = st.number_input("Placement Rest Of Search", min_value=0, max_value=900, value=int(st.session_state.get('cc_sp_place_ros', 0)), step=1)
            with cpm2:
                st.session_state['cc_sp_place_pp'] = st.number_input("Placement Product Page", min_value=0, max_value=900, value=int(st.session_state.get('cc_sp_place_pp', 0)), step=1)
                st.session_state['cc_sp_place_ab'] = st.number_input("Placement Amazon Business", min_value=0, max_value=900, value=int(st.session_state.get('cc_sp_place_ab', 0)), step=1)

            # Products used for campaign generation are those in the Selected Products table (no additional selection)
            id_col = 'ASIN' if id_choice == 'ASIN' else 'SKU'

            # ---- Validation: Ensure required dimensions are present (before Live Preview/launch) ----
            required_missing = []
            try:
                if st.session_state.get('cc_ag_mode_key', 'custom') == 'custom':
                    dims = st.session_state.get('cc_custom_campaign_dims', ["Product Group", "Tag 1", "Tag 2", "Tactic"]) or []
                    # Include Tag requirement if ad group split is Tag-based
                    split_dim = st.session_state.get('cc_custom_adgroup_dim', 'Tag 3')
                    if split_dim in ("Tag 1", "Tag 2", "Tag 3") and split_dim not in dims:
                        dims = list(dims) + [split_dim]
                    # Map dims to Selected Products columns
                    dim_to_col = {
                        'Product Group': 'Product Group',
                        'Tag 1': 'Tag 1',
                        'Tag 2': 'Tag 2',
                        'Tag 3': 'Tag 3',
                    }
                    # Fallback keys in session state
                    fb_keys = {
                        'Product Group': 'cc_pf_fallback_pg',
                        'Tag 1': 'cc_pf_fallback_t1',
                        'Tag 2': 'cc_pf_fallback_t2',
                        'Tag 3': 'cc_pf_fallback_t3',
                    }
                    # Check for missing values per required dim
                    if hasattr(st.session_state, 'cc_products') and isinstance(st.session_state.cc_products, pd.DataFrame) and not st.session_state.cc_products.empty:
                        for d in dims:
                            if d not in dim_to_col:
                                continue
                            col = dim_to_col[d]
                            series = st.session_state.cc_products.get(col)
                            if series is None:
                                # Column entirely missing counts as missing
                                fb_val = str(st.session_state.get(fb_keys[d], '')).strip()
                                if not fb_val:
                                    required_missing.append(d)
                                continue
                            empty_mask = series.astype(str).str.strip().eq('') | series.isna()
                            if empty_mask.any():
                                fb_val = str(st.session_state.get(fb_keys[d], '')).strip()
                                if not fb_val:
                                    required_missing.append(d)
            except Exception:
                # Fail-safe: do not block if validation throws
                required_missing = []

            if required_missing:
                missing_list = ", ".join(sorted(set(required_missing)))
                st.error(f"Missing required values for: {missing_list}. Provide fallbacks in 'Add Products Manually' (Fallback Product Group/Tags) or fill these fields in Selected Products before proceeding.")
                st.stop()

            # ---- Live Preview (counts & sample names) ----
            def _safe_upper_list(xs):
                return [str(x).strip().upper() for x in xs if str(x).strip()]

            def _build_campaign_name_preview(ad_prefix: str, brand_tag: str, category: str, tags: list, tactic_label: str, include_tactic: bool = True):
                parts = [ad_prefix, brand_tag, category]
                for t in tags:
                    if t:
                        parts.append(str(t).strip())
                if include_tactic:
                    parts.append(tactic_label)
                parts.append('BW')
                return " | ".join(parts)

            def _group_products_preview(entries, mode):
                groups = []
                if mode == 'group_all_per_tactic':
                    groups.append({'category': (root_phrase or 'General'), 'tags': [], 'products': list(entries)})
                    return groups
                if mode == 'per_product':
                    for p in entries:
                        groups.append({'category': p.get('category') or (root_phrase or 'General'), 'tags': p.get('tags') or [], 'products': [p]})
                    return groups
                if mode == 'per_pg':
                    buckets = {}
                    for p in entries:
                        cat = (p.get('category') or (root_phrase or 'General')).strip()
                        buckets.setdefault(cat, []).append(p)
                    for cat, plist in buckets.items():
                        groups.append({'category': cat, 'tags': [], 'products': plist})
                    return groups
                if mode == 'custom':
                    # Group by selected campaign dimensions (excluding tactic for grouping)
                    dims = st.session_state.get('cc_custom_campaign_dims', ["Product Group", "Tag 1", "Tag 2", "Tactic"]) or []
                    def _key_for(p):
                        cat = (p.get('category') or (root_phrase or 'General')).strip()
                        t1, t2, t3 = (p.get('tags') or [None, None, None])[:3]
                        parts = []
                        for d in dims:
                            if d == 'Product Group':
                                parts.append(('PG', cat))
                            elif d == 'Tag 1':
                                parts.append(('T1', (t1 or '').strip()))
                            elif d == 'Tag 2':
                                parts.append(('T2', (t2 or '').strip()))
                            elif d == 'Tag 3':
                                parts.append(('T3', (t3 or '').strip()))
                            # Ignore 'Tactic' for grouping at this stage
                        return tuple(parts)
                    buckets = {}
                    for p in entries:
                        buckets.setdefault(_key_for(p), []).append(p)
                    for key, plist in buckets.items():
                        # Derive category and tag list for naming: category from PG, tags from T1/T2/T3 present in dims (exclude tactic)
                        cat_val = next((v for k, v in key if k == 'PG'), (root_phrase or 'General'))
                        tag_vals = []
                        for k, v in key:
                            if k in ('T1', 'T2', 'T3') and v:
                                tag_vals.append(v)
                        groups.append({'category': cat_val, 'tags': tag_vals, 'products': plist})
                    return groups
                buckets = {}
                for p in entries:
                    cat = (p.get('category') or (root_phrase or 'General')).strip()
                    tags_tuple = tuple([t for t in (p.get('tags') or []) if t])
                    key = (cat, tags_tuple)
                buckets.setdefault(key, []).append(p)
                for (cat, tags_tuple), plist in buckets.items():
                    groups.append({'category': cat, 'tags': list(tags_tuple), 'products': plist})
                return groups

            def _extract_existing_campaigns_from_bulk(bulk_data):
                existing = {'Sponsored Products': set(), 'Sponsored Display': set()}
                adgroups = {'Sponsored Products': {}, 'Sponsored Display': {}}
                if not bulk_data:
                    return existing, adgroups
                for sheet_name in [
                    'Sponsored Products Campaigns','Sponsored Display Campaigns',
                    'Sponsored Products Ad Groups','Sponsored Display Ad Groups'
                ]:
                    df = bulk_data.get(sheet_name)
                    if df is None or df.empty:
                        continue
                    is_sp = 'Sponsored Products' in sheet_name
                    prod = 'Sponsored Products' if is_sp else 'Sponsored Display'
                    cols = {c.lower(): c for c in df.columns}
                    camp_col = cols.get('campaign') or cols.get('campaign id') or cols.get('campaign name (informational only)')
                    adg_col = cols.get('ad group') or cols.get('ad group id') or cols.get('ad group name (informational only)')
                    if 'campaigns' in sheet_name.lower():
                        if camp_col:
                            existing[prod].update({str(x).strip() for x in df[camp_col].dropna().unique()})
                    elif 'ad groups' in sheet_name.lower():
                        if camp_col and adg_col:
                            for _, r in df[[camp_col, adg_col]].dropna().iterrows():
                                c = str(r[camp_col]).strip(); a = str(r[adg_col]).strip()
                                if c:
                                    adgroups.setdefault(prod, {}).setdefault(c, set()).add(a)
                return existing, adgroups

            st.markdown("---")
            st.subheader("Live Preview")
            with st.container():
                # Build current product entries
                product_entries_preview = []
                if not st.session_state.cc_products.empty:
                    dfp = st.session_state.cc_products.copy()
                    dfp[id_col] = dfp[id_col].astype(str)
                    for _, row in dfp.iterrows():
                        pid = str(row.get(id_col, '')).strip()
                        product_entries_preview.append({
                            'asin': pid if id_choice=='ASIN' else '',
                            'sku': pid if id_choice=='SKU' else '',
                            'title': row.get('Product Title',''),
                            'group': row.get('Product Group',''),
                            'category': (row.get('Category','') or (root_phrase or 'General')),
                            'tags': [row.get('Tag 1',''), row.get('Tag 2',''), row.get('Tag 3','')],
                        })

                groups_prev = _group_products_preview(product_entries_preview, st.session_state.get('cc_ag_mode_key','group_all_per_tactic'))
                # Tactic defs (mirror main)
                tactic_defs_prev = {
                    "SP Branded Research": {"ad":"SP","brand":"Brand","ttype":"Keyword","match":"broad","label":"Research"},
                    "SP Branded Performance": {"ad":"SP","brand":"Brand","ttype":"Keyword","match":"exact","label":"Performance"},
                    "SP Branded ASIN Targeting": {"ad":"SP","brand":"Brand","ttype":"ASIN","label":"ASIN Targeting"},
                    "SD Branded ASIN Targeting": {"ad":"SD","brand":"Brand","ttype":"ASIN","label":"ASIN Targeting"},
                    "SP Non-Branded Auto": {"ad":"SP","brand":"Non","ttype":"Auto","label":"Auto"},
                    "SP Non-Branded Research": {"ad":"SP","brand":"Non","ttype":"Keyword","match":"broad","label":"Research"},
                    "SP Non-Branded Performance": {"ad":"SP","brand":"Non","ttype":"Keyword","match":"exact","label":"Performance"},
                    "SP Non-Branded ASIN Targeting": {"ad":"SP","brand":"Non","ttype":"ASIN","label":"ASIN Targeting"},
                    "SD Non-Branded ASIN Targeting": {"ad":"SD","brand":"Non","ttype":"ASIN","label":"ASIN Targeting"},
                }
                camp_names = set(); adgroup_keys = set(); product_ad_count = 0
                sample_names = []
                ag_mode_prev = st.session_state.get('cc_ag_mode_key','group_all_per_tactic')
                for g in groups_prev:
                    category = g['category']; tags = g['tags']; entries_for_group = g['products']
                    for tactic in st.session_state.get('cc_selected_tactics', []):
                        td = tactic_defs_prev.get(tactic)
                        if not td:
                            continue
                        if td['ad']=='SD' and td['ttype']!='ASIN':
                            continue
                        ad_prefix = 'SP' if td['ad']=='SP' else 'SD'
                        product_sheet = 'Sponsored Products' if ad_prefix=='SP' else 'Sponsored Display'
                        include_tactic = (ag_mode_prev != 'tactic_level')
                        if ag_mode_prev == 'custom':
                            # Include tactic in name only if selected in campaign dims
                            include_tactic = ('Tactic' in st.session_state.get('cc_custom_campaign_dims', []))
                        c_name = _build_campaign_name_preview(ad_prefix, td['brand'], category, tags, td['label'], include_tactic)
                        camp_names.add((product_sheet, c_name))
                        if ag_mode_prev in ['group_all_per_tactic','per_pg']:
                            ag_name = c_name
                            adgroup_keys.add((product_sheet, c_name, ag_name))
                            if product_sheet=='Sponsored Products':
                                product_ad_count += len(entries_for_group)
                        elif ag_mode_prev=='tactic_level':
                            ag_name = td['label']
                            adgroup_keys.add((product_sheet, c_name, ag_name))
                            if product_sheet=='Sponsored Products':
                                product_ad_count += len(entries_for_group)
                        elif ag_mode_prev=='custom':
                            split = st.session_state.get('cc_custom_adgroup_dim', 'Tag 3')
                            if split == 'Single (one ad group)':
                                ag_name = c_name
                                adgroup_keys.add((product_sheet, c_name, ag_name))
                                if product_sheet=='Sponsored Products':
                                    product_ad_count += len(entries_for_group)
                            elif split == 'Tactic':
                                ag_name = td['label']
                                adgroup_keys.add((product_sheet, c_name, ag_name))
                                if product_sheet=='Sponsored Products':
                                    product_ad_count += len(entries_for_group)
                            elif split == 'Product (ASIN/SKU)':
                                for p in entries_for_group:
                                    pid = p['asin'] if id_choice=='ASIN' else p['sku']
                                    if not pid:
                                        continue
                                    ag_name = (f"{pid} - {p.get('title','')}")[:200]
                                    adgroup_keys.add((product_sheet, c_name, ag_name))
                                    if product_sheet=='Sponsored Products':
                                        product_ad_count += 1
                            else:
                                # Tag 1/2/3 splits
                                idx_map = {'Tag 1':0,'Tag 2':1,'Tag 3':2}
                                ti = idx_map.get(split, 2)
                                vals = sorted({(p.get('tags') or [None,None,None])[ti] or 'Untagged' for p in entries_for_group})
                                for v in vals:
                                    ag_name = str(v)[:200]
                                    adgroup_keys.add((product_sheet, c_name, ag_name))
                                    if product_sheet=='Sponsored Products':
                                        # Count products that belong to this tag bucket
                                        count_here = sum(1 for p in entries_for_group if ((p.get('tags') or [None,None,None])[ti] or 'Untagged') == v)
                                        product_ad_count += count_here
                        else:
                            # per_product
                            for p in entries_for_group:
                                pid = p['asin'] if id_choice=='ASIN' else p['sku']
                                if not pid:
                                    continue
                                c_pp = _build_campaign_name_preview(ad_prefix, td['brand'], category, tags, td['label'], True)
                                ag_name = (f"{pid} - {p.get('title','')}")[:200]
                                adgroup_keys.add((product_sheet, c_pp, ag_name))
                                camp_names.add((product_sheet, c_pp))
                                if product_sheet=='Sponsored Products':
                                    product_ad_count += 1
                        if len(sample_names) < 5:
                            sample_names.append(c_name)

                num_campaigns = len(camp_names)
                num_adgroups = len(adgroup_keys)
                st.markdown(f"Will create {num_campaigns} campaigns, {num_adgroups} ad groups, {product_ad_count} product ads (SP).")
                # Hide example campaign tiles to avoid duplicate visual patterns with accordion

                # Existing campaigns prompt
                existing, existing_ag = _extract_existing_campaigns_from_bulk(st.session_state.get('bulk_data'))
                overlap = []
                for ps, cn in camp_names:
                    if cn in existing.get(ps, set()):
                        overlap.append((ps, cn))
                if overlap:
                    st.info(f"{len(overlap)} campaign(s) already exist in your uploaded bulk file.")
                    st.session_state['cc_merge_into_existing'] = st.checkbox(
                        "Yes, add ASINs and Targets into existing campaigns where they already exist",
                        value=st.session_state.get('cc_merge_into_existing', False)
                    )
                    with st.expander("Show affected campaigns", expanded=False):
                        for _, cn in sorted(overlap, key=lambda x: x[1]):
                            st.write(cn)

                # Accordion-style drilldown preview
                import re as _re
                
                def _sel_prev(df, branding, kind):
                    if df is None or df.empty:
                        return []
                    m = (df['Branding'] == branding) & (df['Kind'] == kind)
                    out = []
                    for _, row in df[m].iterrows():
                        txt = str(row['Text']).strip()
                        bid = row['Bid'] if pd.notna(row.get('Bid')) and row.get('Bid') else st.session_state.get('cc_fallback_bid', 0.5)
                        scope_groups = str(row.get('Scope Product Groups','')).strip()
                        scope_tags = str(row.get('Scope Tags','')).strip()
                        out.append({'text': txt, 'bid': bid, 'scope_groups': scope_groups, 'scope_tags': scope_tags})
                    return out

                brand_kw_prev = _sel_prev(st.session_state.get('cc_targets', pd.DataFrame()), 'Brand', 'Keyword')
                brand_asin_prev = _sel_prev(st.session_state.get('cc_targets', pd.DataFrame()), 'Brand', 'ASIN')
                non_kw_prev = _sel_prev(st.session_state.get('cc_targets', pd.DataFrame()), 'Non', 'Keyword')
                non_asin_prev = _sel_prev(st.session_state.get('cc_targets', pd.DataFrame()), 'Non', 'ASIN')
                non_cat_prev = _sel_prev(st.session_state.get('cc_targets', pd.DataFrame()), 'Non', 'Category')

                # Build structure: { (product_sheet, campaign_name): { 'tactic': td, 'ad_groups': { ag_name: { 'product_ads': [...], 'targets': [...], 'negatives': [...] } } } }
                structure = {}
                for g in groups_prev:
                    category = g['category']; tags = g['tags']; entries_for_group = g['products']
                    for tactic in st.session_state.get('cc_selected_tactics', []):
                        td = tactic_defs_prev.get(tactic)
                        if not td:
                            continue
                        if td['ad']=='SD' and td['ttype']!='ASIN':
                            continue
                        ad_prefix = 'SP' if td['ad']=='SP' else 'SD'
                        product_sheet = 'Sponsored Products' if ad_prefix=='SP' else 'Sponsored Display'
                        include_tactic = (ag_mode_prev != 'tactic_level')
                        campaign_name = _build_campaign_name_preview(ad_prefix, td['brand'], category, tags, td['label'], include_tactic)

                        # Build allowed product-group names for scope matching: Category plus Product Group labels from entries
                        try:
                            allowed_pg_names = {str(category).strip()} | {str(p.get('group','')).strip() for p in entries_for_group if str(p.get('group','')).strip()}
                        except Exception:
                            allowed_pg_names = {str(category).strip()}

                        # Determine ad group names for this campaign
                        ad_groups_list = []
                        if ag_mode_prev in ['group_all_per_tactic', 'per_pg']:
                            ad_groups_list = [campaign_name]
                        elif ag_mode_prev == 'tactic_level':
                            ad_groups_list = [td['label']]
                        elif ag_mode_prev == 'custom':
                            split = st.session_state.get('cc_custom_adgroup_dim', 'Tag 3')
                            if split in ('Single (one ad group)',):
                                ad_groups_list = [campaign_name]
                            elif split == 'Tactic':
                                ad_groups_list = [td['label']]
                            elif split == 'Product (ASIN/SKU)':
                                for p in entries_for_group:
                                    pid = p['asin'] if id_choice=='ASIN' else p['sku']
                                    if not pid:
                                        continue
                                    ad_groups_list.append((f"{pid} - {p.get('title','')}")[:200])
                            else:
                                # Tag-based split: Tag 1/2/3
                                idx_map = {'Tag 1':0,'Tag 2':1,'Tag 3':2}
                                ti = idx_map.get(split, 2)
                                tag_values = set()
                                for p in entries_for_group:
                                    tag_val = (p.get('tags') or [None,None,None])[ti] or 'Untagged'
                                    tag_values.add(str(tag_val)[:200])
                                ad_groups_list = sorted(tag_values)
                        else:  # per_product
                            ad_groups_list = []
                            for p in entries_for_group:
                                pid = p['asin'] if id_choice=='ASIN' else p['sku']
                                if not pid:
                                    continue
                                ad_groups_list.append((f"{pid} - {p.get('title','')}")[:200])

                        # Prepare targets for this tactic
                        targets_for_tactic = []
                        if td['ttype'] == 'Keyword':
                            source = brand_kw_prev if td['brand']=='Brand' else non_kw_prev
                            targets_for_tactic = []
                            for t in source:
                                # Scope filter: Product Group and Tags
                                sg = {x.strip() for x in (t.get('scope_groups','') or '').split(',') if x.strip() and x.strip().lower() not in ('all','none','nan')}
                                # Treat 'All'/'None'/'nan' as no tag restriction
                                stags = {x.strip() for x in (t.get('scope_tags','') or '').split(',') if x.strip() and x.strip().lower() not in ('all','none','nan')}
                                # Allow match to either the group's Category or any Product Group value among the group's products
                                if sg and not (allowed_pg_names & sg):
                                    continue
                                if stags and not (stags.intersection(set(tags))):
                                    continue
                                targets_for_tactic.append({'type':'Keyword', 'text':t['text'], 'match': td.get('match','broad'), 'bid': t['bid']})
                        elif td['ttype'] == 'ASIN':
                            source = brand_asin_prev if td['brand']=='Brand' else non_asin_prev
                            extracted = []
                            for t in source:
                                m = _re.search(r'B0[A-Za-z0-9]{8}', t['text'], flags=_re.I)
                                if not m:
                                    continue
                                # Scope filter before add
                                sg = {x.strip() for x in (t.get('scope_groups','') or '').split(',') if x.strip() and x.strip().lower() not in ('all','none','nan')}
                                # Treat 'All'/'None'/'nan' as no tag restriction
                                stags = {x.strip() for x in (t.get('scope_tags','') or '').split(',') if x.strip() and x.strip().lower() not in ('all','none','nan')}
                                # Allow match to either the group's Category or any Product Group value among the group's products
                                if sg and not (allowed_pg_names & sg):
                                    continue
                                if stags and not (stags.intersection(set(tags))):
                                    continue
                                extracted.append({'type':'ASIN', 'asin': m.group(0).upper(), 'bid': t['bid']})
                            if product_sheet == 'Sponsored Products':
                                targets_for_tactic = [{'type':'Product Targeting', 'expr': f"asin=\"{x['asin']}\"", 'bid': x['bid']} for x in extracted]
                                # Include Category targets only for Non-Branded SP ASIN Targeting
                                if td['brand'] == 'Non' and non_cat_prev:
                                    for c in non_cat_prev:
                                        targets_for_tactic.append({'type':'Product Targeting', 'expr': c['text'], 'bid': c['bid']})
                            else:
                                targets_for_tactic = [{'type':'Contextual Targeting', 'expr': f"asin=\"{x['asin']}\"", 'bid': x['bid']} for x in extracted]
                        elif td['ttype'] == 'Auto' and product_sheet == 'Sponsored Products':
                            targets_for_tactic = [{'type':'Product Targeting', 'expr': e, 'bid': st.session_state.get('cc_fallback_bid', 0.5)} for e in ["close-match","loose-match","complements","substitutes"]]

                        # Negatives (deterministic preview) per rules
                        negatives_for_tactic = []
                        perf_non_selected = 'SP Non-Branded Performance' in st.session_state.get('cc_selected_tactics', [])
                        perf_brand_selected = 'SP Branded Performance' in st.session_state.get('cc_selected_tactics', [])
                        client_cfg_nb = st.session_state.get('client_config', {}) or {}
                        branded_terms_nb = [str(t).strip() for t in (client_cfg_nb.get('branded_keywords', []) or []) if str(t).strip()]
                        branded_asins_nb = [str(a).strip().upper() for a in (client_cfg_nb.get('branded_asins_data', {}) or {}).keys() if str(a).strip()]

                        if product_sheet == 'Sponsored Products':
                            if td['brand'] == 'Non':
                                # Branded terms as negativePhrase for Auto, Research, Performance (Keyword)
                                if td['ttype'] in ('Auto','Keyword'):
                                    for t in branded_terms_nb:
                                        negatives_for_tactic.append({'type':'Negative Keyword', 'text': t, 'match': 'negativePhrase'})
                                # Branded ASINs as negative product targets for Auto and ASIN Targeting
                                if td['ttype'] in ('Auto','ASIN'):
                                    for a in branded_asins_nb:
                                        negatives_for_tactic.append({'type':'Negative Product Targeting', 'expr': f"asin=\"{a}\""})
                                # Cross-negatives: Non-Branded Performance keywords as negativeExact into Auto and Research (not into Performance itself)
                                if perf_non_selected and (td['ttype'] == 'Auto' or (td['ttype'] == 'Keyword' and td.get('label','') == 'Research')):
                                    for t in non_kw_prev:
                                        negatives_for_tactic.append({'type':'Negative Keyword', 'text': t['text'], 'match': 'negativeExact'})
                            elif td['brand'] == 'Brand':
                                # Cross-negatives: Branded Performance keywords as negativeExact into Branded Research
                                if perf_brand_selected and td.get('label','') == 'Research' and td['ttype'] == 'Keyword':
                                    for t in brand_kw_prev:
                                        negatives_for_tactic.append({'type':'Negative Keyword', 'text': t['text'], 'match': 'negativeExact'})

                        # --- Build historical dataset once per tactic for eligibility checks ---
                        try:
                            _st_hist = get_search_term_data(st.session_state.bulk_data, st.session_state.client_config)
                        except Exception:
                            _st_hist = pd.DataFrame()
                        try:
                            _tgt_b_hist, _tgt_nb_hist = get_targeting_performance_data(st.session_state.bulk_data, st.session_state.client_config)
                            _parts_hist = []
                            if isinstance(_tgt_b_hist, pd.DataFrame) and not _tgt_b_hist.empty:
                                _parts_hist.append(_tgt_b_hist)
                            if isinstance(_tgt_nb_hist, pd.DataFrame) and not _tgt_nb_hist.empty:
                                _parts_hist.append(_tgt_nb_hist)
                            _tgt_hist = pd.concat(_parts_hist, ignore_index=True, sort=False) if _parts_hist else pd.DataFrame()
                        except Exception:
                            _tgt_hist = pd.DataFrame()

                        _hist_df = []
                        if isinstance(_st_hist, pd.DataFrame) and not _st_hist.empty:
                            _hist_df.append(_st_hist.copy())
                        if isinstance(_tgt_hist, pd.DataFrame) and not _tgt_hist.empty:
                            _hist_df.append(_tgt_hist.copy())
                        _hist_df = pd.concat(_hist_df, ignore_index=True, sort=False) if _hist_df else pd.DataFrame()

                        if not _hist_df.empty:
                            if 'Campaign' not in _hist_df.columns and 'Campaign Name' in _hist_df.columns:
                                _hist_df['Campaign'] = _hist_df['Campaign Name']
                            if 'Campaign' not in _hist_df.columns and 'Campaign Name (Informational Only)' in _hist_df.columns:
                                _hist_df['Campaign'] = _hist_df['Campaign Name (Informational Only)']
                            if 'Ad Group' not in _hist_df.columns and 'Ad Group Name' in _hist_df.columns:
                                _hist_df['Ad Group'] = _hist_df['Ad Group Name']
                            for _c in ['Campaign','Ad Group','Target','Search Term']:
                                if _c in _hist_df.columns:
                                    _hist_df[_c] = _hist_df[_c].astype(str)
                            if {'Campaign','Ad Group'}.issubset(_hist_df.columns):
                                _hist_df['_pair_key'] = _hist_df['Campaign'].astype(str).str.strip() + '||' + _hist_df['Ad Group'].astype(str).str.strip()
                            else:
                                _hist_df['_pair_key'] = ''
                            def _norm_txt_cc(s: str) -> str:
                                t = _re.sub(r"[+]+", " ", str(s)) if '_re' in globals() else str(s)
                                t = _re.sub(r"\s+", " ", t) if '_re' in globals() else t
                                return t.strip().lower()
                            _hist_df['_norm_target'] = _hist_df.get('Target','').astype(str).map(_norm_txt_cc) if 'Target' in _hist_df.columns else ''
                            _hist_df['_norm_search'] = _hist_df.get('Search Term','').astype(str).map(_norm_txt_cc) if 'Search Term' in _hist_df.columns else ''

                        # Helper to compute allowed (campaign, ad group) for a given list of product IDs
                        def _allowed_from_products_cc(product_ids: list) -> tuple[set, set]:
                            asin_set = {str(x).strip().upper() for x in product_ids} if id_choice == 'ASIN' else set()
                            sku_set = {str(x).strip().upper() for x in product_ids} if id_choice == 'SKU' else set()
                            allowed_campaigns = set(); allowed_pairs = set()
                            bulk_data_loc = st.session_state.get('bulk_data', {}) or {}
                            for sheet_name_loc, df_loc in bulk_data_loc.items():
                                if not isinstance(df_loc, pd.DataFrame) or df_loc.empty:
                                    continue
                                if not any(tok in sheet_name_loc.lower() for tok in ['sponsored products', 'sp', 'campaign']):
                                    continue
                                lower_map = {c.lower(): c for c in df_loc.columns}
                                rec_col = next((lower_map[c] for c in ['record type'] if c in lower_map), None)
                                ent_col = next((lower_map[c] for c in ['entity'] if c in lower_map), None)
                                state_col = next((lower_map[c] for c in ['state', 'status'] if c in lower_map), None)
                                camp_col = next((lower_map[c] for c in ['campaign name (informational only)', 'campaign name', 'campaign'] if c in lower_map), None)
                                ag_col = next((lower_map[c] for c in ['ad group name', 'ad group'] if c in lower_map), None)
                                asin_col = next((lower_map[c] for c in ['asin', 'advertised asin'] if c in lower_map), None)
                                sku_col = next((lower_map[c] for c in ['sku', 'advertised sku'] if c in lower_map), None)
                                if camp_col is None or (asin_col is None and sku_col is None):
                                    continue
                                df2 = df_loc
                                try:
                                    mask_entity = pd.Series([False]*len(df2))
                                    if ent_col is not None:
                                        col = df2[ent_col].astype(str).str.strip().str.lower()
                                        mask_entity = (col == 'product ad') | (col == 'ad')
                                    mask_record = pd.Series([False]*len(df2))
                                    if rec_col is not None:
                                        colr = df2[rec_col].astype(str).str.strip().str.lower()
                                        mask_record = (colr == 'ad') | (colr.str.contains('product ad'))
                                    mask_prod_ad = mask_entity | mask_record
                                    if mask_prod_ad.any():
                                        df2 = df2[mask_prod_ad]
                                except Exception:
                                    pass
                                if state_col is not None:
                                    try:
                                        df2 = df2[df2[state_col].astype(str).str.strip().str.lower() == 'enabled']
                                    except Exception:
                                        pass
                                if df2.empty:
                                    continue
                                mask_match = pd.Series([False]*len(df2))
                                if asin_col and asin_set:
                                    try:
                                        mask_match = mask_match | df2[asin_col].astype(str).str.strip().str.upper().isin(asin_set)
                                    except Exception:
                                        pass
                                if sku_col and sku_set:
                                    try:
                                        mask_match = mask_match | df2[sku_col].astype(str).str.strip().str.upper().isin(sku_set)
                                    except Exception:
                                        pass
                                df3 = df2[mask_match]
                                if df3.empty:
                                    continue
                                for _, rloc in df3.iterrows():
                                    camp = str(rloc.get(camp_col, '')).strip()
                                    if not camp:
                                        continue
                                    allowed_campaigns.add(camp)
                                    if ag_col and pd.notna(rloc.get(ag_col, None)):
                                        adg = str(rloc.get(ag_col, '')).strip()
                                        if adg:
                                            allowed_pairs.add((camp, adg))
                            return allowed_campaigns, allowed_pairs

                        # Fill structure
                        key = (product_sheet, campaign_name)
                        if key not in structure:
                            structure[key] = {'tactic': td, 'ad_groups': {}}
                        for ag_name in ad_groups_list:
                            if ag_name not in structure[key]['ad_groups']:
                                # Defer target assignment until after per-ad-group eligibility filtering
                                structure[key]['ad_groups'][ag_name] = {'product_ads': [], 'targets': [], 'negatives': negatives_for_tactic}
                            # Product ads (SP and SD). Size depends on mode
                            if ag_mode_prev in ['group_all_per_tactic','tactic_level','per_pg']:
                                for p in entries_for_group:
                                    pid = p['asin'] if id_choice=='ASIN' else p['sku']
                                    if pid:
                                        structure[key]['ad_groups'][ag_name]['product_ads'].append(pid)
                            elif ag_mode_prev == 'custom':
                                split = st.session_state.get('cc_custom_adgroup_dim', 'Tag 3')
                                if split in ('Single (one ad group)', 'Tactic'):
                                    for p in entries_for_group:
                                        pid = p['asin'] if id_choice=='ASIN' else p['sku']
                                        if pid:
                                            structure[key]['ad_groups'][ag_name]['product_ads'].append(pid)
                                elif split == 'Product (ASIN/SKU)':
                                    # ag_name is per-product in this branch, so attach single product by matching name prefix
                                    pid_from_ag = ag_name.split(' - ')[0]
                                    if pid_from_ag:
                                        structure[key]['ad_groups'][ag_name]['product_ads'].append(pid_from_ag)
                                else:
                                    # Tag 1/2/3 splits: include products whose tag matches this ad group
                                    idx_map = {'Tag 1':0,'Tag 2':1,'Tag 3':2}
                                    ti = idx_map.get(split, 2)
                                    for p in entries_for_group:
                                        tag_val = (p.get('tags') or [None,None,None])[ti] or 'Untagged'
                                        if str(tag_val)[:200] == ag_name:
                                            pid = p['asin'] if id_choice=='ASIN' else p['sku']
                                            if pid:
                                                structure[key]['ad_groups'][ag_name]['product_ads'].append(pid)
                            else:  # per_product
                                # ag_name corresponds to one product
                                pid = ag_name.split(' - ')[0]
                                if pid:
                                    structure[key]['ad_groups'][ag_name]['product_ads'].append(pid)

                            # Apply per-ad-group eligibility filtering for targets
                            try:
                                restrict_flag = st.session_state.get('cc_agg_restrict_adg', False)
                            except Exception:
                                restrict_flag = False

                            final_targets_for_ag = list(targets_for_tactic)
                            if restrict_flag and isinstance(_hist_df, pd.DataFrame) and not _hist_df.empty:
                                pids_here = [str(x).strip().upper() for x in structure[key]['ad_groups'][ag_name]['product_ads'] if str(x).strip()]
                                allow_camps_ag, allow_pairs_ag = _allowed_from_products_cc(pids_here)
                                if allow_camps_ag or allow_pairs_ag:
                                    allowed_pair_keys = {f"{c}||{a}" for (c,a) in allow_pairs_ag}
                                    mask_allowed = False
                                    if '_pair_key' in _hist_df.columns and allowed_pair_keys:
                                        mask_allowed = _hist_df['_pair_key'].isin(allowed_pair_keys)
                                    if 'Campaign' in _hist_df.columns and allow_camps_ag:
                                        mask_allowed = mask_allowed | _hist_df['Campaign'].astype(str).str.strip().isin(allow_camps_ag)
                                    _hist_allowed = _hist_df[mask_allowed] if isinstance(mask_allowed, pd.Series) else _hist_df.iloc[0:0]

                                    def _kw_norm_ap(s: str) -> str:
                                        t = _re.sub(r"[+]+", " ", str(s)) if '_re' in globals() else str(s)
                                        t = _re.sub(r"\s+", " ", t) if '_re' in globals() else t
                                        return t.strip().lower()

                                    filtered = []
                                    for _t in targets_for_tactic:
                                        if _t.get('type') == 'Keyword':
                                            _norm = _kw_norm_ap(_t.get('text',''))
                                            mk = False
                                            if '_norm_target' in _hist_allowed.columns:
                                                mk = mk | (_hist_allowed['_norm_target'] == _norm)
                                            if '_norm_search' in _hist_allowed.columns:
                                                mk = mk | (_hist_allowed['_norm_search'] == _norm)
                                            if bool(mk.any()) if isinstance(mk, pd.Series) else False:
                                                filtered.append(_t)
                                            continue
                                        expr = str(_t.get('expr',''))
                                        if not expr:
                                            filtered.append(_t)
                                            continue
                                        m_as = _re.search(r'B0[A-Za-z0-9]{8}', expr, flags=_re.I)
                                        if m_as:
                                            asin_v = m_as.group(0)
                                            mk2 = False
                                            if 'Target' in _hist_allowed.columns:
                                                mk2 = mk2 | _hist_allowed['Target'].astype(str).str.contains(asin_v, case=False, na=False)
                                            if 'Search Term' in _hist_allowed.columns:
                                                mk2 = mk2 | _hist_allowed['Search Term'].astype(str).str.contains(asin_v, case=False, na=False)
                                            if bool(mk2.any()) if isinstance(mk2, pd.Series) else False:
                                                filtered.append(_t)
                                            continue
                                        mk3 = False
                                        if 'Target' in _hist_allowed.columns:
                                            mk3 = mk3 | (_hist_allowed['Target'].astype(str).str.strip().str.lower() == expr.strip().lower())
                                        if 'Search Term' in _hist_allowed.columns:
                                            mk3 = mk3 | (_hist_allowed['Search Term'].astype(str).str.strip().str.lower() == expr.strip().lower())
                                        if bool(mk3.any()) if isinstance(mk3, pd.Series) else False:
                                            filtered.append(_t)
                                    final_targets_for_ag = filtered

                            structure[key]['ad_groups'][ag_name]['targets'] = final_targets_for_ag

                # Render accordion: Campaign expanders only (no nested expanders). Inside use selectbox + tabs.
                if structure:
                    for (product_sheet, campaign_name) in sorted(structure.keys(), key=lambda x: (x[0], x[1])):
                        with st.expander(f"{campaign_name}", expanded=False):
                            # Campaign-level Daily Budget override
                            _cb_map = st.session_state.get('cc_campaign_budgets', {})
                            _default_budget = _cb_map.get(campaign_name, st.session_state.get('cc_daily_budget', 10.0))
                            b1, b2 = st.columns([0.5, 1.5])
                            with b1:
                                _bud_key = f"cc_budget_{hash((product_sheet, campaign_name))}"
                                _new_budget = st.number_input("Daily Budget (USD)", min_value=1.0, value=float(_default_budget), step=1.0, key=_bud_key)
                            with b2:
                                st.caption("Campaign-level override for this campaign only.")
                            # Persist override
                            st.session_state['cc_campaign_budgets'][campaign_name] = _new_budget
                            ags = structure[(product_sheet, campaign_name)]['ad_groups']
                            ag_options = sorted(ags.keys())
                            ag_key = f"cc_prev_ag_{hash((product_sheet, campaign_name))}"
                            selected_ag = st.selectbox("Ad Group", ag_options, key=ag_key)
                            details = ags[selected_ag]
                            tab_pa, tab_tgt, tab_neg = st.tabs([f"Product Ads ({len(details['product_ads'])})", f"Targets ({len(details['targets'])})", f"Negative Targets ({len(details['negatives'])})"])
                            with tab_pa:
                                count_pa = len(details['product_ads'])
                                if count_pa == 0:
                                    st.caption("None")
                                else:
                                    sample = details['product_ads'][:50]
                                    st.write("Sample:")
                                    st.code("\n".join(sample))
                                    if count_pa > len(sample):
                                        st.caption(f"â€¦ and {count_pa - len(sample)} more")
                            with tab_tgt:
                                count_tgt = len(details['targets'])
                                if count_tgt == 0:
                                    st.caption("None")
                                else:
                                    lines = []
                                    for t in details['targets'][:50]:
                                        if t.get('type') == 'Keyword':
                                            lines.append(f"Keyword | {t['text']} | {t.get('match','broad')} | bid={t.get('bid')}")
                                        elif 'expr' in t:
                                            lines.append(f"{t['type']} | {t['expr']} | bid={t.get('bid')}")
                                        else:
                                            lines.append(str(t))
                                    st.code("\n".join(lines))
                                    if count_tgt > len(lines):
                                        st.caption(f"â€¦ and {count_tgt - len(lines)} more")
                            with tab_neg:
                                count_neg = len(details['negatives'])
                                if count_neg == 0:
                                    st.caption("None")
                                else:
                                    lines = []
                                    for t in details['negatives'][:50]:
                                        if t.get('type') == 'Negative Keyword':
                                            lines.append(f"Negative Keyword | {t['text']} | {t.get('match','negativePhrase')}")
                                        elif t.get('type') in ('Negative Product Targeting', 'Negative Product Targeting'):
                                            lines.append(f"Negative Product Targeting | {t['expr']}")
                                        else:
                                            lines.append(str(t))
                                    st.code("\n".join(lines))
                                    if count_neg > len(lines):
                                        st.caption(f"â€¦ and {count_neg - len(lines)} more")

            # ---- Save/Load Presets (embedded dropdown) ----
            st.markdown("---")
            with st.expander("Save/Load Presets", expanded=False):
                preset_name = st.text_input("Preset name", value=st.session_state.get('cc_preset_name',''), key='cc_preset_name')
                include_options = [
                    'Product Ads',
                    'Targets',
                    'Fallback Bid',
                    'Daily Budget',
                    'Tactics',
                    'Bidding Strategy',
                    'Placement Modifiers',
                ]
                includes = st.multiselect("Include in preset", include_options, default=include_options, key='cc_preset_includes')

                # Ensure presets container in client config
                c_cfg = st.session_state.get('client_config', {}) or {}
                if 'campaign_creation_presets' not in c_cfg:
                    c_cfg['campaign_creation_presets'] = {}
                st.session_state.client_config = c_cfg

                pcs1, pcs2, pcs3 = st.columns([1.1, 1.4, 0.9])
                with pcs1:
                    if st.button("Save preset", key="cc_preset_save2"):
                        if preset_name.strip():
                            data = {'includes': includes}
                            if 'Product Ads' in includes:
                                try:
                                    data['products'] = st.session_state.cc_products.to_dict('records') if hasattr(st.session_state, 'cc_products') else []
                                except Exception:
                                    data['products'] = []
                            if 'Targets' in includes:
                                try:
                                    data['targets'] = st.session_state.cc_targets.to_dict('records') if hasattr(st.session_state, 'cc_targets') else []
                                except Exception:
                                    data['targets'] = []
                            if 'Fallback Bid' in includes:
                                data['fallback_bid'] = st.session_state.get('cc_fallback_bid')
                            if 'Daily Budget' in includes:
                                data['daily_budget'] = st.session_state.get('cc_daily_budget')
                            if 'Tactics' in includes:
                                data['selected_tactics'] = st.session_state.get('cc_selected_tactics', [])
                            if 'Bidding Strategy' in includes:
                                data['sp_bidding_strategy'] = st.session_state.get('cc_sp_bidding_strategy')
                            if 'Placement Modifiers' in includes:
                                data['sp_place_top'] = st.session_state.get('cc_sp_place_top', 0)
                                data['sp_place_pp'] = st.session_state.get('cc_sp_place_pp', 0)
                                data['sp_place_ros'] = st.session_state.get('cc_sp_place_ros', 0)
                                data['sp_place_ab'] = st.session_state.get('cc_sp_place_ab', 0)

                            st.session_state.client_config['campaign_creation_presets'][preset_name.strip()] = data
                            try:
                                save_client_config(st.session_state.get('selected_client_name', 'unknown'), st.session_state.client_config)
                                st.success(f"Saved preset '{preset_name.strip()}'")
                            except Exception as e:
                                st.error(f"Failed to save preset: {e}")
                        else:
                            st.warning("Enter a preset name first.")
                with pcs2:
                    available_presets = sorted(list(st.session_state.client_config.get('campaign_creation_presets', {}).keys()))
                    st.selectbox("Load preset", options=[""] + available_presets, index=0, key="cc_preset_load_choice2")
                with pcs3:
                    if st.button("Load", key="cc_preset_load2"):
                        name = st.session_state.get('cc_preset_load_choice2', '')
                        if name and name in st.session_state.client_config.get('campaign_creation_presets', {}):
                            preset = st.session_state.client_config['campaign_creation_presets'][name]
                            inc = preset.get('includes', [])
                            # Apply only what was saved in this preset
                            if 'Product Ads' in inc and 'products' in preset:
                                try:
                                    rows = preset.get('products', [])
                                    cols = ['ASIN','SKU','Product Title','Category','Tag 1','Tag 2','Tag 3','Product Group']
                                    st.session_state.cc_products = pd.DataFrame(rows) if rows else pd.DataFrame(columns=cols)
                                except Exception:
                                    st.session_state.cc_products = pd.DataFrame(columns=['ASIN','SKU','Product Title','Category','Tag 1','Tag 2','Tag 3','Product Group'])
                            if 'Targets' in inc and 'targets' in preset:
                                try:
                                    rows = preset.get('targets', [])
                                    st.session_state.cc_targets = pd.DataFrame(rows) if rows else pd.DataFrame(columns=['Text','Kind','Branding','Source','Bid'])
                                except Exception:
                                    st.session_state.cc_targets = pd.DataFrame(columns=['Text','Kind','Branding','Source','Bid'])
                            if 'Fallback Bid' in inc and 'fallback_bid' in preset:
                                st.session_state['cc_fallback_bid'] = preset.get('fallback_bid', st.session_state.get('cc_fallback_bid'))
                            if 'Daily Budget' in inc and 'daily_budget' in preset:
                                st.session_state['cc_daily_budget'] = preset.get('daily_budget', st.session_state.get('cc_daily_budget'))
                            if 'Tactics' in inc and 'selected_tactics' in preset:
                                st.session_state['cc_selected_tactics'] = preset.get('selected_tactics', [])
                            if 'Bidding Strategy' in inc and 'sp_bidding_strategy' in preset:
                                st.session_state['cc_sp_bidding_strategy'] = preset.get('sp_bidding_strategy')
                            if 'Placement Modifiers' in inc:
                                st.session_state['cc_sp_place_top'] = preset.get('sp_place_top', 0)
                                st.session_state['cc_sp_place_pp'] = preset.get('sp_place_pp', 0)
                                st.session_state['cc_sp_place_ros'] = preset.get('sp_place_ros', 0)
                                st.session_state['cc_sp_place_ab'] = preset.get('sp_place_ab', 0)
                            st.success(f"Loaded preset '{name}'")
                        else:
                            st.warning("Choose a preset to load.")

            # ---- Generate Campaigns Button ----
            st.markdown("---")
            # Live: compute campaigns without targets count based on current selections
            def _compute_zero_target_campaigns():
                try:
                    # Build product entries from Selected Products table
                    product_entries_tmp = []
                    if not st.session_state.cc_products.empty:
                        dfp_tmp = st.session_state.cc_products.copy()
                        dfp_tmp[id_col] = dfp_tmp[id_col].astype(str)
                        for _, row in dfp_tmp.iterrows():
                            pid = str(row.get(id_col, '')).strip()
                            product_entries_tmp.append({
                                'asin': pid if id_choice=='ASIN' else '',
                                'sku': pid if id_choice=='SKU' else '',
                                'title': row.get('Product Title',''),
                                'group': row.get('Product Group',''),
                                'category': (row.get('Category','') or (root_phrase or 'General')),
                                'tags': [row.get('Tag 1',''), row.get('Tag 2',''), row.get('Tag 3','')],
                            })

                    # Targets prepared from Selected Targets table
                    def _sel(df, branding, kind):
                        if df.empty:
                            return []
                        m = (df['Branding'] == branding) & (df['Kind'] == kind)
                        out = []
                        for _, row in df[m].iterrows():
                            txt = str(row['Text']).strip()
                            bid = row['Bid'] if pd.notna(row['Bid']) and row['Bid'] else st.session_state.get('cc_fallback_bid', 0.5)
                            out.append({'text': txt, 'bid': bid})
                        return out

                    brand_kw_tmp = _sel(st.session_state.cc_targets, 'Brand', 'Keyword')
                    brand_asin_tmp = _sel(st.session_state.cc_targets, 'Brand', 'ASIN')
                    non_kw_tmp = _sel(st.session_state.cc_targets, 'Non', 'Keyword')
                    non_asin_tmp = _sel(st.session_state.cc_targets, 'Non', 'ASIN')
                    non_cat_tmp = _sel(st.session_state.cc_targets, 'Non', 'Category')

                    # Grouping helper (mirror of generator)
                    def _group_products(entries, mode):
                        groups_local = []
                        if mode == 'group_all_per_tactic':
                            groups_local.append({'category': (root_phrase or 'General'), 'tags': [], 'products': list(entries)})
                            return groups_local
                        if mode == 'per_product':
                            for p in entries:
                                groups_local.append({'category': p.get('category') or (root_phrase or 'General'), 'tags': p.get('tags') or [], 'products': [p]})
                            return groups_local
                        if mode == 'per_pg':
                            buckets = {}
                            for p in entries:
                                cat = (p.get('category') or (root_phrase or 'General')).strip()
                                buckets.setdefault(cat, []).append(p)
                            for cat, plist in buckets.items():
                                groups_local.append({'category': cat, 'tags': [], 'products': plist})
                            return groups_local
                        buckets = {}
                        for p in entries:
                            cat = (p.get('category') or (root_phrase or 'General')).strip()
                            tags_tuple = tuple([t for t in (p.get('tags') or []) if t])
                            key = (cat, tags_tuple)
                            buckets.setdefault(key, []).append(p)
                        for (cat, tags_tuple), plist in buckets.items():
                            groups_local.append({'category': cat, 'tags': list(tags_tuple), 'products': plist})
                        return groups_local

                    tactic_defs_tmp = {
                        "SP Branded Research": {"ad":"SP","brand":"Brand","ttype":"Keyword","match":"broad","label":"Research"},
                        "SP Branded Performance": {"ad":"SP","brand":"Brand","ttype":"Keyword","match":"exact","label":"Performance"},
                        "SP Branded ASIN Targeting": {"ad":"SP","brand":"Brand","ttype":"ASIN","label":"ASIN Targeting"},
                        "SD Branded ASIN Targeting": {"ad":"SD","brand":"Brand","ttype":"ASIN","label":"ASIN Targeting"},
                        "SP Non-Branded Auto": {"ad":"SP","brand":"Non","ttype":"Auto","label":"Auto"},
                        "SP Non-Branded Research": {"ad":"SP","brand":"Non","ttype":"Keyword","match":"broad","label":"Research"},
                        "SP Non-Branded Performance": {"ad":"SP","brand":"Non","ttype":"Keyword","match":"exact","label":"Performance"},
                        "SP Non-Branded ASIN Targeting": {"ad":"SP","brand":"Non","ttype":"ASIN","label":"ASIN Targeting"},
                        "SD Non-Branded ASIN Targeting": {"ad":"SD","brand":"Non","ttype":"ASIN","label":"ASIN Targeting"},
                    }

                    zero_count = 0
                    groups_local = _group_products(product_entries_tmp, ag_mode)
                    for g in groups_local:
                        for tactic in selected_tactics:
                            td = tactic_defs_tmp[tactic]
                            # Auto will always create default targets for SP
                            if td['ttype'] == 'Auto':
                                continue
                            if td['ttype'] == 'Keyword':
                                lst = brand_kw_tmp if td['brand']=='Brand' else non_kw_tmp
                            else:  # ASIN
                                if td['brand'] == 'Brand':
                                    lst = brand_asin_tmp
                                else:
                                    # Include Category targets for Non-Branded ASIN tactics
                                    lst = list(non_asin_tmp) + list(non_cat_tmp)
                            if len(lst) == 0:
                                zero_count += 1
                    return zero_count
                except Exception:
                    return 0

            zero_target_count = _compute_zero_target_campaigns()
            if 'cc_allow_zero_targets' not in st.session_state:
                st.session_state.cc_allow_zero_targets = (zero_target_count == 0)
            st.checkbox(
                f"Allow launching campaigns with zero targets ({zero_target_count} found)",
                key="cc_allow_zero_targets",
                help="When enabled, campaigns without targets will still be created and include only Product Ads."
            )

            # Exclusion controls for negatives (placed above enqueue button)
            st.markdown("---")
            st.subheader("Exclude Negatives")

            # Predict the set of campaign names that will be created with current selections
            def _predict_campaign_names():
                try:
                    groups_preview = _group_products(product_entries_tmp, ag_mode)
                    tactic_defs_preview = tactic_defs_tmp
                    dims = st.session_state.get('cc_custom_campaign_dims', ["Product Group", "Tag 1", "Tag 2", "Tactic"]) or []
                    names = []
                    for g in groups_preview:
                        category = g.get('category') or (root_phrase or 'General')
                        tags = [t for t in (g.get('tags') or []) if t]
                        for tactic in (selected_tactics or []):
                            if tactic not in tactic_defs_preview:
                                continue
                            td = tactic_defs_preview[tactic]
                            ad_prefix = 'SP' if td.get('ad') == 'SP' else 'SD'
                            label = td.get('label','')
                            include_tactic = (ag_mode != 'tactic_level')
                            if ag_mode == 'custom':
                                include_tactic = ('Tactic' in dims)
                            parts = [ad_prefix, td.get('brand','Brand'), category]
                            parts.extend(tags)
                            if include_tactic and label:
                                parts.append(label)
                            parts.append('BW')
                            names.append(" | ".join([str(p).strip() for p in parts if str(p).strip()]))
                    return sorted(set(names))
                except Exception:
                    return []

            predicted_campaigns = _predict_campaign_names()

            # Selector 1: which negative entity types to exclude
            neg_entity_labels = st.multiselect(
                "Exclude Negatives for Entity",
                options=["Negative Keywords", "Negative Product Targeting"],
                default=[],
                help="Choose which negative row types to exclude from the bulk file."
            )
            # Map display labels to internal entity names used in the bulk rows
            label_to_entity = {
                "Negative Keywords": "Negative Keyword",
                "Negative Product Targeting": "Negative Product Targeting",
            }
            st.session_state['cc_exclude_negative_entities'] = [label_to_entity[l] for l in neg_entity_labels if l in label_to_entity]

            # Selector 2: which campaigns to exclude negatives from
            # Add a 'Select all' toggle that pre-selects all predicted campaigns
            _prev_selected = st.session_state.get('cc_exclude_negative_campaigns', [])
            _all_selected_default = bool(predicted_campaigns) and set(_prev_selected) == set(predicted_campaigns)
            _select_all = st.checkbox(
                "Select all campaigns",
                value=_all_selected_default,
                key="cc_exclude_neg_select_all",
                help="Quickly select all predicted campaigns in the list below."
            )
            # Set session state BEFORE widget creation and DO NOT pass 'default' to avoid Streamlit warning
            if _select_all:
                st.session_state['cc_exclude_negative_campaigns'] = list(predicted_campaigns)
            else:
                # Keep user's current picks but clamp to available options
                st.session_state['cc_exclude_negative_campaigns'] = [x for x in _prev_selected if x in predicted_campaigns]

            _ = st.multiselect(
                "Exclude Negatives in Campaign",
                options=predicted_campaigns,
                key='cc_exclude_negative_campaigns',
                help=("Select specific campaign(s) above to exclude the chosen negative entity type(s). "
                      "This list is derived from your current products/tactics.")
            )

            if st.button("Add Campaigns to Bulk Export", type="primary", use_container_width=True):
                if not selected_tactics:
                    st.warning("Select at least one tactic.")
                    st.stop()

                # Enforce zero-target confirmation
                zero_target_count_click = _compute_zero_target_campaigns()
                if zero_target_count_click > 0 and not st.session_state.get('cc_allow_zero_targets', False):
                    st.warning(f"{zero_target_count_click} campaign(s) have no targets. Enable the checkbox to proceed or add targets.")
                    st.stop()

                # Build product entries from Selected Products table
                product_entries = []
                if not st.session_state.cc_products.empty:
                    dfp = st.session_state.cc_products.copy()
                    dfp[id_col] = dfp[id_col].astype(str)
                    for _, row in dfp.iterrows():
                        pid = str(row.get(id_col, '')).strip()
                        product_entries.append({
                            'asin': pid if id_choice=='ASIN' else '',
                            'sku': pid if id_choice=='SKU' else '',
                            'title': row.get('Product Title',''),
                            'group': row.get('Product Group',''),
                            'category': (row.get('Category','') or (root_phrase or 'General')),
                            'tags': [row.get('Tag 1',''), row.get('Tag 2',''), row.get('Tag 3','')],
                        })

                # Prepare targets by branding and type
                def _sel(df, branding, kind):
                    if df.empty:
                        return []
                    m = (df['Branding'] == branding) & (df['Kind'] == kind)
                    out = []
                    for _, row in df[m].iterrows():
                        txt = str(row['Text']).strip()
                        bid = row['Bid'] if pd.notna(row['Bid']) and row['Bid'] else fallback_bid
                        scope_groups = str(row.get('Scope Product Groups','')).strip()
                        scope_tags = str(row.get('Scope Tags','')).strip()
                        out.append({'text': txt, 'bid': bid, 'scope_groups': scope_groups, 'scope_tags': scope_tags})
                    return out

                brand_kw = _sel(st.session_state.cc_targets, 'Brand', 'Keyword')
                brand_asin = _sel(st.session_state.cc_targets, 'Brand', 'ASIN')
                non_kw = _sel(st.session_state.cc_targets, 'Non', 'Keyword')
                non_asin = _sel(st.session_state.cc_targets, 'Non', 'ASIN')
                non_cat = _sel(st.session_state.cc_targets, 'Non', 'Category')

                # Helper to add action row
                def push(row: dict, product_sheet: str):
                    # Honor exclusions for negative rows
                    try:
                        excl_entities = set(st.session_state.get('cc_exclude_negative_entities', []))
                        excl_campaigns = set(st.session_state.get('cc_exclude_negative_campaigns', []))
                        entity = str(row.get('Entity',''))
                        camp_id = str(row.get('Campaign ID',''))
                        # Normalize product-target negative naming so UI selection matches internal variants
                        norm_entity = ("Negative Product Targeting" if entity in ("Negative Product Targeting", "Negative Product Target") else entity)
                        # Determine if row is a negative type we can exclude
                        is_negative = norm_entity in ("Negative Keyword", "Negative Product Targeting")
                        if is_negative and (norm_entity in excl_entities):
                            # If 'Select all campaigns' is enabled, exclude regardless of specific campaign list
                            all_selected = bool(st.session_state.get('cc_exclude_neg_select_all', False))
                            if all_selected:
                                return  # Skip all negatives of selected types
                            # Otherwise, exclude only when this row's Campaign ID is among the selected campaigns
                            if camp_id and (camp_id in excl_campaigns):
                                return  # Skip enqueuing this negative row
                    except Exception:
                        pass
                    action = row.copy()
                    action['action_type'] = 'campaign_creation'
                    action['sheet_type'] = f"{product_sheet} Campaigns"
                    st.session_state.bulk_export_actions.append(action)

                # Campaign name builder
                def build_campaign_name(ad_prefix: str, brand_tag: str, category: str, tags: list, tactic_label: str, include_tactic: bool = True):
                    parts = [ad_prefix, brand_tag, category]
                    for t in tags:
                        if t:
                            parts.append(str(t).strip())
                    if include_tactic:
                        parts.append(tactic_label)
                    parts.append('BW')
                    return " | ".join(parts)

                # Build grouping keys based on CSV vs Root
                def group_products(entries, mode):
                    groups = []
                    # All products together: one group, category uses root phrase, ignore product tags/groups
                    if mode == 'group_all_per_tactic':
                        groups.append({'category': (root_phrase or 'General'), 'tags': [], 'products': list(entries)})
                        return groups
                    # One group per product
                    if mode == 'per_product':
                        for p in entries:
                            groups.append({'category': p.get('category') or (root_phrase or 'General'), 'tags': p.get('tags') or [], 'products': [p]})
                        return groups
                    # One group per product group (category only)
                    if mode == 'per_pg':
                        buckets = {}
                        for p in entries:
                            cat = (p.get('category') or (root_phrase or 'General')).strip()
                            buckets.setdefault(cat, []).append(p)
                        for cat, plist in buckets.items():
                            groups.append({'category': cat, 'tags': [], 'products': plist})
                        return groups
                    if mode == 'custom':
                        dims = st.session_state.get('cc_custom_campaign_dims', ["Product Group", "Tag 1", "Tag 2", "Tactic"]) or []
                        def _key_for(p):
                            cat = (p.get('category') or (root_phrase or 'General')).strip()
                            t1, t2, t3 = (p.get('tags') or [None, None, None])[:3]
                            parts = []
                            for d in dims:
                                if d == 'Product Group':
                                    parts.append(('PG', cat))
                                elif d == 'Tag 1':
                                    parts.append(('T1', (t1 or '').strip()))
                                elif d == 'Tag 2':
                                    parts.append(('T2', (t2 or '').strip()))
                                elif d == 'Tag 3':
                                    parts.append(('T3', (t3 or '').strip()))
                                # ignore Tactic here
                            return tuple(parts)
                        buckets = {}
                        for p in entries:
                            buckets.setdefault(_key_for(p), []).append(p)
                        for key, plist in buckets.items():
                            cat_val = next((v for k, v in key if k == 'PG'), (root_phrase or 'General'))
                            tag_vals = []
                            for k, v in key:
                                if k in ('T1','T2','T3') and v:
                                    tag_vals.append(v)
                            groups.append({'category': cat_val, 'tags': tag_vals, 'products': plist})
                        return groups
                    # Default: group by (category, tags)
                    buckets = {}
                    for p in entries:
                        cat = (p.get('category') or (root_phrase or 'General')).strip()
                        tags_tuple = tuple([t for t in (p.get('tags') or []) if t])
                        key = (cat, tags_tuple)
                        buckets.setdefault(key, []).append(p)
                    for (cat, tags_tuple), plist in buckets.items():
                        groups.append({'category': cat, 'tags': list(tags_tuple), 'products': plist})
                    return groups

                tactic_defs = {
                    "SP Branded Research": {"ad":"SP","brand":"Brand","ttype":"Keyword","match":"broad","label":"Research"},
                    "SP Branded Performance": {"ad":"SP","brand":"Brand","ttype":"Keyword","match":"exact","label":"Performance"},
                    "SP Branded ASIN Targeting": {"ad":"SP","brand":"Brand","ttype":"ASIN","label":"ASIN Targeting"},
                    "SD Branded ASIN Targeting": {"ad":"SD","brand":"Brand","ttype":"ASIN","label":"ASIN Targeting"},
                    "SP Non-Branded Auto": {"ad":"SP","brand":"Non","ttype":"Auto","label":"Auto"},
                    "SP Non-Branded Research": {"ad":"SP","brand":"Non","ttype":"Keyword","match":"broad","label":"Research"},
                    "SP Non-Branded Performance": {"ad":"SP","brand":"Non","ttype":"Keyword","match":"exact","label":"Performance"},
                    "SP Non-Branded ASIN Targeting": {"ad":"SP","brand":"Non","ttype":"ASIN","label":"ASIN Targeting"},
                    "SD Non-Branded ASIN Targeting": {"ad":"SD","brand":"Non","ttype":"ASIN","label":"ASIN Targeting"},
                }

                # Loop through groups and tactics
                groups = group_products(product_entries, ag_mode)
                existing, existing_ag = _extract_existing_campaigns_from_bulk(st.session_state.get('bulk_data'))
                merge_mode = bool(st.session_state.get('cc_merge_into_existing', False))
                merged_campaigns = set()

                start_idx = len(st.session_state.bulk_export_actions)
                for group in groups:
                    category = group['category']
                    tags = group['tags']
                    entries_for_group = group['products']
                    for tactic in selected_tactics:
                        td = tactic_defs[tactic]
                        # Respect SD limitations (safety): SD supports only ASIN Targeting
                        if td['ad'] == 'SD' and td['ttype'] != 'ASIN':
                            continue

                        ad_prefix = 'SP' if td['ad']=='SP' else 'SD'
                        product_sheet = 'Sponsored Products' if ad_prefix=='SP' else 'Sponsored Display'
                        brand_tag = td['brand']
                        tactic_label = td['label']
                        targeting_type_val = 'Auto' if td['ttype']=='Auto' else 'Manual'
                        # Include tactic in campaign name except when tactic-level grouping is selected
                        include_tactic_in_name = (ag_mode != 'tactic_level')
                        if ag_mode == 'custom':
                            include_tactic_in_name = ('Tactic' in st.session_state.get('cc_custom_campaign_dims', []))
                        camp_name = build_campaign_name(ad_prefix, brand_tag, category, tags, tactic_label, include_tactic=include_tactic_in_name)
                        # Allowed Product Group names for scope matching: group's Category plus any Product Group labels among entries
                        try:
                            allowed_pg_names = {str(category).strip()} | {str(p.get('group','')).strip() for p in entries_for_group if str(p.get('group','')).strip()}
                        except Exception:
                            allowed_pg_names = {str(category).strip()}

                        # Campaign row (skip if merging into existing and campaign exists)
                        campaign_exists = camp_name in existing.get(product_sheet, set())
                        if not (merge_mode and campaign_exists):
                            push({
                                'Product': product_sheet,
                                'Entity': 'Campaign',
                                'Operation': 'Create',
                                'Campaign ID': camp_name,
                                'Campaign Name': camp_name,
                                'Start Date': start_date_str,
                                'Targeting Type': targeting_type_val,
                                'State': 'enabled',
                                'Daily Budget': st.session_state.get('cc_campaign_budgets', {}).get(camp_name, daily_budget),
                                **({'Bidding Strategy': st.session_state.get('cc_sp_bidding_strategy')} if product_sheet=='Sponsored Products' else {}),
                                **({'Tactic': 'T00030'} if product_sheet=='Sponsored Display' else {})
                            }, product_sheet)
                            # Placement Modifiers for SP campaigns
                            if product_sheet == 'Sponsored Products':
                                placements = [
                                    ('Placement Top', st.session_state.get('cc_sp_place_top', 0)),
                                    ('Placement Product Page', st.session_state.get('cc_sp_place_pp', 0)),
                                    ('Placement Rest Of Search', st.session_state.get('cc_sp_place_ros', 0)),
                                    ('Placement Amazon Business', st.session_state.get('cc_sp_place_ab', 0)),
                                ]
                                for pname, pct in placements:
                                    # Omit 0% placement rows
                                    try:
                                        pct_val = float(pct) if pct is not None else 0.0
                                    except Exception:
                                        pct_val = 0.0
                                    if pct_val > 0:
                                        push({
                                            'Product': product_sheet,
                                            'Entity': 'Bidding Adjustment',
                                            'Operation': 'Create',
                                            'Campaign ID': camp_name,
                                            'Bidding Strategy': st.session_state.get('cc_sp_bidding_strategy'),
                                            'Placement': pname,
                                            'Percentage': pct_val
                                        }, product_sheet)
                        else:
                            merged_campaigns.add(camp_name)

                        # Determine ad group names
                        if ag_mode in ['group_all_per_tactic', 'per_pg']:
                            ag_names = [camp_name]
                        elif ag_mode == 'tactic_level':
                            ag_names = [tactic_label]
                        elif ag_mode == 'custom':
                            split = st.session_state.get('cc_custom_adgroup_dim', 'Tag 3')
                            if split == 'Single (one ad group)':
                                ag_names = [camp_name]
                            elif split == 'Tactic':
                                ag_names = [tactic_label]
                            elif split == 'Product (ASIN/SKU)':
                                ag_names = []  # per product name handled later
                            else:
                                # Tag 1/2/3 buckets from products in this group
                                idx_map = {'Tag 1':0,'Tag 2':1,'Tag 3':2}
                                ti = idx_map.get(split, 2)
                                vals = sorted({(p.get('tags') or [None,None,None])[ti] or 'Untagged' for p in entries_for_group})
                                ag_names = [str(v)[:200] for v in vals]
                        else:
                            ag_names = []  # will build per product

                        # Create ad groups (skip if merging and campaign exists; choose existing ad group if needed)
                        ag_targets_list = []  # collect ad groups that should receive targets
                        if ag_mode in ['group_all_per_tactic','tactic_level','per_pg']:
                            desired_ag_name = ag_names[0]
                            if merge_mode and campaign_exists:
                                # Use desired ad group if it exists, else fallback to first available in campaign
                                existing_ag_set = existing_ag.get(product_sheet, {}).get(camp_name, set())
                                ag_name = desired_ag_name if desired_ag_name in existing_ag_set else (sorted(existing_ag_set)[0] if existing_ag_set else desired_ag_name)
                            else:
                                ag_name = desired_ag_name
                                push({
                                    'Product': product_sheet,
                                    'Entity': 'Ad Group',
                                    'Operation': 'Create',
                                    'Campaign ID': camp_name,
                                    'Ad Group ID': ag_name,
                                    'Campaign Name': camp_name,
                                    'Ad Group Name': ag_name,
                                    'State': 'enabled',
                                    'Ad Group Default Bid': ag_default_bid,
                                    **({'Tactic': 'T00030'} if product_sheet=='Sponsored Display' else {})
                                }, product_sheet)
                            ag_targets_list = [ag_name]

                            # Product Ads for SP and SD
                            for p in entries_for_group:
                                pid = p['asin'] if id_choice=='ASIN' else p['sku']
                                if not pid:
                                    continue
                                push({
                                    'Product': product_sheet,
                                    'Entity': 'Product Ad',
                                    'Operation': 'Create',
                                    'Campaign ID': camp_name,
                                    'Ad Group ID': ag_name,
                                    id_choice: pid,
                                    'State': 'enabled'
                                }, product_sheet)
                        elif ag_mode == 'custom':
                            split = st.session_state.get('cc_custom_adgroup_dim', 'Tag 3')
                            if split in ('Single (one ad group)','Tactic'):
                                desired_ag_name = camp_name if split == 'Single (one ad group)' else tactic_label
                                if merge_mode and campaign_exists:
                                    existing_ag_set = existing_ag.get(product_sheet, {}).get(camp_name, set())
                                    ag_name = desired_ag_name if desired_ag_name in existing_ag_set else (sorted(existing_ag_set)[0] if existing_ag_set else desired_ag_name)
                                else:
                                    ag_name = desired_ag_name
                                    push({
                                        'Product': product_sheet,
                                        'Entity': 'Ad Group',
                                        'Operation': 'Create',
                                        'Campaign ID': camp_name,
                                        'Ad Group ID': ag_name,
                                        'Campaign Name': camp_name,
                                        'Ad Group Name': ag_name,
                                        'State': 'enabled',
                                        'Ad Group Default Bid': ag_default_bid,
                                        **({'Tactic': 'T00030'} if product_sheet=='Sponsored Display' else {})
                                    }, product_sheet)
                                ag_targets_list = [ag_name]
                                for p in entries_for_group:
                                    pid = p['asin'] if id_choice=='ASIN' else p['sku']
                                    if not pid:
                                        continue
                                    push({
                                        'Product': product_sheet,
                                        'Entity': 'Product Ad',
                                        'Operation': 'Create',
                                        'Campaign ID': camp_name,
                                        'Ad Group ID': ag_name,
                                        id_choice: pid,
                                        'State': 'enabled'
                                    }, product_sheet)
                            elif split == 'Product (ASIN/SKU)':
                                # Per-product handled later in dedicated branch
                                ag_targets_list = []
                            else:
                                # Tag 1/2/3 buckets from products in this group
                                idx_map = {'Tag 1':0,'Tag 2':1,'Tag 3':2}
                                ti = idx_map.get(split, 2)
                                vals = sorted({(p.get('tags') or [None,None,None])[ti] or 'Untagged' for p in entries_for_group})
                                ag_targets_list = []
                                for v in vals:
                                    ag_name = str(v)[:200]
                                    if merge_mode and campaign_exists:
                                        existing_ag_set = existing_ag.get(product_sheet, {}).get(camp_name, set())
                                        ag_to_use = ag_name if ag_name in existing_ag_set else (sorted(existing_ag_set)[0] if existing_ag_set else ag_name)
                                    else:
                                        ag_to_use = ag_name
                                        push({
                                            'Product': product_sheet,
                                            'Entity': 'Ad Group',
                                            'Operation': 'Create',
                                            'Campaign ID': camp_name,
                                            'Ad Group ID': ag_to_use,
                                            'Campaign Name': camp_name,
                                            'Ad Group Name': ag_to_use,
                                            'State': 'enabled',
                                            'Ad Group Default Bid': ag_default_bid,
                                            **({'Tactic': 'T00030'} if product_sheet=='Sponsored Display' else {})
                                        }, product_sheet)
                                    ag_targets_list.append(ag_to_use)
                                    # Add product ads for this tag bucket
                                    for p in entries_for_group:
                                        tag_val = (p.get('tags') or [None,None,None])[ti] or 'Untagged'
                                        if str(tag_val)[:200] != ag_to_use:
                                            continue
                                        pid = p['asin'] if id_choice=='ASIN' else p['sku']
                                        if not pid:
                                            continue
                                        push({
                                            'Product': product_sheet,
                                            'Entity': 'Product Ad',
                                            'Operation': 'Create',
                                            'Campaign ID': camp_name,
                                            'Ad Group ID': ag_to_use,
                                            id_choice: pid,
                                            'State': 'enabled'
                                        }, product_sheet)
                        if False:
                            ag_targets_list = []  # per-product handled below (disabled block)

                            # Add targets to all created ad groups in this campaign (disabled here)
                            for ag_name in ag_targets_list:
                                if td['ttype'] == 'Keyword':
                                    kw_list = brand_kw if brand_tag=='Brand' else non_kw
                                    for k in kw_list:
                                        # Scope filters
                                        sg = {x.strip() for x in (k.get('scope_groups','') or '').split(',') if x.strip() and x.strip().lower() not in ('all','none','nan')}
                                        stags = {x.strip() for x in (k.get('scope_tags','') or '').split(',') if x.strip() and x.strip().lower() not in ('all','none','nan')}
                                        if sg and category not in sg:
                                            continue
                                        if stags and not (stags.intersection(set(tags))):
                                            continue
                                        push({
                                            'Product': product_sheet,
                                            'Entity': 'Keyword',
                                            'Operation': 'Create',
                                            'Campaign ID': camp_name,
                                            'Ad Group ID': ag_name,
                                            'Keyword Text': k['text'],
                                            'Match Type': td.get('match','broad'),
                                            'Bid': k['bid'],
                                            'State': 'enabled'
                                        }, product_sheet)
                                elif td['ttype'] == 'ASIN':
                                    asin_list = brand_asin if brand_tag=='Brand' else non_asin
                                    if product_sheet == 'Sponsored Products':
                                        for a in asin_list:
                                            m = re.search(r'B0[A-Za-z0-9]{8}', a['text'], flags=re.I)
                                            if not m:
                                                continue
                                            expr = f"asin=\"{m.group(0).upper()}\""
                                            # Scope filters
                                            sg = {x.strip() for x in (a.get('scope_groups','') or '').split(',') if x.strip() and x.strip().lower() not in ('all','none','nan')}
                                            stags = {x.strip() for x in (a.get('scope_tags','') or '').split(',') if x.strip() and x.strip().lower() not in ('all','none','nan')}
                                            if sg and category not in sg:
                                                continue
                                            if stags and not (stags.intersection(set(tags))):
                                                continue
                                            push({
                                                'Product': product_sheet,
                                                'Entity': 'Product Targeting',
                                                'Operation': 'Create',
                                                'Campaign ID': camp_name,
                                                'Ad Group ID': ag_name,
                                                'Product Targeting Expression': expr,
                                                'Bid': a['bid'],
                                                'State': 'enabled'
                                            }, product_sheet)
                                        # Include Category targets only for Non-Branded SP ASIN Targeting
                                        if brand_tag == 'Non' and non_cat:
                                            for c in non_cat:
                                                push({
                                                    'Product': product_sheet,
                                                    'Entity': 'Product Targeting',
                                                    'Operation': 'Create',
                                                    'Campaign ID': camp_name,
                                                    'Ad Group ID': ag_name,
                                                    'Product Targeting Expression': c['text'],
                                                    'Bid': c['bid'],
                                                    'State': 'enabled'
                                                }, product_sheet)
                                    else:
                                        for a in asin_list:
                                            m = re.search(r'B0[A-Za-z0-9]{8}', a['text'], flags=re.I)
                                            if not m:
                                                continue
                                            expr = f"asin=\"{m.group(0).upper()}\""
                                            # Scope filters
                                            sg = {x.strip() for x in (a.get('scope_groups','') or '').split(',') if x.strip() and x.strip().lower() not in ('all','none','nan')}
                                            stags = {x.strip() for x in (a.get('scope_tags','') or '').split(',') if x.strip() and x.strip().lower() not in ('all','none','nan')}
                                            if sg and category not in sg:
                                                continue
                                            if stags and not (stags.intersection(set(tags))):
                                                continue
                                            push({
                                                'Product': product_sheet,
                                                'Entity': 'Contextual Targeting',
                                                'Operation': 'Create',
                                                'Campaign ID': camp_name,
                                                'Ad Group ID': ag_name,
                                                'Targeting Expression': expr,
                                                'Bid': a['bid'],
                                                'State': 'enabled',
                                                'Tactic': 'T00030'
                                            }, product_sheet)
                                elif td['ttype'] == 'Auto' and product_sheet == 'Sponsored Products':
                                    for expr in ["close-match","loose-match","complements","substitutes"]:
                                        push({
                                            'Product': product_sheet,
                                            'Entity': 'Product Targeting',
                                            'Operation': 'Create',
                                            'Campaign ID': camp_name,
                                            'Ad Group ID': ag_name,
                                            'Product Targeting Expression': expr,
                                            'Bid': fallback_bid,
                                            'State': 'enabled'
                                        }, product_sheet)

                                # Default negatives for SP Non-Branded Auto (terms + ASINs)
                                if brand_tag == 'Non':
                                    client_cfg_nb2 = st.session_state.get('client_config', {}) or {}
                                    for term in client_cfg_nb2.get('branded_keywords', []) or []:
                                        t = str(term).strip()
                                        if t:
                                            push({
                                                'Product': product_sheet,
                                                'Entity': 'Negative Keyword',
                                                'Operation': 'Create',
                                                'Campaign ID': camp_name,
                                                'Ad Group ID': ag_name,
                                                'Keyword Text': t,
                                                'Match Type': 'negativePhrase',
                                                'State': 'enabled'
                                            }, product_sheet)
                                    for asin_b in (client_cfg_nb2.get('branded_asins_data', {}) or {}).keys():
                                        expr_b = f"asin=\"{str(asin_b).strip().upper()}\""
                                        push({
                                            'Product': product_sheet,
                                            'Entity': 'Negative Product Targeting',
                                            'Operation': 'Create',
                                            'Campaign ID': camp_name,
                                            'Ad Group ID': ag_name,
                                            'Product Targeting Expression': expr_b,
                                            'State': 'enabled'
                                        }, product_sheet)

                            # Additional negatives per rules beyond Auto
                            if product_sheet == 'Sponsored Products':
                                # Non-Branded Research/Performance: branded terms negativePhrase
                                if brand_tag == 'Non' and td['ttype'] == 'Keyword':
                                    client_cfg_nb3 = st.session_state.get('client_config', {}) or {}
                                    for term in client_cfg_nb3.get('branded_keywords', []) or []:
                                        t = str(term).strip()
                                        if t:
                                            push({
                                                'Product': product_sheet,
                                                'Entity': 'Negative Keyword',
                                                'Operation': 'Create',
                                                'Campaign ID': camp_name,
                                                'Ad Group ID': ag_name,
                                                'Keyword Text': t,
                                                'Match Type': 'negativePhrase',
                                                'State': 'enabled'
                                            }, product_sheet)
                                # Non-Branded ASIN Targeting: branded ASINs as negative product targets
                                if brand_tag == 'Non' and td['ttype'] == 'ASIN':
                                    client_cfg_nb4 = st.session_state.get('client_config', {}) or {}
                                    for asin_b in (client_cfg_nb4.get('branded_asins_data', {}) or {}).keys():
                                        expr_b = f"asin=\"{str(asin_b).strip().upper()}\""
                                        push({
                                            'Product': product_sheet,
                                            'Entity': 'Negative Product Targeting',
                                            'Operation': 'Create',
                                            'Campaign ID': camp_name,
                                            'Ad Group ID': ag_name,
                                            'Product Targeting Expression': expr_b,
                                            'State': 'enabled'
                                        }, product_sheet)
                                # Cross-negatives: Non-Branded Performance keywords -> negativeExact into Auto and Research (not into Performance itself)
                                if brand_tag == 'Non' and ('SP Non-Branded Performance' in selected_tactics) and (td['ttype'] == 'Auto' or (td['ttype'] == 'Keyword' and td.get('label','') == 'Research')):
                                    for k in non_kw:
                                        push({
                                            'Product': product_sheet,
                                            'Entity': 'Negative Keyword',
                                            'Operation': 'Create',
                                            'Campaign ID': camp_name,
                                            'Ad Group ID': ag_name,
                                            'Keyword Text': k['text'],
                                            'Match Type': 'negativeExact',
                                            'State': 'enabled'
                                        }, product_sheet)
                                # Cross-negatives: Branded Performance -> negativeExact into Branded Research
                                if brand_tag == 'Brand' and td['ttype'] == 'Keyword' and td.get('label','') == 'Research' and ('SP Branded Performance' in selected_tactics):
                                    for k in brand_kw:
                                        push({
                                            'Product': product_sheet,
                                            'Entity': 'Negative Keyword',
                                            'Operation': 'Create',
                                            'Campaign ID': camp_name,
                                            'Ad Group ID': ag_name,
                                            'Keyword Text': k['text'],
                                            'Match Type': 'negativeExact',
                                            'State': 'enabled'
                                        }, product_sheet)

                        # Add targets and negatives for grouped/custom ad groups (outside per-product branch)
                        if ag_targets_list:
                            for ag_name in ag_targets_list:
                                if td['ttype'] == 'Keyword':
                                    kw_list = brand_kw if brand_tag=='Brand' else non_kw
                                    for k in kw_list:
                                        sg = {x.strip() for x in (k.get('scope_groups','') or '').split(',') if x.strip() and x.strip().lower() not in ('all','none','nan')}
                                        stags = {x.strip() for x in (k.get('scope_tags','') or '').split(',') if x.strip() and x.strip().lower() not in ('all','none','nan')}
                                        # Match against either the group's Category or any Product Group label present in this group
                                        if sg and not (allowed_pg_names & sg):
                                            continue
                                        if stags and not (stags.intersection(set(tags))):
                                            continue
                                        push({
                                            'Product': product_sheet,
                                            'Entity': 'Keyword',
                                            'Operation': 'Create',
                                            'Campaign ID': camp_name,
                                            'Ad Group ID': ag_name,
                                            'Keyword Text': k['text'],
                                            'Match Type': td.get('match','broad'),
                                            'Bid': k['bid'],
                                            'State': 'enabled'
                                        }, product_sheet)
                                elif td['ttype'] == 'ASIN':
                                    asin_list = brand_asin if brand_tag=='Brand' else non_asin
                                    if product_sheet == 'Sponsored Products':
                                        for a in asin_list:
                                            m = re.search(r'B0[A-Za-z0-9]{8}', a['text'], flags=re.I)
                                            if not m:
                                                continue
                                            expr = f"asin=\"{m.group(0).upper()}\""
                                            sg = {x.strip() for x in (a.get('scope_groups','') or '').split(',') if x.strip() and x.strip().lower() not in ('all','none','nan')}
                                            stags = {x.strip() for x in (a.get('scope_tags','') or '').split(',') if x.strip() and x.strip().lower() not in ('all','none','nan')}
                                            if sg and not (allowed_pg_names & sg):
                                                continue
                                            if stags and not (stags.intersection(set(tags))):
                                                continue
                                            push({
                                                'Product': product_sheet,
                                                'Entity': 'Product Targeting',
                                                'Operation': 'Create',
                                                'Campaign ID': camp_name,
                                                'Ad Group ID': ag_name,
                                                'Product Targeting Expression': expr,
                                                'Bid': a['bid'],
                                                'State': 'enabled'
                                            }, product_sheet)
                                        if brand_tag == 'Non' and non_cat:
                                            for c in non_cat:
                                                push({
                                                    'Product': product_sheet,
                                                    'Entity': 'Product Targeting',
                                                    'Operation': 'Create',
                                                    'Campaign ID': camp_name,
                                                    'Ad Group ID': ag_name,
                                                    'Product Targeting Expression': c['text'],
                                                    'Bid': c['bid'],
                                                    'State': 'enabled'
                                                }, product_sheet)
                                    else:
                                        for a in asin_list:
                                            m = re.search(r'B0[A-Za-z0-9]{8}', a['text'], flags=re.I)
                                            if not m:
                                                continue
                                            expr = f"asin=\"{m.group(0).upper()}\""
                                            sg = {x.strip() for x in (a.get('scope_groups','') or '').split(',') if x.strip() and x.strip().lower() not in ('all','none','nan')}
                                            stags = {x.strip() for x in (a.get('scope_tags','') or '').split(',') if x.strip() and x.strip().lower() not in ('all','none','nan')}
                                            if sg and category not in sg:
                                                continue
                                            if stags and not (stags.intersection(set(tags))):
                                                continue
                                            push({
                                                'Product': product_sheet,
                                                'Entity': 'Contextual Targeting',
                                                'Operation': 'Create',
                                                'Campaign ID': camp_name,
                                                'Ad Group ID': ag_name,
                                                'Targeting Expression': expr,
                                                'Bid': a['bid'],
                                                'State': 'enabled',
                                                'Tactic': 'T00030'
                                            }, product_sheet)
                                elif td['ttype'] == 'Auto' and product_sheet == 'Sponsored Products':
                                    for expr in ["close-match","loose-match","complements","substitutes"]:
                                        push({
                                            'Product': product_sheet,
                                            'Entity': 'Product Targeting',
                                            'Operation': 'Create',
                                            'Campaign ID': camp_name,
                                            'Ad Group ID': ag_name,
                                            'Product Targeting Expression': expr,
                                            'Bid': fallback_bid,
                                            'State': 'enabled'
                                        }, product_sheet)
                                    if brand_tag == 'Non':
                                        client_cfg_nb2 = st.session_state.get('client_config', {}) or {}
                                        for term in client_cfg_nb2.get('branded_keywords', []) or []:
                                            t = str(term).strip()
                                            if t:
                                                push({
                                                    'Product': product_sheet,
                                                    'Entity': 'Negative Keyword',
                                                    'Operation': 'Create',
                                                    'Campaign ID': camp_name,
                                                    'Ad Group ID': ag_name,
                                                    'Keyword Text': t,
                                                    'Match Type': 'negativePhrase',
                                                    'State': 'enabled'
                                                }, product_sheet)
                                        for asin_b in (client_cfg_nb2.get('branded_asins_data', {}) or {}).keys():
                                            expr_b = f"asin=\"{str(asin_b).strip().upper()}\""
                                            push({
                                                'Product': product_sheet,
                                                'Entity': 'Negative Product Targeting',
                                                'Operation': 'Create',
                                                'Campaign ID': camp_name,
                                                'Ad Group ID': ag_name,
                                                'Product Targeting Expression': expr_b,
                                                'State': 'enabled'
                                            }, product_sheet)
                                # Additional negatives per rules beyond Auto
                                if product_sheet == 'Sponsored Products':
                                    if brand_tag == 'Non' and td['ttype'] == 'Keyword':
                                        client_cfg_nb3 = st.session_state.get('client_config', {}) or {}
                                        for term in client_cfg_nb3.get('branded_keywords', []) or []:
                                            t = str(term).strip()
                                            if t:
                                                push({
                                                    'Product': product_sheet,
                                                    'Entity': 'Negative Keyword',
                                                    'Operation': 'Create',
                                                    'Campaign ID': camp_name,
                                                    'Ad Group ID': ag_name,
                                                    'Keyword Text': t,
                                                    'Match Type': 'negativePhrase',
                                                    'State': 'enabled'
                                                }, product_sheet)
                                    if brand_tag == 'Non' and ('SP Non-Branded Performance' in selected_tactics) and (td['ttype'] == 'Auto' or (td['ttype'] == 'Keyword' and td.get('label','') == 'Research')):
                                        for k in non_kw:
                                            push({
                                                'Product': product_sheet,
                                                'Entity': 'Negative Keyword',
                                                'Operation': 'Create',
                                                'Campaign ID': camp_name,
                                                'Ad Group ID': ag_name,
                                                'Keyword Text': k['text'],
                                                'Match Type': 'negativeExact',
                                                'State': 'enabled'
                                            }, product_sheet)
                                    if brand_tag == 'Brand' and td['ttype'] == 'Keyword' and td.get('label','') == 'Research' and ('SP Branded Performance' in selected_tactics):
                                        for k in brand_kw:
                                            push({
                                                'Product': product_sheet,
                                                'Entity': 'Negative Keyword',
                                                'Operation': 'Create',
                                                'Campaign ID': camp_name,
                                                'Ad Group ID': ag_name,
                                                'Keyword Text': k['text'],
                                                'Match Type': 'negativeExact',
                                                'State': 'enabled'
                                            }, product_sheet)

                        else:
                            # per_product: create ad group per product and add one Product Ad per group (SP)
                            for p in entries_for_group:
                                pid = p['asin'] if id_choice=='ASIN' else p['sku']
                                if not pid:
                                    continue
                                # For per-product mode, each product should have its own campaign per tactic
                                # Recompute campaign name for each product (still using same category/tags)
                                camp_name_pp = build_campaign_name(ad_prefix, brand_tag, category, tags, tactic_label, include_tactic=True)
                                # Ensure a Campaign/Ad Group row exists unless merging into existing
                                campaign_exists_pp = camp_name_pp in existing.get(product_sheet, set())
                                if not (merge_mode and campaign_exists_pp):
                                    push({
                                        'Product': product_sheet,
                                        'Entity': 'Campaign',
                                        'Operation': 'Create',
                                        'Campaign ID': camp_name_pp,
                                        'Campaign Name (Informational Only)': camp_name_pp,
                                        'Start Date': start_date_str,
                                        'Targeting Type': ('Auto' if td['ttype']=='Auto' else 'Manual'),
                                        'State': 'enabled',
                                        'Daily Budget': st.session_state.get('cc_campaign_budgets', {}).get(camp_name_pp, daily_budget),
                                        **({'Bidding Strategy': st.session_state.get('cc_sp_bidding_strategy')} if product_sheet=='Sponsored Products' else {}),
                                        **({'Tactic': 'T00030'} if product_sheet=='Sponsored Display' else {})
                                    }, product_sheet)
                                    if product_sheet == 'Sponsored Products':
                                        placements = [
                                            ('Placement Top', st.session_state.get('cc_sp_place_top', 0)),
                                            ('Placement Product Page', st.session_state.get('cc_sp_place_pp', 0)),
                                            ('Placement Rest Of Search', st.session_state.get('cc_sp_place_ros', 0)),
                                            ('Placement Amazon Business', st.session_state.get('cc_sp_place_ab', 0)),
                                        ]
                                        for pname, pct in placements:
                                            # Omit 0% placement rows
                                            try:
                                                pct_val = float(pct) if pct is not None else 0.0
                                            except Exception:
                                                pct_val = 0.0
                                            if pct_val > 0:
                                                push({
                                                    'Product': product_sheet,
                                                    'Entity': 'Bidding Adjustment',
                                                    'Operation': 'Create',
                                                    'Campaign ID': camp_name_pp,
                                                    'Bidding Strategy': st.session_state.get('cc_sp_bidding_strategy'),
                                                    'Placement': pname,
                                                    'Percentage': pct_val
                                                }, product_sheet)
                                else:
                                    merged_campaigns.add(camp_name_pp)
                                desired_ag_name_pp = (f"{pid} - {p.get('title','')}")[:200]
                                if merge_mode and campaign_exists_pp:
                                    existing_ag_set_pp = existing_ag.get(product_sheet, {}).get(camp_name_pp, set())
                                    ag_name = desired_ag_name_pp if desired_ag_name_pp in existing_ag_set_pp else (sorted(existing_ag_set_pp)[0] if existing_ag_set_pp else desired_ag_name_pp)
                                else:
                                    ag_name = desired_ag_name_pp
                                    push({
                                        'Product': product_sheet,
                                        'Entity': 'Ad Group',
                                        'Operation': 'Create',
                                        'Campaign ID': camp_name_pp,
                                        'Ad Group ID': ag_name,
                                        'Campaign Name': camp_name_pp,
                                        'Ad Group Name': ag_name,
                                        'State': 'enabled',
                                        'Ad Group Default Bid': ag_default_bid,
                                        **({'Tactic': 'T00030'} if product_sheet=='Sponsored Display' else {})
                                    }, product_sheet)
                                # Product Ads for SP and SD (per-product ad group)
                                push({
                                    'Product': product_sheet,
                                    'Entity': 'Product Ad',
                                    'Operation': 'Create',
                                    'Campaign ID': camp_name_pp,
                                    'Ad Group ID': ag_name,
                                    id_choice: pid,
                                    'State': 'enabled'
                                }, product_sheet)

                                # Targets for this single-product ad group
                                if td['ttype'] == 'Keyword':
                                    kw_list = brand_kw if brand_tag=='Brand' else non_kw
                                    for k in kw_list:
                                        # For per-product ad groups, allow match to group's Category or this product's Product Group
                                        try:
                                            allowed_pg_names_pp = {str(category).strip()}
                                            gp = str(p.get('group','')).strip()
                                            if gp:
                                                allowed_pg_names_pp.add(gp)
                                        except Exception:
                                            allowed_pg_names_pp = {str(category).strip()}
                                        sg = {x.strip() for x in (k.get('scope_groups','') or '').split(',') if x.strip() and x.strip().lower() not in ('all','none','nan')}
                                        stags = {x.strip() for x in (k.get('scope_tags','') or '').split(',') if x.strip() and x.strip().lower() not in ('all','none','nan')}
                                        if sg and not (allowed_pg_names_pp & sg):
                                            continue
                                        if stags and not (stags.intersection(set(tags))):
                                            continue
                                        push({
                                            'Product': product_sheet,
                                            'Entity': 'Keyword',
                                            'Operation': 'Create',
                                            'Campaign ID': camp_name_pp,
                                            'Ad Group ID': ag_name,
                                            'Keyword Text': k['text'],
                                            'Match Type': td.get('match','broad'),
                                            'Bid': k['bid'],
                                            'State': 'enabled'
                                        }, product_sheet)
                                elif td['ttype'] == 'ASIN':
                                    asin_list = brand_asin if brand_tag=='Brand' else non_asin
                                    if product_sheet == 'Sponsored Products':
                                        for a in asin_list:
                                            m = re.search(r'B0[A-Za-z0-9]{8}', a['text'], flags=re.I)
                                            if not m:
                                                continue
                                            expr = f"asin=\"{m.group(0).upper()}\""
                                            # Scope filters for per-product ad group
                                            try:
                                                allowed_pg_names_pp = {str(category).strip()}
                                                gp = str(p.get('group','')).strip()
                                                if gp:
                                                    allowed_pg_names_pp.add(gp)
                                            except Exception:
                                                allowed_pg_names_pp = {str(category).strip()}
                                            sg = {x.strip() for x in (a.get('scope_groups','') or '').split(',') if x.strip() and x.strip().lower() not in ('all','none','nan')}
                                            stags = {x.strip() for x in (a.get('scope_tags','') or '').split(',') if x.strip() and x.strip().lower() not in ('all','none','nan')}
                                            if sg and not (allowed_pg_names_pp & sg):
                                                continue
                                            if stags and not (stags.intersection(set(tags))):
                                                continue
                                            push({
                                                'Product': product_sheet,
                                                'Entity': 'Product Targeting',
                                                'Operation': 'Create',
                                                'Campaign ID': camp_name_pp,
                                                'Ad Group ID': ag_name,
                                                'Product Targeting Expression': expr,
                                                'Bid': a['bid'],
                                                'State': 'enabled'
                                            }, product_sheet)
                                        # Include Category targets only for Non-Branded SP ASIN Targeting
                                        if brand_tag == 'Non' and non_cat:
                                            for c in non_cat:
                                                push({
                                                    'Product': product_sheet,
                                                    'Entity': 'Product Targeting',
                                                    'Operation': 'Create',
                                                    'Campaign ID': camp_name_pp,
                                                    'Ad Group ID': ag_name,
                                                    'Product Targeting Expression': c['text'],
                                                    'Bid': c['bid'],
                                                    'State': 'enabled'
                                                }, product_sheet)
                                        # Non-Branded ASIN Targeting: branded ASINs negatives (per-product)
                                        if brand_tag == 'Non':
                                            client_cfg_nb5 = st.session_state.get('client_config', {}) or {}
                                            for asin_b in (client_cfg_nb5.get('branded_asins_data', {}) or {}).keys():
                                                expr_b = f"asin=\"{str(asin_b).strip().upper()}\""
                                                push({
                                                    'Product': product_sheet,
                                                    'Entity': 'Negative Product Targeting',
                                                    'Operation': 'Create',
                                                    'Campaign ID': camp_name_pp,
                                                    'Ad Group ID': ag_name,
                                                    'Product Targeting Expression': expr_b,
                                                    'State': 'enabled'
                                                }, product_sheet)
                                    else:
                                        for a in asin_list:
                                            m = re.search(r'B0[A-Za-z0-9]{8}', a['text'], flags=re.I)
                                            if not m:
                                                continue
                                            expr = f"asin=\"{m.group(0).upper()}\""
                                            push({
                                                'Product': product_sheet,
                                                'Entity': 'Contextual Targeting',
                                                'Operation': 'Create',
                                                'Campaign ID': camp_name_pp,
                                                'Ad Group ID': ag_name,
                                                'Targeting Expression': expr,
                                                'Bid': a['bid'],
                                                'State': 'enabled',
                                                'Tactic': 'T00030'
                                            }, product_sheet)
                                elif td['ttype'] == 'Auto' and product_sheet == 'Sponsored Products':
                                    for expr in ["close-match","loose-match","complements","substitutes"]:
                                        push({
                                            'Product': product_sheet,
                                            'Entity': 'Product Targeting',
                                            'Operation': 'Create',
                                            'Campaign ID': camp_name_pp,
                                            'Ad Group ID': ag_name,
                                            'Product Targeting Expression': expr,
                                            'Bid': fallback_bid,
                                            'State': 'enabled'
                                        }, product_sheet)
                                    # Default negatives for SP Non-Branded Auto (per-product)
                                    if brand_tag == 'Non':
                                        client_cfg_nb6 = st.session_state.get('client_config', {}) or {}
                                        for term in client_cfg_nb6.get('branded_keywords', []) or []:
                                            t = str(term).strip()
                                            if t:
                                                push({
                                                    'Product': product_sheet,
                                                    'Entity': 'Negative Keyword',
                                                    'Operation': 'Create',
                                                    'Campaign ID': camp_name_pp,
                                                    'Ad Group ID': ag_name,
                                                    'Keyword Text': t,
                                                    'Match Type': 'negativePhrase',
                                                    'State': 'enabled'
                                                }, product_sheet)
                                        for asin_b in (client_cfg_nb6.get('branded_asins_data', {}) or {}).keys():
                                            expr_b = f"asin=\"{str(asin_b).strip().upper()}\""
                                            push({
                                                'Product': product_sheet,
                                                'Entity': 'Negative Product Targeting',
                                                'Operation': 'Create',
                                                'Campaign ID': camp_name_pp,
                                                'Ad Group ID': ag_name,
                                                'Product Targeting Expression': expr_b,
                                                'State': 'enabled'
                                            }, product_sheet)
                                    # Cross-negatives: Non-Branded Performance keywords (per-product) into Auto only
                                    if brand_tag == 'Non' and ('SP Non-Branded Performance' in selected_tactics):
                                        for k in non_kw:
                                            push({
                                                'Product': product_sheet,
                                                'Entity': 'Negative Keyword',
                                                'Operation': 'Create',
                                                'Campaign ID': camp_name_pp,
                                                'Ad Group ID': ag_name,
                                                'Keyword Text': k['text'],
                                                'Match Type': 'negativeExact',
                                                'State': 'enabled'
                                            }, product_sheet)
                                # Cross-negatives for Non-Branded Research (per-product): negate Non-Branded Performance keywords
                                if product_sheet == 'Sponsored Products' and brand_tag == 'Non' and td['ttype'] == 'Keyword' and td.get('label','') == 'Research' and ('SP Non-Branded Performance' in selected_tactics):
                                    for k in non_kw:
                                        push({
                                            'Product': product_sheet,
                                            'Entity': 'Negative Keyword',
                                            'Operation': 'Create',
                                            'Campaign ID': camp_name_pp,
                                            'Ad Group ID': ag_name,
                                            'Keyword Text': k['text'],
                                            'Match Type': 'negativeExact',
                                            'State': 'enabled'
                                        }, product_sheet)
                                # Additional negatives for Non-Branded Research (per-product): branded terms negativePhrase
                                if product_sheet == 'Sponsored Products' and brand_tag == 'Non' and td['ttype'] == 'Keyword':
                                    client_cfg_nb7 = st.session_state.get('client_config', {}) or {}
                                    for term in client_cfg_nb7.get('branded_keywords', []) or []:
                                        t = str(term).strip()
                                        if t:
                                            push({
                                                'Product': product_sheet,
                                                'Entity': 'Negative Keyword',
                                                'Operation': 'Create',
                                                'Campaign ID': camp_name_pp,
                                                'Ad Group ID': ag_name,
                                                'Keyword Text': t,
                                                'Match Type': 'negativePhrase',
                                                'State': 'enabled'
                                            }, product_sheet)
                                # Cross-negatives: Branded Performance -> negativeExact into Branded Research (per-product)
                                if product_sheet == 'Sponsored Products' and brand_tag == 'Brand' and td['ttype'] == 'Keyword' and td.get('label','') == 'Research' and ('SP Branded Performance' in selected_tactics):
                                    for k in brand_kw:
                                        push({
                                            'Product': product_sheet,
                                            'Entity': 'Negative Keyword',
                                            'Operation': 'Create',
                                            'Campaign ID': camp_name_pp,
                                            'Ad Group ID': ag_name,
                                            'Keyword Text': k['text'],
                                            'Match Type': 'negativeExact',
                                            'State': 'enabled'
                                        }, product_sheet)
                                    # Default negatives for SP Non-Branded Auto (per product)
                                    if brand_tag == 'Non':
                                        client_cfg_nb = st.session_state.get('client_config', {}) or {}
                                        for term in client_cfg_nb.get('branded_keywords', []) or []:
                                            t = str(term).strip()
                                            if not t:
                                                continue
                                            push({
                                                'Product': product_sheet,
                                                'Entity': 'Negative Keyword',
                                                'Operation': 'Create',
                                                'Campaign ID': camp_name_pp,
                                                'Ad Group ID': ag_name,
                                                'Keyword Text': t,
                                                'Match Type': 'negativePhrase',
                                                'State': 'enabled'
                                            }, product_sheet)
                                        for asin_b in (client_cfg_nb.get('branded_asins_data', {}) or {}).keys():
                                            expr_b = f"asin=\"{str(asin_b).strip().upper()}\""
                                            push({
                                                'Product': product_sheet,
                                                'Entity': 'Negative Product Targeting',
                                                'Operation': 'Create',
                                                'Campaign ID': camp_name_pp,
                                                'Ad Group ID': ag_name,
                                                'Product Targeting Expression': expr_b,
                                                'State': 'enabled'
                                            }, product_sheet)

                # (Removed redundant global cross-negation block; cross-negatives are handled inline per campaign above)

                # Deduplicate new actions (within this batch and vs existing queue)
                def _action_key(a: dict):
                    fields = [
                        a.get('Product',''), a.get('Entity',''), a.get('Operation',''),
                        a.get('Campaign ID',''), a.get('Ad Group ID',''),
                        a.get('Keyword Text',''), a.get('Product Targeting Expression',''),
                        a.get(id_choice,''), a.get('Targeting Expression',''),
                        # Include placement-related fields so multiple bidding adjustments don't dedupe away
                        a.get('Placement',''), a.get('Percentage',''), a.get('Bidding Strategy','')
                    ]
                    return tuple(fields)

                actions = st.session_state.bulk_export_actions
                pre_existing = {_action_key(a) for a in actions[:start_idx]}
                seen = set(pre_existing)
                cleaned = []
                removed = 0
                for a in actions[start_idx:]:
                    k = _action_key(a)
                    if k in seen:
                        removed += 1
                        continue
                    seen.add(k)
                    cleaned.append(a)
                st.session_state.bulk_export_actions = actions[:start_idx] + cleaned

                with st.expander("Duplicate cleanup summary", expanded=False):
                    st.write(f"Removed {removed} duplicate row(s) from this batch.")
                    st.write(f"Total rows in queue now: {len(st.session_state.bulk_export_actions)}")

                if merged_campaigns:
                    with st.expander("Merged into existing campaigns", expanded=False):
                        for cn in sorted(merged_campaigns):
                            st.write(cn)

                st.success("Campaign creation actions added to Bulk Export queue.")

        with tab5:
            st.header("Bulk File Export")
            st.markdown("Generate and download bulk files with all your planned changes.")
            
            # Initialize bulk export actions if not exists
            if 'bulk_export_actions' not in st.session_state:
                st.session_state.bulk_export_actions = []
            
            if st.session_state.bulk_export_actions:
                st.subheader("ðŸ“‹ Planned Actions")
                
                # Group actions by type
                action_types = {}
                for action in st.session_state.bulk_export_actions:
                    action_type = action['action_type']
                    if action_type not in action_types:
                        action_types[action_type] = []
                    action_types[action_type].append(action)
                
                # Display summary
                col1, col2, col3 = st.columns(3)
                with col1:
                    neg_count = len(action_types.get('negative_keyword', []))
                    st.metric("Negative Keywords", neg_count)
                with col2:
                    pause_count = len(action_types.get('pause', []))
                    st.metric("Pauses", pause_count)
                with col3:
                    bid_count = len(action_types.get('bid_optimization', []))
                    st.metric("Bid Changes", bid_count)
                
                # Display details for each action type
                for action_type, actions in action_types.items():
                    if action_type == 'negative_keyword':
                        st.markdown("**ðŸš« Negative Keywords to Add:**")
                        neg_data = []
                        for action in actions:
                            # Handle both Negative Keywords and Negative Product Targets
                            if action.get('entity') == 'Negative Keyword':
                                # This is a branded term
                                neg_data.append({
                                    'Campaign': action['campaign_name'],
                                    'Type': 'Negative Keyword',
                                    'Keyword/ASIN': action.get('keyword', ''),
                                    'Match Type': action.get('match_type', ''),
                                    'Source': action['source'],
                                    'Level': action.get('level', 'Campaign')
                                })
                            elif action.get('entity') in ('Negative Product Targeting', 'Negative Product Targeting'):
                                # This is a branded ASIN
                                neg_data.append({
                                    'Campaign': action['campaign_name'],
                                    'Type': 'Negative Product Targeting',
                                    'Keyword/ASIN': action.get('asin', ''),
                                    'Match Type': 'N/A',
                                    'Source': action['source'],
                                    'Level': action.get('level', 'Ad Group')
                                })
                            else:
                                # Fallback for other types
                                neg_data.append({
                                    'Campaign': action.get('campaign_name', ''),
                                    'Type': action.get('entity', 'Unknown'),
                                    'Keyword/ASIN': action.get('keyword', action.get('asin', '')),
                                    'Match Type': action.get('match_type', 'N/A'),
                                    'Source': action.get('source', ''),
                                    'Level': action.get('level', '')
                                })
                        
                        if neg_data:
                            neg_df = pd.DataFrame(neg_data)
                            st.dataframe(neg_df, use_container_width=True)
                        else:
                            st.info("No negative keywords to display.")
                    
                    elif action_type == 'pause':
                        st.markdown("**â¸ï¸ Items to Pause:**")
                        pause_data = []
                        for action in actions:
                            data = action.get('data', {})
                            pause_data.append({
                                'Type': data.get('type', ''),
                                'Campaign': data.get('campaign_name', 'N/A'),
                                'Target/Name': data.get('target', data.get('campaign_name', 'N/A')),
                                'Current Spend': data.get('spend', 'N/A'),
                                'Current ACoS': data.get('acos', 'N/A')
                            })
                        pause_df = pd.DataFrame(pause_data)
                        st.dataframe(pause_df, use_container_width=True)
                    
                    elif action_type == 'bid_optimization':
                        st.markdown("**Bid Optimizations:**")
                        bid_data = []
                        for action in actions:
                            # Extract summary data for display (works with both new and old format)
                            if 'row_data' in action:
                                # New format with complete row data
                                row_data = action['row_data']
                                campaign_name = row_data.get('Campaign Name', '') or row_data.get('Campaign Name (Informational Only)', '')
                                old_bid = action.get('old_bid', 0)
                                new_bid = row_data.get('Bid', action.get('new_bid', 0))
                                bid_change = action.get('bid_change', 0)
                                target_acos = row_data.get('Target ACoS', action.get('target_acos', 0))
                            else:
                                # Old format (backward compatibility)
                                campaign_name = action['campaign_name']
                                old_bid = action['old_bid']
                                new_bid = action['new_bid']
                                bid_change = action['bid_change']
                                target_acos = action.get('target_acos', 0)
                            
                            bid_data.append({
                                'Campaign': campaign_name,
                                'Old Bid': f"${old_bid:.2f}",
                                'New Bid': f"${new_bid:.2f}",
                                'Bid Change': f"${bid_change:+.2f}",
                                'Target ACoS': f"{target_acos*100:.1f}%" if isinstance(target_acos, float) and target_acos <= 1 else f"{target_acos:.1f}%"
                            })
                        
                        bid_df = pd.DataFrame(bid_data)
                        st.dataframe(bid_df, use_container_width=True)
                
                # Bulk File Preview
                st.markdown("---")
                st.subheader("ðŸ“‹ Bulk File Preview")
                st.markdown("Preview of what will be included in your bulk file export:")
                
                # Group actions by campaign type for sheet separation
                campaign_type_data = {
                    'Sponsored Products': [],
                    'Sponsored Brands': [],
                    'Sponsored Display': []
                }
                
                # Process all actions and group by campaign type
                for action in st.session_state.bulk_export_actions:
                    # Determine campaign type from sheet_source, sheet_type, or campaign name
                    campaign_type = 'Sponsored Products'  # Default
                    
                    # First priority: check sheet_source (used by bid optimizations and our pauses via data.sheet_source)
                    sheet_source = None
                    if 'sheet_source' in action:
                        sheet_source = action['sheet_source']
                    elif isinstance(action.get('data'), dict) and 'sheet_source' in action.get('data', {}):
                        sheet_source = action.get('data', {}).get('sheet_source')
                    if sheet_source is not None:
                        if 'Brands' in sheet_source or 'SB Multi Ad Group' in sheet_source:
                            campaign_type = 'Sponsored Brands'
                        elif 'Display' in sheet_source:
                            campaign_type = 'Sponsored Display'
                        elif 'Products' in sheet_source:
                            campaign_type = 'Sponsored Products'
                    # Second priority: check sheet_type (used by other actions)
                    elif 'sheet_type' in action:
                        if 'Brands' in action['sheet_type']:
                            campaign_type = 'Sponsored Brands'
                        elif 'Display' in action['sheet_type']:
                            campaign_type = 'Sponsored Display'
                    # Third priority: infer from campaign name
                    elif 'campaign_name' in action or (isinstance(action.get('data'), dict) and 'campaign_name' in action.get('data', {})):
                        campaign_name = (action.get('campaign_name') or action.get('data', {}).get('campaign_name') or '').upper()
                        if 'SB' in campaign_name or 'BRAND' in campaign_name:
                            campaign_type = 'Sponsored Brands'
                        elif 'SD' in campaign_name or 'DISPLAY' in campaign_name:
                            campaign_type = 'Sponsored Display'
                    
                    # Convert action to bulk file format
                    if action['action_type'] == 'negative_keyword':
                        if action.get('entity') == 'Campaign Negative Keyword':
                            bulk_row = {
                                'Product': campaign_type,
                                'Entity': 'Campaign Negative Keyword',
                                'Operation': action.get('operation', 'Create'),
                                'Campaign ID': action.get('campaign_id', ''),
                                'Campaign Name': action.get('campaign_name', ''),
                                'Keyword Text': action.get('keyword', ''),
                                'Match Type': action.get('match_type', 'negativePhrase'),
                                'State': 'enabled'
                            }
                        elif action.get('entity') == 'Negative Keyword':
                            bulk_row = {
                                'Product': campaign_type,
                                'Entity': 'Negative Keyword',
                                'Operation': action.get('operation', 'Create'),
                                'Campaign ID': action.get('campaign_id', ''),
                                'Campaign Name': action.get('campaign_name', ''),
                                'Ad Group ID': action.get('ad_group_id', ''),
                                'Ad Group Name': action.get('ad_group_name', ''),
                                'Keyword Text': action.get('keyword', ''),
                                'Match Type': action.get('match_type', 'negativePhrase'),
                                'State': 'enabled'
                            }
                        elif action.get('entity') in ('Negative Product Targeting', 'Negative Product Targeting'):
                            targeting_col = action.get('targeting_column', 'Product Targeting Expression')
                            bulk_row = {
                                'Product': campaign_type,
                                'Entity': 'Negative Product Targeting',
                                'Operation': action.get('operation', 'Create'),
                                'Campaign ID': action.get('campaign_id', ''),
                                'Campaign Name': action.get('campaign_name', ''),
                                'Ad Group ID': action.get('ad_group_id', ''),
                                'Ad Group Name': action.get('ad_group_name', ''),
                                targeting_col: action.get('targeting_expression', ''),
                                'State': 'enabled'
                            }
                        else:
                            bulk_row = action.copy()  # Fallback
                    elif action['action_type'] == 'pause':
                        data = action.get('data', {})
                        row_data = data.get('row_data', {}) if isinstance(data, dict) else {}
                        # Helper to prefer informational campaign name when present
                        def _campaign_name_from_row(rd):
                            if not isinstance(rd, dict):
                                return data.get('campaign_name', '')
                            for k in rd.keys():
                                if str(k).lower().strip() == 'campaign name (informational only)':
                                    return rd.get(k, '')
                            return rd.get('Campaign Name', data.get('campaign_name', ''))
                        # Campaign pause
                        if data.get('type') == 'campaign':
                            bulk_row = {
                                'Product': campaign_type,
                                'Entity': 'Campaign',
                                'Operation': 'update',
                                'Campaign ID': row_data.get('Campaign ID', ''),
                                'Campaign Name': _campaign_name_from_row(row_data),
                                'State': 'paused'
                            }
                        # Product Ad pause
                        elif data.get('type') == 'product_ad':
                            bulk_row = {
                                'Product': campaign_type,
                                'Entity': 'Product Ad',
                                'Operation': 'update',
                                'Campaign ID': row_data.get('Campaign ID', ''),
                                'Ad Group ID': row_data.get('Ad Group ID', ''),
                                'Campaign Name': _campaign_name_from_row(row_data),
                                'Ad Group Name': row_data.get('Ad Group Name', row_data.get('Ad Group Name (Informational Only)', '')),
                                'Ad ID': row_data.get('Ad ID', ''),
                                'ASIN': row_data.get('ASIN', data.get('asin', '')),
                                'SKU': row_data.get('SKU', data.get('sku', '')),
                                'State': 'paused'
                            }
                        # Target pause (Keyword/Product Targeting/SD Targeting)
                        else:
                            ent = str(row_data.get('Entity', '')).strip().lower()
                            is_sd = (campaign_type == 'Sponsored Display') or ('targeting id' in [c.lower() for c in row_data.keys()])
                            if is_sd:
                                # SD Targeting
                                bulk_row = {
                                    'Product': campaign_type,
                                    'Entity': 'Targeting',
                                    'Operation': 'update',
                                    'Campaign ID': row_data.get('Campaign ID', ''),
                                    'Ad Group ID': row_data.get('Ad Group ID', ''),
                                    'Campaign Name': _campaign_name_from_row(row_data),
                                    'Ad Group Name': row_data.get('Ad Group Name', row_data.get('Ad Group Name (Informational Only)', '')),
                                    'Targeting ID': row_data.get('Targeting ID', ''),
                                    'Targeting Expression': row_data.get('Targeting Expression', data.get('target', '')),
                                    'State': 'paused'
                                }
                            elif 'product' in ent or 'target' in ent or 'product targeting' in ent or ('Product Targeting ID' in row_data):
                                # Product Targeting (SP/SB)
                                bulk_row = {
                                    'Product': campaign_type,
                                    'Entity': 'Product Targeting',
                                    'Operation': 'update',
                                    'Campaign ID': row_data.get('Campaign ID', ''),
                                    'Ad Group ID': row_data.get('Ad Group ID', ''),
                                    'Campaign Name': _campaign_name_from_row(row_data),
                                    'Ad Group Name': row_data.get('Ad Group Name', row_data.get('Ad Group Name (Informational Only)', '')),
                                    'Product Targeting ID': row_data.get('Product Targeting ID', ''),
                                    'Product Targeting Expression': row_data.get('Product Targeting Expression', data.get('target', '')),
                                    'State': 'paused'
                                }
                            else:
                                # Keyword (SP/SB)
                                bulk_row = {
                                    'Product': campaign_type,
                                    'Entity': 'Keyword',
                                    'Operation': 'update',
                                    'Campaign ID': row_data.get('Campaign ID', ''),
                                    'Ad Group ID': row_data.get('Ad Group ID', ''),
                                    'Campaign Name': _campaign_name_from_row(row_data),
                                    'Ad Group Name': row_data.get('Ad Group Name', row_data.get('Ad Group Name (Informational Only)', '')),
                                    'Keyword ID': row_data.get('Keyword ID', ''),
                                    'Keyword Text': row_data.get('Keyword Text', data.get('target', '')),
                                    'Match Type': row_data.get('Match Type', ''),
                                    'State': 'paused'
                                }
                    elif action['action_type'] == 'bid_optimization':
                        if 'row_data' in action:
                            row_data = action['row_data'].copy()
                            
                            # Create bulk row with proper column order based on campaign type
                            if campaign_type == 'Sponsored Display':
                                # Sponsored Display format
                                bulk_row = {
                                    'Product': campaign_type,
                                    'Entity': row_data.get('Entity', 'Targeting'),
                                    'Operation': 'update',
                                    'Campaign ID': row_data.get('Campaign ID', ''),
                                    'Ad Group ID': row_data.get('Ad Group ID', ''),
                                    'Campaign Name': row_data.get('Campaign Name', row_data.get('Campaign Name (Informational Only)', '')),
                                    'Ad Group Name': row_data.get('Ad Group Name', row_data.get('Ad Group Name (Informational Only)', '')),
                                    'Targeting ID': row_data.get('Targeting ID', ''),
                                    'State': row_data.get('State', 'enabled'),
                                    'Targeting Expression': row_data.get('Targeting Expression', ''),
                                    'Bid': row_data.get('Bid', action.get('new_bid', 0))
                                }
                                # Append metrics columns
                                m = action.get('metrics', {})
                                bulk_row['Spend'] = m.get('Spend', '')
                                bulk_row['Ad Sales'] = m.get('Ad Sales', '')
                                bulk_row['ACoS'] = m.get('ACoS', '')
                                bulk_row['ROAS'] = m.get('ROAS', '')
                                bulk_row['CPC'] = m.get('CPC', '')
                                bulk_row['CVR'] = m.get('CVR', '')
                                bulk_row['Orders'] = m.get('Orders', '')
                            else:
                                # Sponsored Products and Sponsored Brands format
                                bulk_row = {
                                    'Product': campaign_type,
                                    'Entity': row_data.get('Entity', 'Keyword'),
                                    'Operation': 'update',
                                    'Campaign ID': row_data.get('Campaign ID', ''),
                                    'Ad Group ID': row_data.get('Ad Group ID', ''),
                                    'Campaign Name': row_data.get('Campaign Name', row_data.get('Campaign Name (Informational Only)', '')),
                                    'Ad Group Name': row_data.get('Ad Group Name', row_data.get('Ad Group Name (Informational Only)', '')),
                                    'Keyword ID': row_data.get('Keyword ID', ''),
                                    'Product Targeting ID': row_data.get('Product Targeting ID', ''),
                                    'State': row_data.get('State', 'enabled'),
                                    'Keyword Text': row_data.get('Keyword Text', row_data.get('Keyword', '')),
                                    'Match Type': row_data.get('Match Type', ''),
                                    'Product Targeting Expression': row_data.get('Product Targeting Expression', ''),
                                    'Bid': row_data.get('Bid', action.get('new_bid', 0))
                                }
                                # Append metrics columns
                                m = action.get('metrics', {})
                                bulk_row['Spend'] = m.get('Spend', '')
                                bulk_row['Ad Sales'] = m.get('Ad Sales', '')
                                bulk_row['ACoS'] = m.get('ACoS', '')
                                bulk_row['ROAS'] = m.get('ROAS', '')
                                bulk_row['CPC'] = m.get('CPC', '')
                                bulk_row['CVR'] = m.get('CVR', '')
                                bulk_row['Orders'] = m.get('Orders', '')
                        else:
                            # Fallback for older format
                            if campaign_type == 'Sponsored Display':
                                bulk_row = {
                                    'Product': campaign_type,
                                    'Entity': 'Targeting',
                                    'Operation': 'update',
                                    'Campaign ID': '',
                                    'Ad Group ID': '',
                                    'Campaign Name': action.get('campaign_name', ''),
                                    'Ad Group Name': '',
                                    'Targeting ID': '',
                                    'State': 'enabled',
                                    'Targeting Expression': '',
                                    'Bid': action.get('new_bid', 0)
                                }
                                # Append metrics columns
                                m = action.get('metrics', {})
                                bulk_row['Spend'] = m.get('Spend', '')
                                bulk_row['Ad Sales'] = m.get('Ad Sales', '')
                                bulk_row['ACoS'] = m.get('ACoS', '')
                                bulk_row['ROAS'] = m.get('ROAS', '')
                                bulk_row['CPC'] = m.get('CPC', '')
                                bulk_row['CVR'] = m.get('CVR', '')
                                bulk_row['Orders'] = m.get('Orders', '')
                            else:
                                bulk_row = {
                                    'Product': campaign_type,
                                    'Entity': 'Keyword',
                                    'Operation': 'update',
                                    'Campaign ID': '',
                                    'Ad Group ID': '',
                                    'Campaign Name': action.get('campaign_name', ''),
                                    'Ad Group Name': '',
                                    'Keyword ID': '',
                                    'Product Targeting ID': '',
                                    'State': 'enabled',
                                    'Keyword Text': '',
                                    'Match Type': '',
                                    'Product Targeting Expression': '',
                                    'Bid': action.get('new_bid', 0)
                                }
                    else:
                        bulk_row = action.copy()  # Fallback
                    
                    campaign_type_data[campaign_type].append(bulk_row)
                
                # Display preview for each campaign type that has data
                preview_tabs = []
                for camp_type, data in campaign_type_data.items():
                    if data:  # Only include if there's data
                        preview_tabs.append(camp_type)
                
                if preview_tabs:
                    preview_tab_objects = st.tabs(preview_tabs)
                    
                    for i, camp_type in enumerate(preview_tabs):
                        with preview_tab_objects[i]:
                            data = campaign_type_data[camp_type]
                            st.markdown(f"**{len(data)} items for {camp_type}:**")
                            
                            if data:
                                # Limit preview rows to avoid exceeding Streamlit message size limits
                                PREVIEW_ROW_LIMIT = 1000
                                preview_df = pd.DataFrame(data)
                                if len(preview_df) > PREVIEW_ROW_LIMIT:
                                    st.caption(f"Showing first {PREVIEW_ROW_LIMIT:,} rows out of {len(preview_df):,} for preview")
                                st.dataframe(preview_df.head(PREVIEW_ROW_LIMIT), use_container_width=True)
                            else:
                                st.info(f"No items for {camp_type}")
                else:
                    st.info("No items to preview. Add some actions first.")
                
                # Export options
                st.markdown("---")
                st.subheader("Export Options")
                
                export_format = st.selectbox(
                    "Export Format",
                    ["Excel (.xlsx)", "CSV (.csv)"],
                    key="export_format"
                )
                
                include_timestamp = st.checkbox("Include timestamp in filename", value=True)
                
                col1, col2, col3 = st.columns(3)

                with col1:
                    if st.button("Generate Bulk File", type="primary") or st.session_state.get("bulk_export_active", False):
                        # Generate/render the bulk file UI and downloads; persist across reruns
                        try:
                            import io
                            from datetime import datetime

                            # Create or reuse a stable filename for this export session
                            client_name = st.session_state.get('client_config', {}).get('client_name', 'Client').replace(' ', '_')
                            if st.session_state.get("bulk_export_active", False) and st.session_state.get("bulk_export_filename"):
                                filename = st.session_state["bulk_export_filename"]
                            else:
                                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S") if include_timestamp else ""
                                filename = f"{client_name}_Advertiser_Actions"
                                if timestamp:
                                    filename += f"_{timestamp}"
                                st.session_state["bulk_export_active"] = True
                                st.session_state["bulk_export_filename"] = filename
                                if "bulk_prepared_csv_parts" not in st.session_state:
                                    st.session_state["bulk_prepared_csv_parts"] = {}
                                # Clear any previously prepared part bytes for a clean session
                                for k in list(st.session_state.keys()):
                                    if isinstance(k, str) and k.startswith("bulk_prepared_xlsx_"):
                                        st.session_state.pop(k, None)
                            
                            if export_format == "Excel (.xlsx)":
                                # Define proper column order for each campaign type
                                def get_column_order(campaign_type):
                                    metrics_cols = ['Spend', 'Ad Sales', 'ACoS', 'ROAS', 'CPC', 'CVR', 'Orders']
                                    if campaign_type == 'Sponsored Display':
                                        return [
                                            'Product', 'Entity', 'Operation', 'Campaign ID', 'Ad Group ID',
                                            'Targeting ID', 'State', 'Targeting Expression',
                                            'Campaign Name', 'Ad Group Name',
                                            'Bid',
                                            'Start Date', 'Targeting Type', 'Daily Budget', 'Ad Group Default Bid', 'ASIN', 'SKU', 'Tactic'
                                        ] + metrics_cols
                                    else:  # Sponsored Products and Sponsored Brands
                                        return [
                                            'Product', 'Entity', 'Operation', 'Campaign ID', 'Ad Group ID',
                                            'Keyword ID', 'Product Targeting ID', 'State', 'Keyword Text',
                                            'Match Type', 'Product Targeting Expression',
                                            'Campaign Name', 'Ad Group Name',
                                            'Bid',
                                            'Bidding Strategy', 'Placement', 'Percentage',
                                            'Start Date', 'Targeting Type', 'Daily Budget', 'Ad Group Default Bid', 'ASIN', 'SKU', 'Tactic'
                                        ] + metrics_cols

                                # Avoid cumulative message size by preparing only one type (and optional part) per render
                                MAX_MSG_BYTES = int(190 * 1024 * 1024)  # keep headroom under Streamlit's 200MB default
                                XLSX_SIZE_LIMIT_BYTES = int(30 * 1024 * 1024)  # enforce 30MB per XLSX file
                                types_with_data = [ct for ct, d in campaign_type_data.items() if d]
                                if not types_with_data:
                                    st.info("No campaign types to export.")
                                else:
                                    # Helper to write a DataFrame to xlsx bytes
                                    def df_to_xlsx_bytes(df, sheet_name):
                                        bio = io.BytesIO()
                                        with pd.ExcelWriter(bio, engine='openpyxl') as writer:
                                            df.to_excel(writer, sheet_name=sheet_name, index=False)
                                        bio.seek(0)
                                        return bio.getvalue()

                                    # Excel per-sheet row limit and Streamlit message size headroom
                                    EXCEL_MAX_ROWS = 1_048_000
                                    SAFETY_FACTOR = 0.6  # keep under Streamlit message limit

                                    for sel_type in types_with_data:
                                        st.markdown(f"### {sel_type}")
                                        data = campaign_type_data.get(sel_type) or []
                                        export_df = pd.DataFrame(data)
                                        required_columns = get_column_order(sel_type)
                                        available_columns = [col for col in required_columns if col in export_df.columns]
                                        if available_columns:
                                            export_df = export_df[available_columns]

                                        # Skip if schema empty
                                        if export_df.shape[1] == 0:
                                            st.info(f"No columns available to export for {sel_type}. Skipping.")
                                            continue

                                        # Determine campaign grouping key to keep campaigns intact within parts
                                        campaign_key = None
                                        for k in ["Campaign ID", "Campaign Name"]:
                                            if k in export_df.columns:
                                                campaign_key = k
                                                break

                                        # Work on a campaign-grouped, stable ordering so all rows from a campaign are contiguous
                                        if campaign_key:
                                            # Ensure consistent dtype to avoid '<' not supported between int and str
                                            try:
                                                export_df[campaign_key] = export_df[campaign_key].astype(str)
                                            except Exception:
                                                pass
                                            working_df = export_df.sort_values(by=[campaign_key], kind="mergesort").reset_index(drop=True)
                                        else:
                                            working_df = export_df.reset_index(drop=True)

                                        total_rows = len(working_df)
                                        if total_rows == 0:
                                            st.info(f"No rows for {sel_type}.")
                                            continue

                                        # Try single file first if well within limits
                                        try:
                                            xlsx_bytes = df_to_xlsx_bytes(working_df, sel_type)
                                        except Exception as e:
                                            xlsx_bytes = b""
                                        if total_rows <= EXCEL_MAX_ROWS and xlsx_bytes and len(xlsx_bytes) <= XLSX_SIZE_LIMIT_BYTES:
                                            st.download_button(
                                                label=f"Download {filename}_{sel_type.replace(' ', '_')}.xlsx",
                                                data=xlsx_bytes,
                                                file_name=f"{filename}_{sel_type.replace(' ', '_')}.xlsx",
                                                mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                                                key=f"dl_single_{sel_type}"
                                            )
                                            st.success("âœ… File ready.")
                                            continue

                                        # Estimate bytes-per-row with a sample to size chunks safely
                                        SAMPLE_ROWS = min(5000, total_rows)
                                        sample_df = working_df.head(SAMPLE_ROWS)
                                        sample_bytes = df_to_xlsx_bytes(sample_df, sel_type)
                                        bytes_per_row = max(1, len(sample_bytes) // max(1, SAMPLE_ROWS))
                                        # Estimate rows per part targeting 30MB per XLSX (with a small safety margin)
                                        max_rows_by_size = max(1, int((XLSX_SIZE_LIMIT_BYTES * 0.95) // bytes_per_row))
                                        default_chunk_rows = max(1, min(EXCEL_MAX_ROWS, max_rows_by_size))
                                        # Allow override from session_state if we previously detected oversize parts
                                        chunk_override_key = f"bulk_chunk_rows_{sel_type}"
                                        CHUNK_ROWS = int(st.session_state.get(chunk_override_key, default_chunk_rows))

                                        # Build part bounds ensuring whole campaigns stay in the same part
                                        bounds = []  # list of (start, end) indices into working_df
                                        if campaign_key:
                                            # Pre-compute contiguous campaign segments in working_df
                                            groups = []  # list of (g_start, g_end)
                                            # working_df is sorted by campaign_key so groups are contiguous
                                            for _, grp in working_df.groupby(campaign_key, sort=False):
                                                g_start = int(grp.index.min())
                                                g_end = int(grp.index.max()) + 1  # exclusive
                                                groups.append((g_start, g_end))

                                            current_start = None
                                            current_end = None
                                            current_rows = 0
                                            for g_start, g_end in groups:
                                                glen = g_end - g_start
                                                if current_rows == 0:
                                                    # start a new part
                                                    current_start = g_start
                                                    current_end = g_end
                                                    current_rows = glen
                                                elif current_rows + glen <= CHUNK_ROWS:
                                                    # add to current part
                                                    current_end = g_end
                                                    current_rows += glen
                                                else:
                                                    # close current part and start a new one with this campaign
                                                    bounds.append((current_start, current_end))
                                                    current_start = g_start
                                                    current_end = g_end
                                                    current_rows = glen
                                            if current_rows > 0:
                                                bounds.append((current_start, current_end))

                                            # Handle pathological case: a single campaign larger than CHUNK_ROWS
                                            # In this case it occupies a single part on its own (may still hit message size; handled below on prepare)
                                        else:
                                            # Fallback: split by rows if we lack a campaign key
                                            start = 0
                                            while start < total_rows:
                                                end = min(total_rows, start + CHUNK_ROWS)
                                                bounds.append((start, end))
                                                start = end

                                        parts = len(bounds)
                                        st.info(f"{sel_type}: {total_rows:,} rows. Split into {parts} file(s). Prepare and download any part below.")

                                        with st.expander(f"{sel_type} parts (â‰¤ {CHUNK_ROWS:,} rows, 30MB max/file)", expanded=True):
                                            for part_idx, (start, end) in enumerate(bounds, start=1):
                                                fname = f"{filename}_{sel_type.replace(' ', '_')}_part{part_idx}of{parts}.xlsx"
                                                state_key = f"bulk_prepared_xlsx_{sel_type}_{part_idx}"
                                                name_key = f"bulk_prepared_xlsx_filename_{sel_type}_{part_idx}"

                                                cols = st.columns([1, 2, 2])
                                                with cols[0]:
                                                    st.caption(f"Rows {start+1:,}-{end:,}")
                                                with cols[1]:
                                                    if st.button(
                                                        f"Prepare part {part_idx}",
                                                        key=f"prepare_{sel_type}_{part_idx}"
                                                    ):
                                                        with st.spinner("Preparing Excel file..."):
                                                            part_df = working_df.iloc[start:end]
                                                            try:
                                                                part_bytes = df_to_xlsx_bytes(part_df, sel_type)
                                                            except Exception as e:
                                                                st.error(f"Failed to build XLSX for {sel_type} part {part_idx}: {e}")
                                                                part_bytes = b""
                                                        # If part is too large, respect campaign grouping constraints
                                                        oversize_by_bytes = bool(part_bytes) and len(part_bytes) > XLSX_SIZE_LIMIT_BYTES
                                                        oversize_by_rows = len(part_df) > EXCEL_MAX_ROWS
                                                        if oversize_by_bytes or oversize_by_rows:
                                                            # Determine if this part contains a single campaign only
                                                            single_campaign = False
                                                            campaign_value = None
                                                            if campaign_key:
                                                                sub = working_df.iloc[start:end]
                                                                uniq = sub[campaign_key].nunique(dropna=False)
                                                                single_campaign = (uniq == 1)
                                                                if single_campaign:
                                                                    try:
                                                                        campaign_value = str(sub[campaign_key].iloc[0])
                                                                    except Exception:
                                                                        campaign_value = None
                                                            if single_campaign:
                                                                size_mb = (len(part_bytes) / (1024*1024)) if part_bytes else 0
                                                                if oversize_by_rows:
                                                                    st.error(
                                                                        f"Campaign {campaign_value or ''} has {len(part_df):,} rows, exceeding the per-sheet limit ({EXCEL_MAX_ROWS:,}). "
                                                                        "This campaign cannot be exported without splitting it, which is not allowed. Please reduce scope or data volume."
                                                                    )
                                                                else:
                                                                    st.error(
                                                                        f"Campaign {campaign_value or ''} produces a part of {size_mb:.1f} MB, exceeding the 30MB per-file limit. "
                                                                        "This campaign cannot be exported without splitting it, which is not allowed. Please reduce scope or data volume."
                                                                    )
                                                                # Do not attempt to reduce chunk rows; cannot split a single campaign
                                                                continue
                                                            # Otherwise reduce rows per part to move campaigns between parts and rerun
                                                            ratio = max(2.0, ((len(part_bytes) if part_bytes else XLSX_SIZE_LIMIT_BYTES) / XLSX_SIZE_LIMIT_BYTES) * 1.25)
                                                            new_rows = max(1000, int(CHUNK_ROWS / ratio))
                                                            st.session_state[chunk_override_key] = new_rows
                                                            st.warning(
                                                                f"Part {part_idx} exceeded the 30MB limit. Reducing rows per part to {new_rows:,} and reloading..."
                                                            )
                                                            try:
                                                                st.rerun()
                                                            except Exception:
                                                                try:
                                                                    st.experimental_rerun()
                                                                except Exception:
                                                                    pass
                                                        st.session_state[state_key] = part_bytes
                                                        st.session_state[name_key] = fname
                                                        st.success(f"âœ… Prepared {fname}")
                                                        try:
                                                            st.toast(f"Prepared {fname}")
                                                        except Exception:
                                                            pass
                                                with cols[2]:
                                                    if st.session_state.get(state_key):
                                                        st.download_button(
                                                            label=f"Download {fname}",
                                                            data=st.session_state.get(state_key),
                                                            file_name=st.session_state.get(name_key, fname),
                                                            mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                                                            key=f"dl_{sel_type}_{part_idx}"
                                                        )
                                                        # Debug: show prepared size
                                                        st.caption(f"Size: {len(st.session_state.get(state_key)) / (1024*1024):.2f} MB")
                                        # Always-visible area listing any prepared downloads for this type
                                        prepared_any = False
                                        for part_idx in range(1, parts + 1):
                                            state_key = f"bulk_prepared_xlsx_{sel_type}_{part_idx}"
                                            name_key = f"bulk_prepared_xlsx_filename_{sel_type}_{part_idx}"
                                            if st.session_state.get(state_key):
                                                if not prepared_any:
                                                    st.markdown("#### Prepared downloads")
                                                    prepared_any = True
                                                fname = st.session_state.get(name_key, f"{filename}_{sel_type.replace(' ', '_')}_part{part_idx}of{parts}.xlsx")
                                                st.download_button(
                                                    label=f"Download {fname}",
                                                    data=st.session_state.get(state_key),
                                                    file_name=fname,
                                                    mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                                                    key=f"dl_prepared_{sel_type}_{part_idx}"
                                                )
                            
                            else:  # CSV format
                                # Combine all actions into a single CSV
                                all_export_data = []
                                
                                for action_type, actions in action_types.items():
                                    if action_type == 'negative_keyword':
                                        for action in actions:
                                            # Check entity type to determine proper handling
                                            if action.get('entity') == 'Campaign Negative Keyword':
                                                # Campaign Negative Keywords
                                                all_export_data.append({
                                                    'Action Type': 'Campaign Negative Keyword',
                                                    'Campaign ID': action.get('campaign_id', ''),
                                                    'Campaign Name': action['campaign_name'],
                                                    'Keyword Text': action['keyword'],
                                                    'Match Type': action['match_type'],
                                                    'State': 'enabled',
                                                    'Entity': action.get('entity', 'Campaign Negative Keyword'),
                                                    'Operation': action.get('operation', 'Create')
                                                })
                                            elif action.get('entity') == 'Negative Keyword':
                                                # Ad Group Negative Keywords
                                                all_export_data.append({
                                                    'Action Type': 'Negative Keyword',
                                                    'Campaign ID': action.get('campaign_id', ''),
                                                    'Campaign Name': action['campaign_name'],
                                                    'Ad Group ID': action.get('ad_group_id', ''),
                                                    'Ad Group Name': action.get('ad_group_name', ''),
                                                    'Keyword Text': action['keyword'],
                                                    'Match Type': action['match_type'],
                                                    'State': 'enabled',
                                                    'Entity': action.get('entity', 'Negative Keyword'),
                                                    'Operation': action.get('operation', 'Create')
                                                })
                                            elif action.get('entity') in ('Negative Product Targeting', 'Negative Product Target'):
                                                # Negative Product Targets (ASINs)
                                                export_row = {
                                                    'Action Type': 'Negative Product Targeting',
                                                    'Campaign ID': action.get('campaign_id', ''),
                                                    'Campaign Name': action['campaign_name'],
                                                    'Ad Group ID': action.get('ad_group_id', ''),
                                                    'Ad Group Name': action.get('ad_group_name', ''),
                                                    'State': 'enabled',
                                                    'Entity': 'Negative Product Targeting',
                                                    'Operation': action.get('operation', 'Create')
                                                }
                                                # Add targeting expression
                                                targeting_column = action.get('targeting_column', 'Product Targeting Expression')
                                                export_row[targeting_column] = action.get('targeting_expression', '')
                                                all_export_data.append(export_row)
                                            else:
                                                # Legacy format
                                                all_export_data.append({
                                                    'Action Type': 'Negative Keyword',
                                                    'Campaign Name': action['campaign_name'],
                                                    'Keyword Text': action.get('keyword', ''),
                                                    'Match Type': action.get('match_type', ''),
                                                    'State': 'enabled',
                                                    'Entity': 'Keyword',
                                                    'Operation': 'CREATE'
                                                })
                                    
                                    elif action_type == 'pause':
                                        for action in actions:
                                            data = action.get('data', {})
                                            all_export_data.append({
                                                'Action Type': 'Pause',
                                                'Campaign Name': data.get('campaign_name', ''),
                                                'Entity': data.get('type', '').title(),
                                                'Target/Item': data.get('target', data.get('campaign_name', '')),
                                                'State': 'paused',
                                                'Operation': 'UPDATE'
                                            })
                                    
                                    elif action_type == 'bid_optimization':
                                        for action in actions:
                                            # Determine campaign type for proper formatting
                                            campaign_type = 'Sponsored Products'  # Default
                                            if 'sheet_source' in action:
                                                sheet_source = action['sheet_source']
                                                if 'Brands' in sheet_source or 'SB' in sheet_source:
                                                    campaign_type = 'Sponsored Brands'
                                                elif 'Display' in sheet_source or 'SD' in sheet_source:
                                                    campaign_type = 'Sponsored Display'
                                            elif 'campaign_name' in action:
                                                campaign_name = action['campaign_name'].upper()
                                                if 'SB' in campaign_name or 'BRAND' in campaign_name:
                                                    campaign_type = 'Sponsored Brands'
                                                elif 'SD' in campaign_name or 'DISPLAY' in campaign_name:
                                                    campaign_type = 'Sponsored Display'
                                            
                                            # Use filtered row data if available
                                            if 'row_data' in action:
                                                row_data = action['row_data'].copy()
                                                
                                                # Create proper bulk file format based on campaign type
                                                if campaign_type == 'Sponsored Display':
                                                    # Sponsored Display format
                                                    bulk_row = {
                                                        'Product': campaign_type,
                                                        'Entity': row_data.get('Entity', 'Targeting'),
                                                        'Operation': 'update',
                                                        'Campaign ID': row_data.get('Campaign ID', ''),
                                                        'Ad Group ID': row_data.get('Ad Group ID', ''),
                                                        'Campaign Name': row_data.get('Campaign Name', row_data.get('Campaign Name (Informational Only)', '')),
                                                        'Ad Group Name': row_data.get('Ad Group Name', row_data.get('Ad Group Name (Informational Only)', '')),
                                                        'Targeting ID': row_data.get('Targeting ID', ''),
                                                        'State': row_data.get('State', 'enabled'),
                                                        'Targeting Expression': row_data.get('Targeting Expression', ''),
                                                        'Bid': row_data.get('Bid', action.get('new_bid', 0))
                                                    }
                                                    # Append metrics columns
                                                    m = action.get('metrics', {})
                                                    bulk_row['Spend'] = m.get('Spend', '')
                                                    bulk_row['Ad Sales'] = m.get('Ad Sales', '')
                                                    bulk_row['ACoS'] = m.get('ACoS', '')
                                                    bulk_row['ROAS'] = m.get('ROAS', '')
                                                    bulk_row['CPC'] = m.get('CPC', '')
                                                    bulk_row['CVR'] = m.get('CVR', '')
                                                    bulk_row['Orders'] = m.get('Orders', '')
                                                else:
                                                    # Sponsored Products and Sponsored Brands format
                                                    bulk_row = {
                                                        'Product': campaign_type,
                                                        'Entity': row_data.get('Entity', 'Keyword'),
                                                        'Operation': 'update',
                                                        'Campaign ID': row_data.get('Campaign ID', ''),
                                                        'Ad Group ID': row_data.get('Ad Group ID', ''),
                                                        'Campaign Name': row_data.get('Campaign Name', row_data.get('Campaign Name (Informational Only)', '')),
                                                        'Ad Group Name': row_data.get('Ad Group Name', row_data.get('Ad Group Name (Informational Only)', '')),
                                                        'Keyword ID': row_data.get('Keyword ID', ''),
                                                        'Product Targeting ID': row_data.get('Product Targeting ID', ''),
                                                        'State': row_data.get('State', 'enabled'),
                                                        'Keyword Text': row_data.get('Keyword Text', row_data.get('Keyword', '')),
                                                        'Match Type': row_data.get('Match Type', ''),
                                                        'Product Targeting Expression': row_data.get('Product Targeting Expression', ''),
                                                        'Bid': row_data.get('Bid', action.get('new_bid', 0))
                                                    }
                                                    # Append metrics columns
                                                    m = action.get('metrics', {})
                                                    bulk_row['Spend'] = m.get('Spend', '')
                                                    bulk_row['Ad Sales'] = m.get('Ad Sales', '')
                                                    bulk_row['ACoS'] = m.get('ACoS', '')
                                                    bulk_row['ROAS'] = m.get('ROAS', '')
                                                    bulk_row['CPC'] = m.get('CPC', '')
                                                    bulk_row['CVR'] = m.get('CVR', '')
                                                    bulk_row['Orders'] = m.get('Orders', '')
                                                all_export_data.append(bulk_row)
                                            else:
                                                # Fallback for older format
                                                if campaign_type == 'Sponsored Display':
                                                    bulk_row = {
                                                        'Product': campaign_type,
                                                        'Entity': 'Targeting',
                                                        'Operation': 'update',
                                                        'Campaign ID': '',
                                                        'Ad Group ID': '',
                                                        'Campaign Name': action.get('campaign_name', ''),
                                                        'Ad Group Name': '',
                                                        'Targeting ID': '',
                                                        'State': 'enabled',
                                                        'Targeting Expression': '',
                                                        'Bid': action.get('new_bid', 0)
                                                    }
                                                    # Append metrics columns
                                                    m = action.get('metrics', {})
                                                    bulk_row['Spend'] = m.get('Spend', '')
                                                    bulk_row['Ad Sales'] = m.get('Ad Sales', '')
                                                    bulk_row['ACoS'] = m.get('ACoS', '')
                                                    bulk_row['ROAS'] = m.get('ROAS', '')
                                                    bulk_row['CPC'] = m.get('CPC', '')
                                                    bulk_row['CVR'] = m.get('CVR', '')
                                                    bulk_row['Orders'] = m.get('Orders', '')
                                                else:
                                                    bulk_row = {
                                                        'Product': campaign_type,
                                                        'Entity': 'Keyword',
                                                        'Operation': 'update',
                                                        'Campaign ID': '',
                                                        'Ad Group ID': '',
                                                        'Campaign Name': action.get('campaign_name', ''),
                                                        'Ad Group Name': '',
                                                        'Keyword ID': '',
                                                        'Product Targeting ID': '',
                                                        'State': 'enabled',
                                                        'Keyword Text': '',
                                                        'Match Type': '',
                                                        'Product Targeting Expression': '',
                                                        'Bid': action.get('new_bid', 0)
                                                    }
                                                    # Append metrics columns
                                                    m = action.get('metrics', {})
                                                    bulk_row['Spend'] = m.get('Spend', '')
                                                    bulk_row['Ad Sales'] = m.get('Ad Sales', '')
                                                    bulk_row['ACoS'] = m.get('ACoS', '')
                                                    bulk_row['ROAS'] = m.get('ROAS', '')
                                                    bulk_row['CPC'] = m.get('CPC', '')
                                                    bulk_row['CVR'] = m.get('CVR', '')
                                                    bulk_row['Orders'] = m.get('Orders', '')
                                                all_export_data.append(bulk_row)
                                
                                if all_export_data:
                                    export_df = pd.DataFrame(all_export_data)
                                    MAX_MSG_BYTES = int(190 * 1024 * 1024)
                                    # Determine campaign grouping key to keep campaigns intact within parts
                                    campaign_key = None
                                    for k in ["Campaign ID", "Campaign Name"]:
                                        if k in export_df.columns:
                                            campaign_key = k
                                            break

                                    # Work on a grouped ordering so all rows from a campaign are contiguous
                                    if campaign_key:
                                        # Ensure consistent dtype to avoid '<' not supported between int and str
                                        try:
                                            export_df[campaign_key] = export_df[campaign_key].astype(str)
                                        except Exception:
                                            pass
                                        working_df = export_df.sort_values(by=[campaign_key], kind="mergesort").reset_index(drop=True)
                                    else:
                                        working_df = export_df.reset_index(drop=True)
                                    csv_all_bytes = working_df.to_csv(index=False).encode('utf-8')
                                    if len(csv_all_bytes) <= MAX_MSG_BYTES:
                                        st.download_button(
                                            label=f"Download {filename}.csv",
                                            data=csv_all_bytes,
                                            file_name=f"{filename}.csv",
                                            mime="text/csv"
                                        )
                                        st.success("âœ… Bulk file generated successfully!")
                                    else:
                                        # Adaptive chunk size by estimated bytes per row
                                        rows = len(working_df)
                                        sample_rows = min(rows, 2000)
                                        sample_bytes = working_df.head(sample_rows).to_csv(index=False).encode('utf-8')
                                        bytes_per_row = max(1, len(sample_bytes) // max(1, sample_rows))
                                        default_chunk_rows = max(1, int((MAX_MSG_BYTES * 0.9) // bytes_per_row))
                                        chunk_override_key = "bulk_chunk_rows_csv"
                                        CHUNK_ROWS_CSV = int(st.session_state.get(chunk_override_key, default_chunk_rows))

                                        # Build part bounds ensuring whole campaigns stay in the same part
                                        bounds = []
                                        if campaign_key:
                                            groups = []
                                            for _, grp in working_df.groupby(campaign_key, sort=False):
                                                g_start = int(grp.index.min())
                                                g_end = int(grp.index.max()) + 1
                                                groups.append((g_start, g_end))
                                            cur_start = None
                                            cur_end = None
                                            cur_rows = 0
                                            for g_start, g_end in groups:
                                                glen = g_end - g_start
                                                if cur_rows == 0:
                                                    cur_start, cur_end, cur_rows = g_start, g_end, glen
                                                elif cur_rows + glen <= CHUNK_ROWS_CSV:
                                                    cur_end, cur_rows = g_end, cur_rows + glen
                                                else:
                                                    bounds.append((cur_start, cur_end))
                                                    cur_start, cur_end, cur_rows = g_start, g_end, glen
                                            if cur_rows > 0:
                                                bounds.append((cur_start, cur_end))
                                        else:
                                            start = 0
                                            while start < rows:
                                                end = min(rows, start + CHUNK_ROWS_CSV)
                                                bounds.append((start, end))
                                                start = end

                                        parts = len(bounds)
                                        st.info(f"CSV is large ({len(csv_all_bytes)/(1024*1024):.1f} MB). Split into {parts} files. Select a part to download.")
                                        csv_part_to_download = st.number_input(
                                            "CSV part to download",
                                            min_value=1,
                                            max_value=parts,
                                            value=1,
                                            step=1,
                                            key="bulk_csv_part_idx"
                                        )
                                        start, end = bounds[int(csv_part_to_download) - 1]
                                        part_df = working_df.iloc[start:end]
                                        part_csv = part_df.to_csv(index=False).encode('utf-8')
                                        # If too large anyway, handle respecting campaign grouping
                                        if len(part_csv) > MAX_MSG_BYTES:
                                            single_campaign = False
                                            campaign_value = None
                                            if campaign_key:
                                                sub = working_df.iloc[start:end]
                                                uniq = sub[campaign_key].nunique(dropna=False)
                                                single_campaign = (uniq == 1)
                                                if single_campaign:
                                                    try:
                                                        campaign_value = str(sub[campaign_key].iloc[0])
                                                    except Exception:
                                                        campaign_value = None
                                            if single_campaign:
                                                st.error(
                                                    f"Campaign {campaign_value or ''} produces a CSV part of {len(part_csv)/(1024*1024):.1f} MB, exceeding the message size limit. "
                                                    "This campaign cannot be exported without splitting it across parts, which is not allowed. Please reduce scope or data volume."
                                                )
                                                # Do not adjust chunk size; cannot split a single campaign
                                            else:
                                                ratio = max(2.0, (len(part_csv) / MAX_MSG_BYTES) * 1.25)
                                                new_rows = max(1000, int(CHUNK_ROWS_CSV / ratio))
                                                st.session_state[chunk_override_key] = new_rows
                                                st.warning(
                                                    f"Selected part is {len(part_csv)/(1024*1024):.1f} MB which exceeds the limit. "
                                                    f"Reducing rows per part to {new_rows:,} and reloading..."
                                                )
                                                try:
                                                    st.rerun()
                                                except Exception:
                                                    try:
                                                        st.experimental_rerun()
                                                    except Exception:
                                                        pass
                                        if csv_part_to_download not in st.session_state.bulk_prepared_csv_parts:
                                            st.session_state.bulk_prepared_csv_parts[csv_part_to_download] = part_csv
                                        st.download_button(
                                            label=f"Download {st.session_state.bulk_export_filename}_part{int(csv_part_to_download)}of{parts}.csv",
                                            data=st.session_state.bulk_prepared_csv_parts[csv_part_to_download],
                                            file_name=f"{st.session_state.bulk_export_filename}_part{int(csv_part_to_download)}of{parts}.csv",
                                            mime="text/csv"
                                        )
                                        st.caption(f"Size: {len(st.session_state.bulk_prepared_csv_parts[csv_part_to_download]) / (1024*1024):.2f} MB")
                                        st.success("âœ… Bulk CSV part ready!")
                        
                        except Exception as e:
                            st.error(f"Error generating bulk file: {str(e)}")
                
                with col2:
                    if st.button("Reset export"):
                        # Clear export session and any prepared data
                        for k in list(st.session_state.keys()):
                            if isinstance(k, str) and (
                                k.startswith("bulk_prepared_xlsx_") or
                                k.startswith("bulk_chunk_rows_") or
                                k in ("bulk_export_active", "bulk_export_filename", "bulk_prepared_csv_parts", "bulk_chunk_rows_csv")
                            ):
                                st.session_state.pop(k, None)
                        try:
                            st.rerun()
                        except Exception:
                            try:
                                st.experimental_rerun()
                            except Exception:
                                pass
                        
                with col2:
                    if st.button("Preview Changes", type="secondary"):
                        st.markdown("**ðŸ“‹ Preview of Changes:**")
                        
                        total_actions = len(st.session_state.bulk_export_actions)
                        st.info(f"Total actions planned: {total_actions}")
                        
                        # Show a summary of what will be changed
                        for action_type, actions in action_types.items():
                            st.markdown(f"**{action_type.replace('_', ' ').title()}:** {len(actions)} items")
                
                with col3:
                    if st.button("Clear All Actions", type="secondary"):
                        st.session_state.bulk_export_actions = []
                        st.success("All planned actions cleared!")
                        st.rerun()
            
            else:
                st.info("ðŸ“ No actions planned yet. Use the other tabs to add negative keywords and pause items.")
                st.markdown("""
                **How to use Bulk File Export:**
                1. Go to **Negative Keywords** tab to add negative keywords
                2. Go to **Pauses** tab to select items to pause  
                3. Return here to generate and download your bulk file
                4. Upload the file to Amazon Ads to apply changes
                """)

    elif st.session_state.current_page == "advertising_audit":
        # Check if settings have been updated and refresh data if needed
        if st.session_state.get('settings_updated', False):
            # Clear any cached targeting data to force recalculation
            if 'targeting_data' in st.session_state:
                del st.session_state.targeting_data
            if 'branded_targets_df' in st.session_state:
                del st.session_state.branded_targets_df
            if 'non_branded_targets_df' in st.session_state:
                del st.session_state.non_branded_targets_df
            if 'classified_campaigns' in st.session_state:
                del st.session_state.classified_campaigns
            
            # Reset the flag
            st.session_state.settings_updated = False
            
            # Add a notification that data has been refreshed
            # Silently apply changes without toast notification
            
        # Professional header with title and logo
        client_name = st.session_state.client_config.get('client_name', 'Client') if st.session_state.get('client_config') else 'Client'
        
        # Load the logo
        logo_path = "assets/hand_logo.png"
        logo_html = ""
        if os.path.exists(logo_path):
            # Convert the image to base64 for inline HTML display
            with open(logo_path, "rb") as img_file:
                img_data = base64.b64encode(img_file.read()).decode('utf-8')
                logo_html = f'<img src="data:image/png;base64,{img_data}" class="header-logo" alt="Hand Logo">'
        
        # Create a professional header with title and logo on the same line
        st.markdown(f"""
            <div class='header-container'>
                <span class='dashboard-hero-title'>
                    Advertising Audit - <span class='client-name'>{client_name}</span>
                </span>
                {logo_html}
            </div>
            <div class='header-divider'></div>
        """, unsafe_allow_html=True)

        
        # Global Sales Attribution Toggle - only shown on Advertising Audit page
        is_companion = st.session_state.get('is_companion_data', False)
        st.session_state.debug_messages.append(f"Is companion data: {is_companion}")

        # Only show Sales Attribution Model section for bulk uploads (not companion exports)
        if not is_companion:
            # Create two columns to place controls side by side
            attr_col, sales_col = st.columns([1, 1])

            with attr_col:
                st.markdown("### Sales Attribution Model")
                # Let user choose attribution window for SD campaigns, but only for bulk uploads
                sd_attribution_choice = st.radio(
                    "Sponsored Display Sales Attribution:",
                    ["Sales", "Sales (Views & Clicks)"],
                    horizontal=True,
                    help="Choose which sales attribution model to use for Sponsored Display campaigns. (Views & Clicks) attribution can cause inflated ad sales numbers, but these are the ad sales that would match the AMS UI",
                    key="global_sd_attribution"
                )
                
                # Store the selection in session state for global access
                if 'sd_attribution_choice' not in st.session_state or st.session_state.sd_attribution_choice != sd_attribution_choice:
                    st.session_state.sd_attribution_choice = sd_attribution_choice
                    clear_caches()
                    st.session_state.debug_messages.append(f"Attribution model changed to: {sd_attribution_choice}, caches cleared")
                    st.rerun()

            with sales_col:
                # This UI should appear for both bulk and companion uploads, as long as a sales report is available
                sales_df = st.session_state.get('sales_report_data')
                available_sales_metrics = []
                
                if isinstance(sales_df, pd.DataFrame):
                    potential_sales_metrics = [
                        'Ordered Revenue', 'Shipped Revenue', 'Ordered Product Sales', 
                        'Shipped Product Sales', 'Product Sales', 'Gross Product Sales', 
                        'Shipped COGS', 'Total Sales', 'Net Sales', 'Revenue'
                    ]
                    for metric in potential_sales_metrics:
                        if metric in sales_df.columns:
                            available_sales_metrics.append(metric)
                
                st.session_state.debug_messages.append(f"Available sales metrics found: {available_sales_metrics}")
                
                if len(available_sales_metrics) > 1:
                    st.markdown("### Total Sales Metric")
                    if 'selected_total_sales_metric' not in st.session_state or st.session_state.selected_total_sales_metric not in available_sales_metrics:
                        st.session_state.selected_total_sales_metric = available_sales_metrics[0]
                    
                    selected_sales_metric = st.selectbox(
                        "Use which Total Sales metric?",
                        options=available_sales_metrics,
                        index=available_sales_metrics.index(st.session_state.selected_total_sales_metric),
                        help="Choose which sales metric column to use for Total Sales calculations throughout the dashboard",
                        key="global_total_sales_metric"
                    )
                    st.session_state.selected_total_sales_metric = selected_sales_metric
                elif len(available_sales_metrics) == 1:
                    st.session_state.selected_total_sales_metric = available_sales_metrics[0]
                    st.session_state.debug_messages.append(f"Only one metric available: {available_sales_metrics[0]}, not showing dropdown")
                else:
                    # No sales metrics found, default to 'Total Sales'
                    st.session_state.selected_total_sales_metric = 'Total Sales'
                    st.session_state.debug_messages.append("No sales metrics found, defaulting to 'Total Sales'")
        else:
            # For companion data, force to "Sales" since Views & Clicks data is not available
            sd_attribution_choice = "Sales"
            # Store the selection in session state for global access
            if 'sd_attribution_choice' not in st.session_state or st.session_state.sd_attribution_choice != sd_attribution_choice:
                st.session_state.sd_attribution_choice = sd_attribution_choice
                clear_caches()
                st.session_state.debug_messages.append(f"Attribution model changed to: {sd_attribution_choice}, caches cleared")
                st.rerun()
            
            # For companion data, still show the sales metric selector if available
            sales_df = st.session_state.get('sales_report_data')
            available_sales_metrics = []
            
            if isinstance(sales_df, pd.DataFrame):
                potential_sales_metrics = [
                    'Ordered Revenue', 'Shipped Revenue', 'Ordered Product Sales', 
                    'Shipped Product Sales', 'Product Sales', 'Gross Product Sales', 
                    'Shipped COGS', 'Total Sales', 'Net Sales', 'Revenue'
                ]
                for metric in potential_sales_metrics:
                    if metric in sales_df.columns:
                        available_sales_metrics.append(metric)
            
            st.session_state.debug_messages.append(f"Available sales metrics found: {available_sales_metrics}")
            
            if len(available_sales_metrics) > 1:
                st.markdown("### Total Sales Metric")
                if 'selected_total_sales_metric' not in st.session_state or st.session_state.selected_total_sales_metric not in available_sales_metrics:
                    st.session_state.selected_total_sales_metric = available_sales_metrics[0]
                
                selected_sales_metric = st.selectbox(
                    "Use which Total Sales metric?",
                    options=available_sales_metrics,
                    index=available_sales_metrics.index(st.session_state.selected_total_sales_metric),
                    help="Choose which sales metric column to use for Total Sales calculations throughout the dashboard",
                    key="companion_total_sales_metric"
                )
                st.session_state.selected_total_sales_metric = selected_sales_metric
            elif len(available_sales_metrics) == 1:
                st.session_state.selected_total_sales_metric = available_sales_metrics[0]
                st.session_state.debug_messages.append(f"Only one metric available: {available_sales_metrics[0]}, not showing dropdown")
            else:
                # No sales metrics found, default to 'Total Sales'
                st.session_state.selected_total_sales_metric = 'Total Sales'
                st.session_state.debug_messages.append("No sales metrics found, defaulting to 'Total Sales'")
        
        st.divider()
        
        # Create navigation bar for Advertising Audit sections
        st.markdown("""
            <style>
            .audit-nav-container {
                display: flex;
                justify-content: flex-end;
                align-items: center;
                padding-right: 1.5rem;
                gap: 0.7rem;
            }
            .audit-nav-link {
                font-size: 0.98rem;
            }
            </style>
            <div class="audit-nav-container" id="nav-bar">
                <a href="#account-overview" class="audit-nav-link" data-section="account-overview">Overview</a>
                <a href="#branded-vs-non-branded-performance" class="audit-nav-link" data-section="branded-vs-non-branded-performance">Branded vs. Non-Branded</a>
                <a href="#targeting-performance" class="audit-nav-link" data-section="targeting-performance">Targeting</a>
                <a href="#search-term-performance" class="audit-nav-link" data-section="search-term-performance">Search Terms</a>
                <a href="#contradicting-targets" class="audit-nav-link" data-section="contradicting-targets">Contradicting Targets</a>
                <a href="#wasted-spend" class="audit-nav-link" data-section="wasted-spend">Wasted Spend</a>
                <a href="#acos-range-spend-distribution" class="audit-nav-link" data-section="acos-range-spend-distribution">ACoS Distribution</a>
                <a href="#performance-by-tactic-ad-type-match-type" class="audit-nav-link" data-section="performance-by-tactic-ad-type-match-type">Tactic Performance</a>
                <a href="#product-analysis" class="audit-nav-link" data-section="product-analysis">Products</a>
            </div>
            
            <script>
            // JavaScript to handle active section highlighting
            document.addEventListener('DOMContentLoaded', function() {
                // Get all section anchors
                const sections = Array.from(document.querySelectorAll('.section-anchor'));
                const navLinks = document.querySelectorAll('.audit-nav-link');

                // Helper: get scroll position and section offset
                function getSectionTops() {
                    return sections.map(section => ({
                        id: section.id,
                        top: section.getBoundingClientRect().top + window.scrollY
                    }));
                }

                function highlightNavOnScroll() {
                    const scrollPos = window.scrollY + 110; // adjust for nav bar height
                    let currentSection = sections[0] ? sections[0].id : '';
                    let sectionTops = getSectionTops();

                    // Find the last section whose top is above scrollPos
                    for (let i = 0; i < sectionTops.length; i++) {
                        if (scrollPos >= sectionTops[i].top) {
                            currentSection = sectionTops[i].id;
                        }
                    }

                    navLinks.forEach(link => {
                        link.classList.remove('active');
                        if (link.getAttribute('data-section') === currentSection) {
                            link.classList.add('active');
                        }
                    });
                }

                // Navigation highlighting disabled per user request
                // window.addEventListener('scroll', highlightNavOnScroll);
                // window.addEventListener('resize', highlightNavOnScroll);
                // highlightNavOnScroll();
            });
            </script>
        """, unsafe_allow_html=True)
        
        # Display Account Overview directly without tabs
        
        # Add anchor for Account Overview with reduced padding
        st.markdown("<div id='account-overview' class='section-anchor'></div>", unsafe_allow_html=True)
        st.markdown("<span class='main-section-header dashboard-section'>Account Overview</span>", unsafe_allow_html=True)
        st.markdown("<div style='margin-bottom:0.5rem;'></div>", unsafe_allow_html=True)
        
        # Check if we have bulk data and sales report data, OR if we have companion data
        is_companion = st.session_state.get('is_companion_data', False)
        has_bulk_data = 'bulk_data' in st.session_state and st.session_state.bulk_data
        has_sales_data = 'sales_report_data' in st.session_state and st.session_state.sales_report_data is not None
        
        # Debug logging for Account Overview execution conditions
        st.session_state.debug_messages.append(f"[Account Overview] Debug - is_companion: {is_companion}")
        st.session_state.debug_messages.append(f"[Account Overview] Debug - has_bulk_data: {has_bulk_data}")
        st.session_state.debug_messages.append(f"[Account Overview] Debug - has_sales_data: {has_sales_data}")
        st.session_state.debug_messages.append(f"[Account Overview] Debug - Condition met: {has_bulk_data and (has_sales_data or is_companion)}")
        
        if has_bulk_data and (has_sales_data or is_companion):
            # Use the global sales attribution choice from session state
            sd_attribution_choice = st.session_state.sd_attribution_choice
            
            # Add debug information about column search
            st.session_state.debug_messages.append(f"Using global Sales Attribution: {sd_attribution_choice}")
            st.session_state.debug_messages.append(f"Looking for patterns: {['Sales', 'Total Sales', '7 Day Total Sales']}")
            st.session_state.debug_messages.append(f"Looking for patterns: {['Orders', '7 Day Total Orders (#)']}")
            
            # Process data for metrics
            try:
                bulk_data = st.session_state.bulk_data
                # For both bulk and companion uploads, use Business Report data if available
                sales_df = st.session_state.get('sales_report_data') if 'sales_report_data' in st.session_state else None
                if sales_df is not None:
                    st.session_state.debug_messages.append(f"[Account Overview] Business Report data loaded: {sales_df.shape[0]} rows, {sales_df.shape[1]} columns")
                    st.session_state.debug_messages.append(f"[Account Overview] Business Report columns: {list(sales_df.columns)}")
                else:
                    st.session_state.debug_messages.append(f"[Account Overview] No Business Report data available")
                
                # Add debug logging for companion vs bulk data processing
                st.session_state.debug_messages.append(f"[Account Overview] Processing {'companion' if is_companion else 'bulk'} data with {'sales report' if has_sales_data else 'no sales report'}")
                combined_ad_data_list = []
                found_sheets = []
                
                # Define sheets to aggregate for ad performance
                ad_performance_sheets = [
                    'Sponsored Products Campaigns',
                    'Sponsored Brands Campaigns',
                    'Sponsored Display Campaigns'
                ]
                
                # Define column patterns to search for
                sales_patterns = ['Sales', 'Total Sales', '7 Day Total Sales']
                orders_patterns = ['Orders', '7 Day Total Orders (#)']
                
                # Define the Helper Function HERE
                def find_col_pattern(df_cols, patterns, case_sensitive=False):
                    """Finds the first matching column name from a list."""
                    # First try exact matches
                    for pattern in patterns:
                        if pattern in df_cols:
                            return pattern
                    
                    # Then try case-insensitive if allowed
                    if not case_sensitive:
                        df_cols_lower = [col.lower() for col in df_cols]
                        for pattern in patterns:
                            pattern_lower = pattern.lower()
                            if pattern_lower in df_cols_lower:
                                idx = df_cols_lower.index(pattern_lower)
                                return df_cols[idx]  # Return original case
                        
                    # Finally try partial matches
                    for pattern in patterns:
                        pattern_lower = pattern.lower()
                        for col in df_cols:
                            if pattern_lower in col.lower():
                                return col
                    
                    # If no match found
                    return None
                
                # --- 1. Aggregate Ad Data from Specific Campaign Sheets ---
                ad_spend_col = 'Spend'
                ad_imp_col = 'Impressions'
                ad_clicks_col = 'Clicks'
                
                # Add debug logging for companion data
                is_companion = st.session_state.get('is_companion_data', False)
                st.session_state.debug_messages.append(f"[Account Overview] Processing {'companion' if is_companion else 'bulk'} data")
                st.session_state.debug_messages.append(f"[Account Overview] Available sheets: {list(bulk_data.keys())}")
                st.session_state.debug_messages.append(f"[Account Overview] Looking for sheets: {ad_performance_sheets}")
                
                # Iterate through the defined sheets
                for sheet_name in ad_performance_sheets:
                    if sheet_name in bulk_data:
                        found_sheets.append(sheet_name)
                        df_sheet = bulk_data[sheet_name]
                        st.session_state.debug_messages.append(f"[Account Overview] Found sheet {sheet_name} with {len(df_sheet)} rows")
                        st.session_state.debug_messages.append(f"[Account Overview] Columns in {sheet_name}: {list(df_sheet.columns)}")
                        
                        # Determine which sales column to use based on sheet type and user selection
                        if 'Sponsored Display' in sheet_name:
                            if sd_attribution_choice == "Sales (Views & Clicks)":
                                current_sales_patterns = ['Sales (Views & Clicks)', 'Total Sales (Views & Clicks)']
                            else:
                                current_sales_patterns = ['Sales']
                        else:
                            # For non-SD campaigns, always use the default sales patterns
                            current_sales_patterns = sales_patterns
                        
                        # Find the appropriate columns in this sheet
                        sheet_sales_col = find_col_pattern(df_sheet.columns, current_sales_patterns)
                        sheet_orders_col = find_col_pattern(df_sheet.columns, orders_patterns)
                        
                        # Check if required columns exist
                        missing_cols = [col for col in [ad_spend_col, ad_imp_col, ad_clicks_col] if col not in df_sheet.columns]
                        if missing_cols:
                            st.session_state.debug_messages.append(f"Missing required columns in {sheet_name}: {missing_cols}")
                            st.session_state.debug_messages.append(f"Available columns in {sheet_name}: {list(df_sheet.columns)}")
                            st.warning(f"Missing required metric columns in {sheet_name}: {missing_cols}. Skipping.")
                            continue
                        
                        # Extract and rename columns
                        cols_to_extract = {
                            ad_spend_col: 'Spend',
                            ad_imp_col: 'Impressions',
                            ad_clicks_col: 'Clicks'
                        }
                        
                        if sheet_sales_col:
                            cols_to_extract[sheet_sales_col] = 'Sales'
                        if sheet_orders_col:
                            cols_to_extract[sheet_orders_col] = 'Orders'
                        
                        # Handle different data types: Campaign vs Targeting data
                        filtered_df = df_sheet
                        
                        # Check if this is companion data
                        is_companion = st.session_state.get('is_companion_data', False)
                        
                        if is_companion:
                            # For companion data, use all rows as they're already at the appropriate aggregation level
                            st.session_state.debug_messages.append(f"Using companion data from {sheet_name}: {len(df_sheet)} rows (no Entity filtering needed)")
                        elif 'Entity' in df_sheet.columns:
                            # Traditional bulk data processing with Entity filtering
                            # Check if we have campaign-level data or targeting-level data
                            entities = df_sheet['Entity'].str.lower().fillna('').unique()
                            has_campaign_rows = any('campaign' in entity for entity in entities)
                            has_targeting_rows = any(entity in ['keyword', 'product targeting', 'auto', 'product target'] for entity in entities)
                            
                            if has_campaign_rows:
                                # Use campaign-level data (traditional bulk data)
                                filtered_df = df_sheet[df_sheet['Entity'].str.lower().fillna('') == 'campaign']
                                st.session_state.debug_messages.append(f"Using campaign-level data from {sheet_name}: {len(filtered_df)} rows from {len(df_sheet)} total")
                            elif has_targeting_rows:
                                # Use all targeting data and aggregate (Targeting Export)
                                # Filter for enabled targeting rows only
                                state_mask = True
                                # Remove state filtering - include all data regardless of state
                                
                                targeting_mask = df_sheet['Entity'].str.lower().fillna('').isin(['keyword', 'product targeting', 'auto', 'product target'])
                                filtered_df = df_sheet[targeting_mask & state_mask]
                                st.session_state.debug_messages.append(f"Using targeting-level data from {sheet_name}: {len(filtered_df)} targeting rows from {len(df_sheet)} total")
                            else:
                                # No recognizable entity types, use all data
                                st.session_state.debug_messages.append(f"No recognized entity types in {sheet_name}, using all {len(df_sheet)} rows")
                        else:
                            # No Entity column (could be companion data or other format)
                            st.session_state.debug_messages.append(f"No Entity column in {sheet_name}, using all {len(df_sheet)} rows")
                        
                        # Extract only the necessary columns
                        st.session_state.debug_messages.append(f"[Account Overview] Extracting columns from {sheet_name}: {list(cols_to_extract.keys())}")
                        extracted_data = filtered_df[list(cols_to_extract.keys())].rename(columns=cols_to_extract)
                        st.session_state.debug_messages.append(f"[Account Overview] Extracted data shape: {extracted_data.shape}")
                        
                        # Ensure all extracted columns are numeric
                        for col in extracted_data.columns:
                            extracted_data[col] = pd.to_numeric(extracted_data[col], errors='coerce')
                        
                        extracted_data = extracted_data.fillna(0)
                        st.session_state.debug_messages.append(f"[Account Overview] Final extracted data totals - Spend: {extracted_data['Spend'].sum()}, Sales: {extracted_data.get('Sales', pd.Series([0])).sum()}")
                        combined_ad_data_list.append(extracted_data)
                    else:
                        st.session_state.debug_messages.append(f"[Account Overview] Sheet {sheet_name} not found in bulk_data")
                    
                st.session_state.debug_messages.append(f"[Account Overview] Combined data list length: {len(combined_ad_data_list)}")
                if combined_ad_data_list:
                    # Combine all the data
                    st.session_state.debug_messages.append(f"[Account Overview] Combining {len(combined_ad_data_list)} data frames")
                    aggregated_ad_data = pd.concat(combined_ad_data_list, ignore_index=True)
                    st.session_state.debug_messages.append(f"[Account Overview] Final aggregated data shape: {aggregated_ad_data.shape}")
                    
                    # Sum up the metrics
                    total_ad_spend = aggregated_ad_data['Spend'].sum()
                    total_impressions = aggregated_ad_data['Impressions'].sum()
                    total_clicks = aggregated_ad_data['Clicks'].sum()
                    total_ad_sales = aggregated_ad_data['Sales'].sum() if 'Sales' in aggregated_ad_data else 0
                    total_ad_orders = aggregated_ad_data['Orders'].sum() if 'Orders' in aggregated_ad_data else 0

                    # Override with Campaign Performance table to ensure consistency across sections
                    try:
                        cp_df = get_campaign_performance_data(bulk_data, st.session_state.client_config)
                        if isinstance(cp_df, pd.DataFrame) and not cp_df.empty:
                            cp_spend = pd.to_numeric(cp_df['Spend'], errors='coerce').fillna(0).sum()
                            cp_sales = pd.to_numeric(cp_df['Ad Sales'], errors='coerce').fillna(0).sum()
                            cp_clicks = pd.to_numeric(cp_df['Clicks'], errors='coerce').fillna(0).sum() if 'Clicks' in cp_df.columns else 0
                            cp_orders = pd.to_numeric(cp_df['Orders'], errors='coerce').fillna(0).sum() if 'Orders' in cp_df.columns else 0
                            total_ad_spend = cp_spend
                            total_ad_sales = cp_sales
                            total_clicks = cp_clicks
                            total_ad_orders = cp_orders
                            st.session_state.debug_messages.append("[Account Overview] Totals overridden from Campaign Performance table for consistency")
                    except Exception as e:
                        st.session_state.debug_messages.append(f"[Account Overview] Could not override from Campaign Performance: {e}")
                    
                    # Find total sales and sessions from sales report (if available)
                    total_sales_col = None
                    sessions_col = None
                    total_sales_revenue = None
                    total_sessions = None
                    
                    if sales_df is not None:
                        # Use user-selected Total Sales metric if available, otherwise fall back to default search
                        if 'selected_total_sales_metric' in st.session_state and st.session_state.selected_total_sales_metric in sales_df.columns:
                            total_sales_col = st.session_state.selected_total_sales_metric
                        else:
                            total_sales_col = find_col_pattern(sales_df.columns, ['Total Sales'])
                        sessions_col = find_col_pattern(sales_df.columns, ['Sessions'])
                        
                        # Calculate total sales revenue
                        if total_sales_col:
                            total_sales_revenue = pd.to_numeric(sales_df[total_sales_col], errors='coerce').sum()
                            
                        # Calculate total sessions
                        if sessions_col:
                            total_sessions = pd.to_numeric(sales_df[sessions_col], errors='coerce').sum()
                    else:
                        st.session_state.debug_messages.append(f"[Account Overview] No sales report data available - using ad sales only for calculations")
                    
                    # --- PROCESS BRANDED/NON-BRANDED DATA BEFORE MAIN DISPLAY ---
                    branded_targets_df = pd.DataFrame()
                    non_branded_targets_df = pd.DataFrame()
                    branded_metrics = None
                    non_branded_metrics = None
                    
                    try:
                        # Process branded/non-branded data directly for Account Overview
                        # For both bulk and companion uploads, use the same function to ensure consistency
                        st.session_state.debug_messages.append(f"[Account Overview] Getting targeting performance data for {'companion' if is_companion else 'bulk'} data")
                        branded_targets_df, non_branded_targets_df = get_targeting_performance_data(
                            bulk_data,
                            st.session_state.client_config
                        )
                        st.session_state.debug_messages.append(f"[Account Overview] Targeting data processed: {len(branded_targets_df)} branded, {len(non_branded_targets_df)} non-branded")

                        # --- Harmonize columns between branded and non-branded targets ---
                        import numpy as np
                        # Define a global list of required columns for harmonization
                        REQUIRED_TARGET_COLUMNS = [
                            'Campaign', 'Target', 'Match Type', 'Spend', 'Ad Sales', '% of Spend', '% of Ad Sales',
                            'ACoS', 'ROAS', 'CPC', 'CVR', 'Impressions', 'Clicks', 'Orders', 'AOV', 'CTR', 'CPA', 'Ad Type'
                        ]
                        branded_cols = set(branded_targets_df.columns)
                        non_branded_cols = set(non_branded_targets_df.columns)
                        all_cols = list(set(REQUIRED_TARGET_COLUMNS).union(branded_cols).union(non_branded_cols))
                        for col in all_cols:
                            if col not in branded_targets_df.columns:
                                branded_targets_df[col] = np.nan
                            if col not in non_branded_targets_df.columns:
                                non_branded_targets_df[col] = np.nan
                        # Ensure 'Ad Sales' is always present and populated (should be from get_targeting_performance_data)
                        # Ensure both 'Sales' and 'Ad Sales' columns exist for compatibility
                        # with different parts of the code
                        if 'Sales' in branded_targets_df.columns and 'Ad Sales' not in branded_targets_df.columns:
                            branded_targets_df['Ad Sales'] = branded_targets_df['Sales']
                        elif 'Ad Sales' in branded_targets_df.columns and 'Sales' not in branded_targets_df.columns:
                            branded_targets_df['Sales'] = branded_targets_df['Ad Sales']
                            
                        if 'Sales' in non_branded_targets_df.columns and 'Ad Sales' not in non_branded_targets_df.columns:
                            non_branded_targets_df['Ad Sales'] = non_branded_targets_df['Sales']
                        elif 'Ad Sales' in non_branded_targets_df.columns and 'Sales' not in non_branded_targets_df.columns:
                            non_branded_targets_df['Sales'] = non_branded_targets_df['Ad Sales']
                        
                        # Ensure 'Ad Sales' is always present
                        if 'Ad Sales' not in branded_targets_df.columns:
                            branded_targets_df['Ad Sales'] = np.nan
                        if 'Ad Sales' not in non_branded_targets_df.columns:
                            non_branded_targets_df['Ad Sales'] = np.nan
                        
                        # Defensive: If DataFrames are None, create empty DataFrames with required columns
                        if branded_targets_df is None or not isinstance(branded_targets_df, pd.DataFrame):
                            branded_targets_df = pd.DataFrame(columns=REQUIRED_TARGET_COLUMNS)
                        if non_branded_targets_df is None or not isinstance(non_branded_targets_df, pd.DataFrame):
                            non_branded_targets_df = pd.DataFrame(columns=REQUIRED_TARGET_COLUMNS)

                        # Create a function to calculate metrics from targeting dataframes
                        def calculate_metrics_from_targets(df):
                            if df.empty:
                                return {
                                    'Total Spend': 0.0,
                                    'Total Ad Sales': 0.0,
                                    'ACoS': 0.0,
                                    'ROAS': 0.0,
                                    'CPC': 0.0,
                                    'CVR': 0.0,
                                    'CTR': 0.0,
                                    'Total Impressions': 0,
                                    'Total Clicks': 0,
                                    'AOV': 0.0,
                                    'CPA': 0.0,
                                    'Total Orders': 0
                                }
                            
                            # Calculate aggregated metrics
                            total_spend = df['Spend'].sum()
                            total_sales = df['Sales'].sum() if 'Sales' in df.columns else df['Ad Sales'].sum()
                            total_impressions = df['Impressions'].sum()
                            total_clicks = df['Clicks'].sum()
                            total_orders = df['Orders'].sum()
                            
                            # Calculate derived metrics (handling division by zero)
                            acos = (total_spend / total_sales) * 100 if total_sales > 0 else 0
                            roas = total_sales / total_spend if total_spend > 0 else 0
                            cpc = total_spend / total_clicks if total_clicks > 0 else 0
                            ctr = (total_clicks / total_impressions) * 100 if total_impressions > 0 else 0
                            cvr = (total_orders / total_clicks) * 100 if total_clicks > 0 else 0
                            aov = total_sales / total_orders if total_orders > 0 else 0
                            cpa = total_spend / total_orders if total_orders > 0 else 0
                            
                            return {
                                'Total Spend': total_spend,
                                'Total Ad Sales': total_sales,
                                'ACoS': round(acos, 2),
                                'ROAS': roas,
                                'CPC': cpc,
                                'CVR': cvr,
                                'CTR': ctr,
                                'Total Impressions': total_impressions,
                                'Total Clicks': total_clicks,
                                'AOV': aov,
                                'CPA': cpa,
                                'Total Orders': total_orders
                            }
                        
                        # Calculate metrics for branded and non-branded targets
                        branded_metrics = calculate_metrics_from_targets(branded_targets_df)
                        non_branded_metrics = calculate_metrics_from_targets(non_branded_targets_df)
                        
                        st.session_state.debug_messages.append(f"[Account Overview] Branded metrics calculated: Spend=${branded_metrics['Total Spend']:.2f}, Sales=${branded_metrics['Total Ad Sales']:.2f}")
                        st.session_state.debug_messages.append(f"[Account Overview] Non-branded metrics calculated: Spend=${non_branded_metrics['Total Spend']:.2f}, Sales=${non_branded_metrics['Total Ad Sales']:.2f}")
                        
                    except Exception as e:
                        st.session_state.debug_messages.append(f"[Account Overview] Error processing branded/non-branded data: {str(e)}")
                        branded_metrics = None
                        non_branded_metrics = None
                    
                    # Calculate KPIs
                    acos = (total_ad_spend / total_ad_sales) * 100 if total_ad_sales > 0 else 0
                    roas = total_ad_sales / total_ad_spend if total_ad_spend > 0 else 0
                    cpc = total_ad_spend / total_clicks if total_clicks > 0 else 0
                    cvr = (total_ad_orders / total_clicks) * 100 if total_clicks > 0 else 0
                    ctr = (total_clicks / total_impressions) * 100 if total_impressions > 0 else 0
                    aov = total_ad_sales / total_ad_orders if total_ad_orders > 0 else 0
                    cpa = total_ad_spend / total_ad_orders if total_ad_orders > 0 else 0
                    
                    # Calculate TACoS and Ad Traffic %
                    tacos = (total_ad_spend / total_sales_revenue) * 100 if total_sales_revenue and total_sales_revenue > 0 else None
                    ad_sales_perc = (total_ad_sales / total_sales_revenue) * 100 if total_sales_revenue and total_sales_revenue > 0 else None
                    ad_traffic_perc_sessions = (total_clicks / total_sessions) * 100 if total_sessions and total_sessions > 0 else None
                    
                    # Display metrics in a clean layout
                    st.markdown("""
    <h4 style='font-family:'Inter','Roboto','Segoe UI',Arial,sans-serif; font-size:1.0rem; font-weight:600; color:#bfa94a; margin-top:1.5rem; margin-bottom:0.7rem;'>
        Advertising Performance
    </h4>""", unsafe_allow_html=True)
                    
                    # Check if this is companion data to determine formatting for counting metrics only
                    is_companion = st.session_state.get('is_companion_data', False)
                    
                    # First row of metrics
                    col1, col2, col3, col4 = st.columns(4)
                    col1.metric("Spend", f"${total_ad_spend:,.2f}")
                    col2.metric("Ad Sales", f"${total_ad_sales:,.2f}" if total_ad_sales is not None else "N/A")
                    col3.metric("ACoS", f"{acos:.2f}%" if acos is not None else "N/A")
                    col4.metric("ROAS", f"{roas:.2f}x" if roas is not None else "N/A")
                    
                    # Second row of metrics
                    col5, col6, col7, col8 = st.columns(4)
                    col5.metric("CPC", f"${cpc:.2f}" if cpc is not None else "N/A")
                    col6.metric("CVR", f"{cvr:.2f}%" if cvr is not None else "N/A")
                    col7.metric("CTR", f"{ctr:.2f}%" if ctr is not None else "N/A")
                    col8.metric("AOV", f"${aov:.2f}" if aov is not None else "N/A")
                    
                    # Third row of metrics - only format counting metrics differently for companion data
                    col9, col10, col11, col12 = st.columns(4)
                    col9.metric("CPA", f"${cpa:.2f}" if cpa is not None else "N/A")
                    
                    # Format counting metrics without decimals for companion data
                    if is_companion:
                        col10.metric("Impressions", f"{int(total_impressions):,}" if total_impressions is not None else "N/A")
                        col11.metric("Clicks", f"{int(total_clicks):,}" if total_clicks is not None else "N/A")
                        col12.metric("Orders", f"{int(total_ad_orders):,}" if total_ad_orders is not None else "N/A")
                    else:
                        col10.metric("Impressions", f"{int(total_impressions):,}" if total_impressions is not None else "N/A")
                        col11.metric("Clicks", f"{int(total_clicks):,}" if total_clicks is not None else "N/A")
                        col12.metric("Orders", f"{int(total_ad_orders):,}" if total_ad_orders is not None else "N/A")
                    
                    st.divider()
                    st.markdown("""
    <h4 style='font-family:'Inter','Roboto','Segoe UI',Arial,sans-serif; font-size:1.0rem; font-weight:600; color:#bfa94a; margin-top:1.5rem; margin-bottom:0.7rem;'>
        Topline & Advertising Performance
    </h4>""", unsafe_allow_html=True)
                    
                    # Topline metrics
                    col13, col14, col15 = st.columns(3)
                    if total_sales_revenue is not None:
                        col13.metric("Total Sales", f"${total_sales_revenue:,.2f}", help=f"Using '{total_sales_col}' column")
                    else:
                        col13.metric("Total Sales", "N/A", help="'Total Sales' column not found in Sales Report.")
                    
                    if tacos is not None:
                        col14.metric("TACoS", f"{tacos:.2f}%", help="Total Advertising Cost of Sales (Spend / Total Sales)")
                    else:
                        col14.metric("TACoS", "N/A", help="Cannot calculate - Total Sales missing.")
                    
                    if ad_sales_perc is not None:
                        col15.metric("Ad Sales % of Total", f"{ad_sales_perc:.2f}%", help="Percentage of Total Sales from Advertising")
                    else:
                        col15.metric("Ad Sales % of Total", "N/A", help="Cannot calculate - Total Sales missing.")
                    
                    col16, col17 = st.columns(2)
                    if total_sessions is not None:
                        col16.metric("Total Sessions", f"{total_sessions:,}", help=f"Using '{sessions_col}' column")
                    else:
                        col16.metric("Total Sessions", "-", help="'Sessions' column not found in Sales Report.")
                    
                    if ad_traffic_perc_sessions is not None:
                        col17.metric("Ad Traffic % Sessions", f"{ad_traffic_perc_sessions:.2f}%", help="Percentage of Sessions from Ad Clicks")
                    else:
                        col17.metric("Ad Traffic % Sessions", "-", help="Cannot calculate - Sessions or Ad Clicks missing.")
                    

                    
                    # Add Sales Distribution Pie Chart
                    if total_sales_revenue is not None and total_sales_revenue > 0 and total_ad_sales is not None:
                        st.divider()
                        # No need for the main title anymore as each chart has its own title
                        
                        # Calculate organic sales (total sales - ad sales)
                        organic_sales = max(0, total_sales_revenue - total_ad_sales)  # Ensure it's not negative
                        
                        # Calculate percentages
                        ad_sales_percentage = (total_ad_sales / total_sales_revenue) * 100
                        organic_sales_percentage = (organic_sales / total_sales_revenue) * 100
                        
                        # Create a centered container for the pie chart
                        # Create two columns for side-by-side charts
                        col_sales, col_traffic = st.columns(2)
                        
                        with col_sales:
                            # Add title for sales chart
                            st.markdown("""
                            <h2 style='text-align: center; color: #2196f3; font-family: Inter,Roboto,Segoe UI,Arial,sans-serif; font-size:1.6rem; font-weight:700; margin-top:1.2rem; margin-bottom:1.0rem;'>
                                Organic vs Paid Sales
                            </h2>""", unsafe_allow_html=True)
                            
                            # Format the data for the pie chart
                            sales_data = {
                                'Category': ['Ad Sales', 'Organic Sales'],
                                'Sales': [total_ad_sales, organic_sales],
                                'Percentage': [ad_sales_percentage, organic_sales_percentage]
                            }
                            sales_df = pd.DataFrame(sales_data)
                            
                            # Create a horizontal bar chart with context-aware styling
                            fig_sales = {
                                    'data': [
                                        {
                                            'type': 'bar',
                                            'orientation': 'h',
                                            'y': sales_df['Category'],
                                            'x': sales_df['Sales'],
                                            'text': [f'${val:,.2f}' for val in sales_df['Sales']],
                                            'textposition': 'inside',
                                            'textfont': {'color': 'white', 'size': 14, 'weight': 'bold'},
                                            'hoverinfo': 'text',
                                            'hovertext': [f'{cat}: ${val:,.2f} ({pct:.1f}%)' for cat, val, pct in zip(sales_df['Category'], sales_df['Sales'], sales_df['Percentage'])],
                                            'marker': {
                                                'color': ['#1E3A8A', '#3B82F6'],  # Darker blue for Ad Sales, lighter blue for Organic Sales
                                                'line': {
                                                    'width': 1,
                                                    'color': 'rgba(229, 231, 235, 0.3)'
                                                }
                                            }
                                        }
                                    ],
                                    'layout': {
                                        'title': None,  # Remove title from chart itself
                                        'showlegend': False,
                                        'height': 250,
                                        'margin': {'t': 20, 'b': 40, 'l': 120, 'r': 60},
                                        'xaxis': {
                                            'title': 'Sales ($)',
                                            'tickformat': '$,.0f',
                                            'gridcolor': 'rgba(229, 231, 235, 0.3)',
                                            'gridwidth': 0.5,
                                            'color': 'rgba(255, 255, 255, 0.8)'  # Text color that works in both light/dark modes
                                        },
                                        'yaxis': {
                                            'title': '',
                                            'tickfont': {'size': 14},
                                            'ticksuffix': '  ',
                                            'color': 'rgba(255, 255, 255, 0.8)'  # Text color that works in both light/dark modes
                                        },
                                        'annotations': [
                                            {
                                                'x': sales_df['Sales'][i],
                                                'y': sales_df['Category'][i],
                                                'text': f'{sales_df["Percentage"][i]:.1f}%',
                                                'showarrow': False,
                                                'font': {'size': 14, 'color': 'rgba(255, 255, 255, 0.9)'},
                                                'xanchor': 'left',
                                                'xshift': 10,
                                                'bgcolor': 'rgba(0, 0, 0, 0.4)',
                                                'borderpad': 2,
                                                'bordercolor': 'rgba(255, 255, 255, 0.2)',
                                                'borderwidth': 1
                                            } for i in range(len(sales_df))
                                        ],
                                        'plot_bgcolor': 'rgba(0, 0, 0, 0.1)',  # Transparent background
                                        'paper_bgcolor': 'rgba(0, 0, 0, 0)'    # Transparent paper
                                    }
                            }
                            # Add config to make chart responsive and add the dark-mode-compatible class
                            st.plotly_chart(fig_sales, use_container_width=True, config={'responsive': True, 'displayModeBar': False})
                            
                            # --- Organic vs Paid Traffic Chart ---
                            if total_sessions is not None and total_sessions > 0 and ad_traffic_perc_sessions is not None:
                                ad_traffic_sessions = total_sessions * (ad_traffic_perc_sessions / 100)
                                organic_traffic_sessions = total_sessions - ad_traffic_sessions
                                ad_traffic_pct = (ad_traffic_sessions / total_sessions) * 100 if total_sessions > 0 else 0
                                organic_traffic_pct = (organic_traffic_sessions / total_sessions) * 100 if total_sessions > 0 else 0
                                
                                with col_traffic:
                                    # Add title for traffic chart
                                    st.markdown("""
                                    <h2 style='text-align: center; color: #2196f3; font-family: Inter,Roboto,Segoe UI,Arial,sans-serif; font-size:1.6rem; font-weight:700; margin-top:1.2rem; margin-bottom:1.0rem;'>
                                        Organic vs Paid Traffic
                                    </h2>""", unsafe_allow_html=True)
                                    traffic_data = {
                                        'Category': ['Ad Traffic', 'Organic Traffic'],
                                        'Sessions': [ad_traffic_sessions, organic_traffic_sessions],
                                        'Percentage': [ad_traffic_pct, organic_traffic_pct]
                                    }
                                    traffic_df = pd.DataFrame(traffic_data)
                                    fig_traffic = {
                                        'data': [
                                            {
                                                'type': 'bar',
                                                'orientation': 'h',
                                                'y': traffic_df['Category'],
                                                'x': traffic_df['Sessions'],
                                                'text': [f'{val:,.0f}' for val in traffic_df['Sessions']],
                                                'textposition': 'inside',
                                                'textfont': {'color': 'white', 'size': 14, 'weight': 'bold'},
                                                'hoverinfo': 'text',
                                                'hovertext': [f'{cat}: {val:,.0f} ({pct:.1f}%)' for cat, val, pct in zip(traffic_df['Category'], traffic_df['Sessions'], traffic_df['Percentage'])],
                                                'marker': {
                                                    'color': ['#0f5132', '#10b981'],
                                                    'line': {
                                                        'width': 1,
                                                        'color': 'rgba(229, 231, 235, 0.3)'
                                                    }
                                                }
                                            }
                                        ],
                                        'layout': {
                                            'title': None,
                                            'showlegend': False,
                                            'height': 250,
                                            'margin': {'t': 20, 'b': 40, 'l': 120, 'r': 60},
                                            'xaxis': {
                                                'title': 'Sessions',
                                                'tickformat': ',.0f',
                                                'gridcolor': 'rgba(229, 231, 235, 0.3)',
                                                'gridwidth': 0.5,
                                                'color': 'rgba(255, 255, 255, 0.8)'
                                            },
                                            'yaxis': {
                                                'title': '',
                                                'tickfont': {'size': 14},
                                                'ticksuffix': '  ',
                                                'color': 'rgba(255, 255, 255, 0.8)'
                                            },
                                            'annotations': [
                                                {
                                                    # Position the annotation at the end of the bar plus some padding
                                                    'x': traffic_df['Sessions'][i] * 1.05,
                                                    'y': traffic_df['Category'][i],
                                                    'text': f'{traffic_df["Percentage"][i]:.1f}%',
                                                    'showarrow': False,
                                                    'font': {'size': 14, 'color': 'rgba(255, 255, 255, 0.9)'},
                                                    'xanchor': 'left',
                                                    'xshift': 5,
                                                    'bgcolor': 'rgba(0, 0, 0, 0.4)',
                                                    'borderpad': 2,
                                                    'bordercolor': 'rgba(255, 255, 255, 0.2)',
                                                    'borderwidth': 1
                                                } for i in range(len(traffic_df))
                                            ],
                                            'plot_bgcolor': 'rgba(0, 0, 0, 0.1)',
                                            'paper_bgcolor': 'rgba(0, 0, 0, 0)'
                                        }
                                    }
                                    st.plotly_chart(fig_traffic, use_container_width=True, config={'responsive': True, 'displayModeBar': False})

                        
                        # Add Branded vs. Non-Branded Performance section
                        st.divider()
                        st.markdown("<div id='branded-vs-non-branded-performance' class='section-anchor'></div>", unsafe_allow_html=True)
                        st.markdown("<hr style='height:2px;border-width:0;color:gold;background-color:gold;margin-top:25px;margin-bottom:15px;margin-left:10px;margin-right:10px'>", unsafe_allow_html=True)
                        st.markdown("<span class='main-section-header'>Branded vs. Non-Branded Performance</span>", unsafe_allow_html=True)
                        st.markdown("<div style='margin-bottom:1.2rem;'></div>", unsafe_allow_html=True)
                        # Add toggle for data source (Search Terms or Targets)
                        if 'branded_data_source' not in st.session_state:
                            st.session_state.branded_data_source = "Targets"
                            
                        # Left-align the data source picker
                        toggle_col, desc_col = st.columns([0.2, 0.8])
                        with toggle_col:
                            data_source = st.radio(
                                "Data Source",
                                options=["Targets", "Search Terms"],
                                horizontal=True,
                                key="branded_data_source"
                            )
                            
                        # No descriptions needed
                        
                        # Get the appropriate data based on the selected source
                        if data_source == "Targets":
                            # Get the targeting performance data
                            branded_targets_df, non_branded_targets_df = get_targeting_performance_data(
                                bulk_data, 
                                st.session_state.client_config
                            )
                            st.session_state.debug_messages.append(f"[Branded Performance] Using Targets data source")
                        else:  # Search Terms
                            # Get search term data and split into branded/non-branded
                            search_term_df = get_search_term_data(bulk_data, st.session_state.client_config)
                            
                            if not search_term_df.empty:
                                # Check if 'Is Branded' column exists, if not, add it based on client configuration
                                if 'Is Branded' not in search_term_df.columns:
                                    st.session_state.debug_messages.append(f"[Branded Performance] 'Is Branded' column not found in search term data, adding it")
                                    
                                    # Get branded terms and ASINs from client config
                                    client_config = st.session_state.client_config
                                    branded_terms = []
                                    branded_asins = []
                                    
                                    if client_config:
                                        # Extract branded terms from client config (handling different possible formats)
                                        if 'Branded Terms' in client_config:
                                            if isinstance(client_config['Branded Terms'], list):
                                                branded_terms = client_config['Branded Terms']
                                            elif isinstance(client_config['Branded Terms'], str):
                                                branded_terms = [term.strip() for term in client_config['Branded Terms'].split(',')]
                                        
                                        # Extract branded ASINs from the correct source: branded_asins_data
                                        branded_asins_data = client_config.get('branded_asins_data', {})
                                        branded_asins = [str(asin).strip().upper() for asin in branded_asins_data.keys() if str(asin).strip()]
                                        
                                        # Fallback: check old format for backward compatibility
                                        if not branded_asins and 'Branded ASINs' in client_config:
                                            if isinstance(client_config['Branded ASINs'], list):
                                                branded_asins = [str(asin).strip().upper() for asin in client_config['Branded ASINs'] if str(asin).strip()]
                                            elif isinstance(client_config['Branded ASINs'], str):
                                                branded_asins = [asin.strip().upper() for asin in client_config['Branded ASINs'].split(',') if asin.strip()]
                                    
                                    # Function to check if a search term contains any branded terms
                                    def is_branded_term(term):
                                        if pd.isna(term) or not isinstance(term, str):
                                            return False
                                        
                                        term_lower = term.lower()
                                        
                                        # Check if term contains any branded terms
                                        for branded_term in branded_terms:
                                            if branded_term and isinstance(branded_term, str) and branded_term.lower() in term_lower:
                                                return True
                                        
                                        # Check if term contains any branded ASINs (using uppercase comparison for ASINs)
                                        for branded_asin in branded_asins:
                                            if branded_asin and isinstance(branded_asin, str) and branded_asin in term.upper():
                                                return True
                                        
                                        return False
                                    
                                    # Apply the function to classify search terms
                                    search_term_col = 'Search Term' if 'Search Term' in search_term_df.columns else 'Customer Search Term'
                                    if search_term_col in search_term_df.columns:
                                        # Enhanced classification: check both search term and target for branded content
                                        def classify_search_term_row(row):
                                            search_term = row.get(search_term_col, '')
                                            target = row.get('Target', '')
                                            
                                            # Check if search term is branded
                                            if is_branded_term(search_term):
                                                return True
                                            
                                            # Check if target contains branded ASINs
                                            if target and isinstance(target, str) and not pd.isna(target):
                                                target_upper = target.upper()
                                                for branded_asin in branded_asins:
                                                    if branded_asin and isinstance(branded_asin, str) and branded_asin in target_upper:
                                                        return True
                                            
                                            return False
                                        
                                        search_term_df['Is Branded'] = search_term_df.apply(classify_search_term_row, axis=1)
                                    else:
                                        # If no search term column found, default all to non-branded
                                        search_term_df['Is Branded'] = False
                                        st.session_state.debug_messages.append(f"[Branded Performance] No search term column found in data")
                                
                                # Check which column name is being used for branding classification
                                if 'Is_Branded' in search_term_df.columns:
                                    branded_column = 'Is_Branded'
                                elif 'Is Branded' in search_term_df.columns:
                                    branded_column = 'Is Branded'
                                else:
                                    # If neither column exists, add it with default values
                                    search_term_df['Is Branded'] = False
                                    branded_column = 'Is Branded'
                                    st.session_state.debug_messages.append(f"[Branded Performance] No branding column found, defaulting all to non-branded")
                                
                                # Split into branded and non-branded using the correct column
                                branded_targets_df = search_term_df[search_term_df[branded_column] == True].copy()
                                non_branded_targets_df = search_term_df[search_term_df[branded_column] == False].copy()
                                
                                # Add debug information
                                st.session_state.debug_messages.append(f"[Branded Performance] Using column '{branded_column}' for branding classification")
                                st.session_state.debug_messages.append(f"[Branded Performance] Using Search Terms data source: {len(branded_targets_df)} branded, {len(non_branded_targets_df)} non-branded")
                            else:
                                # If search term data is empty, create empty DataFrames
                                branded_targets_df = pd.DataFrame()
                                non_branded_targets_df = pd.DataFrame()
                                st.session_state.debug_messages.append(f"[Branded Performance] Search term data is empty")

                        # --- Harmonize columns between branded and non-branded targets ---
                        import numpy as np
                        # Define a global list of required columns for harmonization
                        REQUIRED_TARGET_COLUMNS = [
                            'Campaign', 'Target', 'Match Type', 'Spend', 'Ad Sales', '% of Spend', '% of Ad Sales',
                            'ACoS', 'ROAS', 'CPC', 'CVR', 'Impressions', 'Clicks', 'Orders', 'AOV', 'CTR', 'CPA', 'Ad Type'
                        ]
                        branded_cols = set(branded_targets_df.columns)
                        non_branded_cols = set(non_branded_targets_df.columns)
                        all_cols = list(set(REQUIRED_TARGET_COLUMNS).union(branded_cols).union(non_branded_cols))
                        for col in all_cols:
                            if col not in branded_targets_df.columns:
                                branded_targets_df[col] = np.nan
                            if col not in non_branded_targets_df.columns:
                                non_branded_targets_df[col] = np.nan
                        # Ensure 'Ad Sales' is always present and populated (should be from get_targeting_performance_data)
                        # Ensure both 'Sales' and 'Ad Sales' columns exist for compatibility
                        # with different parts of the code
                        if 'Sales' in branded_targets_df.columns and 'Ad Sales' not in branded_targets_df.columns:
                            branded_targets_df['Ad Sales'] = branded_targets_df['Sales']
                        elif 'Ad Sales' in branded_targets_df.columns and 'Sales' not in branded_targets_df.columns:
                            branded_targets_df['Sales'] = branded_targets_df['Ad Sales']
                            
                        if 'Sales' in non_branded_targets_df.columns and 'Ad Sales' not in non_branded_targets_df.columns:
                            non_branded_targets_df['Ad Sales'] = non_branded_targets_df['Sales']
                        elif 'Ad Sales' in non_branded_targets_df.columns and 'Sales' not in non_branded_targets_df.columns:
                            non_branded_targets_df['Sales'] = non_branded_targets_df['Ad Sales']
                        
                        # Ensure 'Ad Sales' is always present
                        if 'Ad Sales' not in branded_targets_df.columns:
                            branded_targets_df['Ad Sales'] = np.nan
                        if 'Ad Sales' not in non_branded_targets_df.columns:
                            non_branded_targets_df['Ad Sales'] = np.nan
                        # Reorder columns to match REQUIRED_TARGET_COLUMNS order (with extras at the end)
                        # Make sure 'Bid' is the very last column
                        ordered_cols = [col for col in REQUIRED_TARGET_COLUMNS if col in all_cols]
                        extra_cols = [col for col in all_cols if col not in REQUIRED_TARGET_COLUMNS and col != 'Bid']
                        if 'Bid' in all_cols:
                            ordered_cols = ordered_cols + extra_cols + ['Bid']
                        else:
                            ordered_cols = ordered_cols + extra_cols
                        branded_targets_df = branded_targets_df[ordered_cols]
                        non_branded_targets_df = non_branded_targets_df[ordered_cols]
                        # Debug: Show harmonized columns
                        st.session_state.debug_messages.append(f"[Harmonized Branded Columns] {list(branded_targets_df.columns)}")
                        st.session_state.debug_messages.append(f"[Harmonized Non-Branded Columns] {list(non_branded_targets_df.columns)}")
                        st.session_state.debug_messages.append(f"[Ad Sales Column] Branded targets has {branded_targets_df['Ad Sales'].notna().sum()} non-NaN values, Non-Branded has {non_branded_targets_df['Ad Sales'].notna().sum()} non-NaN values.")

                        # Defensive: If DataFrames are None, create empty DataFrames with required columns
                        if branded_targets_df is None or not isinstance(branded_targets_df, pd.DataFrame):
                            branded_targets_df = pd.DataFrame(columns=REQUIRED_TARGET_COLUMNS)
                        if non_branded_targets_df is None or not isinstance(non_branded_targets_df, pd.DataFrame):
                            non_branded_targets_df = pd.DataFrame(columns=REQUIRED_TARGET_COLUMNS)

                        # Create a function to calculate metrics from targeting dataframes
                        def calculate_metrics_from_targets(df):
                            if df.empty:
                                return {
                                    'Total Spend': 0.0,
                                    'Total Ad Sales': 0.0,
                                    'ACoS': 0.0,
                                    'ROAS': 0.0,
                                    'CPC': 0.0,
                                    'CVR': 0.0,
                                    'CTR': 0.0,
                                    'Total Impressions': 0,
                                    'Total Clicks': 0,
                                    'AOV': 0.0,
                                    'CPA': 0.0,
                                    'Total Orders': 0
                                }
                            
                            # Calculate aggregated metrics
                            total_spend = df['Spend'].sum()
                            total_sales = df['Sales'].sum()
                            total_impressions = df['Impressions'].sum()
                            total_clicks = df['Clicks'].sum()
                            total_orders = df['Orders'].sum()
                            
                            # Calculate derived metrics (handling division by zero)
                            acos = (total_spend / total_sales) * 100 if total_sales > 0 else 0
                            roas = total_sales / total_spend if total_spend > 0 else 0
                            cpc = total_spend / total_clicks if total_clicks > 0 else 0
                            ctr = (total_clicks / total_impressions) * 100 if total_impressions > 0 else 0
                            cvr = (total_orders / total_clicks) * 100 if total_clicks > 0 else 0
                            aov = total_sales / total_orders if total_orders > 0 else 0
                            cpa = total_spend / total_orders if total_orders > 0 else 0
                            
                            return {
                                'Total Spend': total_spend,
                                'Total Ad Sales': total_sales,
                                'ACoS': round(acos, 2),
                                'ROAS': roas,
                                'CPC': cpc,
                                'CVR': cvr,
                                'CTR': ctr,
                                'Total Impressions': total_impressions,
                                'Total Clicks': total_clicks,
                                'AOV': aov,
                                'CPA': cpa,
                                'Total Orders': total_orders
                            }
                        
                        # Calculate metrics for branded and non-branded targets
                        branded_metrics = calculate_metrics_from_targets(branded_targets_df)
                        non_branded_metrics = calculate_metrics_from_targets(non_branded_targets_df)
                        
                        if not (branded_targets_df.empty and non_branded_targets_df.empty):
                            # Calculate totals for each metric
                            total_metrics = {
                                'Total Spend': branded_metrics['Total Spend'] + non_branded_metrics['Total Spend'],
                                'Total Ad Sales': branded_metrics['Total Ad Sales'] + non_branded_metrics['Total Ad Sales'],
                                'ACoS': (branded_metrics['Total Spend'] + non_branded_metrics['Total Spend']) / (branded_metrics['Total Ad Sales'] + non_branded_metrics['Total Ad Sales']) * 100 if (branded_metrics['Total Ad Sales'] + non_branded_metrics['Total Ad Sales']) != 0 else 0,
                                'ROAS': (branded_metrics['Total Ad Sales'] + non_branded_metrics['Total Ad Sales']) / (branded_metrics['Total Spend'] + non_branded_metrics['Total Spend']) if (branded_metrics['Total Spend'] + non_branded_metrics['Total Spend']) != 0 else 0,
                                'CPC': (branded_metrics['Total Spend'] + non_branded_metrics['Total Spend']) / (branded_metrics['Total Clicks'] + non_branded_metrics['Total Clicks']) if (branded_metrics['Total Clicks'] + non_branded_metrics['Total Clicks']) != 0 else 0,
                                'CVR': (branded_metrics['Total Orders'] + non_branded_metrics['Total Orders']) / (branded_metrics['Total Clicks'] + non_branded_metrics['Total Clicks']) * 100 if (branded_metrics['Total Clicks'] + non_branded_metrics['Total Clicks']) != 0 else 0,
                                'CTR': (branded_metrics['Total Clicks'] + non_branded_metrics['Total Clicks']) / (branded_metrics['Total Impressions'] + non_branded_metrics['Total Impressions']) * 100 if (branded_metrics['Total Impressions'] + non_branded_metrics['Total Impressions']) != 0 else 0,
                                'AOV': (branded_metrics['Total Ad Sales'] + non_branded_metrics['Total Ad Sales']) / (branded_metrics['Total Orders'] + non_branded_metrics['Total Orders']) if (branded_metrics['Total Orders'] + non_branded_metrics['Total Orders']) != 0 else 0,
                                'CPA': (branded_metrics['Total Spend'] + non_branded_metrics['Total Spend']) / (branded_metrics['Total Orders'] + non_branded_metrics['Total Orders']) if (branded_metrics['Total Orders'] + non_branded_metrics['Total Orders']) != 0 else 0,
                                'Total Impressions': branded_metrics['Total Impressions'] + non_branded_metrics['Total Impressions'],
                                'Total Clicks': branded_metrics['Total Clicks'] + non_branded_metrics['Total Clicks'],
                                'Total Orders': branded_metrics['Total Orders'] + non_branded_metrics['Total Orders'],
                            }

                            # Prepare formatted values for display
                            metrics = {
                                "Spend": [f"${branded_metrics['Total Spend']:,.2f}", f"${non_branded_metrics['Total Spend']:,.2f}", f"${total_metrics['Total Spend']:,.2f}"],
                                "Ad Sales": [f"${branded_metrics['Total Ad Sales']:,.2f}", f"${non_branded_metrics['Total Ad Sales']:,.2f}", f"${total_metrics['Total Ad Sales']:,.2f}"],
                                "ACoS": [f"{branded_metrics['ACoS']:.2f}%", f"{non_branded_metrics['ACoS']:.2f}%", f"{total_metrics['ACoS']:.2f}%"],
                                "ROAS": [f"{branded_metrics['ROAS']:.2f}x", f"{non_branded_metrics['ROAS']:.2f}x", f"{total_metrics['ROAS']:.2f}x"],
                                "CPC": [f"${branded_metrics['CPC']:.2f}", f"${non_branded_metrics['CPC']:.2f}", f"${total_metrics['CPC']:,.2f}"],
                                "CVR": [f"{branded_metrics['CVR']:.2f}%", f"{non_branded_metrics['CVR']:.2f}%", f"{total_metrics['CVR']:.2f}%"],
                                "CTR": [f"{branded_metrics['CTR']:.2f}%", f"{non_branded_metrics['CTR']:.2f}%", f"{total_metrics['CTR']:.2f}%"],
                                "Impressions": [f"{int(branded_metrics['Total Impressions']):,}", f"{int(non_branded_metrics['Total Impressions']):,}", f"{int(total_metrics['Total Impressions']):,}"],
                                "Clicks": [f"{int(branded_metrics['Total Clicks']):,}", f"{int(non_branded_metrics['Total Clicks']):,}", f"{int(total_metrics['Total Clicks']):,}"],
                                "Orders": [f"{int(branded_metrics['Total Orders']):,}", f"{int(non_branded_metrics['Total Orders']):,}", f"{int(total_metrics['Total Orders']):,}"],
                                "AOV": [f"${branded_metrics['AOV']:.2f}", f"${non_branded_metrics['AOV']:.2f}", f"${total_metrics['AOV']:,.2f}"],
                                "CPA": [f"${branded_metrics['CPA']:.2f}", f"${non_branded_metrics['CPA']:.2f}", f"${total_metrics['CPA']:,.2f}"],
                            }
                            index = ["Branded", "Non-Branded", "Total"]
                            df_metrics = pd.DataFrame(metrics, index=index)
                            
                            # Prepare ACoS targets for each row
                            config = st.session_state.get('client_config', {})
                            goals = config.get('goals', {})
                            branded_target = goals.get('branded_acos')
                            non_branded_target = goals.get('non_branded_acos')
                            account_target = goals.get('account_wide_acos')

                            def highlight_and_acos(row):
                                # Function simplified to remove ACoS coloring based on user goals
                                styles = ['' for _ in row]
                                # Keep Total row styling
                                if row.name == 'Total':
                                    styles = ['border-top: 3px solid #FFD700; font-weight: bold;'.strip() for _ in row]
                                # Keep CPA column styling for contrast
                                if 'CPA' in row.index:
                                    cpa_idx = list(row.index).index('CPA')
                                    styles[cpa_idx] += 'color: white;'
                                return [s.replace('\n', ' ').strip() for s in styles]

                            # Filter out rows with all zeros before displaying
                            # Create a copy to avoid modifying the original
                            df_metrics_filtered = df_metrics.copy()
                            
                            # Remove empty rows (rows where both Spend and Ad Sales are zero/empty)
                            rows_to_keep = []
                            for idx, row in df_metrics.iterrows():
                                # Extract numeric values from formatted strings
                                spend_val = float(row['Spend'].replace('$', '').replace(',', ''))
                                sales_val = float(row['Ad Sales'].replace('$', '').replace(',', ''))
                                
                                # Keep row if either spend or sales is non-zero, or if it's the Total row
                                if spend_val > 0 or sales_val > 0 or idx == 'Total':
                                    rows_to_keep.append(idx)
                            
                            # Filter the dataframe to keep only non-empty rows and the Total row
                            if len(rows_to_keep) > 0:  # Make sure we have at least one row to keep
                                df_metrics_filtered = df_metrics_filtered.loc[rows_to_keep]
                            
                            # Display the dataframe with styling
                            st.dataframe(
                                df_metrics_filtered.style.apply(highlight_and_acos, axis=1),
                                height=150  # Reduced height since we're removing empty rows
                            )

                            st.markdown("</div>", unsafe_allow_html=True)
                            
                            # Add charts for branded vs. non-branded spend and sales distribution
                            st.divider()
                            st.subheader(f"Branded vs. Non-Branded Distribution ({data_source})")
                            
                            # Calculate total spend and sales across both branded and non-branded
                            total_spend = branded_metrics['Total Spend'] + non_branded_metrics['Total Spend']
                            total_sales = branded_metrics['Total Ad Sales'] + non_branded_metrics['Total Ad Sales']
                            
                            # Calculate percentages
                            if total_spend > 0:
                                branded_spend_pct = (branded_metrics['Total Spend'] / total_spend) * 100
                                non_branded_spend_pct = (non_branded_metrics['Total Spend'] / total_spend) * 100
                            else:
                                branded_spend_pct = 0
                                non_branded_spend_pct = 0
                                     
                            if total_sales > 0:
                                branded_sales_pct = (branded_metrics['Total Ad Sales'] / total_sales) * 100
                                non_branded_sales_pct = (non_branded_metrics['Total Ad Sales'] / total_sales) * 100
                            else:
                                branded_sales_pct = 0
                                non_branded_sales_pct = 0
                            
                            # Create two columns for the charts with a small gap between them
                            pie_col1, gap, pie_col2 = st.columns([0.48, 0.04, 0.48])
                            
                            with pie_col1:
                                # Center the header above the chart
                                st.markdown("<h3 style='text-align: center;'>Spend Distribution</h3>", unsafe_allow_html=True)
                                
                                # Format the data for the pie chart
                                spend_data = {
                                    'Category': ['Branded', 'Non-Branded'],
                                    'Spend': [branded_metrics['Total Spend'], non_branded_metrics['Total Spend']],
                                    'Percentage': [branded_spend_pct, non_branded_spend_pct]
                                }
                                spend_df = pd.DataFrame(spend_data)
                                
                                # Create the chart
                                if total_spend > 0:
                                    # Create a horizontal bar chart with context-aware styling
                                    fig_spend = {
                                        'data': [
                                            {
                                                'type': 'bar',
                                                'orientation': 'h',
                                                'y': spend_df['Category'],
                                                'x': spend_df['Spend'],
                                                'text': [f'${val:,.2f}' for val in spend_df['Spend']],
                                                'textposition': 'inside',
                                                'textfont': {'color': 'white', 'size': 14, 'weight': 'bold'},
                                                'hoverinfo': 'text',
                                                'hovertext': [f'{cat}: ${val:,.2f} ({pct:.1f}%)' for cat, val, pct in zip(spend_df['Category'], spend_df['Spend'], spend_df['Percentage'])],
                                                'marker': {
                                                    'color': ['#1E3A8A', '#3B82F6'],
                                                    'line': {
                                                        'width': 1,
                                                        'color': 'rgba(229, 231, 235, 0.3)'
                                                    }
                                                }
                                            }
                                        ],
                                        'layout': {
                                            'title': None,  # Remove title from chart itself
                                            'showlegend': False,
                                            'height': 250,
                                            'margin': {'t': 20, 'b': 40, 'l': 120, 'r': 60},
                                            'xaxis': {
                                                'title': 'Spend',
                                                'tickformat': '$,.0f',
                                                'gridcolor': 'rgba(229, 231, 235, 0.3)',
                                                'gridwidth': 0.5,
                                                'color': 'rgba(255, 255, 255, 0.8)'  # Text color that works in both light/dark modes
                                            },
                                            'yaxis': {
                                                'title': '',
                                                'tickfont': {'size': 14},
                                                'ticksuffix': '  ',
                                                'color': 'rgba(255, 255, 255, 0.8)'  # Text color that works in both light/dark modes
                                            },
                                            'annotations': [
                                                {
                                                    # Position the annotation at the end of the bar plus some padding
                                                    'x': spend_df['Spend'][i] * 1.05,
                                                    'y': spend_df['Category'][i],
                                                    'text': f'{spend_df["Percentage"][i]:.1f}%',
                                                    'showarrow': False,
                                                    'font': {'size': 14, 'color': 'rgba(255, 255, 255, 0.9)'},
                                                    'xanchor': 'left',
                                                    'xshift': 5,
                                                    'bgcolor': 'rgba(0, 0, 0, 0.4)',
                                                    'borderpad': 2,
                                                    'bordercolor': 'rgba(255, 255, 255, 0.2)',
                                                    'borderwidth': 1
                                                } for i in range(len(spend_df))
                                            ],
                                            'plot_bgcolor': 'rgba(0, 0, 0, 0.1)',  # Transparent background
                                            'paper_bgcolor': 'rgba(0, 0, 0, 0)'    # Transparent paper
                                        }
                                    }
                                    # Add config to make chart responsive and add the dark-mode-compatible class
                                    st.plotly_chart(fig_spend, use_container_width=True, config={'responsive': True, 'displayModeBar': False})
                                else:
                                    st.info("No spend data available to generate chart.")
                            
                            with pie_col2:
                                # Center the header above the chart
                                st.markdown("<h3 style='text-align: center;'>Ad Sales Distribution</h3>", unsafe_allow_html=True)
                                
                                # Format the data for the pie chart
                                sales_data = {
                                    'Category': ['Branded', 'Non-Branded'],
                                    'Sales': [branded_metrics['Total Ad Sales'], non_branded_metrics['Total Ad Sales']],
                                    'Percentage': [branded_sales_pct, non_branded_sales_pct]
                                }
                                sales_df = pd.DataFrame(sales_data)
                                
                                # Create the chart
                                if total_sales > 0:
                                    # Create a horizontal bar chart with context-aware styling
                                    fig_sales = {
                                        'data': [
                                            {
                                                'type': 'bar',
                                                'orientation': 'h',
                                                'y': sales_df['Category'],
                                                'x': sales_df['Sales'],
                                                'text': [f'${val:,.2f}' for val in sales_df['Sales']],
                                                'textposition': 'inside',
                                                'textfont': {'color': 'white', 'size': 14, 'weight': 'bold'},
                                                'hoverinfo': 'text',
                                                'hovertext': [f'{cat}: ${val:,.2f} ({pct:.1f}%)' for cat, val, pct in zip(sales_df['Category'], sales_df['Sales'], sales_df['Percentage'])],
                                                'marker': {
                                                    'color': ['#0f5132', '#10b981'],
                                                    'line': {
                                                        'width': 1,
                                                        'color': 'rgba(229, 231, 235, 0.3)'
                                                    }
                                                }
                                            }
                                        ],
                                        'layout': {
                                            'title': None,  # Remove title from chart itself
                                            'showlegend': False,
                                            'height': 250,
                                            'margin': {'t': 20, 'b': 40, 'l': 120, 'r': 60},
                                            'xaxis': {
                                                'title': 'Sales ($)',
                                                'tickformat': '$,.0f',
                                                'gridcolor': 'rgba(229, 231, 235, 0.3)',
                                                'gridwidth': 0.5,
                                                'color': 'rgba(255, 255, 255, 0.8)'  # Text color that works in both light/dark modes
                                            },
                                            'yaxis': {
                                                'title': '',
                                                'tickfont': {'size': 14},
                                                'ticksuffix': '  ',
                                                'color': 'rgba(255, 255, 255, 0.8)'  # Text color that works in both light/dark modes
                                            },
                                            'annotations': [
                                                {
                                                    # Position the annotation at the end of the bar plus some padding
                                                    'x': sales_df['Sales'][i] * 1.05,
                                                    'y': sales_df['Category'][i],
                                                    'text': f'{sales_df["Percentage"][i]:.1f}%',
                                                    'showarrow': False,
                                                    'font': {'size': 14, 'color': 'rgba(255, 255, 255, 0.9)'},
                                                    'xanchor': 'left',
                                                    'xshift': 5,
                                                    'bgcolor': 'rgba(0, 0, 0, 0.4)',
                                                    'borderpad': 2,
                                                    'bordercolor': 'rgba(255, 255, 255, 0.2)',
                                                    'borderwidth': 1
                                                } for i in range(len(sales_df))
                                            ],
                                            'plot_bgcolor': 'rgba(0, 0, 0, 0.1)',  # Transparent background
                                            'paper_bgcolor': 'rgba(0, 0, 0, 0)'    # Transparent paper
                                        }
                                    }
                                    # Add config to make chart responsive and add the dark-mode-compatible class
                                    st.plotly_chart(fig_sales, use_container_width=True, config={'responsive': True, 'displayModeBar': False})
                                else:
                                    st.info("No sales data available to generate chart.")
                    else:
                        st.info("No branded/non-branded targeting data available. Please ensure you have uploaded a bulk file and configured branded terms and ASINs in the Client Settings Center.")
                else:
                    st.warning("No advertising campaign data found or processed for aggregation.")
                
            except Exception as e:
                st.error(f"An error occurred calculating overview KPIs: {e}")
                with st.expander("Error Details"):
                    import traceback
                    st.code(traceback.format_exc())
        else:
            st.info("Upload Bulk Operations & Sales reports to populate the overview.")
    
    # The 'Advertising Audit: Comply Foam' section has been removed as requested
    # KPIs are now only shown in the 'Account Overview' section of the 'Advertising Audit' tab
    # --- Campaign Performance Section ---
    if st.session_state.current_page == "advertising_audit":
        # Gold main header for Campaign Performance with anchor
        st.markdown("<div id='campaign-performance' class='section-anchor'></div>", unsafe_allow_html=True)
        st.markdown("<hr style='height:2px;border-width:0;color:gold;background-color:gold;margin-top:25px;margin-bottom:15px;margin-left:10px;margin-right:10px'>", unsafe_allow_html=True)
        st.markdown("<span class='main-section-header dashboard-section'>Campaign Performance</span>", unsafe_allow_html=True)
        st.markdown("<div style='margin-bottom:1.2rem;'></div>", unsafe_allow_html=True)
        
        if st.session_state.get('bulk_data') and st.session_state.get('client_config'):
            try:
                # Get campaign performance data
                campaign_performance_df = get_campaign_performance_data(
                    st.session_state.bulk_data,
                    st.session_state.client_config
                )
                
                if not campaign_performance_df.empty:
                    # Get available product groups from campaign tagging
                    campaign_product_groups = set()
                    if 'campaign_tags_data' in st.session_state.client_config:
                        for campaign_data in st.session_state.client_config['campaign_tags_data'].values():
                            if 'tag_1' in campaign_data and campaign_data['tag_1']:
                                campaign_product_groups.add(campaign_data['tag_1'])
                    
                        # Add Untagged Group as a filter option for campaigns that could be untagged
                        if st.session_state.client_config["campaign_tags_data"]:
                            campaign_product_groups.add("Untagged Group")
                    # Get available ad types from the data
                    available_ad_types = set()
                    if 'Ad Type' in campaign_performance_df.columns:
                        available_ad_types = set(campaign_performance_df['Ad Type'].dropna().unique())
                    
                    # Filters section
                    filter_col1, filter_col2, filter_col3 = st.columns(3)
                    
                    with filter_col1:
                        # Campaign search bar
                        campaign_search = st.text_input(
                            "Campaign Search:",
                            placeholder="Enter campaign name phrase...",
                            key="campaign_performance_campaign_search"
                        )
                    
                    with filter_col2:
                        # Ad Type filter - show dropdown for each that exists
                        selected_ad_types = []
                        if available_ad_types:
                            ad_type_options = sorted(list(available_ad_types))
                            selected_ad_types = st.multiselect(
                                "Ad Type:",
                                options=ad_type_options,
                                key="campaign_performance_ad_type_filter"
                            )
                    
                    with filter_col3:
                        # Product Group filter - only show if there are product groups
                        selected_product_groups = []
                        if campaign_product_groups:
                            product_group_options = sorted(list(campaign_product_groups))
                            selected_product_groups = st.multiselect(
                                "Product Group:",
                                options=product_group_options,
                                key="campaign_performance_product_group_filter"
                            )
                    
                    # Apply filters to the data
                    filtered_df = campaign_performance_df.copy()
                    
                    # Apply product group filter
                    if selected_product_groups and 'Product Group' in filtered_df.columns:
                        filtered_df = filtered_df[filtered_df['Product Group'].isin(selected_product_groups)]
                    
                    # Apply ad type filter
                    if selected_ad_types and 'Ad Type' in filtered_df.columns:
                        filtered_df = filtered_df[filtered_df['Ad Type'].isin(selected_ad_types)]
                    
                    # Apply campaign search filter (case-insensitive)
                    if campaign_search and 'Campaign' in filtered_df.columns:
                        matched_campaigns = filtered_df['Campaign'].astype(str).apply(lambda x: phrase_match(x, campaign_search))
                        filtered_df = filtered_df[matched_campaigns]
                    
                    # Advanced Filters Section
                    st.markdown("---")
                    with st.expander("ðŸ”§ Advanced Filters", expanded=False):
                        # Initialize advanced filter state
                        if 'campaign_filter_groups' not in st.session_state:
                            st.session_state.campaign_filter_groups = []
                        st.markdown("**Advanced Filtering with Filter Groups**")
                        st.caption("Create multiple filter groups and combine them with AND/OR logic for complex filtering.")
                        
                        # Determine which columns are shown in the table so filters include ALL visible columns
                        display_columns_for_filters = ['Ad Type', 'Campaign', 'Spend', 'Ad Sales', '% Ad Spend', '% Ad Sales', 'ACoS', 'ROAS', 'CPC', 'CVR', 'AOV', 'Clicks', 'Orders']
                        if campaign_product_groups:
                            display_columns_for_filters.insert(1, 'Product Group')
                        filterable_columns = [c for c in display_columns_for_filters if c in filtered_df.columns]

                        # Classify columns as numeric vs text based on data (handles $/%/comma formatted strings)
                        numeric_columns = []
                        for col in filterable_columns:
                            try:
                                test_series = pd.to_numeric(
                                    filtered_df[col].astype(str).str.replace('$', '').str.replace('%', '').str.replace(',', ''),
                                    errors='coerce'
                                )
                                if not test_series.isna().all():
                                    numeric_columns.append(col)
                            except Exception:
                                pass

                        numeric_ops = ['>', '>=', '=', '<=', '<']
                        text_ops = ['contains', "doesn't contain", 'equals', "doesn't equal"]
                        
                        # Control buttons
                        col_btn1, col_btn2, col_btn3, col_spacer = st.columns([1.5, 1.5, 1.5, 5.5])
                        
                        with col_btn1:
                            add_group = st.button("âž• Add Filter Group", key="add_filter_group")
                        
                        with col_btn2:
                            clear_all = False
                            if st.session_state.campaign_filter_groups:
                                clear_all = st.button("ðŸ—‘ï¸ Clear All", key="clear_all_filter_groups")
                        
                        # Handle button actions without immediate rerun
                        if add_group:
                            # Choose sensible defaults based on available columns
                            default_col = (filterable_columns[0] if filterable_columns else (numeric_columns[0] if numeric_columns else 'Campaign'))
                            default_ops = numeric_ops if default_col in numeric_columns else text_ops
                            st.session_state.campaign_filter_groups.append({
                                'filters': [{
                                    'column': default_col,
                                    'operator': default_ops[0],
                                    'value': '',
                                    'logic': 'AND'
                                }],
                                'group_logic': 'AND'
                            })
                        
                        if clear_all:
                            st.session_state.campaign_filter_groups = []
                        
                        # Display filter groups
                        groups_to_remove = []
                        for group_idx, group in enumerate(st.session_state.campaign_filter_groups):
                            with st.container():
                                # Group header
                                group_col1, group_col2, group_col3 = st.columns([6, 2, 1])
                                
                                with group_col1:
                                    st.markdown(f"**Filter Group {group_idx + 1}**")
                                
                                with group_col2:
                                    if group_idx > 0:  # Only show group logic for groups after the first
                                        group['group_logic'] = st.selectbox(
                                            "Group Logic",
                                            options=['AND', 'OR'],
                                            index=['AND', 'OR'].index(group['group_logic']),
                                            key=f"group_logic_{group_idx}",
                                            help="How this group combines with previous groups"
                                        )
                                
                                with group_col3:
                                    if st.button("ðŸ—‘ï¸", key=f"remove_group_{group_idx}", help="Remove entire group"):
                                        groups_to_remove.append(group_idx)
                                
                                # Display filters within this group
                                filters_to_remove = []
                                for filter_idx, filter_config in enumerate(group['filters']):
                                    col1, col2, col3, col4, col5, col6 = st.columns([2, 1.2, 1.2, 1, 0.8, 0.8])
                                    
                                    with col1:
                                        filter_config['column'] = st.selectbox(
                                            "Column" if filter_idx == 0 else "",
                                            options=filterable_columns,
                                            index=(filterable_columns.index(filter_config['column']) if filter_config['column'] in filterable_columns else 0),
                                            key=f"filter_column_{group_idx}_{filter_idx}",
                                            label_visibility="visible" if filter_idx == 0 else "collapsed"
                                        )
                                    
                                    with col2:
                                        # Dynamically set operator options based on column type
                                        curr_col = filter_config.get('column')
                                        curr_ops = numeric_ops if curr_col in numeric_columns else text_ops
                                        if filter_config.get('operator') not in curr_ops:
                                            # Reset to default if previous operator is incompatible with new column type
                                            filter_config['operator'] = curr_ops[0]
                                        filter_config['operator'] = st.selectbox(
                                            "Operator" if filter_idx == 0 else "",
                                            options=curr_ops,
                                            index=curr_ops.index(filter_config['operator']) if filter_config['operator'] in curr_ops else 0,
                                            key=f"filter_operator_{group_idx}_{filter_idx}",
                                            label_visibility="visible" if filter_idx == 0 else "collapsed"
                                        )
                                    
                                    with col3:
                                        filter_config['value'] = st.text_input(
                                            "Value" if filter_idx == 0 else "",
                                            value=str(filter_config['value']),
                                            key=f"filter_value_{group_idx}_{filter_idx}",
                                            label_visibility="visible" if filter_idx == 0 else "collapsed"
                                        )
                                    
                                    with col4:
                                        if filter_idx > 0:  # Only show logic for filters after the first in each group
                                            filter_config['logic'] = st.selectbox(
                                                "Logic" if filter_idx == 1 else "",
                                                options=['AND', 'OR'],
                                                index=['AND', 'OR'].index(filter_config['logic']),
                                                key=f"filter_logic_{group_idx}_{filter_idx}",
                                                label_visibility="visible" if filter_idx == 1 else "collapsed"
                                            )
                                        else:
                                            st.write("")  # Empty space for first filter in group
                                    
                                    with col5:
                                        if st.button("âž•", key=f"add_filter_{group_idx}_{filter_idx}", help="Add filter to this group"):
                                            default_col = (filterable_columns[0] if filterable_columns else (numeric_columns[0] if numeric_columns else 'Campaign'))
                                            default_ops = numeric_ops if default_col in numeric_columns else text_ops
                                            group['filters'].append({
                                                'column': default_col,
                                                'operator': default_ops[0],
                                                'value': '',
                                                'logic': 'AND'
                                            })
                                    
                                    with col6:
                                        if len(group['filters']) > 1:  # Don't allow removing the last filter in a group
                                            if st.button("ðŸ—‘ï¸", key=f"remove_filter_{group_idx}_{filter_idx}", help="Remove filter"):
                                                filters_to_remove.append(filter_idx)
                                
                                # Remove filters marked for removal (process after all buttons to avoid key conflicts)
                                for filter_idx in reversed(filters_to_remove):
                                    group['filters'].pop(filter_idx)
                                
                                st.markdown("---")
                        
                        # Remove groups marked for removal (process after all buttons to avoid key conflicts)  
                        for group_idx in reversed(groups_to_remove):
                            st.session_state.campaign_filter_groups.pop(group_idx)
                        
                        # Apply advanced filters
                        if st.session_state.campaign_filter_groups:
                            try:
                                group_masks = []
                                
                                # Process each filter group
                                for group in st.session_state.campaign_filter_groups:
                                    group_mask = None
                                    
                                    # Process filters within the group
                                    for filter_idx, filter_config in enumerate(group['filters']):
                                        if filter_config['value'].strip():  # Only apply if value is provided
                                            column = filter_config['column']
                                            operator = filter_config['operator']
                                            value_str = filter_config['value'].strip()
                                            logic = filter_config['logic']
                                            
                                            # Apply numeric or text filtering depending on column type
                                            if column in filtered_df.columns:
                                                if column in numeric_columns:
                                                    # Numeric comparison
                                                    numeric_series = pd.to_numeric(
                                                        filtered_df[column].astype(str).str.replace('$', '').str.replace('%', '').str.replace(',', ''), 
                                                        errors='coerce'
                                                    )
                                                    try:
                                                        value = float(value_str.replace('$', '').replace('%', '').replace(',', ''))
                                                        if operator == '>':
                                                            condition_mask = numeric_series > value
                                                        elif operator == '>=':
                                                            condition_mask = numeric_series >= value
                                                        elif operator == '=':
                                                            condition_mask = numeric_series == value
                                                        elif operator == '<=':
                                                            condition_mask = numeric_series <= value
                                                        elif operator == '<':
                                                            condition_mask = numeric_series < value
                                                        else:
                                                            condition_mask = pd.Series([True] * len(numeric_series), index=numeric_series.index)
                                                        condition_mask = condition_mask.fillna(False)
                                                    except ValueError:
                                                        st.error(f"Invalid numeric value in Group {len(group_masks)+1}, Filter {filter_idx+1}: '{value_str}'")
                                                        condition_mask = pd.Series([False] * len(filtered_df), index=filtered_df.index)
                                                else:
                                                    # Text comparison (case-insensitive)
                                                    text_series = filtered_df[column].astype(str).fillna('')
                                                    val_norm = value_str
                                                    if operator == 'contains':
                                                        condition_mask = text_series.str.contains(val_norm, case=False, na=False)
                                                    elif operator == "doesn't contain":
                                                        condition_mask = ~text_series.str.contains(val_norm, case=False, na=False)
                                                    elif operator == 'equals':
                                                        condition_mask = text_series.str.strip().str.lower() == val_norm.strip().lower()
                                                    elif operator == "doesn't equal":
                                                        condition_mask = text_series.str.strip().str.lower() != val_norm.strip().lower()
                                                    else:
                                                        condition_mask = pd.Series([True] * len(text_series), index=text_series.index)
                                                    condition_mask = condition_mask.fillna(False)

                                                # Combine with previous conditions in this group
                                                if filter_idx == 0:
                                                    group_mask = condition_mask
                                                else:
                                                    if logic == 'AND':
                                                        group_mask = group_mask & condition_mask
                                                    else:  # OR
                                                        group_mask = group_mask | condition_mask
                                    
                                    # Add this group's mask to the list
                                    if group_mask is not None:
                                        group_masks.append((group_mask, group.get('group_logic', 'AND')))
                                
                                # Combine all group masks
                                if group_masks:
                                    final_mask = group_masks[0][0]  # Start with first group
                                    
                                    for i in range(1, len(group_masks)):
                                        mask, logic = group_masks[i]
                                        if logic == 'AND':
                                            final_mask = final_mask & mask
                                        else:  # OR
                                            final_mask = final_mask | mask
                                    
                                    # Apply the final combined mask
                                    filtered_df = filtered_df[final_mask]
                                    # Enforce ROAS > 0 if any ROAS '>' filter exists in Campaign Performance filters
                                    try:
                                        if any(
                                            (isinstance(g, dict) and isinstance(g.get('filters', []), list) and any(
                                                (isinstance(f, dict) and f.get('column') == 'ROAS' and f.get('operator') == '>' and str(f.get('value', '')).strip() != '')
                                                for f in g.get('filters', [])
                                            ))
                                            for g in st.session_state.get('campaign_filter_groups', [])
                                        ):
                                            if 'ROAS' in filtered_df.columns:
                                                _roas = pd.to_numeric(
                                                    filtered_df['ROAS'].astype(str).str.replace('$', '').str.replace('%', '').str.replace(',', ''),
                                                    errors='coerce'
                                                )
                                                filtered_df = filtered_df[_roas > 0]
                                    except Exception:
                                        pass
                                    
                            except Exception as e:
                                st.error(f"Error applying advanced filters: {e}")
                        
                        # Show active filter summary
                        if st.session_state.campaign_filter_groups:
                            active_filters = sum(len([f for f in group['filters'] if f['value'].strip()]) for group in st.session_state.campaign_filter_groups)
                            if active_filters > 0:
                                st.info(f"ðŸ” {active_filters} active filter(s) across {len(st.session_state.campaign_filter_groups)} group(s)")
                    
                    # Summary cards
                    if not filtered_df.empty:
                        # Calculate summary metrics
                        total_spend = filtered_df['Spend'].sum() if 'Spend' in filtered_df.columns else 0
                        total_ad_sales = filtered_df['Ad Sales'].sum() if 'Ad Sales' in filtered_df.columns else 0
                        total_clicks = filtered_df['Clicks'].sum() if 'Clicks' in filtered_df.columns else 0
                        total_orders = filtered_df['Orders'].sum() if 'Orders' in filtered_df.columns else 0
                        
                        # Calculate derived metrics
                        avg_acos = (total_spend / total_ad_sales * 100) if total_ad_sales > 0 else 0
                        avg_roas = (total_ad_sales / total_spend) if total_spend > 0 else 0
                        avg_cpc = (total_spend / total_clicks) if total_clicks > 0 else 0
                        avg_cvr = (total_orders / total_clicks * 100) if total_clicks > 0 else 0
                        avg_aov = (total_ad_sales / total_orders) if total_orders > 0 else 0
                        
                        # Display summary cards
                        st.subheader("Campaign Performance Summary")
                        col1, col2, col3, col4 = st.columns(4)
                        with col1:
                            st.metric("Spend", f"${total_spend:,.2f}")
                            st.metric("CPC", f"${avg_cpc:.2f}")
                        with col2:
                            st.metric("Ad Sales", f"${total_ad_sales:,.2f}")
                            st.metric("CVR", f"{avg_cvr:.2f}%")
                        with col3:
                            st.metric("ACoS", f"{avg_acos:.2f}%")
                            st.metric("AOV", f"${avg_aov:.2f}")
                        with col4:
                            st.metric("ROAS", f"{avg_roas:.2f}")
                            st.metric("Orders", f"{total_orders:,.0f}")
                        
                        
                        # Define columns to display
                        display_columns = ['Ad Type', 'Campaign', 'Spend', 'Ad Sales', '% Ad Spend', '% Ad Sales', 'ACoS', 'ROAS', 'CPC', 'CVR', 'AOV', 'Clicks', 'Orders']
                        
                        # Only include Product Group column if there are product groups
                        if campaign_product_groups:
                            display_columns.insert(1, 'Product Group')
                        
                        # Filter to only show available columns
                        available_columns = [col for col in display_columns if col in filtered_df.columns]
                        
                        if available_columns:
                            # Sort by spend for display
                            display_df = filtered_df[available_columns].sort_values('Spend', ascending=False).reset_index(drop=True)
                            
                            # Convert numeric columns for styling
                            numeric_df = display_df.copy()
                            for col in numeric_df.columns:
                                if col not in ['Ad Type', 'Product Group', 'Campaign']:
                                    numeric_df[col] = pd.to_numeric(numeric_df[col].astype(str).str.replace('$', '').str.replace('%', '').str.replace(',', ''), errors='coerce')
                            
                            # Create formatting dictionary
                            fmt_dict = {}
                            for col in numeric_df.columns:
                                if col in ['Ad Type', 'Product Group', 'Campaign']:
                                    continue
                                elif col == 'ROAS':
                                    fmt_dict[col] = lambda x: f"{x:.2f}"
                                elif col in ['ACoS', 'CVR', '% Ad Spend', '% Ad Sales']:
                                    fmt_dict[col] = lambda x: f"{x:.2f}%"
                                elif col in ['CPC', 'AOV']:
                                    fmt_dict[col] = lambda x: f"${x:.2f}"
                                elif col in ['Spend', 'Ad Sales']:
                                    fmt_dict[col] = lambda x: f"${x:,.2f}"
                                else:
                                    fmt_dict[col] = lambda x: f"{x:,.0f}" if pd.notnull(x) else "0"
                            
                            # Style the dataframe
                            styled_df = numeric_df.style.format(fmt_dict)
                            
                            # Apply color gradients to percentage columns
                            if '% Ad Spend' in numeric_df.columns:
                                styled_df = styled_df.apply(lambda x: [color_gradient_blue(v, 0, 100) 
                                                            if not pd.isna(v) else '' 
                                                            for v in x] if x.name == '% Ad Spend' else [''] * len(x), axis=0)
                            if '% Ad Sales' in numeric_df.columns:
                                styled_df = styled_df.apply(lambda x: [color_gradient_green(v, 0, 100) 
                                                            if not pd.isna(v) else '' 
                                                            for v in x] if x.name == '% Ad Sales' else [''] * len(x), axis=0)
                            
                            # Display the styled table
                            st.dataframe(
                                styled_df,
                                use_container_width=True,
                                hide_index=True
                            )
                        else:
                            st.info("No data available to display.")
                    else:
                        st.info("No campaigns match the current filters.")
                else:
                    st.info("No campaign performance data available. Please ensure you have uploaded bulk advertising files.")
                    
            except Exception as e:
                st.error(f"An error occurred while loading campaign performance data: {e}")
                import traceback
                with st.expander("Error Details"):
                    st.code(traceback.format_exc())     # --- Targeting Performance Section ---
        else:
            st.info("Upload Bulk Operations & Sales reports to populate campaign performance data.")
    
    # --- Targeting Performance Section ---
    if st.session_state.current_page == "advertising_audit":
        # Gold main header for Targeting Performance with anchor
        st.markdown("<div id='targeting-performance' class='section-anchor'></div>", unsafe_allow_html=True)
        st.markdown("<hr style='height:2px;border-width:0;color:gold;background-color:gold;margin-top:25px;margin-bottom:15px;margin-left:10px;margin-right:10px'>", unsafe_allow_html=True)
        st.markdown("<span class='main-section-header dashboard-section'>Targeting Performance</span>", unsafe_allow_html=True)
        st.markdown("<div style='margin-bottom:1.2rem;'></div>", unsafe_allow_html=True)
        if st.session_state.get('bulk_data') and st.session_state.get('client_config'):
            import math
            goals = st.session_state.client_config.get('goals', {})

            shared_column_config = {
                # Currency
                "Spend": st.column_config.NumberColumn(label="Spend", format="dollar"),
                "Ad Sales": st.column_config.NumberColumn(label="Ad Sales", format="dollar"),
                "Total Sales": st.column_config.NumberColumn(label="Total Sales ($)", format="dollar"),
                "AOV": st.column_config.NumberColumn(label="AOV ($)", format="dollar"),
                "CPA": st.column_config.NumberColumn(label="CPA ($)", format="dollar"),
                "CPC": st.column_config.NumberColumn(label="CPC ($)", format="dollar"),
                "Bid": st.column_config.NumberColumn(label="Bid ($)", format="dollar"),
                # Percentages
                "ACoS": st.column_config.NumberColumn(label="ACoS (%)", format="%.2f%%"),
                "CVR": st.column_config.NumberColumn(label="CVR (%)", format="%.2f%%"),
                "CTR": st.column_config.NumberColumn(label="CTR (%)", format="%.2f%%"),
                "TACoS": st.column_config.NumberColumn(label="TACoS (%)", format="%.2f%%"),
                "% Ad Sales": st.column_config.NumberColumn(label="% Ad Sales", format="%.2f%%"),
                "Ad Traffic % Sessions": st.column_config.NumberColumn(label="Ad Traffic % Sessions", format="%.2f%%"),
                # Integers with Commas
                "Impressions": st.column_config.NumberColumn(label="Impressions", format="localized"),
                "Clicks": st.column_config.NumberColumn(label="Clicks", format="localized"),
                "Orders": st.column_config.NumberColumn(label="Orders", format="localized"),
                "Units Sold": st.column_config.NumberColumn(label="Units Sold", format="localized"),
                "Sessions": st.column_config.NumberColumn(label="Sessions", format="localized"),
                # General Numbers
                "ROAS": st.column_config.NumberColumn(label="ROAS", format="%.2f"),
                # Text columns
                "Campaign": st.column_config.TextColumn(label="Campaign"),
                "Target": st.column_config.TextColumn(label="Target"),
                "Match Type": st.column_config.TextColumn(label="Match Type"),
                "Target Type": st.column_config.TextColumn(label="Target Type"),
                "Ad Type": st.column_config.TextColumn(label="Ad Type"),
                "Search Term": st.column_config.TextColumn(label="Search Term"),
                "Product Group": st.column_config.TextColumn(label="Product Group")
            }

            # Retrieve ACoS targets
            branded_acos = goals.get('branded_acos', None)
            non_branded_acos = goals.get('non_branded_acos', None)
            account_wide_acos = goals.get('account_wide_acos', None)

            # Get targeting data
            # No descriptions needed
            
            try:
                branded_targets_df, non_branded_targets_df = get_targeting_performance_data(
                    st.session_state.bulk_data,
                    st.session_state.client_config
                )
                st.session_state.branded_targets_df = branded_targets_df
                st.session_state.non_branded_targets_df = non_branded_targets_df
                st.session_state.all_targets_df = pd.concat([branded_targets_df, non_branded_targets_df], ignore_index=True)

                # Extract product groups for filtering
                product_groups = set()
                for df in [st.session_state.all_targets_df, branded_targets_df, non_branded_targets_df]:
                    if not df.empty and 'Product Group' in df.columns:
                        groups = df['Product Group'].dropna().unique()
                        product_groups.update([g for g in groups if g])
                
                # Initialize session state for product group filter if not exists
                if 'targeting_performance_product_group_filter' not in st.session_state:
                    st.session_state.targeting_performance_product_group_filter = []
                
                # Basic Filters Section
                st.markdown("**Basic Filters**")
                
                # First row of basic filters
                basic_filter_cols = st.columns(3)
                with basic_filter_cols[0]:
                    campaign_search = st.text_input("Filter by Campaign", key="targeting_campaign_search", help="Search for campaigns containing this text")
                with basic_filter_cols[1]:
                    target_search = st.text_input("Filter by Target", key="targeting_target_search", help="Search for targets containing this text")
                with basic_filter_cols[2]:
                    # Add product group filter to the right of target filter
                    if product_groups:
                        product_groups = sorted(list(product_groups))
                        selected_product_groups = st.multiselect(
                            "Filter by Product Group(s):",
                            options=product_groups,
                            key="targeting_performance_product_group_filter"
                        )
                        # Update the filter active state based on selection
                        st.session_state.targeting_performance_filter_active = len(selected_product_groups) > 0
                    else:
                        st.write("")  # Empty space if no product groups
                
                # Second row of basic filters
                basic_filter_cols2 = st.columns(3)
                with basic_filter_cols2[0]:
                    # Get unique match types from all data
                    all_match_types = set()
                    for df in [st.session_state.all_targets_df, branded_targets_df, non_branded_targets_df]:
                        if not df.empty and 'Match Type' in df.columns:
                            all_match_types.update(df['Match Type'].dropna().unique())
                    match_types = ['All'] + sorted(list(all_match_types))
                    selected_match_types = st.multiselect("Filter by Match Type", match_types, default=['All'], key="targeting_match_type_filter")
                    if 'All' in selected_match_types:
                        selected_match_types = [mt for mt in match_types if mt != 'All']
                
                with basic_filter_cols2[1]:
                    # Get unique target types from all data
                    all_target_types = set()
                    for df in [st.session_state.all_targets_df, branded_targets_df, non_branded_targets_df]:
                        if not df.empty and 'Target Type' in df.columns:
                            all_target_types.update(df['Target Type'].dropna().unique())
                    target_types = ['All'] + sorted(list(all_target_types))
                    selected_target_types = st.multiselect("Filter by Target Type", target_types, default=['All'], key="targeting_target_type_filter")
                    if 'All' in selected_target_types:
                        selected_target_types = [tt for tt in target_types if tt != 'All']
                
                with basic_filter_cols2[2]:
                    # Get unique ad types from all data
                    all_ad_types = set()
                    for df in [st.session_state.all_targets_df, branded_targets_df, non_branded_targets_df]:
                        temp_df = df.copy()
                        if 'Sheet Source' in temp_df.columns and 'Ad Type' not in temp_df.columns:
                            temp_df['Ad Type'] = temp_df['Sheet Source']
                        if not temp_df.empty and 'Ad Type' in temp_df.columns:
                            all_ad_types.update(temp_df['Ad Type'].dropna().unique())
                    ad_types = ['All'] + sorted(list(all_ad_types))
                    selected_ad_types = st.multiselect("Filter by Ad Type", ad_types, default=['All'], key="targeting_ad_type_filter")
                    if 'All' in selected_ad_types:
                        selected_ad_types = [at for at in ad_types if at != 'All']
                
                # Advanced Filters Section
                st.markdown("---")
                with st.expander("ðŸ”§ Advanced Filters", expanded=False):
                    # Initialize advanced filter state
                    if 'targeting_filter_groups' not in st.session_state:
                        st.session_state.targeting_filter_groups = []
                    st.markdown("**Advanced Filtering with Filter Groups**")
                    st.caption("Create multiple filter groups and combine them with AND/OR logic for complex filtering.")
                    
                    # Determine filterable columns based on visible table columns
                    display_columns_for_filters = [
                        'Ad Type', 'Product Group', 'Campaign', 'Target', 'Match Type', 'Target Type',
                        'Spend', 'Ad Sales', '% Ad Spend', '% Ad Sales', 'ACoS', 'ROAS', 'CPC', 'CVR', 'AOV',
                        'Clicks', 'Orders', 'Impressions', 'CTR', 'CPA', 'Bid'
                    ]
                    filterable_columns = [c for c in display_columns_for_filters if c in st.session_state.all_targets_df.columns]

                    # Classify columns as numeric or text by attempting numeric conversion (strip $, %, ,)
                    numeric_columns = []
                    for col in filterable_columns:
                        try:
                            test_series = pd.to_numeric(
                                st.session_state.all_targets_df[col].astype(str).str.replace('$', '').str.replace('%', '').str.replace(',', ''),
                                errors='coerce'
                            )
                            if not test_series.isna().all():
                                numeric_columns.append(col)
                        except Exception:
                            pass
                    # Force 'Target' to be treated as text, never numeric
                    numeric_columns = [c for c in numeric_columns if c != 'Target']
                    text_columns = [c for c in filterable_columns if c not in numeric_columns]

                    numeric_ops = ['>', '>=', '=', '<=', '<']
                    text_ops = ['contains', "doesn't contain", 'equals', "doesn't equal"]

                    # Control buttons
                    button_cols = st.columns([1, 1, 3])
                    with button_cols[0]:
                        add_group = st.button("âž• Add Filter Group", key="targeting_add_group")
                    with button_cols[1]:
                        clear_all = st.button("ðŸ—‘ï¸ Clear All Filters", key="targeting_clear_all")

                    # Handle button actions
                    if add_group:
                        default_col = (filterable_columns[0] if filterable_columns else (numeric_columns[0] if numeric_columns else 'Campaign'))
                        default_ops = numeric_ops if default_col in numeric_columns else text_ops
                        st.session_state.targeting_filter_groups.append({
                            'filters': [{
                                'column': default_col,
                                'operator': default_ops[0],
                                'value': '',
                                'logic': 'AND'
                            }],
                            'group_logic': 'AND'
                        })

                    if clear_all:
                        st.session_state.targeting_filter_groups = []
                        # Force immediate UI refresh so clearing reflects right away
                        st.rerun()

                    # Display filter groups
                    groups_to_remove = []
                    for group_idx, group in enumerate(st.session_state.targeting_filter_groups):
                        st.markdown(f"**Filter Group {group_idx + 1}**")

                        # Group controls
                        group_col1, group_col2, group_col3 = st.columns([2, 1, 1])
                        with group_col1:
                            st.markdown(f"*Group {group_idx + 1} Logic*")
                        with group_col2:
                            group['group_logic'] = st.selectbox(
                                "Group Logic",
                                ["AND", "OR"],
                                index=0 if group.get('group_logic', 'AND') == 'AND' else 1,
                                key=f"targeting_group_logic_{group_idx}",
                                label_visibility="collapsed"
                            )
                        with group_col3:
                            if st.button("ðŸ—‘ï¸", key=f"targeting_remove_group_{group_idx}", help="Remove entire group"):
                                groups_to_remove.append(group_idx)

                        # Display filters within this group
                        filters_to_remove = []
                        for filter_idx, filter_item in enumerate(group['filters']):
                            col1, col2, col3, col4, col5, col6 = st.columns([2, 1, 2, 1, 1, 1])

                            with col1:
                                k_col = f"targeting_filter_col_{group_idx}_{filter_idx}"
                                # Pre-seed widget state once, then let the widget manage its own value
                                if k_col not in st.session_state:
                                    st.session_state[k_col] = (
                                        filter_item.get('column')
                                        if filter_item.get('column') in filterable_columns
                                        else (filterable_columns[0] if filterable_columns else '')
                                    )
                                st.selectbox(
                                    "Column",
                                    options=filterable_columns,
                                    key=k_col,
                                    label_visibility="collapsed" if filter_idx > 0 else "visible"
                                )
                                filter_item['column'] = st.session_state[k_col]

                            with col2:
                                curr_col = filter_item.get('column')
                                curr_ops = numeric_ops if curr_col in numeric_columns else text_ops
                                k_op = f"targeting_filter_op_{group_idx}_{filter_idx}"
                                # Pre-seed operator respecting available ops
                                if k_op not in st.session_state or st.session_state[k_op] not in curr_ops:
                                    st.session_state[k_op] = (
                                        filter_item.get('operator') if filter_item.get('operator') in curr_ops else curr_ops[0]
                                    )
                                st.selectbox(
                                    "Operator",
                                    options=curr_ops,
                                    key=k_op,
                                    label_visibility="collapsed" if filter_idx > 0 else "visible"
                                )
                                filter_item['operator'] = st.session_state[k_op]

                            with col3:
                                k_val = f"targeting_filter_val_{group_idx}_{filter_idx}"
                                if k_val not in st.session_state:
                                    st.session_state[k_val] = str(filter_item.get('value', ''))
                                st.text_input(
                                    "Value",
                                    key=k_val,
                                    label_visibility="collapsed" if filter_idx > 0 else "visible"
                                )
                                filter_item['value'] = st.session_state[k_val]

                            with col4:
                                if filter_idx < len(group['filters']) - 1:  # Not the last filter in group
                                    k_logic = f"targeting_filter_logic_{group_idx}_{filter_idx}"
                                    if k_logic not in st.session_state:
                                        st.session_state[k_logic] = filter_item.get('logic', 'AND') if filter_item.get('logic', 'AND') in ["AND", "OR"] else "AND"
                                    st.selectbox(
                                        "Logic",
                                        ["AND", "OR"],
                                        key=k_logic,
                                        label_visibility="collapsed" if filter_idx > 0 else "visible"
                                    )
                                    filter_item['logic'] = st.session_state[k_logic]
                                else:
                                    st.write("")  # Empty space for last filter in group

                            with col5:
                                if st.button("âž•", key=f"targeting_add_filter_{group_idx}_{filter_idx}", help="Add filter to this group"):
                                    default_col = (filterable_columns[0] if filterable_columns else (numeric_columns[0] if numeric_columns else 'Campaign'))
                                    default_ops = numeric_ops if default_col in numeric_columns else text_ops
                                    group['filters'].append({
                                        'column': default_col,
                                        'operator': default_ops[0],
                                        'value': '',
                                        'logic': 'AND'
                                    })

                            with col6:
                                if len(group['filters']) > 1:  # Don't allow removing the last filter in a group
                                    if st.button("ðŸ—‘ï¸", key=f"targeting_remove_filter_{group_idx}_{filter_idx}", help="Remove filter"):
                                        filters_to_remove.append(filter_idx)

                        # Remove filters marked for removal
                        if filters_to_remove:
                            for filter_idx in reversed(filters_to_remove):
                                group['filters'].pop(filter_idx)
                            # Force immediate UI refresh so deletion reflects right away
                            st.rerun()

                        st.markdown("---")

                    # Remove groups marked for removal
                    if groups_to_remove:
                        for group_idx in reversed(groups_to_remove):
                            st.session_state.targeting_filter_groups.pop(group_idx)
                        # Force immediate UI refresh so deletion reflects right away
                        st.rerun()
                
                # Move tabs here - under filters and above scorecard
                st.markdown("---")
                tab_labels = ["All Targets", "Branded Targets", "Non-Branded Targets"]
                tabs = st.tabs(tab_labels)
                target_dfs = [st.session_state.all_targets_df, branded_targets_df, non_branded_targets_df]
                target_acos_list = [account_wide_acos, branded_acos, non_branded_acos]

                for tab, label, df, target_acos in zip(tabs, tab_labels, target_dfs, target_acos_list):
                    with tab:
                        # Defensive: If df is a dict, display warning and show each DataFrame
                        if isinstance(df, dict):
                            st.warning(f"{label} data is a dictionary. Displaying each item individually.")
                            for subkey, subdf in df.items():
                                st.subheader(subkey)
                                try:
                                    st.dataframe(subdf)
                                except Exception as e:
                                    st.error(f"Error displaying {subkey}: {e}")
                            continue

                        # Apply all filters to the data
                        filtered_df = df.copy()
                        
                        # --- Patch: Ensure Product Group is always campaign-level ---
                        # Build a mapping from campaign name to tag_1 (campaign-level product group)
                        campaign_tag_map = {}
                        if 'client_config' in st.session_state and 'campaign_tags_data' in st.session_state.client_config:
                            for cname, cinfo in st.session_state.client_config['campaign_tags_data'].items():
                                if cinfo.get('tag_1'):
                                    campaign_tag_map[cname.upper()] = cinfo['tag_1']
                        # Overwrite Product Group column with the campaign-level tag_1
                        # Only populate if there are actually product groups defined in Campaign Tagging
                        if 'Campaign' in filtered_df.columns:
                            if campaign_tag_map:  # Only if there are product groups defined
                                filtered_df['Product Group'] = filtered_df['Campaign'].apply(lambda x: campaign_tag_map.get(str(x).upper(), '') or 'Untagged Group')
                            else:
                                filtered_df['Product Group'] = 'Untagged Group'  # Use 'Untagged Group' if no product groups defined
                        
                        # Apply basic filters
                        # Campaign search filter
                        if campaign_search and 'Campaign' in filtered_df.columns:
                            def phrase_match(value, filter_value):
                                if not isinstance(value, str):
                                    return False
                                import re
                                pattern = re.escape(filter_value)
                                regex = rf"(^|[|\s]){pattern}([|\s]|$)"
                                return re.search(regex, value, re.IGNORECASE) is not None or value.strip().lower() == filter_value.strip().lower()
                            
                            matched_campaigns = filtered_df['Campaign'].astype(str).apply(lambda x: phrase_match(x, campaign_search))
                            filtered_df = filtered_df[matched_campaigns]
                        
                        # Target search filter
                        if target_search and 'Target' in filtered_df.columns:
                            import re
                            search_terms = [term.strip().lower() for term in target_search.split('|') if term.strip()]
                            mask = pd.Series(False, index=filtered_df.index)
                            for term in search_terms:
                                if '|' in term:
                                    sub_terms = [t.strip() for t in term.split('|') if t.strip()]
                                    for sub_term in sub_terms:
                                        mask = mask | filtered_df['Target'].astype(str).str.lower().str.contains(re.escape(sub_term), regex=True, na=False)
                                else:
                                    mask = mask | filtered_df['Target'].astype(str).str.lower().str.contains(re.escape(term), regex=True, na=False)
                            filtered_df = filtered_df[mask]
                        
                        # Product group filter
                        if st.session_state.get('targeting_performance_filter_active', False) and len(st.session_state.targeting_performance_product_group_filter) > 0:
                            if 'Product Group' in filtered_df.columns:
                                filtered_df = filtered_df[filtered_df['Product Group'].isin(st.session_state.targeting_performance_product_group_filter)]
                        
                        # Match type filter
                        if selected_match_types and 'Match Type' in filtered_df.columns:
                            filtered_df = filtered_df[filtered_df['Match Type'].isin(selected_match_types)]
                        
                        # Target type filter
                        if selected_target_types and 'Target Type' in filtered_df.columns:
                            filtered_df = filtered_df[filtered_df['Target Type'].isin(selected_target_types)]
                        
                        # Ad type filter
                        if selected_ad_types and 'Product' in filtered_df.columns:
                            filtered_df = filtered_df[filtered_df['Product'].isin(selected_ad_types)]
                        
                        # Apply advanced filters (numeric and text with AND/OR logic)
                        if st.session_state.targeting_filter_groups:
                            try:
                                group_conditions = []
                                for group in st.session_state.targeting_filter_groups:
                                    if not group['filters']:
                                        continue
                                    filter_conditions = []
                                    for filter_item in group['filters']:
                                        if not str(filter_item.get('value', '')).strip():
                                            continue
                                        column = filter_item.get('column')
                                        operator = filter_item.get('operator')
                                        value_raw = str(filter_item.get('value', '')).strip()
                                        if column not in filtered_df.columns:
                                            continue
                                        if column in numeric_columns:
                                            series_num = pd.to_numeric(
                                                filtered_df[column].astype(str).str.replace('$', '').str.replace('%', '').str.replace(',', ''),
                                                errors='coerce'
                                            )
                                            try:
                                                comp_val = float(value_raw.replace('$', '').replace('%', '').replace(',', ''))
                                            except ValueError:
                                                st.error(f"Invalid numeric value '{value_raw}' for column '{column}'.")
                                                continue
                                            if operator == '>':
                                                condition = series_num > comp_val
                                            elif operator == '>=':
                                                condition = series_num >= comp_val
                                            elif operator == '=':
                                                condition = series_num == comp_val
                                            elif operator == '<=':
                                                condition = series_num <= comp_val
                                            elif operator == '<':
                                                condition = series_num < comp_val
                                            else:
                                                continue
                                            condition = condition.fillna(False)
                                        else:
                                            series_txt = filtered_df[column].astype(str).fillna('')
                                            v = value_raw
                                            if operator == 'contains':
                                                condition = series_txt.str.contains(v, case=False, na=False)
                                            elif operator == "doesn't contain":
                                                condition = ~series_txt.str.contains(v, case=False, na=False)
                                            elif operator == 'equals':
                                                condition = series_txt.str.strip().str.lower() == v.strip().lower()
                                            elif operator == "doesn't equal":
                                                condition = series_txt.str.strip().str.lower() != v.strip().lower()
                                            else:
                                                continue
                                            condition = condition.fillna(False)
                                        filter_conditions.append(condition)
                                    if filter_conditions:
                                        group_condition = filter_conditions[0]
                                        for i, condition in enumerate(filter_conditions[1:], 1):
                                            filter_logic = group['filters'][i-1]['logic']
                                            if filter_logic == 'AND':
                                                group_condition = group_condition & condition
                                            else:
                                                group_condition = group_condition | condition
                                        group_conditions.append(group_condition)
                                    if group_conditions:
                                        final_condition = group_conditions[0]
                                        for i, condition in enumerate(group_conditions[1:], 1):
                                            group_logic = st.session_state.targeting_filter_groups[i-1]['group_logic']
                                            if group_logic == 'AND':
                                                final_condition = final_condition & condition
                                            else:
                                                final_condition = final_condition | condition
                                    filtered_df = filtered_df[final_condition]
                                    # Enforce ROAS > 0 if any ROAS '>' filter exists in Targeting Performance filters
                                    try:
                                        if any(
                                            (isinstance(g, dict) and isinstance(g.get('filters', []), list) and any(
                                                (isinstance(f, dict) and f.get('column') == 'ROAS' and f.get('operator') == '>' and str(f.get('value', '')).strip() != '')
                                                for f in g.get('filters', [])
                                            ))
                                            for g in st.session_state.get('targeting_filter_groups', [])
                                        ):
                                            if 'ROAS' in filtered_df.columns:
                                                _roas = pd.to_numeric(
                                                    filtered_df['ROAS'].astype(str).str.replace('$', '').str.replace('%', '').str.replace(',', ''),
                                                    errors='coerce'
                                                )
                                                filtered_df = filtered_df[_roas > 0]
                                    except Exception:
                                        pass
                            except Exception as e:
                                st.error(f"Error applying advanced filters: {e}")
                        
                            
                        # Sort by Ad Sales if the column exists
                        sales_col = None
                        if 'sd_attribution_choice' in st.session_state and st.session_state.sd_attribution_choice == "Sales (Views & Clicks)" and 'Sponsored Display' in str(filtered_df.get('Campaign Type', '')).upper():
                            sales_col = 'Sales (Views & Clicks)' if 'Sales (Views & Clicks)' in filtered_df.columns else 'Sales'
                        else:
                            sales_col = 'Ad Sales'
                                
                        if sales_col in filtered_df.columns:
                            filtered_df = filtered_df.sort_values(by=sales_col, ascending=False)

                        # Remove KPI Summary Cards
                        if not filtered_df.empty:
                            # Debug information only
                            st.session_state.debug_messages.append(f"Targeting Performance Summary - Using sales column: {sales_col}")
                            st.session_state.debug_messages.append(f"Available columns: {filtered_df.columns.tolist()}")



                        # --- Summary Metrics from Filtered Data ---
                        if not filtered_df.empty:
                            spend = filtered_df['Spend'].replace('[\$,]', '', regex=True).astype(float).sum() if 'Spend' in filtered_df.columns else 0
                            
                            # Use the appropriate sales column based on attribution choice
                            sales_col = None
                            if 'sd_attribution_choice' in st.session_state and st.session_state.sd_attribution_choice == "Sales (Views & Clicks)" and 'Sponsored Display' in str(filtered_df.get('Campaign Type', '')).upper():
                                sales_col = 'Sales (Views & Clicks)' if 'Sales (Views & Clicks)' in filtered_df.columns else 'Ad Sales'
                            else:
                                sales_col = 'Ad Sales'
                                
                            sales = filtered_df[sales_col].replace('[\$,]', '', regex=True).astype(float).sum() if sales_col in filtered_df.columns else 0
                            acos = (spend / sales * 100) if sales > 0 else 0
                            roas = (sales / spend) if spend > 0 else 0
                            clicks = filtered_df['Clicks'].replace(',', '', regex=True).astype(float).sum() if 'Clicks' in filtered_df.columns else 0
                            cpc = (spend / clicks) if clicks > 0 else 0
                            cvr = (filtered_df['Orders'].replace(',', '', regex=True).astype(float).sum() / filtered_df['Clicks'].replace(',', '', regex=True).astype(float).sum() * 100) if 'Clicks' in filtered_df.columns and 'Orders' in filtered_df.columns and filtered_df['Clicks'].sum() > 0 else 0
                            aov = (sales / filtered_df['Orders'].replace(',', '', regex=True).astype(float).sum()) if 'Orders' in filtered_df.columns and filtered_df['Orders'].sum() > 0 else 0
                            
                            # Display summary cards
                            kpi_cols = st.columns(6)
                            kpi_cols[0].metric("TOTAL SPEND", f"${spend:,.2f}")
                            kpi_cols[1].metric("AD SALES", f"${sales:,.2f}")
                            kpi_cols[2].metric("ACOS", f"{acos:.2f}%")
                            kpi_cols[3].metric("CPC", f"${cpc:.2f}")
                            kpi_cols[4].metric("CVR", f"{cvr:.2f}%")
                            kpi_cols[5].metric("AOV", f"${aov:.2f}")

                        # --- Main Table: Paginated, Styled ---
                        if not filtered_df.empty:
                            # --- Main Table: Styled ---
                            # No pagination - display all rows
                            display_df = filtered_df.copy()

                            # --- Remove unwanted columns ---
                            cols_to_remove = ["Classification", "Classification Reason", "Ad Group"]
                            
                            # Also remove Product Group column if no product groups are defined in Campaign Tagging
                            has_product_groups = False
                            if 'client_config' in st.session_state and 'campaign_tags_data' in st.session_state.client_config:
                                has_product_groups = any(v.get('tag_1', '') for v in st.session_state.client_config['campaign_tags_data'].values())
                            
                            if not has_product_groups:
                                cols_to_remove.append("Product Group")
                            
                            display_df = display_df.drop(columns=[col for col in cols_to_remove if col in display_df.columns], errors="ignore")

                            # --- Rename and reorder columns ---
                            if "Sheet Source" in display_df.columns:
                                # Check if 'Ad Type' column already exists
                                if 'Ad Type' in display_df.columns:
                                    # If Ad Type already exists, don't rename Sheet Source, just drop it
                                    display_df = display_df.drop(columns=['Sheet Source'])
                                else:
                                    # If Ad Type doesn't exist, rename Sheet Source to Ad Type
                                    display_df = display_df.rename(columns={"Sheet Source": "Ad Type"})
                                    
                            # Ensure no duplicate column names exist
                            cols = list(display_df.columns)
                            seen = set()
                            deduped_cols = []
                            for col in cols:
                                if col in seen:
                                    # Skip duplicate columns
                                    continue
                                seen.add(col)
                                deduped_cols.append(col)
                            display_df = display_df[deduped_cols]
                                    
                            # Make sure 'Ad Sales' exists, but keep 'Sales' for the dataframe
                            # Only remove 'Sales' from the display
                            if 'Sales' in display_df.columns and 'Ad Sales' not in display_df.columns:
                                display_df['Ad Sales'] = display_df['Sales']
                            elif 'Ad Sales' in display_df.columns and 'Sales' not in display_df.columns:
                                display_df['Sales'] = display_df['Ad Sales']
                                
                            # For display purposes, we'll hide the 'Sales' column
                            display_cols = [col for col in display_df.columns if col != 'Sales']
                                
                            # Move 'Bid' column to the end if it exists
                            if 'Bid' in display_df.columns:
                                cols = list(display_df.columns)
                                cols.remove('Bid')
                                cols.append('Bid')
                                display_df = display_df[display_cols]

                            # --- Main Table: Display with formatting and sorting preserved ---
                            try:
                                if target_acos is not None:
                                    use_avg_fallback = False
                                    if 'client_config' in st.session_state:
                                        goals = st.session_state.client_config.get('goals', {})
                                        if 'Branded' in label:
                                            use_avg_fallback = goals.get('use_avg_acos_branded', False)
                                        elif 'Non-Branded' in label:
                                            use_avg_fallback = goals.get('use_avg_acos_nonbranded', False)
                                        else:
                                            use_avg_fallback = goals.get('use_avg_acos_account', False)
                                    style_acos(display_df, target_acos, column_config=shared_column_config, use_avg_as_fallback=use_avg_fallback, title=f"{label} Targeting", use_expander=True)
                                else:
                                    st.dataframe(display_df, use_container_width=True, hide_index=True, column_config=shared_column_config)
                            except Exception as e:
                                st.error(f"Error displaying styled table: {e}")
                                st.session_state.debug_messages.append(f"[Targeting Table Error] {e}")

                            # Display row count
                            st.caption(f"Total Rows: {len(filtered_df)}")
                            
                            # --- Export: Targeting Performance (Filtered) -> Excel ---
                            try:
                                if not filtered_df.empty:
                                    export_df = display_df.copy()
                                    # Convert numeric-like columns
                                    currency_cols = ['Spend', 'Ad Sales', 'CPC', 'AOV', 'Bid']
                                    percent_cols = ['ACoS', 'CTR', 'CVR', '% Ad Spend', '% Ad Sales', '% of Spend', '% of Ad Sales']
                                    int_like_cols = ['Orders', 'Impressions', 'Clicks']
                                    float_like_cols = ['ROAS']
                                    for col in export_df.columns:
                                        if col in currency_cols:
                                            export_df[col] = pd.to_numeric(export_df[col].astype(str).str.replace(r'[^0-9\.-]', '', regex=True), errors='coerce')
                                        elif col in percent_cols:
                                            export_df[col] = pd.to_numeric(export_df[col].astype(str).str.replace(r'[^0-9\.-]', '', regex=True), errors='coerce')
                                        elif col in int_like_cols:
                                            export_df[col] = pd.to_numeric(export_df[col].astype(str).str.replace(',', '', regex=True), errors='coerce')
                                        elif col in float_like_cols:
                                            export_df[col] = pd.to_numeric(export_df[col].astype(str).str.replace(',', '', regex=True), errors='coerce')

                                    # Build workbook

                                    wb = Workbook()
                                    ws = wb.active
                                    ws.title = str(label)

                                    for r_idx, row in enumerate(dataframe_to_rows(export_df, index=False, header=True), 1):
                                        ws.append(row)
                                        if r_idx == 1:
                                            for c_idx, _ in enumerate(row, 1):
                                                cell = ws.cell(row=1, column=c_idx)
                                                cell.fill = PatternFill(start_color="B8860B", end_color="B8860B", fill_type="solid")
                                                cell.font = Font(color="FFFFFF", bold=True)
                                                cell.alignment = Alignment(horizontal="center", vertical="center")

                                    # Conditional formatting for ACoS against target
                                    acos_target = None
                                    if 'client_config' in st.session_state:
                                        goals = st.session_state.client_config.get('goals', {})
                                        if 'Branded' in label:
                                            acos_target = goals.get('branded_acos') or goals.get('account_wide_acos')
                                        elif 'Non-Branded' in label:
                                            acos_target = goals.get('non_branded_acos') or goals.get('account_wide_acos')
                                        else:
                                            acos_target = goals.get('account_wide_acos')
                                    if acos_target is None or not isinstance(acos_target, (int, float)):
                                        acos_target = 25

                                    if 'ACoS' in export_df.columns:
                                        acos_col_idx = list(export_df.columns).index('ACoS') + 1
                                        range_str = f"{get_column_letter(acos_col_idx)}2:{get_column_letter(acos_col_idx)}{len(export_df) + 1}"
                                        rule = ColorScaleRule(start_type='num', start_value=0, start_color='90EE90',
                                                              mid_type='num', mid_value=acos_target, mid_color='FFFF00',
                                                              end_type='num', end_value=acos_target * 2, end_color='FF0000')
                                        ws.conditional_formatting.add(range_str, rule)

                                    # Number formats
                                    for col_name in export_df.columns:
                                        col_letter = get_column_letter(list(export_df.columns).index(col_name) + 1)
                                        if col_name in currency_cols:
                                            for row in range(2, len(export_df) + 2):
                                                ws[f"{col_letter}{row}"].number_format = '"$"#,##0.00'
                                        elif col_name in percent_cols:
                                            for row in range(2, len(export_df) + 2):
                                                ws[f"{col_letter}{row}"].number_format = '0.00"%"'
                                        elif col_name in int_like_cols:
                                            for row in range(2, len(export_df) + 2):
                                                ws[f"{col_letter}{row}"].number_format = '#,##0'
                                        elif col_name in float_like_cols:
                                            for row in range(2, len(export_df) + 2):
                                                ws[f"{col_letter}{row}"].number_format = '0.00'

                                    # Auto-adjust column widths
                                    for column in ws.columns:
                                        max_len = 0
                                        col_letter = column[0].column_letter
                                        for cell in column:
                                            try:
                                                if cell.value is not None and len(str(cell.value)) > max_len:
                                                    max_len = len(str(cell.value))
                                            except Exception:
                                                pass
                                        ws.column_dimensions[col_letter].width = min(max_len + 2, 50)

                                    # Save and provide download
                                    excel_buffer = io.BytesIO()
                                    wb.save(excel_buffer)
                                    excel_buffer.seek(0)
                                    from datetime import datetime as _dt
                                    ts = _dt.now().strftime("%Y%m%d_%H%M%S")
                                    st.download_button(
                                        label="ðŸ’¾ Download Targeting Performance (.xlsx)",
                                        data=excel_buffer.getvalue(),
                                        file_name=f"Targeting_Performance_{str(label).replace(' ', '_')}_{ts}.xlsx",
                                        mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                                        key=f"download_targeting_{str(label).replace(' ', '_')}_{ts}"
                                    )
                            except Exception as e:
                                st.error(f"Error creating export: {e}")
                                
                        else:
                            st.info(f"No {label.lower()} found.")
                            
                # After all tabs are processed, add the ACoS Range Distribution section with its own tabs
                # --- Target ACoS Range Distribution section commented out by request ---
                # (existing commented code remains unchanged)

                # --- Dynamic Word Cloud Section ---
                if not st.session_state.branded_targets_df.empty or not st.session_state.non_branded_targets_df.empty or not st.session_state.all_targets_df.empty:
                    st.markdown("<div style='margin-top:2rem;'></div>", unsafe_allow_html=True)
                    with st.expander("Show Target Word Cloud", expanded=False):
                        st.markdown("""
                            <div style='font-size:1.1rem; color:#bfa23a; font-weight:600; margin-bottom:0.5rem;'>Target Word Cloud</div>
                            <style>
                            .wordcloud-container {
                                background: #181818;
                                border-radius: 16px;
                                box-shadow: 0 2px 12px rgba(0,0,0,0.25);
                                padding: 1.5rem 1rem 1rem 1rem;
                                margin-bottom: 1.5rem;
                            }
                            </style>
                        """, unsafe_allow_html=True)
                        
                        # Always remove ASINs from target word cloud
                        remove_asins_target = True

                        # Outer tabs for All, Branded, Non-Branded
                        wc_outer_labels = ["All", "Branded", "Non-Branded"]
                        wc_outer_tabs = st.tabs(wc_outer_labels)
                        wc_dfs = [st.session_state.all_targets_df, st.session_state.branded_targets_df, st.session_state.non_branded_targets_df]

                        for wc_outer_idx, (wc_outer_tab, wc_outer_label, wc_df) in enumerate(zip(wc_outer_tabs, wc_outer_labels, wc_dfs)):
                            with wc_outer_tab:
                                wc_tab_labels = ["Spend", "Ad Sales"]
                                wc_tabs = st.tabs(wc_tab_labels)
                                for wc_tab_label, wc_metric in zip(wc_tab_labels, ["Spend", "Ad Sales"]):
                                    with wc_tabs[wc_tab_labels.index(wc_tab_label)]:
                                        st.markdown("<div class='wordcloud-container'>", unsafe_allow_html=True)
                                        # Defensive: check columns
                                        if wc_df is not None and not wc_df.empty and 'Target' in wc_df.columns and wc_metric in wc_df.columns:
                                            wc_data = wc_df[["Target", wc_metric]].copy()
                                            wc_data[wc_metric] = (
                                                wc_data[wc_metric]
                                                .replace('[\$,]', '', regex=True)
                                                .replace(',', '', regex=True)
                                                .astype(str)
                                                .str.replace(',', '')
                                                .astype(float)
                                            )
                                            wc_data = wc_data.groupby("Target")[wc_metric].sum().reset_index()
                                            wc_dict = dict(zip(wc_data["Target"], wc_data[wc_metric]))
                                            wc_dict = {k: v for k, v in wc_dict.items() if v > 0}
                                            
                                            # Always filter out ASINs from target word cloud
                                            wc_dict = {k: v for k, v in wc_dict.items() 
                                                      if not (len(str(k)) == 10 and str(k).lower().startswith('b0'))}
                                            
                                            if wc_dict:
                                                # Lazy import heavy libs
                                                WordCloud, _STOP = get_wordcloud()
                                                plt = get_plt()
                                                wc = WordCloud(
                                                    width=900,
                                                    height=350,
                                                    background_color="#181818",
                                                    colormap="summer",
                                                    prefer_horizontal=0.95,
                                                    contour_color="#bfa23a",
                                                    contour_width=2
                                                ).generate_from_frequencies(wc_dict)
                                                fig, ax = plt.subplots(figsize=(12, 4), facecolor="#181818")
                                                ax.imshow(wc, interpolation="bilinear")
                                                ax.axis("off")
                                                fig.patch.set_facecolor('#181818')
                                                plt.tight_layout(pad=0)
                                                st.pyplot(fig)
                                                plt.close(fig)
                                            else:
                                                st.info(f"No {wc_metric.lower()} data available for word cloud.")
                                        else:
                                            st.info(f"No data available for {wc_metric} word cloud.")
                                        st.markdown("</div>", unsafe_allow_html=True)

                    # --- Additional Targeting Information: Counts by Match Type ---
                    with st.expander("Additional Targeting Information", expanded=False):
                        st.caption("Counts of rows with at least one Click and at least one Order by Match Type. Includes any available types beyond Exact/Broad/Phrase/Auto (e.g., Category Targeting, Product/ASIN Targeting).")

                        def _build_match_type_counts_table(src_df: pd.DataFrame) -> pd.DataFrame:
                            try:
                                if src_df is None or src_df.empty:
                                    return pd.DataFrame()
                                required_cols = {'Match Type', 'Clicks', 'Orders'}
                                if not required_cols.issubset(set(src_df.columns)):
                                    return pd.DataFrame()

                                df = src_df.copy()
                                # Normalize numerics that may be stored as strings with commas
                                df['Clicks_num'] = pd.to_numeric(df['Clicks'].astype(str).str.replace(',', ''), errors='coerce').fillna(0)
                                df['Orders_num'] = pd.to_numeric(df['Orders'].astype(str).str.replace(',', ''), errors='coerce').fillna(0)

                                # Group by Match Type and count rows meeting thresholds
                                grp = df.groupby('Match Type', dropna=False)
                                clicks_counts = grp.apply(lambda g: (g['Clicks_num'] >= 1).sum())
                                orders_counts = grp.apply(lambda g: (g['Orders_num'] >= 1).sum())

                                out = (
                                    pd.DataFrame({
                                        'Match Type': clicks_counts.index.astype(str),
                                        '# Terms w/ Clicks': clicks_counts.values,
                                        '# of Converting Terms': orders_counts.values,
                                    })
                                    .sort_values(by=['# Terms w/ Clicks', 'Match Type'], ascending=[False, True])
                                    .reset_index(drop=True)
                                )
                                return out
                            except Exception as e:
                                st.session_state.debug_messages.append(f"[Additional Targeting Info] Error building table: {e}")
                                return pd.DataFrame()

                        counts_outer_labels = ["All", "Branded", "Non-Branded"]
                        counts_outer_tabs = st.tabs(counts_outer_labels)
                        counts_dfs = [
                            st.session_state.all_targets_df if 'all_targets_df' in st.session_state else pd.DataFrame(),
                            st.session_state.branded_targets_df if 'branded_targets_df' in st.session_state else pd.DataFrame(),
                            st.session_state.non_branded_targets_df if 'non_branded_targets_df' in st.session_state else pd.DataFrame(),
                        ]

                        for counts_tab, tab_label, base_df in zip(counts_outer_tabs, counts_outer_labels, counts_dfs):
                            with counts_tab:
                                table_df = _build_match_type_counts_table(base_df)
                                if table_df.empty:
                                    st.info(f"No data available to summarize for {tab_label}.")
                                else:
                                    st.dataframe(
                                        table_df,
                                        use_container_width=True,
                                        hide_index=True,
                                        column_config={
                                            '# Terms w/ Clicks': st.column_config.NumberColumn(format='%d'),
                                            '# of Converting Terms': st.column_config.NumberColumn(format='%d'),
                                        },
                                    )

                    # --- Search Term Table Section ---
                    st.markdown("<div style='margin-top:3rem;'></div>", unsafe_allow_html=True)
                    st.markdown("<div id='search-term-performance' class='section-anchor'></div>", unsafe_allow_html=True)
                    st.markdown("<hr style='height:2px;border-width:0;color:gold;background-color:gold;margin-top:25px;margin-bottom:15px;margin-left:10px;margin-right:10px'>", unsafe_allow_html=True)
                    st.markdown("<span class='main-section-header'>Search Term Performance</span>", unsafe_allow_html=True)
                    
                    # No descriptions needed
                    
                    # Initialize session state variables for search term filters if they don't exist
                    if 'search_term_product_group_filter' not in st.session_state:
                        st.session_state.search_term_product_group_filter = []
                    if 'search_term_filter_active' not in st.session_state:
                        st.session_state.search_term_filter_active = False
                    if 'search_term_filter_groups' not in st.session_state:
                        st.session_state.search_term_filter_groups = []
                    
                    # Get search term data for filter setup
                    search_term_df = pd.DataFrame()
                    product_groups = []
                    if 'bulk_data' in st.session_state:
                        search_term_df = get_search_term_data(st.session_state.bulk_data, st.session_state.client_config)
                        if not search_term_df.empty and 'Product Group' in search_term_df.columns:
                            product_groups = sorted([str(g) for g in search_term_df['Product Group'].dropna().unique() if g])
                    
                    # Basic Filters Section
                    st.markdown("**Basic Filters**")
                    
                    # First row of basic filters
                    basic_filter_cols = st.columns(3)
                    with basic_filter_cols[0]:
                        campaign_search = st.text_input("Filter by Campaign", key="search_term_campaign_search", help="Search for campaigns containing this text")
                    with basic_filter_cols[1]:
                        search_term_search = st.text_input("Filter by Search Term", key="search_term_search", help="Search for search terms containing this text")
                    with basic_filter_cols[2]:
                        # Add product group filter to the right of search term filter
                        if product_groups:
                            selected_product_groups = st.multiselect(
                                "Filter by Product Group(s):",
                                options=product_groups,
                                key="search_term_product_group_filter"
                            )
                            # Update the filter active state based on selection
                            st.session_state.search_term_filter_active = len(selected_product_groups) > 0
                        else:
                            st.write("")  # Empty space if no product groups
                    
                    # Second row of basic filters
                    basic_filter_cols2 = st.columns(3)
                    with basic_filter_cols2[0]:
                        # Get unique match types from search term data
                        all_match_types = set()
                        if not search_term_df.empty and 'Match Type' in search_term_df.columns:
                            all_match_types.update(search_term_df['Match Type'].dropna().unique())
                        match_types = ['All'] + sorted(list(all_match_types))
                        selected_match_types = st.multiselect("Filter by Match Type", match_types, default=['All'], key="search_term_match_type_filter")
                        if 'All' in selected_match_types:
                            selected_match_types = [mt for mt in match_types if mt != 'All']
                    
                    with basic_filter_cols2[1]:
                        # Get unique target types from search term data
                        all_target_types = set()
                        if not search_term_df.empty and 'Target Type' in search_term_df.columns:
                            all_target_types.update(search_term_df['Target Type'].dropna().unique())
                        target_types = ['All'] + sorted(list(all_target_types))
                        selected_target_types = st.multiselect("Filter by Target Type", target_types, default=['All'], key="search_term_target_type_filter")
                        if 'All' in selected_target_types:
                            selected_target_types = [tt for tt in target_types if tt != 'All']
                    
                    with basic_filter_cols2[2]:
                        # Get unique ad types from search term data
                        all_ad_types = set()
                        if not search_term_df.empty:
                            temp_df = search_term_df.copy()
                            # Use the proper infer_ad_type function to get ad types
                            if 'Product' not in temp_df.columns:
                                # Import the infer_ad_type function from the ad type tab
                                def infer_ad_type(df):
                                    if 'Product' in df.columns and df['Product'].notna().any() and (df['Product'] != '').any():
                                        return df
                                    
                                    # Try to infer from Sheet Source or Campaign Type
                                    df = df.copy()
                                
                                    # First, check if we can infer from Sheet Source
                                    if 'Sheet Source' in df.columns:
                                        def _adtype_from_sheet(row):
                                            s = str(row['Sheet Source']).lower()
                                            if 'sponsored products' in s:
                                                return 'SP'
                                            elif 'sponsored brands' in s:
                                                return 'SB'
                                            elif 'sponsored display' in s:
                                                return 'SD'
                                            return 'Unknown'
                                        df['Product'] = df.apply(_adtype_from_sheet, axis=1)
                                
                                    # If we still don't have valid Product values, try to infer from Campaign Type
                                    if 'Product' not in df.columns or not df['Product'].notna().any() or (df['Product'] == '').all() or (df['Product'] == 'Unknown').all():
                                        if 'Campaign Type' in df.columns:
                                            def _adtype_from_campaign_type(row):
                                                ct = str(row['Campaign Type']).lower()
                                                if 'product' in ct or 'sp' in ct or 'sponsored product' in ct:
                                                    return 'SP'
                                                elif 'brand' in ct or 'sb' in ct or 'sponsored brand' in ct:
                                                    return 'SB'
                                                elif 'display' in ct or 'sd' in ct or 'sponsored display' in ct:
                                                    return 'SD'
                                                return 'Unknown'
                                            df['Product'] = df.apply(_adtype_from_campaign_type, axis=1)
                                
                                    # Last resort: try to infer from Campaign Name
                                    if 'Product' not in df.columns or not df['Product'].notna().any() or (df['Product'] == '').all() or (df['Product'] == 'Unknown').all():
                                        campaign_col = None
                                        for col in df.columns:
                                            if col in ['Campaign Name (Informational Only)', 'Campaign Name', 'Campaign']:
                                                campaign_col = col
                                                break
                                            
                                        if campaign_col:
                                            def _adtype_from_campaign(row):
                                                campaign = str(row[campaign_col]).lower()
                                                if 'sp' in campaign.split() or 'sp|' in campaign or '| sp' in campaign or 'sponsored product' in campaign:
                                                    return 'SP'
                                                elif 'sb' in campaign.split() or 'sb|' in campaign or '| sb' in campaign or 'sponsored brand' in campaign:
                                                    return 'SB'
                                                elif 'sd' in campaign.split() or 'sd|' in campaign or '| sd' in campaign or 'sponsored display' in campaign:
                                                    return 'SD'
                                                return 'Unknown'
                                            df['Product'] = df.apply(_adtype_from_campaign, axis=1)
                                    
                                    return df
                                
                                temp_df = infer_ad_type(temp_df)
                            
                            if 'Product' in temp_df.columns:
                                all_ad_types.update(temp_df['Product'].dropna().unique())
                        ad_types = ['All'] + sorted(list(all_ad_types))
                        selected_ad_types = st.multiselect("Filter by Ad Type", ad_types, default=['All'], key="search_term_ad_type_filter")
                        if 'All' in selected_ad_types:
                            selected_ad_types = [at for at in ad_types if at != 'All']
                    
                    # Advanced Filters Section
                    st.markdown("---")
                    with st.expander("ðŸ”§ Advanced Filters", expanded=False):
                        st.markdown("**Advanced Filtering with Filter Groups**")
                        # Ensure filter groups state exists for search terms
                        if 'search_term_filter_groups' not in st.session_state:
                            st.session_state.search_term_filter_groups = []
                        st.caption("Create multiple filter groups and combine them with AND/OR logic for complex filtering.")
                        
                        # Determine filterable columns based on visible table columns
                        display_columns_for_filters = [
                            'Ad Type', 'Product Group', 'Campaign', 'Target', 'Search Term', 'Match Type', 'Target Type',
                            'Spend', 'Ad Sales', '% Ad Spend', '% Ad Sales', 'ACoS', 'ROAS', 'CPC', 'CVR', 'AOV',
                            'Clicks', 'Orders', 'Impressions', 'CTR', 'CPA', 'Bid',
                            # Synthetic, aggregated by Campaign+Target+Match Type over current filtered data
                            'Target ROAS', 'Target Spend', 'Target Ad Sales', 'Target Orders'
                        ]
                        synthetic_filter_cols = {'Target ROAS', 'Target Spend', 'Target Ad Sales', 'Target Orders'}
                        # Allow synthetic columns even if not present in raw dataframe
                        filterable_columns = [
                            c for c in display_columns_for_filters
                            if (not search_term_df.empty and (c in search_term_df.columns or c in synthetic_filter_cols))
                        ]

                        # Classify columns as numeric or text by attempting numeric conversion (strip $, %, ,)
                        numeric_columns = []
                        for col in filterable_columns:
                            if col in synthetic_filter_cols:
                                # Synthetic targets are numeric by definition
                                numeric_columns.append(col)
                            else:
                                try:
                                    test_series = pd.to_numeric(
                                        search_term_df[col].astype(str).str.replace('$', '').str.replace('%', '').str.replace(',', ''),
                                        errors='coerce'
                                    )
                                    if not test_series.isna().all():
                                        numeric_columns.append(col)
                                except Exception:
                                    pass
                        # Force 'Search Term' and 'Target' to be treated as text, never numeric
                        numeric_columns = [c for c in numeric_columns if c not in ['Search Term', 'Target']]
                        text_columns = [c for c in filterable_columns if c not in numeric_columns]

                        numeric_ops = ['>', '>=', '=', '<=', '<']
                        text_ops = ['contains', "doesn't contain", 'equals', "doesn't equal"]

                        # Control buttons
                        button_cols = st.columns([1, 1, 3])
                        with button_cols[0]:
                            add_group = st.button("âž• Add Filter Group", key="search_term_add_group")
                        with button_cols[1]:
                            clear_all = st.button("ðŸ—‘ï¸ Clear All Filters", key="search_term_clear_all")

                        # Handle button actions
                        if add_group:
                            default_col = (filterable_columns[0] if filterable_columns else (numeric_columns[0] if numeric_columns else 'Campaign'))
                            default_ops = numeric_ops if default_col in numeric_columns else text_ops
                            st.session_state.search_term_filter_groups.append({
                                'filters': [{
                                    'column': default_col,
                                    'operator': default_ops[0],
                                    'value': '',
                                    'logic': 'AND'
                                }],
                                'group_logic': 'AND'
                            })

                        if clear_all:
                            st.session_state.search_term_filter_groups = []
                            # Immediate refresh so UI reflects cleared groups without extra click
                            st.rerun()

                        # Display filter groups
                        groups_to_remove = []
                        for group_idx, group in enumerate(st.session_state.search_term_filter_groups):
                            st.markdown(f"**Filter Group {group_idx + 1}**")

                            # Group controls
                            group_col1, group_col2, group_col3 = st.columns([2, 1, 1])
                            with group_col1:
                                st.markdown(f"*Group {group_idx + 1} Logic*")
                            with group_col2:
                                group['group_logic'] = st.selectbox(
                                    "Group Logic",
                                    ["AND", "OR"],
                                    index=0 if group.get('group_logic', 'AND') == 'AND' else 1,
                                    key=f"search_term_group_logic_{group_idx}",
                                    label_visibility="collapsed"
                                )
                            with group_col3:
                                if st.button("ðŸ—‘ï¸", key=f"search_term_remove_group_{group_idx}", help="Remove entire group"):
                                    groups_to_remove.append(group_idx)

                            # Display filters within this group
                            filters_to_remove = []
                            for filter_idx, filter_item in enumerate(group['filters']):
                                col1, col2, col3, col4, col5, col6 = st.columns([2, 1, 2, 1, 1, 1])

                                with col1:
                                    k_col = f"search_term_filter_col_{group_idx}_{filter_idx}"
                                    # Pre-seed widget state once, then let the widget manage its own value
                                    if k_col not in st.session_state:
                                        st.session_state[k_col] = (
                                            filter_item.get('column')
                                            if filter_item.get('column') in filterable_columns
                                            else (filterable_columns[0] if filterable_columns else '')
                                        )
                                    st.selectbox(
                                        "Column",
                                        options=filterable_columns,
                                        key=k_col,
                                        label_visibility="collapsed" if filter_idx > 0 else "visible"
                                    )
                                    filter_item['column'] = st.session_state[k_col]

                                with col2:
                                    curr_col = filter_item.get('column')
                                    curr_ops = numeric_ops if curr_col in numeric_columns else text_ops
                                    k_op = f"search_term_filter_op_{group_idx}_{filter_idx}"
                                    # Pre-seed operator respecting available ops
                                    if k_op not in st.session_state or st.session_state[k_op] not in curr_ops:
                                        st.session_state[k_op] = (
                                            filter_item.get('operator') if filter_item.get('operator') in curr_ops else curr_ops[0]
                                        )
                                    st.selectbox(
                                        "Operator",
                                        options=curr_ops,
                                        key=k_op,
                                        label_visibility="collapsed" if filter_idx > 0 else "visible"
                                    )
                                    filter_item['operator'] = st.session_state[k_op]

                                with col3:
                                    k_val = f"search_term_filter_val_{group_idx}_{filter_idx}"
                                    if k_val not in st.session_state:
                                        st.session_state[k_val] = str(filter_item.get('value', ''))
                                    st.text_input(
                                        "Value",
                                        key=k_val,
                                        label_visibility="collapsed" if filter_idx > 0 else "visible"
                                    )
                                    filter_item['value'] = st.session_state[k_val]

                                with col4:
                                    if filter_idx < len(group['filters']) - 1:  # Not the last filter in group
                                        k_logic = f"search_term_filter_logic_{group_idx}_{filter_idx}"
                                        if k_logic not in st.session_state:
                                            st.session_state[k_logic] = filter_item.get('logic', 'AND') if filter_item.get('logic', 'AND') in ["AND", "OR"] else "AND"
                                        st.selectbox(
                                            "Logic",
                                            ["AND", "OR"],
                                            key=k_logic,
                                            label_visibility="collapsed" if filter_idx > 0 else "visible"
                                        )
                                        filter_item['logic'] = st.session_state[k_logic]
                                    else:
                                        st.write("")  # Empty space for last filter in group

                                with col5:
                                    if st.button("âž•", key=f"search_term_add_filter_{group_idx}_{filter_idx}", help="Add filter to this group"):
                                        default_col = (filterable_columns[0] if filterable_columns else (numeric_columns[0] if numeric_columns else 'Campaign'))
                                        default_ops = numeric_ops if default_col in numeric_columns else text_ops
                                        group['filters'].append({
                                            'column': default_col,
                                            'operator': default_ops[0],
                                            'value': '',
                                            'logic': 'AND'
                                        })

                                with col6:
                                    if len(group['filters']) > 1:  # Don't allow removing the last filter in a group
                                        if st.button("ðŸ—‘ï¸", key=f"search_term_remove_filter_{group_idx}_{filter_idx}", help="Remove filter"):
                                            filters_to_remove.append(filter_idx)

                            # Remove filters marked for removal
                            if filters_to_remove:
                                for filter_idx in reversed(filters_to_remove):
                                    group['filters'].pop(filter_idx)
                                # Immediate refresh so deletion reflects without extra click
                                st.rerun()

                            st.markdown("---")

                        # Remove groups marked for removal
                        if groups_to_remove:
                            for group_idx in reversed(groups_to_remove):
                                st.session_state.search_term_filter_groups.pop(group_idx)
                            # Immediate refresh so deletion reflects without extra click
                            st.rerun()
                    
                    # Move tabs here - under filters and above scorecard
                    st.markdown("---")
                    all_tab, branded_tab, non_branded_tab = st.tabs(["All Terms", "Branded Terms", "Non-Branded Terms"])
                    
                    # Function to display table with pagination for each tab
                    def display_search_term_table(df, tab_key, is_branded=None, show_metrics=False, show_filters=False):
                        # Apply all filters to the data
                        filtered_df = df.copy()
                        
                        # Apply basic filters
                        # Campaign search filter
                        if campaign_search and 'Campaign' in filtered_df.columns:
                            def phrase_match(value, filter_value):
                                if not isinstance(value, str):
                                    return False
                                import re
                                pattern = re.escape(filter_value)
                                regex = rf"(^|[|\s]){pattern}([|\s]|$)"
                                return re.search(regex, value, re.IGNORECASE) is not None or value.strip().lower() == filter_value.strip().lower()
                            
                            matched_campaigns = filtered_df['Campaign'].astype(str).apply(lambda x: phrase_match(x, campaign_search))
                            filtered_df = filtered_df[matched_campaigns]
                        
                        # Search term search filter
                        if search_term_search and 'Search Term' in filtered_df.columns:
                            import re
                            search_terms = [term.strip().lower() for term in search_term_search.split('|') if term.strip()]
                            mask = pd.Series(False, index=filtered_df.index)
                            for term in search_terms:
                                if '|' in term:
                                    sub_terms = [t.strip() for t in term.split('|') if t.strip()]
                                    for sub_term in sub_terms:
                                        mask = mask | filtered_df['Search Term'].astype(str).str.lower().str.contains(re.escape(sub_term), regex=True, na=False)
                                else:
                                    mask = mask | filtered_df['Search Term'].astype(str).str.lower().str.contains(re.escape(term), regex=True, na=False)
                            filtered_df = filtered_df[mask]
                        
                        # Product group filter
                        if st.session_state.get('search_term_filter_active', False) and len(st.session_state.search_term_product_group_filter) > 0:
                            if 'Product Group' in filtered_df.columns:
                                filtered_df = filtered_df[filtered_df['Product Group'].isin(st.session_state.search_term_product_group_filter)]
                        
                        # Match type filter
                        if selected_match_types and 'Match Type' in filtered_df.columns:
                            filtered_df = filtered_df[filtered_df['Match Type'].isin(selected_match_types)]
                        
                        # Target type filter
                        if selected_target_types and 'Target Type' in filtered_df.columns:
                            filtered_df = filtered_df[filtered_df['Target Type'].isin(selected_target_types)]
                        
                        # Ad type filter
                        if selected_ad_types and 'Product' in filtered_df.columns:
                            filtered_df = filtered_df[filtered_df['Product'].isin(selected_ad_types)]
                        
                        # Compute synthetic aggregated targets for current basic-filtered data
                        # Grouping by Campaign + Target + Match Type
                        try:
                            needed_cols = ['Campaign', 'Target', 'Match Type']
                            have_keys = all(col in filtered_df.columns for col in needed_cols)
                            if have_keys:
                                tmp = filtered_df.copy()
                                # Numeric conversions
                                if 'Spend' in tmp.columns:
                                    tmp['__Spend_num'] = pd.to_numeric(tmp['Spend'].astype(str).str.replace(r'[^0-9\.-]', '', regex=True), errors='coerce').fillna(0)
                                else:
                                    tmp['__Spend_num'] = 0
                                sales_col_name = 'Ad Sales' if 'Ad Sales' in tmp.columns else ('Sales' if 'Sales' in tmp.columns else None)
                                if sales_col_name:
                                    tmp['__AdSales_num'] = pd.to_numeric(tmp[sales_col_name].astype(str).str.replace(r'[^0-9\.-]', '', regex=True), errors='coerce').fillna(0)
                                else:
                                    tmp['__AdSales_num'] = 0
                                if 'Orders' in tmp.columns:
                                    tmp['__Orders_num'] = pd.to_numeric(tmp['Orders'].astype(str).str.replace(',', '', regex=True), errors='coerce').fillna(0)
                                else:
                                    tmp['__Orders_num'] = 0

                                agg = tmp.groupby(['Campaign', 'Target', 'Match Type'], dropna=False).agg({
                                    '__Spend_num': 'sum',
                                    '__AdSales_num': 'sum',
                                    '__Orders_num': 'sum'
                                }).reset_index().rename(columns={
                                    '__Spend_num': 'Target Spend',
                                    '__AdSales_num': 'Target Ad Sales',
                                    '__Orders_num': 'Target Orders'
                                })
                                # Compute ROAS safely
                                agg['Target ROAS'] = np.where(agg['Target Spend'] > 0, agg['Target Ad Sales'] / agg['Target Spend'], 0.0)
                                # Merge back so advanced filters can reference these columns directly
                                filtered_df = filtered_df.merge(
                                    agg[['Campaign', 'Target', 'Match Type', 'Target Spend', 'Target Ad Sales', 'Target Orders', 'Target ROAS']],
                                    on=['Campaign', 'Target', 'Match Type'],
                                    how='left'
                                )
                            else:
                                # Ensure columns exist to avoid key errors later
                                for _c in ['Target Spend', 'Target Ad Sales', 'Target Orders', 'Target ROAS']:
                                    if _c not in filtered_df.columns:
                                        filtered_df[_c] = 0
                        except Exception as _e:
                            # Fail safe: ensure columns exist
                            for _c in ['Target Spend', 'Target Ad Sales', 'Target Orders', 'Target ROAS']:
                                if _c not in filtered_df.columns:
                                    filtered_df[_c] = 0

                        # Apply advanced filters (numeric and text with AND/OR logic)
                        if st.session_state.search_term_filter_groups:
                            try:
                                group_conditions = []
                                for group in st.session_state.search_term_filter_groups:
                                    if not group['filters']:
                                        continue
                                    filter_conditions = []
                                    for filter_item in group['filters']:
                                        if not str(filter_item.get('value', '')).strip():
                                            continue
                                        column = filter_item.get('column')
                                        operator = filter_item.get('operator')
                                        value_raw = str(filter_item.get('value', '')).strip()
                                        # Allow synthetic columns as well as real columns
                                        if column not in filtered_df.columns and column not in synthetic_filter_cols:
                                            continue
                                        if column in numeric_columns:
                                            series_num = pd.to_numeric(
                                                filtered_df[column].astype(str).str.replace('$', '').str.replace('%', '').str.replace(',', ''),
                                                errors='coerce'
                                            )
                                            try:
                                                comp_val = float(value_raw.replace('$', '').replace('%', '').replace(',', ''))
                                            except ValueError:
                                                st.error(f"Invalid numeric value '{value_raw}' for column '{column}'.")
                                                continue
                                            if operator == '>':
                                                condition = series_num > comp_val
                                            elif operator == '>=':
                                                condition = series_num >= comp_val
                                            elif operator == '=':
                                                condition = series_num == comp_val
                                            elif operator == '<=':
                                                condition = series_num <= comp_val
                                            elif operator == '<':
                                                condition = series_num < comp_val
                                            else:
                                                continue
                                            condition = condition.fillna(False)
                                        else:
                                            series_txt = filtered_df[column].astype(str).fillna('')
                                            v = value_raw
                                            if operator == 'contains':
                                                condition = series_txt.str.contains(v, case=False, na=False)
                                            elif operator == "doesn't contain":
                                                condition = ~series_txt.str.contains(v, case=False, na=False)
                                            elif operator == 'equals':
                                                condition = series_txt.str.strip().str.lower() == v.strip().lower()
                                            elif operator == "doesn't equal":
                                                condition = series_txt.str.strip().str.lower() != v.strip().lower()
                                            else:
                                                continue
                                            condition = condition.fillna(False)
                                        filter_conditions.append(condition)
                                    if filter_conditions:
                                        group_condition = filter_conditions[0]
                                        for i, condition in enumerate(filter_conditions[1:], 1):
                                            filter_logic = group['filters'][i-1]['logic']
                                            if filter_logic == 'AND':
                                                group_condition = group_condition & condition
                                            else:
                                                group_condition = group_condition | condition
                                        group_conditions.append(group_condition)
                                if group_conditions:
                                    final_condition = group_conditions[0]
                                    for i, condition in enumerate(group_conditions[1:], 1):
                                        group_logic = st.session_state.search_term_filter_groups[i-1]['group_logic']
                                        if group_logic == 'AND':
                                            final_condition = final_condition & condition
                                        else:
                                            final_condition = final_condition | condition
                                    filtered_df = filtered_df[final_condition]
                                    # Enforce ROAS > 0 if any ROAS '>' filter exists in Search Term Performance filters
                                    try:
                                        if any(
                                            (isinstance(g, dict) and isinstance(g.get('filters', []), list) and any(
                                                (isinstance(f, dict) and f.get('column') == 'ROAS' and f.get('operator') == '>' and str(f.get('value', '')).strip() != '')
                                                for f in g.get('filters', [])
                                            ))
                                            for g in st.session_state.get('search_term_filter_groups', [])
                                        ):
                                            if 'ROAS' in filtered_df.columns:
                                                _roas = pd.to_numeric(
                                                    filtered_df['ROAS'].astype(str).str.replace('$', '').str.replace('%', '').str.replace(',', ''),
                                                    errors='coerce'
                                                )
                                                filtered_df = filtered_df[_roas > 0]
                                    except Exception:
                                        pass
                            except Exception as e:
                                st.error(f"Error applying advanced filters: {e}")
                        
                        # Use filtered_df instead of df for the rest of the function
                        df = filtered_df
                        
                        # Display summary metrics from filtered data
                        if not df.empty:
                            spend = df['Spend'].replace('[\$,]', '', regex=True).astype(float).sum() if 'Spend' in df.columns else 0
                            sales = df['Ad Sales'].replace('[\$,]', '', regex=True).astype(float).sum() if 'Ad Sales' in df.columns else 0
                            acos = (spend / sales * 100) if sales > 0 else 0
                            roas = (sales / spend) if spend > 0 else 0
                            clicks = df['Clicks'].replace(',', '', regex=True).astype(float).sum() if 'Clicks' in df.columns else 0
                            cpc = (spend / clicks) if clicks > 0 else 0
                            cvr = (df['Orders'].replace(',', '', regex=True).astype(float).sum() / df['Clicks'].replace(',', '', regex=True).astype(float).sum() * 100) if 'Clicks' in df.columns and 'Orders' in df.columns and df['Clicks'].sum() > 0 else 0
                            aov = (sales / df['Orders'].replace(',', '', regex=True).astype(float).sum()) if 'Orders' in df.columns and df['Orders'].sum() > 0 else 0
                            
                            # Display summary cards
                            kpi_cols = st.columns(6)
                            kpi_cols[0].metric("TOTAL SPEND", f"${spend:,.2f}")
                            kpi_cols[1].metric("AD SALES", f"${sales:,.2f}")
                            kpi_cols[2].metric("ACOS", f"{acos:.2f}%")
                            kpi_cols[3].metric("CPC", f"${cpc:.2f}")
                            kpi_cols[4].metric("CVR", f"{cvr:.2f}%")
                            kpi_cols[5].metric("AOV", f"${aov:.2f}")
                            
                            st.markdown("---")
                        
                        # Get ACoS targets from client config
                        goals = st.session_state.client_config.get('goals', {}) if 'client_config' in st.session_state else {}
                        branded_target = goals.get('branded_acos')
                        non_branded_target = goals.get('non_branded_acos')
                        account_target = goals.get('account_wide_acos')
                        
                        # Determine which target to use based on tab type
                        target_acos = None
                        if is_branded is True:
                            target_acos = branded_target or account_target
                            use_avg_fallback = goals.get('use_avg_acos_branded', False)
                        elif is_branded is False:
                            target_acos = non_branded_target or account_target
                            use_avg_fallback = goals.get('use_avg_acos_nonbranded', False)
                        else:
                            target_acos = account_target
                            use_avg_fallback = goals.get('use_avg_acos_account', False)
                        
                        # No pagination - display all rows
                    
                        # Ensure we have all required columns and use shared_column_config for formatting
                        required_columns = ['Campaign', 'Product', 'Target Type', 'Match Type', 'Target', 'Search Term', 'Spend', 'Ad Sales', 'Orders', 'Impressions', 'Clicks', 'ACoS', 'ROAS', 'CPC', 'CTR', 'CVR', 'AOV']
                        
                        # Check if Product Group should be included based on whether there are product groups defined
                        has_product_groups = False
                        if 'client_config' in st.session_state and 'campaign_tags_data' in st.session_state.client_config:
                            has_product_groups = any(v.get('tag_1', '') for v in st.session_state.client_config['campaign_tags_data'].values())
                        
                        # Only include Product Group in required columns if there are product groups defined
                        if has_product_groups and 'Product Group' in df.columns:
                            required_columns.insert(1, 'Product Group')  # Insert after Campaign
                        
                        for col in required_columns:
                            if col not in df.columns:
                                # Use 0 for numeric columns, '' for text
                                if col in ['Spend', 'Ad Sales', 'Orders', 'Impressions', 'Clicks', 'ROAS', 'CPC', 'CTR', 'CVR', 'AOV']:
                                    df[col] = 0
                                else:
                                    df[col] = ''

                        # Reorder columns as specified
                        display_columns = [col for col in required_columns if col in df.columns]

                        # Sort by Ad Sales numerically if present
                        if 'Ad Sales' in df.columns and not df.empty:
                            df = df.copy()
                            # Clean numeric columns to fix weird characters
                            for col in ['Spend', 'Ad Sales']:
                                if col in df.columns:
                                    # Convert to numeric, removing any currency symbols or formatting
                                    df[col] = pd.to_numeric(df[col].astype(str).str.replace(r'[^0-9.-]', '', regex=True), errors='coerce').fillna(0)
                            df['Ad_Sales_Numeric'] = df['Ad Sales']
                            df = df.sort_values(by='Ad_Sales_Numeric', ascending=False)
                            df = df.drop(columns=['Ad_Sales_Numeric'])

                        # Remove any manual string formatting for display
                        # Use shared_column_config for column formatting
                        try:
                            from . import shared_column_config
                        except ImportError:
                            shared_column_config = {
                                "Spend": st.column_config.NumberColumn(label="Spend", format="dollar"),
                                "Ad Sales": st.column_config.NumberColumn(label="Ad Sales", format="dollar"),
                                "Orders": st.column_config.NumberColumn(label="Orders", format="localized"),
                                "Impressions": st.column_config.NumberColumn(label="Impressions", format="localized"),
                                "Clicks": st.column_config.NumberColumn(label="Clicks", format="localized"),
                                "ACoS": st.column_config.NumberColumn(label="ACoS (%)", format="%.2f%%"),
                                "ROAS": st.column_config.NumberColumn(label="ROAS", format="%.2f"),
                                "CPC": st.column_config.NumberColumn(label="CPC ($)", format="dollar"),
                                "CTR": st.column_config.NumberColumn(label="CTR (%)", format="%.2f%%"),
                                "CVR": st.column_config.NumberColumn(label="CVR (%)", format="%.2f%%"),
                                "AOV": st.column_config.NumberColumn(label="AOV ($)", format="dollar"),
                                "Target Type": st.column_config.TextColumn(label="Target Type"),
                                "Match Type": st.column_config.TextColumn(label="Match Type"),
                                "Target": st.column_config.TextColumn(label="Target"),
                                "Search Term": st.column_config.TextColumn(label="Search Term"),
                                "Campaign": st.column_config.TextColumn(label="Campaign"),
                                "Product": st.column_config.TextColumn(label="Ad Type"),
                            }



                        # Use style_acos for ACoS highlighting if available, otherwise st.dataframe
                        if 'ACoS' in filtered_df.columns and (target_acos is not None or use_avg_fallback):
                            try:
                                style_acos(filtered_df[display_columns], target_acos, column_config=shared_column_config, use_avg_as_fallback=use_avg_fallback, title=f"{'Branded' if is_branded else 'Non-Branded' if is_branded is not None else 'All'} Search Terms")
                            except Exception as e:
                                st.dataframe(filtered_df[display_columns], use_container_width=True, column_config=shared_column_config, hide_index=True)
                        else:
                            st.dataframe(filtered_df[display_columns], use_container_width=True, column_config=shared_column_config, hide_index=True)

                        # --- Export: Search Term Performance (Filtered) -> Excel ---
                        try:
                            if not filtered_df.empty:
                                export_df = filtered_df[display_columns].copy()

                                # Convert numeric-like columns
                                currency_cols = ['Spend', 'Ad Sales', 'CPC', 'AOV', 'Bid']
                                percent_cols = ['ACoS', 'CTR', 'CVR', '% Ad Spend', '% Ad Sales']
                                int_like_cols = ['Orders', 'Impressions', 'Clicks']
                                float_like_cols = ['ROAS']
                                for col in export_df.columns:
                                    if col in currency_cols:
                                        export_df[col] = pd.to_numeric(export_df[col].astype(str).str.replace(r'[^0-9\.-]', '', regex=True), errors='coerce')
                                    elif col in percent_cols:
                                        export_df[col] = pd.to_numeric(export_df[col].astype(str).str.replace(r'[^0-9\.-]', '', regex=True), errors='coerce')
                                    elif col in int_like_cols:
                                        export_df[col] = pd.to_numeric(export_df[col].astype(str).str.replace(',', '', regex=True), errors='coerce')
                                    elif col in float_like_cols:
                                        export_df[col] = pd.to_numeric(export_df[col].astype(str).str.replace(',', '', regex=True), errors='coerce')

                                # Build workbook

                                wb = Workbook()
                                ws = wb.active
                                # Name the sheet by tab key for clarity
                                ws.title = str(tab_key).replace('_', ' ').title()

                                for r_idx, row in enumerate(dataframe_to_rows(export_df, index=False, header=True), 1):
                                    ws.append(row)
                                    if r_idx == 1:
                                        for c_idx, _ in enumerate(row, 1):
                                            cell = ws.cell(row=1, column=c_idx)
                                            cell.fill = PatternFill(start_color="B8860B", end_color="B8860B", fill_type="solid")
                                            cell.font = Font(color="FFFFFF", bold=True)
                                            cell.alignment = Alignment(horizontal="center", vertical="center")

                                # Conditional formatting for ACoS using computed target_acos
                                acos_tgt = target_acos if isinstance(target_acos, (int, float)) else 25
                                if 'ACoS' in export_df.columns:
                                    acos_col_idx = list(export_df.columns).index('ACoS') + 1
                                    range_str = f"{get_column_letter(acos_col_idx)}2:{get_column_letter(acos_col_idx)}{len(export_df) + 1}"
                                    rule = ColorScaleRule(start_type='num', start_value=0, start_color='90EE90',
                                                          mid_type='num', mid_value=acos_tgt, mid_color='FFFF00',
                                                          end_type='num', end_value=acos_tgt * 2, end_color='FF0000')
                                    ws.conditional_formatting.add(range_str, rule)

                                # Number formats
                                for col_name in export_df.columns:
                                    col_letter = get_column_letter(list(export_df.columns).index(col_name) + 1)
                                    if col_name in currency_cols:
                                        for row in range(2, len(export_df) + 2):
                                            ws[f"{col_letter}{row}"].number_format = '"$"#,##0.00'
                                    elif col_name in percent_cols:
                                        for row in range(2, len(export_df) + 2):
                                            ws[f"{col_letter}{row}"].number_format = '0.00"%"'
                                    elif col_name in int_like_cols:
                                        for row in range(2, len(export_df) + 2):
                                            ws[f"{col_letter}{row}"].number_format = '#,##0'
                                    elif col_name in float_like_cols:
                                        for row in range(2, len(export_df) + 2):
                                            ws[f"{col_letter}{row}"].number_format = '0.00'

                                # Auto-adjust column widths
                                for column in ws.columns:
                                    max_len = 0
                                    col_letter = column[0].column_letter
                                    for cell in column:
                                        try:
                                            if cell.value is not None and len(str(cell.value)) > max_len:
                                                max_len = len(str(cell.value))
                                        except Exception:
                                            pass
                                    ws.column_dimensions[col_letter].width = min(max_len + 2, 50)

                                # Save and provide download
                                excel_buffer = io.BytesIO()
                                wb.save(excel_buffer)
                                excel_buffer.seek(0)
                                from datetime import datetime as _dt
                                ts = _dt.now().strftime("%Y%m%d_%H%M%S")
                                st.download_button(
                                    label="ðŸ’¾ Download Search Term Performance (.xlsx)",
                                    data=excel_buffer.getvalue(),
                                    file_name=f"Search_Term_Performance_{str(tab_key)}_{ts}.xlsx",
                                    mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                                    key=f"download_search_terms_{str(tab_key)}_{ts}"
                                )
                        except Exception as e:
                            st.error(f"Error creating export: {e}")

                    with all_tab:
                        if 'bulk_data' in st.session_state:
                            # Get search term data with branding classification
                            search_term_df = get_search_term_data(st.session_state.bulk_data, st.session_state.client_config)
                            
                            # Apply product group filter if active
                            if st.session_state.get('search_term_filter_active', False) and 'Product Group' in search_term_df.columns and st.session_state.search_term_product_group_filter:
                                search_term_df = search_term_df[search_term_df['Product Group'].isin(st.session_state.search_term_product_group_filter)]
                            
                            if search_term_df.empty:
                                st.info("Search Term data not present in bulk file.")
                            else:
                                # Display the table with summary cards between filters and table
                                display_search_term_table(search_term_df, "all", is_branded=None, show_metrics=True, show_filters=True)
                        else:
                            st.info("No bulk advertising data available. Please upload bulk advertising files with search term data.")
                    
                    with branded_tab:
                        if 'bulk_data' in st.session_state:
                            # Get search term data with branding classification
                            search_term_df = get_search_term_data(st.session_state.bulk_data, st.session_state.client_config)
                            
                            # Apply product group filter if active
                            if st.session_state.get('search_term_filter_active', False) and 'Product Group' in search_term_df.columns and st.session_state.search_term_product_group_filter:
                                search_term_df = search_term_df[search_term_df['Product Group'].isin(st.session_state.search_term_product_group_filter)]
                            
                            if search_term_df.empty:
                                st.info("Search Term data not present in bulk file.")
                            else:
                                # Get targeting data to merge for Target Type
                                targeting_data = None
                                # Check if we have both branded and non-branded targeting data in session state
                                if 'branded_targets_df' in st.session_state and 'non_branded_targets_df' in st.session_state:
                                    branded_df = st.session_state.branded_targets_df
                                    non_branded_df = st.session_state.non_branded_targets_df
                                    if not branded_df.empty or not non_branded_df.empty:
                                        targeting_data = pd.concat([branded_df, non_branded_df], ignore_index=True)
                                        st.session_state.debug_messages.append(f"[Search Term Table] Combined targeting data: {len(targeting_data)} rows")
                                
                                # Merge Target Type from targeting data if available
                                if targeting_data is not None and 'Target Type' in targeting_data.columns:
                                    # Create a mapping dictionary from Target to Target Type
                                    target_type_map = dict(zip(targeting_data['Target'], targeting_data['Target Type']))
                                    
                                    # Apply the mapping to search term data
                                    search_term_df['Target Type'] = search_term_df['Target'].map(target_type_map)
                                    
                                    # For any missing Target Types, use a default value
                                    search_term_df['Target Type'] = search_term_df['Target Type'].fillna('Keyword')
                                    
                                    st.session_state.debug_messages.append(f"[Search Term Table] Added Target Type from targeting data")
                                else:
                                    # If targeting data not available, use a default value
                                    search_term_df['Target Type'] = 'Keyword'
                                    st.session_state.debug_messages.append(f"[Search Term Table] Using default Target Type 'Keyword'")
                                
                                # Filter for branded terms only
                                branded_df = search_term_df[search_term_df['Is_Branded'] == True].copy() if 'Is_Branded' in search_term_df.columns else pd.DataFrame()
                                
                                # --- Table Filters: Text Input (Contains) and Dropdown ---
                                campaign_filter, target_filter, search_term_filter, match_type_filter = '', '', '', ''
                                
                                # First row of filters
                                row1_cols = st.columns(2)
                                with row1_cols[0]:
                                    campaign_filter = st.text_input("Filter by Campaign", key="filter_search_term_campaign_branded")
                                with row1_cols[1]:
                                    target_filter = st.text_input("Filter by Target", key="filter_search_term_target_branded")
                                
                                # Second row of filters
                                row2_cols = st.columns(2)
                                with row2_cols[0]:
                                    search_term_filter = st.text_input("Filter by Search Term", key="filter_search_term_branded")
                                with row2_cols[1]:
                                    # Get unique match types from the data
                                    match_types = ['All'] + sorted(branded_df['Match Type'].dropna().unique().tolist())
                                    match_type_filter = st.selectbox("Filter by Match Type", match_types, key="filter_search_term_match_type_branded")
                                
                                # Apply filters
                                filtered_df = branded_df.copy()
                                filter_changed = False
                                
                                if campaign_filter:
                                    matched_campaigns = filtered_df['Campaign'].astype(str).apply(lambda x: phrase_match(x, campaign_filter))
                                    filtered_df = filtered_df[matched_campaigns]
                                    filter_changed = True
                                if target_filter:
                                    filtered_df = filtered_df[filtered_df['Target'].astype(str).str.contains(target_filter, case=False, na=False)]
                                    filter_changed = True
                                if search_term_filter:
                                    filtered_df = filtered_df[filtered_df['Search Term'].astype(str).str.contains(search_term_filter, case=False, na=False)]
                                    filter_changed = True
                                if match_type_filter and match_type_filter != 'All':
                                    filtered_df = filtered_df[filtered_df['Match Type'].astype(str) == match_type_filter]
                                    filter_changed = True
                                
                                # Reset to page 1 if filters changed
                                if filter_changed:
                                    st.session_state["search_term_table_branded_page_number"] = 1
                                
                                if not filtered_df.empty:
                                    # Display the table with summary cards between filters and table
                                    display_search_term_table(filtered_df, "branded", is_branded=True, show_metrics=True, show_filters=False)
                                else:
                                    st.info("No branded search term data found.")
                        else:
                            st.info("No bulk advertising data available. Please upload bulk advertising files with search term data.")
                    
                    with non_branded_tab:
                        if 'bulk_data' in st.session_state:
                            # Get search term data with branding classification
                            search_term_df = get_search_term_data(st.session_state.bulk_data, st.session_state.client_config)
                            
                            # Apply product group filter if active
                            if st.session_state.get('search_term_filter_active', False) and 'Product Group' in search_term_df.columns and st.session_state.search_term_product_group_filter:
                                search_term_df = search_term_df[search_term_df['Product Group'].isin(st.session_state.search_term_product_group_filter)]
                            
                            if search_term_df.empty:
                                st.info("Search Term data not present in bulk file.")
                            else:
                                # Get targeting data to merge for Target Type
                                targeting_data = None
                                # Check if we have both branded and non-branded targeting data in session state
                                if 'branded_targets_df' in st.session_state and 'non_branded_targets_df' in st.session_state:
                                    branded_df = st.session_state.branded_targets_df
                                    non_branded_df = st.session_state.non_branded_targets_df
                                    if not branded_df.empty or not non_branded_df.empty:
                                        targeting_data = pd.concat([branded_df, non_branded_df], ignore_index=True)
                                        st.session_state.debug_messages.append(f"[Search Term Table] Combined targeting data: {len(targeting_data)} rows")
                                
                                # Merge Target Type from targeting data if available
                                if targeting_data is not None and 'Target Type' in targeting_data.columns:
                                    # Create a mapping dictionary from Target to Target Type
                                    target_type_map = dict(zip(targeting_data['Target'], targeting_data['Target Type']))
                                    
                                    # Apply the mapping to search term data
                                    search_term_df['Target Type'] = search_term_df['Target'].map(target_type_map)
                                    
                                    # For any missing Target Types, use a default value
                                    search_term_df['Target Type'] = search_term_df['Target Type'].fillna('Keyword')
                                    
                                    st.session_state.debug_messages.append(f"[Search Term Table] Added Target Type from targeting data")
                                else:
                                    # If targeting data not available, use a default value
                                    search_term_df['Target Type'] = 'Keyword'
                                    st.session_state.debug_messages.append(f"[Search Term Table] Using default Target Type 'Keyword'")
                                
                                # Filter for non-branded terms only
                                non_branded_df = search_term_df[search_term_df['Is_Branded'] == False].copy() if 'Is_Branded' in search_term_df.columns else pd.DataFrame()
                                
                                # --- Table Filters: Text Input (Contains) and Dropdown ---
                                campaign_filter, target_filter, search_term_filter, match_type_filter = '', '', '', ''
                                
                                # First row of filters
                                row1_cols = st.columns(2)
                                with row1_cols[0]:
                                    campaign_filter = st.text_input("Filter by Campaign", key="filter_search_term_campaign_nonbranded")
                                with row1_cols[1]:
                                    target_filter = st.text_input("Filter by Target", key="filter_search_term_target_nonbranded")
                                
                                # Second row of filters
                                row2_cols = st.columns(2)
                                with row2_cols[0]:
                                    search_term_filter = st.text_input("Filter by Search Term", key="filter_search_term_nonbranded")
                                with row2_cols[1]:
                                    # Get unique match types from the data
                                    match_types = ['All'] + sorted(non_branded_df['Match Type'].dropna().unique().tolist())
                                    match_type_filter = st.selectbox("Filter by Match Type", match_types, key="filter_search_term_match_type_nonbranded")
                                
                                # Apply filters
                                filtered_df = non_branded_df.copy()
                                filter_changed = False
                                
                                if campaign_filter:
                                    matched_campaigns = filtered_df['Campaign'].astype(str).apply(lambda x: phrase_match(x, campaign_filter))
                                    filtered_df = filtered_df[matched_campaigns]
                                    filter_changed = True
                                if target_filter:
                                    filtered_df = filtered_df[filtered_df['Target'].astype(str).str.contains(target_filter, case=False, na=False)]
                                    filter_changed = True
                                if search_term_filter:
                                    filtered_df = filtered_df[filtered_df['Search Term'].astype(str).str.contains(search_term_filter, case=False, na=False)]
                                    filter_changed = True
                                if match_type_filter and match_type_filter != 'All':
                                    filtered_df = filtered_df[filtered_df['Match Type'].astype(str) == match_type_filter]
                                    filter_changed = True
                                
                                # Reset to page 1 if filters changed
                                if filter_changed:
                                    st.session_state["search_term_table_non_branded_page_number"] = 1
                                
                                if not filtered_df.empty:
                                    # Display the table with summary cards between filters and table
                                    display_search_term_table(filtered_df, "non_branded", is_branded=False, show_metrics=True, show_filters=False)
                                else:
                                    st.info("No non-branded search term data found.")
                        else:
                            st.info("No bulk advertising data available. Please upload bulk advertising files with search term data.")
                    
                    # --- Search Term Word Cloud Section ---
                    st.markdown("<div style='margin-top:2rem;'></div>", unsafe_allow_html=True)
                    if 'bulk_data' in st.session_state:
                        # Get search term data
                        search_term_df = get_search_term_data(st.session_state.bulk_data, st.session_state.client_config)
                        
                        if not search_term_df.empty:
                            with st.expander("Show Search Term Word Cloud", expanded=False):
                                st.markdown("""
                                    <div style='font-size:1.1rem; color:#bfa23a; font-weight:600; margin-bottom:0.5rem;'>Search Term Word Cloud</div>
                                    <style>
                                    .wordcloud-container {
                                        background: #181818;
                                        border-radius: 16px;
                                        box-shadow: 0 2px 12px rgba(0,0,0,0.25);
                                        padding: 1.5rem 1rem 1rem 1rem;
                                        margin-bottom: 1.5rem;
                                    }
                                    </style>
                                """, unsafe_allow_html=True)
                                
                                # Add option to remove ASINs
                                remove_asins_search = st.checkbox("Remove ASINs from search term word cloud", 
                                                          help="Remove terms that appear to be ASINs (starting with B0 and 10 characters long)")
                                
                                # Create tabs for All, Branded, Non-Branded search terms
                                wc_labels = ["All Search Terms", "Branded Search Terms", "Non-Branded Search Terms"]
                                wc_tabs = st.tabs(wc_labels)
                                
                                # Filter data for each tab
                                if 'Is_Branded' in search_term_df.columns:
                                    all_terms = search_term_df
                                    branded_terms = search_term_df[search_term_df['Is_Branded'] == True]
                                    non_branded_terms = search_term_df[search_term_df['Is_Branded'] == False]
                                else:
                                    all_terms = search_term_df
                                    branded_terms = pd.DataFrame()
                                    non_branded_terms = pd.DataFrame()
                                
                                term_dfs = [all_terms, branded_terms, non_branded_terms]
                                term_types = ['all', 'branded', 'non-branded']
                                
                                # Generate word clouds for each tab
                                for idx, (wc_tab, term_df, term_type) in enumerate(zip(wc_tabs, term_dfs, term_types)):
                                    with wc_tab:
                                        st.markdown("<div class='wordcloud-container'>", unsafe_allow_html=True)
                                        if not term_df.empty:
                                            # Generate the word cloud
                                            fig = generate_search_term_wordcloud(term_df, filter_type=term_type, remove_asins=remove_asins_search)
                                            st.pyplot(fig)
                                        else:
                                            st.info(f"No {term_type.replace('-', ' ')} search term data available.")
                                        st.markdown("</div>", unsafe_allow_html=True)
                    
                    # Helper function to calculate wasted spend score
                    def calculate_wasted_spend_score(df):
                            # Create a mapping of targets to their classification (Branded/Non-Branded)
                            target_classification = {}
                            
                            # Add Branded targets to the mapping
                            if not branded_targets_df.empty and 'Target' in branded_targets_df.columns:
                                for target in branded_targets_df['Target'].dropna().unique():
                                    target_classification[str(target).lower()] = "Branded"
                            
                            # Add Non-Branded targets to the mapping
                            if not non_branded_targets_df.empty and 'Target' in non_branded_targets_df.columns:
                                for target in non_branded_targets_df['Target'].dropna().unique():
                                    target_classification[str(target).lower()] = "Non-Branded"
                            
                            # Filter for Sponsored Products and Sponsored Brands only
                            sp_sb_search_terms = search_term_df[
                                search_term_df['Campaign Type'].str.contains('Sponsored Products|Sponsored Brands', case=False, na=False)
                            ].copy() if 'Campaign Type' in search_term_df.columns else search_term_df.copy()
                            
                            # Ensure we have the necessary columns
                            required_cols = ['Search Term', 'Target', 'Is_Branded', 'Campaign', 'Spend', 'Sales', 'Orders', 'Clicks', 'Impressions']
                            for col in required_cols:
                                if col not in sp_sb_search_terms.columns:
                                    sp_sb_search_terms[col] = None
                            
                            # Create a column for search term classification
                            sp_sb_search_terms['Search Term Classification'] = sp_sb_search_terms['Is_Branded'].apply(
                                lambda x: "Branded" if x else "Non-Branded"
                            )
                            
                            # Create a column for target classification
                            sp_sb_search_terms['Target Classification'] = sp_sb_search_terms['Target'].apply(
                                lambda x: target_classification.get(str(x).lower(), "Unknown") if pd.notna(x) else "Unknown"
                            )
                            
                            # Ensure Match Type column exists
                            if 'Match Type' not in sp_sb_search_terms.columns:
                                sp_sb_search_terms['Match Type'] = "Unknown"
                            
                            # Filter for contradicting classifications
                            contradicting_terms = sp_sb_search_terms[
                                (sp_sb_search_terms['Search Term Classification'] != sp_sb_search_terms['Target Classification']) & 
                                (sp_sb_search_terms['Target Classification'] != "Unknown")
                            ].copy()
                            
                            # Calculate metrics
                            if not contradicting_terms.empty:
                                # Calculate ACoS and ROAS as numeric values
                                contradicting_terms['ACoS'] = contradicting_terms.apply(
                                    lambda row: (row['Spend'] / row['Sales'] * 100) if pd.notna(row['Sales']) and row['Sales'] > 0 else 0,
                                    axis=1
                                )
                                contradicting_terms['ROAS'] = contradicting_terms.apply(
                                    lambda row: (row['Sales'] / row['Spend']) if pd.notna(row['Spend']) and row['Spend'] > 0 else 0,
                                    axis=1
                                )
                                
                                # Rename 'Sales' to 'Ad Sales' for clarity, but first check if it already exists
                                if 'Ad Sales' not in contradicting_terms.columns and 'Sales' in contradicting_terms.columns:
                                    contradicting_terms = contradicting_terms.rename(columns={'Sales': 'Ad Sales'})
                                
                                # Create filters
                                col1, col2, col3 = st.columns(3)
                                with col1:
                                    search_term_filter = st.text_input("Filter by Search Term", "", key="contradicting_search_term_filter")
                                with col2:
                                    target_filter = st.text_input("Filter by Target", "", key="contradicting_target_filter")
                                with col3:
                                    campaign_filter = st.text_input("Filter by Campaign", "", key="contradicting_campaign_filter")
                                
                                col4, col5 = st.columns(2)
                                with col4:
                                    search_term_class_filter = st.selectbox(
                                        "Filter by Search Term Classification",
                                        ["All", "Branded", "Non-Branded"],
                                        key="contradicting_search_term_class_filter"
                                    )
                                with col5:
                                    # Get unique match types from the data
                                    match_types = ["All"] + sorted(contradicting_terms['Match Type'].dropna().unique().tolist()) if 'Match Type' in contradicting_terms.columns else ["All"]
                                    match_type_filter = st.selectbox(
                                        "Filter by Match Type",
                                        match_types,
                                        key="contradicting_match_type_filter"
                                    )
                                
                                # Apply filters
                                filtered_df = contradicting_terms.copy()
                                if search_term_filter:
                                    filtered_df = filtered_df[filtered_df['Search Term'].astype(str).str.contains(search_term_filter, case=False, na=False)]
                                if target_filter:
                                    filtered_df = filtered_df[filtered_df['Target'].astype(str).str.contains(target_filter, case=False, na=False)]
                                if campaign_filter:
                                    filtered_df = filtered_df[filtered_df['Campaign'].astype(str).str.contains(campaign_filter, case=False, na=False)]
                                if search_term_class_filter != "All":
                                    filtered_df = filtered_df[filtered_df['Search Term Classification'] == search_term_class_filter]
                                if match_type_filter != "All" and 'Match Type' in filtered_df.columns:
                                    filtered_df = filtered_df[filtered_df['Match Type'] == match_type_filter]
                                
                                # Display the table
                                if not filtered_df.empty:
                                    # Calculate summary metrics
                                    # Calculate summary metrics directly without creating new columns
                                    try:
                                        # For Spend
                                        if 'Spend' in filtered_df.columns:
                                            # Convert to numeric safely
                                            numeric_spend = pd.to_numeric(
                                                filtered_df['Spend'].replace('[$,]', '', regex=True) if isinstance(filtered_df['Spend'], pd.Series) else 
                                                filtered_df['Spend'].astype(str), 
                                                errors='coerce'
                                            ).fillna(0)
                                            spend = numeric_spend.sum()
                                        else:
                                            spend = 0
                                            
                                        # For Ad Sales
                                        if 'Ad Sales' in filtered_df.columns:
                                            # Convert to numeric safely
                                            numeric_sales = pd.to_numeric(
                                                filtered_df['Ad Sales'].replace('[$,]', '', regex=True) if isinstance(filtered_df['Ad Sales'], pd.Series) else 
                                                filtered_df['Ad Sales'].astype(str), 
                                                errors='coerce'
                                            ).fillna(0)
                                            sales = numeric_sales.sum()
                                        else:
                                            sales = 0
                                    except Exception as e:
                                        # Fallback if there's any error in conversion
                                        st.error(f"Error calculating metrics: {str(e)}")
                                        spend = 0
                                        sales = 0
                                        
                                    # Calculate ACoS and ROAS using scalar values
                                    acos = (spend / sales * 100) if sales > 0 else 0
                                    roas = (sales / spend) if spend > 0 else 0
                                    
                                    # Display summary cards
                                    kpi_cols = st.columns(4)
                                    kpi_cols[0].metric("TOTAL SPEND", f"${spend:,.2f}")
                                    kpi_cols[1].metric("AD SALES", f"${sales:,.2f}")
                                    kpi_cols[2].metric("ACoS", f"{acos:.2f}%")
                                    kpi_cols[3].metric("ROAS", f"{roas:.2f}")
                                    
                                    # Check for duplicate columns in a safer way
                                    if 'Ad Sales' in filtered_df.columns and filtered_df.columns.duplicated().any():
                                        # Create a new DataFrame with unique column names
                                        # This approach preserves the first occurrence of each column
                                        filtered_df = filtered_df.loc[:, ~filtered_df.columns.duplicated()]
                                    
                                    # Select columns to display
                                    display_cols = [
                                        'Campaign', 'Search Term', 'Search Term Classification', 
                                        'Target', 'Target Classification', 'Match Type', 'Spend', 'Ad Sales', 
                                        'ACoS', 'ROAS'
                                    ]
                                    
                                    # Ensure all required columns exist
                                    for col in display_cols:
                                        if col not in filtered_df.columns:
                                            filtered_df[col] = "N/A"
                                    
                                    # Clean numeric columns to fix weird characters
                                    for col in ['Spend', 'Ad Sales']:
                                        if col in filtered_df.columns:
                                            # Convert to numeric, removing any currency symbols or formatting
                                            filtered_df[col] = pd.to_numeric(filtered_df[col].astype(str).str.replace(r'[^0-9.-]', '', regex=True), errors='coerce').fillna(0)
                                    
                                    # Select columns to display
                                    display_df = filtered_df[display_cols].copy()
                                    
                                    # Create column configuration for proper formatting and sorting
                                    column_config = {
                                        'Campaign': st.column_config.TextColumn(label="Campaign"),
                                        'Search Term': st.column_config.TextColumn(label="Search Term"),
                                        'Search Term Classification': st.column_config.TextColumn(label="Search Term Classification"),
                                        'Target': st.column_config.TextColumn(label="Target"),
                                        'Target Classification': st.column_config.TextColumn(label="Target Classification"),
                                        'Match Type': st.column_config.TextColumn(label="Match Type"),
                                        'Spend': st.column_config.NumberColumn(label="Spend", format="dollar"),
                                        'Ad Sales': st.column_config.NumberColumn(label="Ad Sales", format="dollar"),
                                        'ACoS': st.column_config.NumberColumn(label="ACoS (%)", format="%.2f%%"),
                                        'ROAS': st.column_config.NumberColumn(label="ROAS", format="%.2f")
                                    }
                                    
                                    # Convert ACoS and ROAS to numeric values for proper display
                                    if 'ACoS' in display_df.columns:
                                        display_df['ACoS'] = pd.to_numeric(display_df['ACoS'].astype(str).str.replace(r'[^0-9.-]', '', regex=True), errors='coerce').fillna(0)
                                    
                                    if 'ROAS' in display_df.columns:
                                        display_df['ROAS'] = pd.to_numeric(display_df['ROAS'].astype(str).str.replace(r'[^0-9.-]', '', regex=True), errors='coerce').fillna(0)
                                    
                                    # Display the table with column config for proper formatting and sorting
                                    st.dataframe(
                                        display_df,
                                        use_container_width=True,
                                        hide_index=True,
                                        column_config=column_config
                                    )
                    # --- Wasted Spend Analysis Section ---
                    st.markdown("<div style='margin-top:3rem;'></div>", unsafe_allow_html=True)
                    st.markdown("<div id='wasted-spend' class='section-anchor'></div>", unsafe_allow_html=True)
                    st.markdown("<hr style='height:2px;border-width:0;color:gold;background-color:gold;margin-top:25px;margin-bottom:15px;margin-left:10px;margin-right:10px'>", unsafe_allow_html=True)
                    st.markdown("<span class='main-section-header'>Wasted Spend Analysis</span>", unsafe_allow_html=True)
                    
                    # Helper function to calculate wasted spend score
                    def calculate_wasted_spend_score(df):
                        """Calculate a wasted spend score for each target focused on ROAS efficiency and spend impact.
                        The score prioritizes targets with poor ROAS performance relative to budget consumption.
                        
                        Args:
                            df: DataFrame containing targeting data
                        
                        Returns:
                            DataFrame with wasted spend score and components
                        """
                        if df.empty:
                            return pd.DataFrame()
                            
                        # Make a copy to avoid modifying the original
                        result_df = df.copy()
                        
                        # Ensure we have the necessary columns
                        required_cols = ['ROAS', 'Spend', 'Ad Sales'] 
                        for col in required_cols:
                            if col not in result_df.columns:
                                if 'debug_messages' in st.session_state:
                                    st.session_state.debug_messages.append(f"[Wasted Spend Analysis] Missing required column: {col}")
                                return result_df
                        
                        # Calculate segment metrics
                        avg_roas = result_df['ROAS'].mean() if not result_df.empty else 0
                        total_spend = result_df['Spend'].sum() if not result_df.empty else 0
                        median_spend = result_df['Spend'].median() if not result_df.empty else 0
                        
                        # Set minimum spend threshold - filter out targets with less than $30 spend
                        min_spend_threshold = 30.0
                        result_df['Below_Min_Spend'] = result_df['Spend'] < min_spend_threshold
                        
                        # Add debug info
                        if 'debug_messages' in st.session_state:
                            st.session_state.debug_messages.append(f"[Wasted Spend Analysis] Segment metrics - Avg ROAS: {avg_roas:.2f}, Total Spend: ${total_spend:.2f}, Median Spend: ${median_spend:.2f}")
                            st.session_state.debug_messages.append(f"[Wasted Spend Analysis] Filtering out {result_df['Below_Min_Spend'].sum()} targets with spend below ${min_spend_threshold}")
                        
                        # Flag targets with no sales
                        result_df['No_Sales'] = (result_df['Ad Sales'] == 0) & (result_df['Spend'] > 0)
                        
                        # Calculate ROAS efficiency score - how efficiently spend is converted to sales
                        result_df['ROAS_Efficiency'] = 0.0
                        
                        # Calculate ROAS threshold based on segment average
                        # Using a dynamic threshold that adapts to the segment's performance
                        roas_threshold = max(avg_roas * 0.75, 0.5)  # At least 0.5 to avoid extreme thresholds in low ROAS segments
                        
                        # Add threshold to debug info
                        if 'debug_messages' in st.session_state:
                            st.session_state.debug_messages.append(f"[Wasted Spend Analysis] ROAS threshold: {roas_threshold:.6f}")
                        
                        # Handle targets with ROAS > 0
                        has_sales_mask = (~result_df['No_Sales']) & (result_df['ROAS'] > 0)
                        
                        # Safety check - if no targets with sales or avg_roas is 0, skip this section
                        if avg_roas <= 0:
                            if 'debug_messages' in st.session_state:
                                st.session_state.debug_messages.append(f"[Wasted Spend Analysis] Skipping ROAS efficiency calculation - avg_roas is {avg_roas}")
                        elif not has_sales_mask.any():
                            if 'debug_messages' in st.session_state:
                                st.session_state.debug_messages.append(f"[Wasted Spend Analysis] No targets with sales found")
                        else:
                            # Calculate ROAS efficiency for all targets with sales
                            # This is a more nuanced approach that considers how far below optimal each target is
                            
                            # Calculate ROAS efficiency for all targets with sales
                            # Higher values = worse efficiency
                            result_df.loc[has_sales_mask, 'ROAS_Efficiency'] = 1 - (result_df.loc[has_sales_mask, 'ROAS'] / roas_threshold)
                            
                            # Cap efficiency scores between 0 and 1
                            # 0 = meeting or exceeding threshold (good)
                            # 1 = extremely inefficient (bad)
                            result_df.loc[has_sales_mask, 'ROAS_Efficiency'] = result_df.loc[has_sales_mask, 'ROAS_Efficiency'].clip(0, 1)
                            
                            # Apply exponential scaling to emphasize very poor performers
                            # This creates more separation between slightly poor and very poor performers
                            result_df.loc[has_sales_mask, 'ROAS_Efficiency'] = result_df.loc[has_sales_mask, 'ROAS_Efficiency'] ** 1.5
                            
                            # Targets with ROAS above threshold get a score of 0 (not wasteful)
                            above_threshold_mask = (result_df['ROAS'] >= roas_threshold) & has_sales_mask
                            result_df.loc[above_threshold_mask, 'ROAS_Efficiency'] = 0.0
                            
                            # Flag targets above the ROAS threshold to be excluded later
                            result_df['Above_Threshold'] = False
                            result_df.loc[above_threshold_mask, 'Above_Threshold'] = True
                            
                            if 'debug_messages' in st.session_state:
                                st.session_state.debug_messages.append(f"[Wasted Spend Analysis] Found {above_threshold_mask.sum()} targets with ROAS above threshold (will be excluded)")
                            
                            # Add debug info about efficiency scores
                            if 'debug_messages' in st.session_state and has_sales_mask.any():
                                avg_efficiency = result_df.loc[has_sales_mask, 'ROAS_Efficiency'].mean()
                                st.session_state.debug_messages.append(f"[Wasted Spend Analysis] Average ROAS efficiency score: {avg_efficiency:.6f}")
                        
                        # Initialize Below_Guardrail as all False since we're not using guardrail anymore
                        result_df['Below_Guardrail'] = False
                        
                        # Handle targets with no sales - they get maximum inefficiency score
                        # but will be categorized based on spend amount
                        no_sales_mask = result_df['No_Sales']
                        if no_sales_mask.any():
                            # Base value for no sales is maximum (1.0)
                            result_df.loc[no_sales_mask, 'ROAS_Efficiency'] = 1.0
                            
                            # Create a new column for no sales categorization
                            result_df['No_Sales_Category'] = 'Low'
                            
                            # Categorize no sales targets based on spend
                            # $100+ spend and no sales = Critical
                            critical_no_sales_mask = no_sales_mask & (result_df['Spend'] >= 100)
                            result_df.loc[critical_no_sales_mask, 'No_Sales_Category'] = 'Critical'
                            
                            # $75-99.99 spend and no sales = High
                            high_no_sales_mask = no_sales_mask & (result_df['Spend'] >= 75) & (result_df['Spend'] < 100)
                            result_df.loc[high_no_sales_mask, 'No_Sales_Category'] = 'High'
                            
                            # $50-74.99 spend and no sales = Medium
                            medium_no_sales_mask = no_sales_mask & (result_df['Spend'] >= 50) & (result_df['Spend'] < 75)
                            result_df.loc[medium_no_sales_mask, 'No_Sales_Category'] = 'Medium'
                            
                            # Below $50 spend and no sales = Low (default)
                            
                            if 'debug_messages' in st.session_state:
                                st.session_state.debug_messages.append(f"[Wasted Spend Analysis] Found {no_sales_mask.sum()} targets with no sales")
                                st.session_state.debug_messages.append(f"[Wasted Spend Analysis] No sales categories: Critical={critical_no_sales_mask.sum()}, High={high_no_sales_mask.sum()}, Medium={medium_no_sales_mask.sum()}, Low={(no_sales_mask & (result_df['Spend'] < 50)).sum()}")
                        
                        # Calculate spend impact - how much of the total budget is being consumed
                        # Normalize spend to create a 0-1 scale where higher values = higher spend
                        if total_spend > 0:
                            # Calculate raw spend percentage
                            result_df['Spend_Pct'] = result_df['Spend'] / total_spend
                            
                            # Apply a more aggressive scaling to emphasize high-spend targets
                            # Using a power function instead of logarithmic to give more weight to higher spend
                            max_spend_pct = result_df['Spend_Pct'].max()
                            if max_spend_pct > 0:
                                # Power scaling with exponent 0.7 gives more emphasis to high spenders
                                # while still preventing extreme domination
                                result_df['Spend_Impact'] = (result_df['Spend_Pct'] / max_spend_pct) ** 0.7
                            else:
                                result_df['Spend_Impact'] = 0.0
                        else:
                            result_df['Spend_Pct'] = 0.0
                            result_df['Spend_Impact'] = 0.0
                            
                        # Calculate final wasted spend score combining ROAS efficiency and spend impact
                        # This creates a score that prioritizes inefficient targets that consume significant budget
                        # Significantly increased weight for spend impact to emphasize high-spending targets
                        result_df['Wasted_Spend_Score'] = (
                            (result_df['ROAS_Efficiency'] * 0.25) +  # Reduced weight for ROAS efficiency (25%)
                            (result_df['Spend_Impact'] * 0.75)       # Significantly increased weight for spend impact (75%)
                        )
                        
                        # Set score to 0 for targets below minimum spend threshold, below ROAS guardrail, or above ROAS threshold
                        result_df.loc[result_df['Below_Min_Spend'], 'Wasted_Spend_Score'] = 0.0
                        result_df.loc[result_df['Below_Guardrail'], 'Wasted_Spend_Score'] = 0.0
                        result_df.loc[result_df['Above_Threshold'], 'Wasted_Spend_Score'] = 0.0
                        
                        # Normalize waste scores to 0-100 scale for better interpretability
                        if not result_df.empty and result_df['Wasted_Spend_Score'].max() > 0:
                            # Scale scores to 0-100 range
                            max_score = result_df['Wasted_Spend_Score'].max()
                            result_df['Wasted_Spend_Score'] = (result_df['Wasted_Spend_Score'] / max_score) * 100
                            
                            # Round to 2 decimal places for cleaner display
                            result_df['Wasted_Spend_Score'] = result_df['Wasted_Spend_Score'].round(2)
                            
                            if 'debug_messages' in st.session_state:
                                st.session_state.debug_messages.append(f"[Wasted Spend Analysis] Normalized waste scores to 0-100 scale")
                        
                        # Sort by wasted spend score in descending order
                        result_df = result_df.sort_values('Wasted_Spend_Score', ascending=False)
                        
                        # Remove targets below minimum spend threshold
                        result_df = result_df[~result_df['Below_Min_Spend']]
                        
                        # Remove targets above the ROAS threshold (we keep all targets below threshold regardless of ROAS)
                        result_df = result_df[~result_df['Above_Threshold']]
                        
                        # Remove targets with zero waste score (but keep all others, even with very low ROAS)
                        result_df = result_df[result_df['Wasted_Spend_Score'] > 0]
                        
                        # Add waste score category for easier interpretation
                        result_df['Waste_Category'] = pd.cut(
                            result_df['Wasted_Spend_Score'], 
                            bins=[0, 25, 50, 75, 100], 
                            labels=['Low', 'Medium', 'High', 'Critical']
                        )
                        
                        # Override waste category for no sales targets based on spend amount
                        no_sales_mask = result_df['No_Sales']
                        if no_sales_mask.any():
                            result_df.loc[no_sales_mask, 'Waste_Category'] = result_df.loc[no_sales_mask, 'No_Sales_Category']
                        
                        if 'debug_messages' in st.session_state:
                            st.session_state.debug_messages.append(f"[Wasted Spend Analysis] Final count of targets with waste score > 0: {len(result_df)}")
                            category_counts = result_df['Waste_Category'].value_counts().to_dict()
                            st.session_state.debug_messages.append(f"[Wasted Spend Analysis] Waste categories: {category_counts}")
                        
                        return result_df
                    
                    # Create tabs for the Wasted Spend Analysis
                    waste_tab_labels = ["All Targets", "Branded Targets", "Non-Branded Targets"]
                    waste_tabs = st.tabs(waste_tab_labels)
                    
                    # Get the dataframes for each tab
                    waste_dfs = [st.session_state.all_targets_df, st.session_state.branded_targets_df, st.session_state.non_branded_targets_df]
                    waste_titles = ["All", "Branded", "Non-Branded"]
                    
                    # Process each tab
                    for i, (tab, df, title) in enumerate(zip(waste_tabs, waste_dfs, waste_titles)):
                        with tab:
                            if not df.empty:
                                # Calculate wasted spend scores for all targets
                                waste_df = calculate_wasted_spend_score(df)
                                
                                if not waste_df.empty:
                                    # Allow user to select how many targets to view
                                    col1, col2 = st.columns([1, 3])
                                    with col1:
                                        num_targets = st.selectbox(
                                            f"Number of {title} Targets to View", 
                                            options=[5, 10, 15, 20, 25, 50, 100], 
                                            index=1,  # Default to 10
                                            key=f"waste_num_targets_{i}"
                                        )
                                    
                                    # Sort by priority first (Critical -> High -> Medium -> Low), then by waste score
                                    # Create a priority order mapping (higher value = higher priority)
                                    priority_order = {'Critical': 3, 'High': 2, 'Medium': 1, 'Low': 0}
                                    waste_df['Priority_Order'] = waste_df['Waste_Category'].map(priority_order)
                                    
                                    # Sort the dataframe - priority first (descending to put highest priority on top), then waste score
                                    waste_df = waste_df.sort_values(['Priority_Order', 'Wasted_Spend_Score'], 
                                                                 ascending=[False, False])
                                    
                                    # Now limit to the selected number of targets AFTER sorting
                                    selected_waste_df = waste_df.head(num_targets).copy()
                                    
                                    # Calculate metrics for the selected targets
                                    selected_avg_roas = selected_waste_df['ROAS'].mean() if 'ROAS' in selected_waste_df.columns else 0
                                    selected_total_spend = selected_waste_df['Spend'].sum() if 'Spend' in selected_waste_df.columns else 0
                                    
                                    with col2:
                                        st.markdown(f"<div style='padding-top:2rem;'><b>Average ROAS:</b> {selected_avg_roas:.2f}x &nbsp;&nbsp; <b>Total Spend:</b> ${selected_total_spend:,.2f}</div>", unsafe_allow_html=True)
                                    
                                    # Create a display dataframe with relevant columns
                                    display_cols = [
                                        'Campaign', 'Target', 'Match Type', 'Spend', 'Ad Sales', 
                                        'ROAS', 'ACoS', 'Clicks', 'Orders', 'CPC', 'Bid', 'Waste_Category'
                                    ]
                                    # Filter to only columns that exist
                                    display_cols = [col for col in display_cols if col in selected_waste_df.columns]
                                    
                                    # Use the already limited dataframe
                                    display_df = selected_waste_df[display_cols].copy()
                                    
                                    # Format the display dataframe
                                    display_df['Spend'] = display_df['Spend'].apply(lambda x: f"${x:,.2f}" if x >= 1000 else f"${x:.2f}")
                                    display_df['Ad Sales'] = display_df['Ad Sales'].apply(lambda x: f"${x:,.2f}" if x >= 1000 else f"${x:.2f}")
                                    display_df['ROAS'] = display_df['ROAS'].apply(lambda x: f"{x:.2f}x")
                                    display_df['CPC'] = display_df['CPC'].apply(lambda x: f"${x:.2f}")
                                    display_df['ACoS'] = display_df['ACoS'].apply(lambda x: f"{x:.2f}%")
                                    # Format Bid column if it exists
                                    if 'Bid' in display_df.columns:
                                        display_df['Bid'] = display_df['Bid'].apply(lambda x: f"${x:.2f}")
                                    display_df['Clicks'] = display_df['Clicks'].apply(lambda x: f"{int(x):,}")
                                    display_df['Orders'] = display_df['Orders'].apply(lambda x: f"{int(x):,}")
                                    
                                    # Rename columns for display
                                    display_df = display_df.rename(columns={
                                        'Waste_Category': 'Priority'
                                    })
                                    
                                    # Apply conditional formatting based on waste category
                                    def highlight_waste_score(val):
                                        if 'Critical' in str(val):
                                            return 'background-color: rgba(255, 0, 0, 0.2); color: #ff4d4d; font-weight: bold'
                                        elif 'High' in str(val):
                                            return 'background-color: rgba(255, 165, 0, 0.2); color: #ffa500; font-weight: bold'
                                        elif 'Medium' in str(val):
                                            return 'background-color: rgba(255, 255, 0, 0.1); color: #cccc00; font-weight: bold'
                                        elif 'Low' in str(val):
                                            return 'background-color: rgba(0, 128, 0, 0.1); color: #00cc00; font-weight: bold'
                                        return ''
                                    
                                    # Apply styling to the waste score column
                                    styled_df = display_df.style.map(
                                        highlight_waste_score, 
                                        subset=['Priority']
                                    )
                                    
                                    # Create column configuration for proper formatting and sorting
                                    column_config = {
                                        'Campaign': st.column_config.TextColumn(
                                            'Campaign',
                                            width='medium'
                                        ),
                                        'Target': st.column_config.TextColumn(
                                            'Target',
                                            width='medium'
                                        ),
                                        'Match Type': st.column_config.TextColumn(
                                            'Match Type',
                                            width='small'
                                        ),
                                        'Spend': st.column_config.NumberColumn(
                                            'Spend',
                                            format='$%.2f',
                                            width='small'
                                        ),
                                        'Ad Sales': st.column_config.NumberColumn(
                                            'Ad Sales',
                                            format='$%.2f',
                                            width='small'
                                        ),
                                        'ROAS': st.column_config.NumberColumn(
                                            'ROAS',
                                            format='%.2fx',
                                            width='small'
                                        ),
                                        'ACoS': st.column_config.NumberColumn(
                                            'ACoS',
                                            format='%.2f%%',
                                            width='small'
                                        ),
                                        'Clicks': st.column_config.NumberColumn(
                                            'Clicks',
                                            format='%d',
                                            width='small'
                                        ),
                                        'Orders': st.column_config.NumberColumn(
                                            'Orders',
                                            format='%d',
                                            width='small'
                                        ),
                                        'CPC': st.column_config.NumberColumn(
                                            'CPC',
                                            format='$%.2f',
                                            width='small'
                                        ),
                                        'Bid': st.column_config.NumberColumn(
                                            'Bid',
                                            format='$%.2f',
                                            width='small'
                                        ),
                                        'Priority': st.column_config.TextColumn(
                                            'Priority',
                                            width='small'
                                        )
                                    }

                                    # Convert formatted string values to numeric for sorting
                                    numeric_df = display_df.copy()
                                    
                                    # Convert currency and numeric columns from formatted strings to numeric values
                                    for col in ['Spend', 'Ad Sales', 'CPC', 'Bid']:
                                        if col in numeric_df.columns:
                                            numeric_df[col] = numeric_df[col].apply(lambda x: float(str(x).replace('$', '').replace(',', '')) if pd.notna(x) and str(x).strip() != '' else 0)
                                    
                                    # Convert percentage columns
                                    if 'ACoS' in numeric_df.columns:
                                        numeric_df['ACoS'] = numeric_df['ACoS'].apply(lambda x: float(str(x).replace('%', '')) if pd.notna(x) and str(x) != 'N/A' else 0)
                                    
                                    # Convert ROAS
                                    if 'ROAS' in numeric_df.columns:
                                        numeric_df['ROAS'] = numeric_df['ROAS'].apply(lambda x: float(str(x).replace('x', '')) if pd.notna(x) and str(x) != 'N/A' else 0)
                                    
                                    # Convert Clicks and Orders
                                    for col in ['Clicks', 'Orders']:
                                        if col in numeric_df.columns:
                                            numeric_df[col] = numeric_df[col].apply(lambda x: int(str(x).replace(',', '')) if pd.notna(x) and str(x).strip() != '' else 0)
                                    
                                    # Apply styling to the priority column
                                    styled_df = numeric_df.style.map(
                                        highlight_waste_score, 
                                        subset=['Priority']
                                    )
                                    
                                    # Display the table with column config for proper formatting and sorting
                                    st.dataframe(
                                        styled_df,
                                        use_container_width=True,
                                        hide_index=True,
                                        column_config=column_config
                                    )
                                    

                                    
                                    # Add explanation of the waste score
                                    # Add explanation of the waste score
                                    with st.expander("How is the Waste Score calculated?"):
                                        st.markdown("""
                                        The **Waste Score** is calculated on a scale of 0-100 using an improved formula that focuses on two key factors:
                                        
                                        1. **ROAS Efficiency (25%)**: How efficiently your Spend is generating sales relative to the segment's threshold
                                        2. **Spend Impact (75%)**: How much of your total budget this target consumes
                                        
                                        **Key features of this improved approach:**
                                        
                                        - **Minimum Spend Threshold**: Targets with less than $30 in spend are excluded entirely
                                        - **ROAS Guardrail**: Targets with ROAS more than 75% below segment average are excluded
                                        - **Priority Categories**: Scores are grouped into Low (0-25), Medium (25-50), High (50-75), and Critical (75-100)
                                        - **No Sales Categorization**:
                                          - $100+ spend with no sales: Critical priority
                                          - $75-99.99 spend with no sales: High priority
                                          - $50-74.99 spend with no sales: Medium priority
                                          - Under $50 spend with no sales: Low priority
                                        
                                        This approach helps you identify targets that are both inefficient (poor ROAS) and significant budget consumers, with strong emphasis on high-spending targets.
                                        """)
                                else:
                                    st.info(f"No {title} targets available for analysis.")
                            else:
                                st.info(f"No {title} targets available for analysis.")
                    
                    # --- Contradicting Targets and Search Terms Section ---
                    st.markdown("<div style='margin-top:3rem;'></div>", unsafe_allow_html=True)
                    st.markdown("<div id='contradicting-targets' class='section-anchor'></div>", unsafe_allow_html=True)
                    st.markdown("<hr style='height:2px;border-width:0;color:gold;background-color:gold;margin-top:25px;margin-bottom:15px;margin-left:10px;margin-right:10px'>", unsafe_allow_html=True)
                    st.markdown("<span class='main-section-header'>Contradicting Targets and Search Terms</span>", unsafe_allow_html=True)
                    
                    # Hide function docstrings by not displaying them
                    if 'bulk_data' in st.session_state and 'client_config' in st.session_state:
                        # Get search term data with branding classification
                        search_term_df = get_search_term_data(st.session_state.bulk_data, st.session_state.client_config)
                        
                        # Get targeting data with branding classification
                        branded_targets_df, non_branded_targets_df = get_targeting_performance_data(
                            st.session_state.bulk_data,
                            st.session_state.client_config
                        )
                        
                        # Combine branded and non-branded targets
                        all_targets_df = pd.concat([branded_targets_df, non_branded_targets_df], ignore_index=True) if not (branded_targets_df.empty and non_branded_targets_df.empty) else pd.DataFrame()
                        if not search_term_df.empty and not all_targets_df.empty:
                        # Create a mapping of targets to their classification (Branded/Non-Branded)
                            target_classification = {}
                            
                            # Add Branded targets to the mapping
                            if not branded_targets_df.empty and 'Target' in branded_targets_df.columns:
                                for target in branded_targets_df['Target'].dropna().unique():
                                    target_classification[str(target).lower()] = "Branded"
                            
                            # Add Non-Branded targets to the mapping
                            if not non_branded_targets_df.empty and 'Target' in non_branded_targets_df.columns:
                                for target in non_branded_targets_df['Target'].dropna().unique():
                                    target_classification[str(target).lower()] = "Non-Branded"
                            
                            # Filter for Sponsored Products and Sponsored Brands only
                            sp_sb_search_terms = search_term_df[
                                search_term_df['Campaign Type'].str.contains('Sponsored Products|Sponsored Brands', case=False, na=False)
                            ].copy() if 'Campaign Type' in search_term_df.columns else search_term_df.copy()
                            
                            # Ensure we have the necessary columns
                            required_cols = ['Search Term', 'Target', 'Is_Branded', 'Campaign', 'Spend', 'Sales', 'Orders', 'Clicks', 'Impressions']
                            for col in required_cols:
                                if col not in sp_sb_search_terms.columns:
                                    sp_sb_search_terms[col] = None
                            
                            # Create a column for search term classification
                            sp_sb_search_terms['Search Term Classification'] = sp_sb_search_terms['Is_Branded'].apply(
                                lambda x: "Branded" if x else "Non-Branded"
                            )
                            
                            # Create a column for target classification
                            sp_sb_search_terms['Target Classification'] = sp_sb_search_terms['Target'].apply(
                                lambda x: target_classification.get(str(x).lower(), "Unknown") if pd.notna(x) else "Unknown"
                            )
                            
                            # Ensure Match Type column exists
                            if 'Match Type' not in sp_sb_search_terms.columns:
                                sp_sb_search_terms['Match Type'] = "Unknown"
                            
                            # Filter for contradicting classifications
                            contradicting_terms = sp_sb_search_terms[
                                (sp_sb_search_terms['Search Term Classification'] != sp_sb_search_terms['Target Classification']) & 
                                (sp_sb_search_terms['Target Classification'] != "Unknown")
                            ].copy()
                            
                            # Calculate metrics
                            if not contradicting_terms.empty:
                                # Calculate ACoS and ROAS as numeric values
                                contradicting_terms['ACoS'] = contradicting_terms.apply(
                                    lambda row: (row['Spend'] / row['Sales'] * 100) if pd.notna(row['Sales']) and row['Sales'] > 0 else 0,
                                    axis=1
                                )
                                contradicting_terms['ROAS'] = contradicting_terms.apply(
                                    lambda row: (row['Sales'] / row['Spend']) if pd.notna(row['Spend']) and row['Spend'] > 0 else 0,
                                    axis=1
                                )
                                
                                # Rename 'Sales' to 'Ad Sales' for clarity, but first check if it already exists
                                if 'Ad Sales' not in contradicting_terms.columns and 'Sales' in contradicting_terms.columns:
                                    contradicting_terms = contradicting_terms.rename(columns={'Sales': 'Ad Sales'})
                                
                                # Create filters
                                col1, col2, col3 = st.columns(3)
                                with col1:
                                    search_term_filter = st.text_input("Filter by Search Term", "", key="contradicting_search_term_filter")
                                with col2:
                                    target_filter = st.text_input("Filter by Target", "", key="contradicting_target_filter")
                                with col3:
                                    campaign_filter = st.text_input("Filter by Campaign", "", key="contradicting_campaign_filter")
                                
                                col4, col5 = st.columns(2)
                                with col4:
                                    search_term_class_filter = st.selectbox(
                                        "Filter by Search Term Classification",
                                        ["All", "Branded", "Non-Branded"],
                                        key="contradicting_search_term_class_filter"
                                    )
                                with col5:
                                    # Get unique match types from the data
                                    match_types = ["All"] + sorted(contradicting_terms['Match Type'].dropna().unique().tolist()) if 'Match Type' in contradicting_terms.columns else ["All"]
                                    match_type_filter = st.selectbox(
                                        "Filter by Match Type",
                                        match_types,
                                        key="contradicting_match_type_filter"
                                    )
                                
                                # Apply filters
                                filtered_df = contradicting_terms.copy()
                                if search_term_filter:
                                    filtered_df = filtered_df[filtered_df['Search Term'].astype(str).str.contains(search_term_filter, case=False, na=False)]
                                if target_filter:
                                    filtered_df = filtered_df[filtered_df['Target'].astype(str).str.contains(target_filter, case=False, na=False)]
                                if campaign_filter:
                                    filtered_df = filtered_df[filtered_df['Campaign'].astype(str).str.contains(campaign_filter, case=False, na=False)]
                                if search_term_class_filter != "All":
                                    filtered_df = filtered_df[filtered_df['Search Term Classification'] == search_term_class_filter]
                                if match_type_filter != "All" and 'Match Type' in filtered_df.columns:
                                    filtered_df = filtered_df[filtered_df['Match Type'] == match_type_filter]
                                
                                # Display the table
                                if not filtered_df.empty:
                                    # Calculate summary metrics
                                    # Calculate summary metrics directly without creating new columns
                                    try:
                                        # For Spend
                                        if 'Spend' in filtered_df.columns:
                                            # Convert to numeric safely
                                            numeric_spend = pd.to_numeric(
                                                filtered_df['Spend'].replace('[$,]', '', regex=True) if isinstance(filtered_df['Spend'], pd.Series) else 
                                                filtered_df['Spend'].astype(str), 
                                                errors='coerce'
                                            ).fillna(0)
                                            spend = numeric_spend.sum()
                                        else:
                                            spend = 0
                                            
                                        # For Ad Sales
                                        if 'Ad Sales' in filtered_df.columns:
                                            # Convert to numeric safely
                                            numeric_sales = pd.to_numeric(
                                                filtered_df['Ad Sales'].replace('[$,]', '', regex=True) if isinstance(filtered_df['Ad Sales'], pd.Series) else 
                                                filtered_df['Ad Sales'].astype(str), 
                                                errors='coerce'
                                            ).fillna(0)
                                            sales = numeric_sales.sum()
                                        else:
                                            sales = 0
                                    except Exception as e:
                                        # Fallback if there's any error in conversion
                                        st.error(f"Error calculating metrics: {str(e)}")
                                        spend = 0
                                        sales = 0
                                        
                                    # Calculate ACoS and ROAS using scalar values
                                    acos = (spend / sales * 100) if sales > 0 else 0
                                    roas = (sales / spend) if spend > 0 else 0
                                    
                                    # Display summary cards
                                    kpi_cols = st.columns(4)
                                    kpi_cols[0].metric("TOTAL SPEND", f"${spend:,.2f}")
                                    kpi_cols[1].metric("AD SALES", f"${sales:,.2f}")
                                    kpi_cols[2].metric("ACoS", f"{acos:.2f}%")
                                    kpi_cols[3].metric("ROAS", f"{roas:.2f}")
                                    
                                    # Check for duplicate columns in a safer way
                                    if 'Ad Sales' in filtered_df.columns and filtered_df.columns.duplicated().any():
                                        # Create a new DataFrame with unique column names
                                        # This approach preserves the first occurrence of each column
                                        filtered_df = filtered_df.loc[:, ~filtered_df.columns.duplicated()]
                                    
                                    # Select columns to display
                                    display_cols = [
                                        'Campaign', 'Search Term', 'Search Term Classification', 
                                        'Target', 'Target Classification', 'Match Type', 'Spend', 'Ad Sales', 
                                        'ACoS', 'ROAS'
                                    ]
                                    
                                    # Ensure all required columns exist
                                    for col in display_cols:
                                        if col not in filtered_df.columns:
                                            filtered_df[col] = "N/A"
                                    
                                    # Clean numeric columns to fix weird characters
                                    for col in ['Spend', 'Ad Sales']:
                                        if col in filtered_df.columns:
                                            # Convert to numeric, removing any currency symbols or formatting
                                            filtered_df[col] = pd.to_numeric(filtered_df[col].astype(str).str.replace(r'[^0-9.-]', '', regex=True), errors='coerce').fillna(0)
                                    
                                    # Select columns to display
                                    display_df = filtered_df[display_cols].copy()
                                    
                                    # Create column configuration for proper formatting and sorting
                                    column_config = {
                                        'Campaign': st.column_config.TextColumn(label="Campaign"),
                                        'Search Term': st.column_config.TextColumn(label="Search Term"),
                                        'Search Term Classification': st.column_config.TextColumn(label="Search Term Classification"),
                                        'Target': st.column_config.TextColumn(label="Target"),
                                        'Target Classification': st.column_config.TextColumn(label="Target Classification"),
                                        'Match Type': st.column_config.TextColumn(label="Match Type"),
                                        'Spend': st.column_config.NumberColumn(label="Spend", format="$%.2f"),
                                        'Ad Sales': st.column_config.NumberColumn(label="Ad Sales", format="$%.2f"),
                                        'ACoS': st.column_config.NumberColumn(label="ACoS", format="%.2f"),
                                        'ROAS': st.column_config.NumberColumn(label="ROAS", format="%.2f")
                                    }
                                    
                                    # Display the table
                                    st.dataframe(
                                        display_df,
                                        column_config=column_config,
                                        use_container_width=True,
                                        hide_index=True
                                    )
                                    

                                else:
                                    st.info("No contradicting targets and search terms found with the current filters.")
                            else:
                                st.info("No contradicting targets and search terms found in the data.")
                        else:
                            st.info("Insufficient data to analyze contradicting targets and search terms. Please upload bulk advertising files with search term and targeting data.")
                    else:
                        st.info("Please upload bulk advertising files to analyze contradicting targets and search terms.")
                    # --- Contradicting Targets in Campaigns Section ---
                    st.markdown("<div style='margin-top:3rem;'></div>", unsafe_allow_html=True)
                    st.markdown("<div id='contradicting-targets-campaigns' class='section-anchor'></div>", unsafe_allow_html=True)
                    st.markdown("<hr style='height:2px;border-width:0;color:gold;background-color:gold;margin-top:25px;margin-bottom:15px;margin-left:10px;margin-right:10px'>", unsafe_allow_html=True)
                    st.markdown("<span class='main-section-header'>Contradicting Targets in Campaigns</span>", unsafe_allow_html=True)
                    
                    if 'bulk_data' in st.session_state and 'client_config' in st.session_state:
                        # Get search term data with branding classification
                        search_term_df = get_search_term_data(st.session_state.bulk_data, st.session_state.client_config)
                        
                        # Get targeting data with branding classification
                        branded_targets_df, non_branded_targets_df = get_targeting_performance_data(
                            st.session_state.bulk_data,
                            st.session_state.client_config
                        )
                        
                        # Combine branded and non-branded targets
                        all_targets_df = pd.concat([branded_targets_df, non_branded_targets_df], ignore_index=True) if not (branded_targets_df.empty and non_branded_targets_df.empty) else pd.DataFrame()
                        
                        # Function to classify campaigns
                        def classify_campaign(campaign_name):
                            if pd.isna(campaign_name):
                                return "Unknown"
                            campaign_str = str(campaign_name).lower()
                            if 'brand' in campaign_str:
                                return "Branded"
                            elif 'non' in campaign_str and 'brand' not in campaign_str:
                                return "Non-Branded"
                            else:
                                return "Unknown"
                        
                        # Create tabs for Targets and Search Terms
                        contradicting_campaigns_tabs = st.tabs(["Search Terms", "Targets"])
                        
                        # Tab 1: Search Terms
                        with contradicting_campaigns_tabs[0]:
                            if not search_term_df.empty:
                                # Filter for Sponsored Products and Sponsored Brands only
                                sp_sb_search_terms = search_term_df[
                                    search_term_df['Campaign Type'].str.contains('Sponsored Products|Sponsored Brands', case=False, na=False)
                                ].copy() if 'Campaign Type' in search_term_df.columns else search_term_df.copy()
                                
                                if not sp_sb_search_terms.empty:
                                    # Add campaign classification
                                    sp_sb_search_terms['Campaign Classification'] = sp_sb_search_terms['Campaign'].apply(classify_campaign)
                                    
                                    # Create search term classification
                                    sp_sb_search_terms['Search Term Classification'] = sp_sb_search_terms['Is_Branded'].apply(
                                        lambda x: "Branded" if x else "Non-Branded"
                                    )
                                    
                                    # Filter for contradicting classifications
                                    contradicting_search_terms = sp_sb_search_terms[
                                        (sp_sb_search_terms['Campaign Classification'] != sp_sb_search_terms['Search Term Classification']) & 
                                        (sp_sb_search_terms['Campaign Classification'] != "Unknown")
                                    ].copy()
                                    
                                    if not contradicting_search_terms.empty:
                                        # Calculate ACoS
                                        contradicting_search_terms['ACoS'] = contradicting_search_terms.apply(
                                            lambda row: (row['Spend'] / row['Sales'] * 100) if pd.notna(row['Sales']) and row['Sales'] > 0 else 0,
                                            axis=1
                                        )
                                        
                                        # Rename 'Sales' to 'Ad Sales' for consistency
                                        if 'Ad Sales' not in contradicting_search_terms.columns and 'Sales' in contradicting_search_terms.columns:
                                            contradicting_search_terms = contradicting_search_terms.rename(columns={'Sales': 'Ad Sales'})
                                        
                                        # Create filters
                                        col1, col2, col3 = st.columns(3)
                                        with col1:
                                            campaign_filter_st = st.text_input("Filter by Campaign", "", key="contradicting_campaign_st_filter")
                                        with col2:
                                            search_term_filter_st = st.text_input("Filter by Search Term", "", key="contradicting_search_term_st_filter")
                                        with col3:
                                            match_type_options_st = ["All"] + sorted(contradicting_search_terms['Match Type'].dropna().unique().tolist()) if 'Match Type' in contradicting_search_terms.columns else ["All"]
                                            match_type_filter_st = st.selectbox(
                                                "Filter by Match Type",
                                                match_type_options_st,
                                                key="contradicting_match_type_st_filter"
                                            )
                                        
                                        # Apply filters
                                        filtered_search_terms = contradicting_search_terms.copy()
                                        if campaign_filter_st:
                                            filtered_search_terms = filtered_search_terms[filtered_search_terms['Campaign'].astype(str).str.contains(campaign_filter_st, case=False, na=False)]
                                        if search_term_filter_st:
                                            filtered_search_terms = filtered_search_terms[filtered_search_terms['Search Term'].astype(str).str.contains(search_term_filter_st, case=False, na=False)]
                                        if match_type_filter_st != "All" and 'Match Type' in filtered_search_terms.columns:
                                            filtered_search_terms = filtered_search_terms[filtered_search_terms['Match Type'] == match_type_filter_st]
                                        
                                        if not filtered_search_terms.empty:
                                            # Store contradicting targets data in session state for use in Negatives tab
                                            st.session_state.contradicting_targets_data = contradicting_search_terms.copy()
                                            
                                            # Calculate summary metrics
                                            spend = pd.to_numeric(filtered_search_terms['Spend'].replace('[$,]', '', regex=True), errors='coerce').fillna(0).sum()
                                            sales = pd.to_numeric(filtered_search_terms['Ad Sales'].replace('[$,]', '', regex=True), errors='coerce').fillna(0).sum()
                                            acos = (spend / sales * 100) if sales > 0 else 0
                                            
                                            # Display summary cards
                                            kpi_cols = st.columns(3)
                                            kpi_cols[0].metric("TOTAL SPEND", f"${spend:,.2f}")
                                            kpi_cols[1].metric("AD SALES", f"${sales:,.2f}")
                                            kpi_cols[2].metric("ACoS", f"{acos:.2f}%")
                                            
                                            # Select and display columns
                                            display_cols = [
                                                'Campaign', 'Campaign Classification', 'Target', 'Match Type',
                                                'Search Term', 'Search Term Classification', 'Spend', 'Ad Sales', 'ACoS'
                                            ]
                                            
                                            # Ensure all required columns exist
                                            for col in display_cols:
                                                if col not in filtered_search_terms.columns:
                                                    filtered_search_terms[col] = "N/A"
                                            
                                            # Clean numeric columns
                                            for col in ['Spend', 'Ad Sales']:
                                                if col in filtered_search_terms.columns:
                                                    filtered_search_terms[col] = pd.to_numeric(filtered_search_terms[col].astype(str).str.replace(r'[^0-9.-]', '', regex=True), errors='coerce').fillna(0)
                                            
                                            display_df = filtered_search_terms[display_cols].copy()
                                            
                                            # Sort by Spend in descending order by default
                                            display_df = display_df.sort_values("Spend", ascending=False)
                                            
                                            # Create column configuration
                                            column_config = {
                                                'Campaign': st.column_config.TextColumn(label="Campaign Name"),
                                                'Campaign Classification': st.column_config.TextColumn(label="Campaign Classification"),
                                                'Target': st.column_config.TextColumn(label="Target"),
                                                'Match Type': st.column_config.TextColumn(label="Match Type"),
                                                'Search Term': st.column_config.TextColumn(label="Search Term"),
                                                'Search Term Classification': st.column_config.TextColumn(label="Search Term Classification"),
                                                'Spend': st.column_config.NumberColumn(label="Spend", format="$%.2f"),
                                                'Ad Sales': st.column_config.NumberColumn(label="Ad Sales", format="$%.2f"),
                                                'ACoS': st.column_config.NumberColumn(label="ACoS", format="%.2f%%")
                                            }
                                            
                                            # Display the table
                                            st.dataframe(
                                                display_df,
                                                column_config=column_config,
                                                use_container_width=True,
                                                hide_index=True
                                            )
                                        else:
                                            st.info("No contradicting search terms found with the current filters.")
                                    else:
                                        st.info("No contradicting search terms found between campaigns and their search term classifications.")
                                else:
                                    st.info("No search term data available for Sponsored Products/Sponsored Brands campaigns.")
                            else:
                                st.info("No search term data available for analysis.")
                        
                        # Tab 2: Targets
                        with contradicting_campaigns_tabs[1]:
                            if not all_targets_df.empty:
                                # Add campaign classification
                                all_targets_df['Campaign Classification'] = all_targets_df['Campaign'].apply(classify_campaign)
                                
                                # Add target classification based on whether it came from branded or non-branded dataframe
                                all_targets_df['Target Classification'] = 'Unknown'
                                if not branded_targets_df.empty:
                                    branded_mask = all_targets_df['Target'].isin(branded_targets_df['Target'])
                                    all_targets_df.loc[branded_mask, 'Target Classification'] = 'Branded'
                                if not non_branded_targets_df.empty:
                                    non_branded_mask = all_targets_df['Target'].isin(non_branded_targets_df['Target'])
                                    all_targets_df.loc[non_branded_mask, 'Target Classification'] = 'Non-Branded'
                                
                                # Filter for contradicting classifications
                                contradicting_targets = all_targets_df[
                                    (all_targets_df['Campaign Classification'] != all_targets_df['Target Classification']) & 
                                    (all_targets_df['Campaign Classification'] != "Unknown") &
                                    (all_targets_df['Target Classification'] != "Unknown")
                                ].copy()
                                
                                if not contradicting_targets.empty:
                                    # Calculate ACoS
                                    contradicting_targets['ACoS'] = contradicting_targets.apply(
                                        lambda row: (row['Spend'] / row['Ad Sales'] * 100) if pd.notna(row['Ad Sales']) and row['Ad Sales'] > 0 else 0,
                                        axis=1
                                    )
                                    
                                    # Create filters
                                    col1, col2, col3 = st.columns(3)
                                    with col1:
                                        campaign_filter_targets = st.text_input("Filter by Campaign", "", key="contradicting_campaign_targets_filter")
                                    with col2:
                                        target_filter_targets = st.text_input("Filter by Target", "", key="contradicting_target_targets_filter")
                                    with col3:
                                        match_type_options = ["All"] + sorted(contradicting_targets['Match Type'].dropna().unique().tolist()) if 'Match Type' in contradicting_targets.columns else ["All"]
                                        match_type_filter_targets = st.selectbox(
                                            "Filter by Match Type",
                                            match_type_options,
                                            key="contradicting_match_type_targets_filter"
                                        )
                                    
                                    # Apply filters
                                    filtered_targets = contradicting_targets.copy()
                                    if campaign_filter_targets:
                                        filtered_targets = filtered_targets[filtered_targets['Campaign'].astype(str).str.contains(campaign_filter_targets, case=False, na=False)]
                                    if target_filter_targets:
                                        filtered_targets = filtered_targets[filtered_targets['Target'].astype(str).str.contains(target_filter_targets, case=False, na=False)]
                                    if match_type_filter_targets != "All" and 'Match Type' in filtered_targets.columns:
                                        filtered_targets = filtered_targets[filtered_targets['Match Type'] == match_type_filter_targets]
                                    
                                    if not filtered_targets.empty:
                                        # Calculate summary metrics
                                        spend = pd.to_numeric(filtered_targets['Spend'].replace('[$,]', '', regex=True), errors='coerce').fillna(0).sum()
                                        sales = pd.to_numeric(filtered_targets['Ad Sales'].replace('[$,]', '', regex=True), errors='coerce').fillna(0).sum()
                                        acos = (spend / sales * 100) if sales > 0 else 0
                                        
                                        # Display summary cards
                                        kpi_cols = st.columns(3)
                                        kpi_cols[0].metric("TOTAL SPEND", f"${spend:,.2f}")
                                        kpi_cols[1].metric("AD SALES", f"${sales:,.2f}")
                                        kpi_cols[2].metric("ACoS", f"{acos:.2f}%")
                                        
                                        # Select and display columns
                                        display_cols = [
                                            'Campaign', 'Campaign Classification', 'Target', 'Match Type',
                                            'Target Classification', 'Spend', 'Ad Sales', 'ACoS'
                                        ]
                                        
                                        # Ensure all required columns exist
                                        for col in display_cols:
                                            if col not in filtered_targets.columns:
                                                filtered_targets[col] = "N/A"
                                        
                                        # Clean numeric columns
                                        for col in ['Spend', 'Ad Sales']:
                                            if col in filtered_targets.columns:
                                                filtered_targets[col] = pd.to_numeric(filtered_targets[col].astype(str).str.replace(r'[^0-9.-]', '', regex=True), errors='coerce').fillna(0)
                                        
                                        display_df = filtered_targets[display_cols].copy()
                                        
                                        # Sort by Spend in descending order by default
                                        display_df = display_df.sort_values("Spend", ascending=False)
                                        
                                        # Create column configuration
                                        column_config = {
                                            'Campaign': st.column_config.TextColumn(label="Campaign Name"),
                                            'Campaign Classification': st.column_config.TextColumn(label="Campaign Classification"),
                                            'Target': st.column_config.TextColumn(label="Target"),
                                            'Match Type': st.column_config.TextColumn(label="Match Type"),
                                            'Target Classification': st.column_config.TextColumn(label="Target Classification"),
                                            'Spend': st.column_config.NumberColumn(label="Spend", format="$%.2f"),
                                            'Ad Sales': st.column_config.NumberColumn(label="Ad Sales", format="$%.2f"),
                                            'ACoS': st.column_config.NumberColumn(label="ACoS", format="%.2f%%")
                                        }
                                        
                                        # Display the table
                                        st.dataframe(
                                            display_df,
                                            column_config=column_config,
                                            use_container_width=True,
                                            hide_index=True
                                        )
                                    else:
                                        st.info("No contradicting targets found with the current filters.")
                                else:
                                    st.info("No contradicting targets found between campaigns and their target classifications.")
                            else:
                                st.info("No targeting data available for analysis.")
                    else:
                        st.info("Please upload bulk advertising files to analyze contradicting targets in campaigns.")
                        
                    # ACoS Range Tables Section
                    st.markdown("<div style='margin-top:3rem;'></div>", unsafe_allow_html=True)
                    st.markdown("<div id='acos-range-tables' class='section-anchor'></div>", unsafe_allow_html=True)
                    st.markdown("<hr style='height:2px;border-width:0;color:gold;background-color:gold;margin-top:25px;margin-bottom:15px;margin-left:10px;margin-right:10px'>", unsafe_allow_html=True)
                    st.markdown("<span class='main-section-header'>ACoS Range Tables</span>", unsafe_allow_html=True)
                    
                    # Create main tabs for Target Performance and Search Term Performance views
                    acos_tables_main_tabs = st.tabs(["Target Performance", "Search Term Performance"])
                    
                    # TARGET PERFORMANCE TAB
                    with acos_tables_main_tabs[0]:
                        # Extract product groups for filtering from all target dataframes
                        product_groups = set()
                        for df in [st.session_state.all_targets_df, st.session_state.branded_targets_df, st.session_state.non_branded_targets_df]:
                            if not df.empty and 'Product Group' in df.columns:
                                groups = df['Product Group'].dropna().unique()
                                product_groups.update([g for g in groups if g])
                        
                        # Initialize session state variables for target product group filter
                        if 'target_product_group_filter' not in st.session_state:
                            st.session_state.target_product_group_filter = []
                        if 'target_filter_active' not in st.session_state:
                            st.session_state.target_filter_active = False
                        
                        # Define a callback function to update related session state variables
                        def update_target_filter_state():
                            # Update filter active state based on current selection
                            st.session_state.target_filter_active = len(st.session_state.target_product_group_filter) > 0
                        
                        # Add product group filter above the tabs
                        filter_col1, filter_col2 = st.columns([0.4, 0.6])
                        with filter_col1:
                            if product_groups:
                                product_groups = sorted(list(product_groups))
                                st.multiselect(
                                    "Filter by Product Group(s):",
                                    options=product_groups,
                                    key="target_product_group_filter",
                                    on_change=update_target_filter_state
                                )
                                # The callback will handle updating session state
                            else:
                                # Show disabled multiselect with informative label
                                st.multiselect(
                                    "Filter by Product Group(s) (Add in Client Settings):",
                                    options=[],
                                    disabled=True,
                                    key="target_product_group_filter_disabled"
                                )
                                # Reset filter state safely
                                if st.session_state.target_product_group_filter:
                                    st.session_state.target_product_group_filter = []
                                    st.session_state.target_filter_active = False
                        
                        # Create tabs for the Target tables
                        target_tab_labels = ["All Targets", "Branded Targets", "Non-Branded Targets"]
                        target_tabs = st.tabs(target_tab_labels)
                        
                        # Get the dataframes for each target tab
                        target_dfs = [st.session_state.all_targets_df, st.session_state.branded_targets_df, st.session_state.non_branded_targets_df]
                        
                        # Apply product group filter to all dataframes if active
                        if st.session_state.get('target_filter_active', False) and len(st.session_state.get('target_product_group_filter', [])) > 0:
                            filtered_target_dfs = []
                            for df in target_dfs:
                                if not df.empty and 'Product Group' in df.columns:
                                    filtered_df = df[df['Product Group'].isin(st.session_state.target_product_group_filter)]
                                    filtered_target_dfs.append(filtered_df)
                                else:
                                    filtered_target_dfs.append(df)
                            target_dfs = filtered_target_dfs
                            st.caption(f"Filtered by Product Group(s): {', '.join(st.session_state.target_product_group_filter)}")
                        
                        # Process each target tab
                        for i, (target_label, target_df) in enumerate(zip(target_tab_labels, target_dfs)):
                            with target_tabs[i]:
                                if not target_df.empty and 'ACoS' in target_df.columns and 'Spend' in target_df.columns:
                                    # Calculate ACoS range distribution
                                    # Get number of ranges from session state or use default
                                    range_key = f"num_acos_ranges_{target_label.replace(' ', '_')}"
                                    num_ranges = st.session_state.get(range_key, 5)
                                    
                                    # Calculate ACoS range distribution with user-selected number of ranges
                                    acos_distribution = calculate_acos_range_distribution(target_df, num_ranges=num_ranges)
                                    
                                    if not acos_distribution.empty:
                                        # Create a column layout for the range selector controls
                                        range_cols = st.columns([1, 2])
                                        
                                        # Add number of ranges input in the first column
                                        with range_cols[0]:
                                            # Initialize session state for number of ranges if not exists
                                            range_key = f"num_acos_ranges_{target_label.replace(' ', '_')}"
                                            if range_key not in st.session_state:
                                                st.session_state[range_key] = 5
                                                
                                            # Input for number of ACoS ranges with help text
                                            num_ranges = st.number_input(
                                                "Number of ACoS Ranges",
                                                min_value=3,
                                                max_value=20,
                                                value=st.session_state[range_key],
                                                step=1,
                                                help="Set how many ACoS ranges to display (100%+ and No Sales ranges will be added automatically)",
                                                key=f"num_ranges_input_{target_label.replace(' ', '_')}"
                                            )
                                            
                                            # Update session state when value changes
                                            if num_ranges != st.session_state[range_key]:
                                                st.session_state[range_key] = num_ranges
                                                # Use st.rerun() instead of experimental_rerun
                                                st.rerun()
                                        
                                        # Add ACoS range dropdown in the second column
                                        with range_cols[1]:
                                            # Create a dropdown selector for ACoS ranges
                                            acos_range = st.selectbox(
                                                "Select ACoS Range", 
                                                acos_distribution['ACoS Range'].tolist(),
                                                key=f"acos_range_selector_{target_label.replace(' ', '_')}"
                                            )
                                        
                                        # Filter targets for this ACoS range
                                        if acos_range == 'No Sales':
                                            # Create a safe numeric version of Ad Sales
                                            target_df_copy = target_df.copy()
                                            target_df_copy['Ad_Sales_numeric'] = safe_convert_to_numeric(target_df_copy['Ad Sales'])
                                            range_df = target_df_copy[target_df_copy['Ad_Sales_numeric'] == 0]
                                        else:
                                            # Extract the range bounds
                                            if acos_range == '100%+':
                                                lower = 100
                                                upper = float('inf')
                                            else:
                                                bounds = acos_range.replace('%', '').split('-')
                                                lower = float(bounds[0])
                                                upper = float(bounds[1])
                                        
                                            # Clean ACoS values and create numeric Ad Sales
                                            target_df_copy = target_df.copy()
                                            target_df_copy['ACoS_numeric'] = target_df_copy['ACoS'].apply(lambda x: clean_acos(x))
                                            target_df_copy['Ad_Sales_numeric'] = safe_convert_to_numeric(target_df_copy['Ad Sales'])
                                            
                                            # Filter by ACoS range
                                            range_df = target_df_copy[(target_df_copy['ACoS_numeric'] >= lower) & 
                                                                     (target_df_copy['ACoS_numeric'] < upper) & 
                                                                     (target_df_copy['Ad_Sales_numeric'] > 0)]
                                            
                                            if not range_df.empty:
                                                # Group by Target and Match Type
                                                if 'Match Type' not in range_df.columns:
                                                    range_df['Match Type'] = range_df.get('Target Type', 'Unknown')
                                                
                                                # Select and prepare columns for display
                                                display_cols = ['Target', 'Match Type', 'Spend', 'Ad Sales', 'ACoS', 'ROAS', 'CPC', 'CVR']
                                                
                                                # Ensure all required columns exist
                                                for col in display_cols:
                                                    if col not in range_df.columns:
                                                        range_df[col] = None
                                                
                                                # Group by Target and Match Type
                                                # Create numeric columns for aggregation
                                                range_df = range_df.copy()
                                                
                                                # Convert Spend and Ad Sales to numeric using the global helper function
                                                range_df['Spend_numeric'] = safe_convert_to_numeric(range_df['Spend'])
                                                range_df['Ad_Sales_numeric'] = safe_convert_to_numeric(range_df['Ad Sales'])
                                                
                                                # Group by Target and Match Type
                                                grouped_df = range_df.groupby(['Target', 'Match Type']).agg({
                                                    'Spend_numeric': 'sum',
                                                    'Ad_Sales_numeric': 'sum',
                                                    'Impressions': 'sum',
                                                    'Clicks': 'sum',
                                                    'Orders': 'sum'
                                                }).reset_index()
                                                
                                                # Calculate metrics with both display and numeric versions for proper sorting
                                                # ACoS calculation
                                                grouped_df['ACoS_sort'] = grouped_df.apply(
                                                    lambda row: (row['Spend_numeric'] / row['Ad_Sales_numeric'] * 100) if row['Ad_Sales_numeric'] > 0 else float('inf'), 
                                                    axis=1
                                                )
                                                grouped_df['ACoS'] = grouped_df['ACoS_sort'].apply(
                                                    lambda x: f"{x:.2f}%" if x != float('inf') else 'N/A'
                                                )
                                                
                                                # ROAS calculation
                                                grouped_df['ROAS_sort'] = grouped_df.apply(
                                                    lambda row: (row['Ad_Sales_numeric'] / row['Spend_numeric']) if row['Spend_numeric'] > 0 else 0, 
                                                    axis=1
                                                )
                                                grouped_df['ROAS'] = grouped_df['ROAS_sort'].apply(
                                                    lambda x: f"{x:.2f}" if x > 0 else 'N/A'
                                                )
                                                
                                                # CPC calculation
                                                grouped_df['CPC_sort'] = grouped_df.apply(
                                                    lambda row: (row['Spend_numeric'] / row['Clicks']) if row['Clicks'] > 0 else 0, 
                                                    axis=1
                                                )
                                                grouped_df['CPC'] = grouped_df['CPC_sort'].apply(
                                                    lambda x: f"${x:.2f}" if x > 0 else 'N/A'
                                                )
                                                
                                                # CVR calculation
                                                grouped_df['CVR_sort'] = grouped_df.apply(
                                                    lambda row: (row['Orders'] / row['Clicks'] * 100) if row['Clicks'] > 0 else 0, 
                                                    axis=1
                                                )
                                                grouped_df['CVR'] = grouped_df['CVR_sort'].apply(
                                                    lambda x: f"{x:.2f}%" if x > 0 else 'N/A'
                                                )
                                                
                                                # Format currency columns
                                                # Always format currency columns with commas for consistent display
                                                grouped_df['Spend'] = grouped_df['Spend_numeric'].apply(lambda x: f"${x:,.2f}")
                                                grouped_df['Ad Sales'] = grouped_df['Ad_Sales_numeric'].apply(lambda x: f"${x:,.2f}")
                                                
                                                # Create a dataframe with the display columns and the numeric values for sorting
                                                # First, create a copy of the numeric columns for sorting
                                                sort_df = grouped_df.copy()
                                                
                                                # Create a display dataframe with formatted columns for display
                                                display_df = sort_df.copy()
                                                
                                                # Format the display columns
                                                display_df['ACoS'] = sort_df['ACoS_sort'].apply(lambda x: f"{x:.2f}%" if x != float('inf') else 'N/A')
                                                display_df['ROAS'] = sort_df['ROAS_sort'].apply(lambda x: f"{x:.2f}" if x > 0 else 'N/A')
                                                display_df['CPC'] = sort_df['CPC_sort'].apply(lambda x: f"${x:.2f}" if x > 0 else 'N/A')
                                                display_df['CVR'] = sort_df['CVR_sort'].apply(lambda x: f"{x:.2f}%" if x > 0 else 'N/A')
                                                # Always format currency columns with commas for consistent display
                                                display_df['Spend'] = sort_df['Spend_numeric'].apply(lambda x: f"${x:,.2f}")
                                                display_df['Ad Sales'] = sort_df['Ad_Sales_numeric'].apply(lambda x: f"${x:,.2f}")
                                                
                                                # Select only the display columns
                                                display_df = display_df[display_cols]
                                                
                                                # For proper sorting, we'll use a different approach that works with older Streamlit versions
                                                # Create a DataFrame where the display columns have their original numeric values
                                                # but are formatted for display using the Streamlit column configuration
                                                
                                                # First, create a DataFrame with the numeric columns for sorting
                                                sort_df = pd.DataFrame({
                                                    "Target": grouped_df["Target"],
                                                    "Match Type": grouped_df["Match Type"],
                                                    "Spend": grouped_df["Spend_numeric"],
                                                    "Ad Sales": grouped_df["Ad_Sales_numeric"],
                                                    "ACoS": grouped_df["ACoS_sort"],
                                                    "ROAS": grouped_df["ROAS_sort"],
                                                    "CPC": grouped_df["CPC_sort"],
                                                    "CVR": grouped_df["CVR_sort"]
                                                })
                                                
                                                # Store the numeric values for sorting
                                                sort_df_numeric = sort_df.copy()
                                                
                                                # Format the display columns with dollar signs and commas
                                                formatted_df = sort_df.copy()
                                                formatted_df["Spend"] = formatted_df["Spend"].apply(lambda x: "${:,.2f}".format(x))
                                                formatted_df["Ad Sales"] = formatted_df["Ad Sales"].apply(lambda x: "${:,.2f}".format(x))
                                                
                                                # Create column configuration for display formatting
                                                column_config = {
                                                    "Target": st.column_config.TextColumn("Target"),
                                                    "Match Type": st.column_config.TextColumn("Match Type"),
                                                    "Spend": st.column_config.NumberColumn("Spend", help="Ad spend", format="dollar"),
                                                    "Ad Sales": st.column_config.NumberColumn("Ad Sales", help="Sales attributed to ads", format="dollar"),
                                                    "ACoS": st.column_config.NumberColumn("ACoS", help="Advertising Cost of Sales", format="%.2f%%"),
                                                    "ROAS": st.column_config.NumberColumn("ROAS", help="Return on Ad Spend", format="%.2f"),
                                                    "CPC": st.column_config.NumberColumn("CPC", help="Cost Per Click", format="$%.2f"),
                                                    "CVR": st.column_config.NumberColumn("CVR", help="Conversion Rate", format="%.2f%%")
                                                }
                                                
                                                # Sort by Ad Sales in descending order
                                                sort_df = sort_df.sort_values(by='Ad Sales', ascending=False)
                                                st.dataframe(sort_df, use_container_width=True, hide_index=True, column_config=column_config)
                                            else:
                                                st.info(f"No targets found in the {acos_range} range.")
                                    else:
                                        st.info(f"Unable to generate tables for {target_label}. Check if ACoS and Spend columns are available.")
                                else:
                                    st.info(f"No data available for {target_label}. Check if ACoS and Spend columns are available.")
                    
                    # SEARCH TERM PERFORMANCE TAB
                    with acos_tables_main_tabs[1]:
                        if 'bulk_data' in st.session_state:
                            # Get search term data with branding classification
                            search_term_df = get_search_term_data(st.session_state.bulk_data, st.session_state.client_config)
                            
                            if search_term_df.empty:
                                st.info("No search term data available. Please upload bulk advertising files with search term data.")
                            else:
                                # Extract product groups for filtering
                                product_groups = set()
                                if 'Product Group' in search_term_df.columns:
                                    groups = search_term_df['Product Group'].dropna().unique()
                                    product_groups.update([g for g in groups if g])
                                
                                # Initialize all session state variables if they don't exist
                                if 'search_term_product_group_filter1' not in st.session_state:
                                    st.session_state.search_term_product_group_filter1 = []
                                if 'search_term_filter_active1' not in st.session_state:
                                    st.session_state.search_term_filter_active1 = False
                                
                                # Define a callback function to update related session state variables
                                def update_search_term_filter1_state():
                                    # Update filter active state based on current selection
                                    st.session_state.search_term_filter_active1 = len(st.session_state.search_term_product_group_filter1) > 0
                                
                                # Add product group filter above the tabs
                                filter_col1, filter_col2 = st.columns([0.4, 0.6])
                                with filter_col1:
                                    if product_groups:
                                        product_groups = sorted(list(product_groups))
                                        st.multiselect(
                                            "Filter by Product Group(s):",
                                            options=product_groups,
                                            key="search_term_product_group_filter1",
                                            on_change=update_search_term_filter1_state
                                        )
                                        # The callback will handle updating session state
                                    else:
                                        # Show disabled multiselect with informative label
                                        st.multiselect(
                                            "Filter by Product Group(s) (Add in Client Settings):",
                                            options=[],
                                            disabled=True,
                                            key="search_term_product_group_filter1_disabled"
                                        )
                                        # Reset filter state safely
                                        if st.session_state.search_term_product_group_filter1:
                                            st.session_state.search_term_product_group_filter1 = []
                                            st.session_state.search_term_filter_active1 = False
                                        
                                # Apply product group filter if active
                                filtered_search_term_df = search_term_df.copy()
                                st.subheader("Contradicting Targets and Search Terms")
        
                                if st.session_state.get('search_term_filter_active1', False) and len(st.session_state.get('search_term_product_group_filter1', [])) > 0:
                                    if 'Product Group' in filtered_search_term_df.columns:
                                        filtered_search_term_df = filtered_search_term_df[filtered_search_term_df['Product Group'].isin(st.session_state.search_term_product_group_filter1)]
                                        st.caption(f"Filtered by Product Group(s): {', '.join(st.session_state.search_term_product_group_filter1)}")
                                    else:
                                        st.warning("Product Group column not found in data. Cannot apply filter.")
                                        st.session_state.debug_messages.append("[Search Term Performance] Product Group column not found in data.")
                                
                                # Create tabs for the Search Term tables
                                search_tab_labels = ["All Search Terms", "Branded Search Terms", "Non-Branded Search Terms"]
                                search_tabs = st.tabs(search_tab_labels)
                                
                                # Filter data for each tab
                                if 'Is_Branded' in filtered_search_term_df.columns:
                                    all_terms = filtered_search_term_df
                                    branded_terms = filtered_search_term_df[filtered_search_term_df['Is_Branded'] == True]
                                    non_branded_terms = filtered_search_term_df[filtered_search_term_df['Is_Branded'] == False]
                                else:
                                    all_terms = filtered_search_term_df
                                    branded_terms = pd.DataFrame()
                                    non_branded_terms = pd.DataFrame()
                                
                                term_dfs = [all_terms, branded_terms, non_branded_terms]

                                # Initialize session state for ACoS Range Spend Distribution if not exists
                                if "acos_range_spend_distribution_ranges" not in st.session_state:
                                    st.session_state.acos_range_spend_distribution_ranges = 10
                                
                                # Calculate consistent ACoS ranges for all search term tabs
                                search_consistent_ranges, search_consistent_labels = get_consistent_acos_ranges(term_dfs, st.session_state.acos_range_spend_distribution_ranges)
                                
                                # Process each search term tab
                                for i, (search_label, search_df) in enumerate(zip(search_tab_labels, term_dfs)):
                                    with search_tabs[i]:
                                        if not search_df.empty and 'ACoS' in search_df.columns and 'Spend' in search_df.columns:
                                            # Calculate ACoS range distribution
                                            # Get number of ranges from session state or use default
                                            range_key = f"num_acos_ranges_{search_label.replace(' ', '_')}"
                                            num_ranges = st.session_state.get(range_key, 5)
                                            
                                            # Calculate ACoS range distribution with user-selected number of ranges
                                            acos_distribution = calculate_acos_range_distribution(search_df, num_ranges=num_ranges)
                                            
                                            if not acos_distribution.empty:
                                                # Create a column layout for the range selector controls
                                                range_cols = st.columns([1, 2])
                                                
                                                # Add number of ranges input in the first column
                                                with range_cols[0]:
                                                    # Initialize session state for number of ranges if not exists
                                                    range_key = f"num_acos_ranges_{search_label.replace(' ', '_')}"
                                                    if range_key not in st.session_state:
                                                        st.session_state[range_key] = 5
                                                        
                                                    # Input for number of ACoS ranges with help text
                                                    num_ranges = st.number_input(
                                                        "Number of ACoS Ranges",
                                                        min_value=3,
                                                        max_value=20,
                                                        value=st.session_state[range_key],
                                                        step=1,
                                                        help="Set how many ACoS ranges to display (100%+ and No Sales ranges will be added automatically)",
                                                        key=f"num_ranges_input_{search_label.replace(' ', '_')}"
                                                    )
                                                    
                                                    # Update session state when value changes
                                                    if num_ranges != st.session_state[range_key]:
                                                        st.session_state[range_key] = num_ranges
                                                        # Use st.rerun() instead of experimental_rerun
                                                        st.rerun()
                                                
                                                # Add ACoS range dropdown in the second column
                                                with range_cols[1]:
                                                    # Create a dropdown selector for ACoS ranges
                                                    acos_range = st.selectbox(
                                                        "Select ACoS Range", 
                                                        acos_distribution['ACoS Range'].tolist(),
                                                        key=f"acos_range_selector_{search_label.replace(' ', '_')}"
                                                    )
                                                
                                                # Filter search terms for this ACoS range
                                                if acos_range == 'No Sales':
                                                    # Create a safe numeric version of Ad Sales
                                                    search_df_copy = search_df.copy()
                                                    search_df_copy['Ad_Sales_numeric'] = safe_convert_to_numeric(search_df_copy['Ad Sales'])
                                                    range_df = search_df_copy[search_df_copy['Ad_Sales_numeric'] == 0]
                                                else:
                                                    # Extract the range bounds
                                                    if acos_range == '100%+':
                                                        lower = 100
                                                        upper = float('inf')
                                                    else:
                                                        bounds = acos_range.replace('%', '').split('-')
                                                        lower = float(bounds[0])
                                                        upper = float(bounds[1])
                                                    
                                                    # Clean ACoS values and create numeric Ad Sales
                                                    search_df_copy = search_df.copy()
                                                    search_df_copy['ACoS_numeric'] = search_df_copy['ACoS'].apply(lambda x: clean_acos(x))
                                                    search_df_copy['Ad_Sales_numeric'] = safe_convert_to_numeric(search_df_copy['Ad Sales'])
                                                    
                                                    # Filter by ACoS range
                                                    range_df = search_df_copy[(search_df_copy['ACoS_numeric'] >= lower) & 
                                                                             (search_df_copy['ACoS_numeric'] < upper) & 
                                                                             (search_df_copy['Ad_Sales_numeric'] > 0)]
                                                
                                                if not range_df.empty:
                                                    # Group by Search Term, Target, and Match Type
                                                    if 'Match Type' not in range_df.columns:
                                                        range_df['Match Type'] = range_df.get('Target Type', 'Unknown')
                                                    
                                                    # Select and prepare columns for display
                                                    display_cols = ['Search Term', 'Target', 'Match Type', 'Spend', 'Ad Sales', 'ACoS', 'ROAS', 'CPC', 'CVR']
                                                    
                                                    # Ensure all required columns exist
                                                    for col in display_cols:
                                                        if col not in range_df.columns:
                                                            range_df[col] = None
                                                    
                                                    # Create numeric columns for aggregation
                                                    range_df = range_df.copy()
                                                    
                                                    # Convert Spend and Ad Sales to numeric using the global helper function
                                                    range_df['Spend_numeric'] = safe_convert_to_numeric(range_df['Spend'])
                                                    range_df['Ad_Sales_numeric'] = safe_convert_to_numeric(range_df['Ad Sales'])
                                                    
                                                    # Group by Search Term, Target and Match Type
                                                    grouped_df = range_df.groupby(['Search Term', 'Target', 'Match Type']).agg({
                                                        'Spend_numeric': 'sum',
                                                        'Ad_Sales_numeric': 'sum',
                                                        'Impressions': 'sum',
                                                        'Clicks': 'sum',
                                                        'Orders': 'sum'
                                                    }).reset_index()
                                                    
                                                    # Calculate metrics with both display and numeric versions for proper sorting
                                                    # ACoS calculation
                                                    grouped_df['ACoS_sort'] = grouped_df.apply(
                                                        lambda row: (row['Spend_numeric'] / row['Ad_Sales_numeric'] * 100) if row['Ad_Sales_numeric'] > 0 else float('inf'), 
                                                        axis=1
                                                    )
                                                    grouped_df['ACoS'] = grouped_df['ACoS_sort'].apply(
                                                        lambda x: f"{x:.2f}%" if x != float('inf') else 'N/A'
                                                    )
                                                    
                                                    # ROAS calculation
                                                    grouped_df['ROAS_sort'] = grouped_df.apply(
                                                        lambda row: (row['Ad_Sales_numeric'] / row['Spend_numeric']) if row['Spend_numeric'] > 0 else 0, 
                                                        axis=1
                                                    )
                                                    grouped_df['ROAS'] = grouped_df['ROAS_sort'].apply(
                                                        lambda x: f"{x:.2f}" if x > 0 else 'N/A'
                                                    )
                                                    
                                                    # CPC calculation
                                                    grouped_df['CPC_sort'] = grouped_df.apply(
                                                        lambda row: (row['Spend_numeric'] / row['Clicks']) if row['Clicks'] > 0 else 0, 
                                                        axis=1
                                                    )
                                                    grouped_df['CPC'] = grouped_df['CPC_sort'].apply(
                                                        lambda x: f"${x:.2f}" if x > 0 else 'N/A'
                                                    )
                                                    
                                                    # CVR calculation
                                                    grouped_df['CVR_sort'] = grouped_df.apply(
                                                        lambda row: (row['Orders'] / row['Clicks'] * 100) if row['Clicks'] > 0 else 0, 
                                                        axis=1
                                                    )
                                                    grouped_df['CVR'] = grouped_df['CVR_sort'].apply(
                                                        lambda x: f"{x:.2f}%" if x > 0 else 'N/A'
                                                    )
                                                    
                                                    # Format currency columns
                                                    grouped_df['Spend'] = grouped_df['Spend_numeric'].apply(lambda x: f"${x:,.2f}")
                                                    grouped_df['Ad Sales'] = grouped_df['Ad_Sales_numeric'].apply(lambda x: f"${x:,.2f}")
                                                    
                                                    # For proper sorting, we'll use a different approach that works with older Streamlit versions
                                                    # Create a DataFrame where the display columns have their original numeric values
                                                    # but are formatted for display using the Streamlit column configuration
                                                    
                                                    # First, create a DataFrame with the numeric columns for sorting
                                                    sort_df = pd.DataFrame({
                                                        "Search Term": grouped_df["Search Term"],
                                                        "Target": grouped_df["Target"],
                                                        "Match Type": grouped_df["Match Type"],
                                                        "Spend": grouped_df["Spend_numeric"],
                                                        "Ad Sales": grouped_df["Ad_Sales_numeric"],
                                                        "ACoS": grouped_df["ACoS_sort"],
                                                        "ROAS": grouped_df["ROAS_sort"],
                                                        "CPC": grouped_df["CPC_sort"],
                                                        "CVR": grouped_df["CVR_sort"]
                                                    })
                                                    
                                                    # Store the numeric values for sorting
                                                    sort_df_numeric = sort_df.copy()
                                                    
                                                    # Format the display columns with dollar signs and commas
                                                    formatted_df = sort_df.copy()
                                                    formatted_df["Spend"] = formatted_df["Spend"].apply(lambda x: "${:,.2f}".format(x))
                                                    formatted_df["Ad Sales"] = formatted_df["Ad Sales"].apply(lambda x: "${:,.2f}".format(x))
                                                    
                                                    # Create column configuration for display formatting
                                                    column_config = {
                                                        "Search Term": st.column_config.TextColumn("Search Term"),
                                                        "Target": st.column_config.TextColumn("Target"),
                                                        "Match Type": st.column_config.TextColumn("Match Type"),
                                                        "Spend": st.column_config.NumberColumn("Spend", help="Ad spend", format="dollar"),
                                                        "Ad Sales": st.column_config.NumberColumn("Ad Sales", help="Sales attributed to ads", format="dollar"),
                                                        "ACoS": st.column_config.NumberColumn("ACoS", help="Advertising Cost of Sales", format="%.2f%%"),
                                                        "ROAS": st.column_config.NumberColumn("ROAS", help="Return on Ad Spend", format="%.2f"),
                                                        "CPC": st.column_config.NumberColumn("CPC", help="Cost Per Click", format="$%.2f"),
                                                        "CVR": st.column_config.NumberColumn("CVR", help="Conversion Rate", format="%.2f%%")
                                                    }
                                                    
                                                    # Display the table with sorting enabled and hide index
                                                    # Sort by Ad Sales in descending order
                                                    sort_df = sort_df.sort_values(by='Ad Sales', ascending=False)
                                                    st.dataframe(sort_df, use_container_width=True, hide_index=True, column_config=column_config)
                                                else:
                                                    st.info(f"No search terms found in the {acos_range} range.")
                                            else:
                                                st.info(f"Unable to generate tables for {search_label}. Check if ACoS and Spend columns are available.")
                                        else:
                                            st.info(f"No data available for {search_label}. Check if ACoS and Spend columns are available.")
                        else:
                            st.info("No bulk advertising data available. Please upload bulk advertising files with search term data.")
                    
                    # ACoS Range Spend Distribution Section
                    st.markdown("<div style='margin-top:3rem;'></div>", unsafe_allow_html=True)
                    st.markdown("<div id='acos-range-spend-distribution' class='section-anchor'></div>", unsafe_allow_html=True)
                    st.markdown("<span class='main-section-header'>ACoS Range Spend Distribution</span>", unsafe_allow_html=True)
                    
                    # Add a dedicated ACoS Range number selector for this section
                    col1, col2 = st.columns([1, 3])
                    with col1:
                        # Initialize session state for ACoS Range Spend Distribution if not exists
                        if "acos_range_spend_distribution_ranges" not in st.session_state:
                            st.session_state.acos_range_spend_distribution_ranges = 10
                        
                        # Number input for ACoS Range Spend Distribution
                        num_ranges_value = st.number_input(
                            "Number of ACoS Ranges",
                            min_value=3,
                            max_value=20,
                            value=st.session_state.acos_range_spend_distribution_ranges,
                            step=1,
                            help="Set how many ACoS ranges to display in the distribution charts (100%+ and No Sales ranges will be added automatically)",
                            key="acos_range_spend_distribution_input"
                        )
                        
                        # Update session state when value changes
                        st.session_state.acos_range_spend_distribution_ranges = num_ranges_value
                    
                    # Create main tabs for Target Performance and Search Term Performance views
                    main_tabs = st.tabs(["Target Performance", "Search Term Performance"])
                    
                    # TARGET PERFORMANCE TAB
                    with main_tabs[0]:
                        # Add Targeting Performance Summary header
                        st.subheader("Targeting Performance Summary")
                        
                        # Extract product groups for filtering from targets data
                        target_product_groups = set()
                        
                        # Check all target dataframes for Product Group column
                        for df in [st.session_state.all_targets_df, st.session_state.branded_targets_df, st.session_state.non_branded_targets_df]:
                            if not df.empty and 'Product Group' in df.columns:
                                groups = df['Product Group'].dropna().unique()
                                target_product_groups.update([g for g in groups if g])
                        
                        # Initialize all session state variables for target product group filter if they don't exist
                        if 'target_product_group_filter_acos_range' not in st.session_state:
                            st.session_state.target_product_group_filter_acos_range = []
                        if 'target_filter_active_acos_range' not in st.session_state:
                            st.session_state.target_filter_active_acos_range = False
                        
                        # Define a callback function to update related session state variables
                        def update_target_filter_acos_range_state():
                            # Update filter active state based on current selection
                            st.session_state.target_filter_active_acos_range = len(st.session_state.target_product_group_filter_acos_range) > 0
                        
                        # Add product group filter above the tabs
                        filter_col1, filter_col2 = st.columns([0.4, 0.6])
                        with filter_col1:
                            if target_product_groups:
                                target_product_groups = sorted(list(target_product_groups))
                                st.multiselect(
                                    "Filter by Product Group(s):",
                                    options=target_product_groups,
                                    key="target_product_group_filter_acos_range",
                                    on_change=update_target_filter_acos_range_state
                                )
                                # The callback will handle updating session state
                            else:
                                # Show disabled multiselect with informative label
                                st.multiselect(
                                    "Filter by Product Group(s) (Add in Client Settings):",
                                    options=[],
                                    disabled=True,
                                    key="target_product_group_filter_acos_range_disabled"
                                )
                                # Reset filter state safely
                                if st.session_state.target_product_group_filter_acos_range:
                                    st.session_state.target_product_group_filter_acos_range = []
                                    st.session_state.target_filter_active_acos_range = False
                        
                        # Apply product group filter to each dataframe if active
                        filtered_all_targets_df = st.session_state.all_targets_df.copy()
                        filtered_branded_targets_df = st.session_state.branded_targets_df.copy()
                        filtered_non_branded_targets_df = st.session_state.non_branded_targets_df.copy()
                        
                        if st.session_state.get('target_filter_active_acos_range', False) and len(st.session_state.get('target_product_group_filter_acos_range', [])) > 0:
                            # Filter all targets
                            if 'Product Group' in filtered_all_targets_df.columns:
                                filtered_all_targets_df = filtered_all_targets_df[filtered_all_targets_df['Product Group'].isin(st.session_state.target_product_group_filter_acos_range)]
                            else:
                                st.warning("Product Group column not found in All Targets data. Cannot apply filter.")
                                st.session_state.debug_messages.append("[Target Performance] Product Group column not found in All Targets data.")
                            
                            # Filter branded targets
                            if not filtered_branded_targets_df.empty and 'Product Group' in filtered_branded_targets_df.columns:
                                filtered_branded_targets_df = filtered_branded_targets_df[filtered_branded_targets_df['Product Group'].isin(st.session_state.target_product_group_filter_acos_range)]
                            
                            # Filter non-branded targets
                            if not filtered_non_branded_targets_df.empty and 'Product Group' in filtered_non_branded_targets_df.columns:
                                filtered_non_branded_targets_df = filtered_non_branded_targets_df[filtered_non_branded_targets_df['Product Group'].isin(st.session_state.target_product_group_filter_acos_range)]
                            
                            # Show filter caption
                            st.caption(f"Filtered by Product Group(s): {', '.join(st.session_state.target_product_group_filter_acos_range)}")
                        
                        # Create tabs for the Target Spend Distribution Graph
                        graph_tab_labels = ["All Targets", "Branded Targets", "Non-Branded Targets"]
                        graph_tabs = st.tabs(graph_tab_labels)
                        
                        # Get the filtered dataframes for each target tab
                        graph_dfs = [filtered_all_targets_df, filtered_branded_targets_df, filtered_non_branded_targets_df]
                        
                        # Calculate consistent ACoS ranges for all target tabs
                        consistent_ranges, consistent_labels = get_consistent_acos_ranges(graph_dfs, st.session_state.acos_range_spend_distribution_ranges)
                        
                        # Process each target tab
                        for graph_tab, graph_label, graph_df in zip(graph_tabs, graph_tab_labels, graph_dfs):
                            with graph_tab:
                                if not graph_df.empty and 'ACoS' in graph_df.columns and 'Spend' in graph_df.columns:
                                    # Calculate ACoS range distribution for the graph using the dedicated selector value
                                    if consistent_ranges and consistent_labels:
                                        acos_distribution = calculate_acos_distribution_with_ranges(graph_df, consistent_ranges, consistent_labels)
                                    else:
                                        acos_distribution = calculate_acos_range_distribution(graph_df, num_ranges=st.session_state.acos_range_spend_distribution_ranges)
                                    
                                    
                                    # Prepare data for the graph
                                    if not acos_distribution.empty:
                                        # Convert spend values from formatted strings to numeric
                                        acos_distribution['Spend_numeric'] = acos_distribution['Spend'].str.replace('$', '').str.replace(',', '').astype(float)
                                        
                                        # Create a bar chart of spend by ACoS range
                                        # Create a custom color array for the bars (green to red)
                                        num_ranges = len(acos_distribution)
                                        # Create a list of indices for each ACoS range
                                        range_indices = list(range(num_ranges))
                                        
                                        # Create a bar chart of spend by ACoS range
                                        fig = px.bar(
                                            acos_distribution,
                                            x='ACoS Range',
                                            y='Spend_numeric',
                                            labels={'Spend_numeric': 'Spend', 'ACoS Range': 'ACoS Range'},
                                            title=f'Spend Distribution by ACoS Range - {graph_label}',
                                            color=range_indices,  # Use indices for coloring
                                            color_continuous_scale=['#2ecc71', '#27ae60', '#2ecc71', '#7dce70', '#a4d86e', '#d4e157', '#f1c40f', '#f39c12', '#f5b041', '#e67e22', '#d35400', '#ff0000'],
                                            height=500
                                        )
                                        
                                        # Customize the graph
                                        fig.update_layout(
                                            xaxis_title='ACoS Range',
                                            yaxis_title='Spend',
                                            plot_bgcolor='rgba(0,0,0,0)',
                                            paper_bgcolor='rgba(0,0,0,0)',
                                            font=dict(color='#FFFFFF'),
                                            coloraxis_showscale=False,
                                            margin=dict(l=40, r=40, t=50, b=40)
                                        )
                                        
                                        # Add spend values as text on top of bars
                                        fig.update_traces(
                                            text=acos_distribution['Spend'],
                                            textposition='outside'
                                        )
                                        
                                        # Display the graph
                                        st.plotly_chart(fig, use_container_width=True)
                                    else:
                                        st.info(f"Unable to generate spend distribution graph for {graph_label}. Check if ACoS and Spend columns are available.")
                                else:
                                    st.info(f"No data available for {graph_label} graph. Check if ACoS and Spend columns are available.")
                    
                    # SEARCH TERM PERFORMANCE TAB
                    with main_tabs[1]:
                        # Add Search Term Performance Summary header
                        st.subheader("Search Term Performance Summary")
                        
                        if 'bulk_data' in st.session_state:
                            # Get search term data with branding classification
                            search_term_df = get_search_term_data(st.session_state.bulk_data, st.session_state.client_config)
                            
                            if search_term_df.empty:
                                st.info("ðŸ“Š Search Term data will only reflect Sponsored Display targets.")
                                st.info("No search term data available. Please upload bulk advertising files with search term data.")
                            else:
                                # Extract product groups for filtering
                                product_groups = set()
                                if 'Product Group' in search_term_df.columns:
                                    groups = search_term_df['Product Group'].dropna().unique()
                                    product_groups.update([g for g in groups if g])
                                
                                # Initialize all session state variables if they don't exist
                                if 'search_term_product_group_filter2' not in st.session_state:
                                    st.session_state.search_term_product_group_filter2 = []
                                if 'search_term_filter_active2' not in st.session_state:
                                    st.session_state.search_term_filter_active2 = False
                                
                                # Define a callback function to update related session state variables
                                def update_search_term_filter2_state():
                                    # Update filter active state based on current selection
                                    st.session_state.search_term_filter_active2 = len(st.session_state.search_term_product_group_filter2) > 0
                                
                                # Add product group filter above the tabs
                                filter_col1, filter_col2 = st.columns([0.4, 0.6])
                                with filter_col1:
                                    if product_groups:
                                        product_groups = sorted(list(product_groups))
                                        st.multiselect(
                                            "Filter by Product Group(s):",
                                            options=product_groups,
                                            key="search_term_product_group_filter2",
                                            on_change=update_search_term_filter2_state
                                        )
                                        # The callback will handle updating session state
                                    else:
                                        # Show disabled multiselect with informative label
                                        st.multiselect(
                                            "Filter by Product Group(s) (Add in Client Settings):",
                                            options=[],
                                            disabled=True,
                                            key="search_term_product_group_filter2_disabled"
                                        )
                                        # Reset filter state safely
                                        if st.session_state.search_term_product_group_filter2:
                                            st.session_state.search_term_product_group_filter2 = []
                                            st.session_state.search_term_filter_active2 = False
                                        
                                # Apply product group filter if active
                                filtered_search_term_df = search_term_df.copy()
                                if st.session_state.get('search_term_filter_active2', False) and len(st.session_state.get('search_term_product_group_filter2', [])) > 0:
                                    if 'Product Group' in filtered_search_term_df.columns:
                                        filtered_search_term_df = filtered_search_term_df[filtered_search_term_df['Product Group'].isin(st.session_state.search_term_product_group_filter2)]
                                        st.caption(f"Filtered by Product Group(s): {', '.join(st.session_state.search_term_product_group_filter2)}")
                                    else:
                                        st.warning("Product Group column not found in data. Cannot apply filter.")
                                        st.session_state.debug_messages.append("[Search Term Performance] Product Group column not found in data.")
                                # Create tabs for the Search Term Spend Distribution Graph
                                graph_tab_labels = ["All Search Terms", "Branded Search Terms", "Non-Branded Search Terms"]
                                graph_tabs = st.tabs(graph_tab_labels)
                                
                                # Filter data for each tab
                                if 'Is_Branded' in filtered_search_term_df.columns:
                                    all_terms = filtered_search_term_df
                                    branded_terms = filtered_search_term_df[filtered_search_term_df['Is_Branded'] == True]
                                    non_branded_terms = filtered_search_term_df[filtered_search_term_df['Is_Branded'] == False]
                                else:
                                    all_terms = filtered_search_term_df
                                    branded_terms = pd.DataFrame()
                                    non_branded_terms = pd.DataFrame()
                                
                                term_dfs = [all_terms, branded_terms, non_branded_terms]
                                
                                # Calculate consistent ACoS ranges for all search term tabs
                                search_consistent_ranges, search_consistent_labels = get_consistent_acos_ranges(term_dfs, st.session_state.acos_range_spend_distribution_ranges)
                                
                                # Process each search term tab
                                for graph_tab, graph_label, graph_df in zip(graph_tabs, graph_tab_labels, term_dfs):
                                    with graph_tab:
                                        if not graph_df.empty and 'ACoS' in graph_df.columns and 'Spend' in graph_df.columns:
                                            # Calculate ACoS range distribution for the graph using the dedicated selector value
                                            if search_consistent_ranges and search_consistent_labels:
                                                acos_distribution = calculate_acos_distribution_with_ranges(graph_df, search_consistent_ranges, search_consistent_labels)
                                            else:
                                                acos_distribution = calculate_acos_range_distribution(graph_df, num_ranges=st.session_state.acos_range_spend_distribution_ranges)
                                            
                                            # Prepare data for the graph
                                            if not acos_distribution.empty:
                                                # Convert spend values from formatted strings to numeric
                                                acos_distribution['Spend_numeric'] = acos_distribution['Spend'].str.replace('$', '').str.replace(',', '').astype(float)
                                                
                                                # Create a bar chart of spend by ACoS range
                                                # Create a custom color array for the bars (green to red)
                                                num_ranges = len(acos_distribution)
                                                # Create a list of indices for each ACoS range
                                                range_indices = list(range(num_ranges))
                                                
                                                # Create a bar chart of spend by ACoS range
                                                fig = px.bar(
                                                    acos_distribution,
                                                    x='ACoS Range',
                                                    y='Spend_numeric',
                                                    labels={'Spend_numeric': 'Spend', 'ACoS Range': 'ACoS Range'},
                                                    title=f'Spend Distribution by ACoS Range - {graph_label}',
                                                    color=range_indices,  # Use indices for coloring
                                                    color_continuous_scale=['#2ecc71', '#27ae60', '#2ecc71', '#7dce70', '#a4d86e', '#d4e157', '#f1c40f', '#f39c12', '#f5b041', '#e67e22', '#d35400', '#ff0000'],
                                                    height=500
                                                )
                                                
                                                # Customize the graph
                                                fig.update_layout(
                                                    xaxis_title='ACoS Range',
                                                    yaxis_title='Spend',
                                                    plot_bgcolor='rgba(0,0,0,0)',
                                                    paper_bgcolor='rgba(0,0,0,0)',
                                                    font=dict(color='#FFFFFF'),
                                                    coloraxis_showscale=False,
                                                    margin=dict(l=40, r=40, t=50, b=40)
                                                )
                                                
                                                # Add spend values as text on top of bars
                                                fig.update_traces(
                                                    text=acos_distribution['Spend'],
                                                    textposition='outside'
                                                )
                                                
                                                # Display the graph
                                                st.plotly_chart(fig, use_container_width=True)
                                            else:
                                                st.info(f"Unable to generate spend distribution graph for {graph_label}. Check if ACoS and Spend columns are available.")
                                        else:
                                            st.info(f"No data available for {graph_label} graph. Check if ACoS and Spend columns are available.")
                        else:
                            st.info("No bulk advertising data available. Please upload bulk advertising files with search term data.")
                     
                    # The aggregation tables are now implemented in the sections below
                    # No need for placeholder tabs here

            except Exception as e:
                import traceback
                st.error(f"An error occurred calculating targeting performance: {e}\n{traceback.format_exc()}")
                if 'debug_messages' in st.session_state:
                    st.session_state.debug_messages.append(f"[Targeting Performance Error] {e}\n{traceback.format_exc()}")
                # Initialize empty DataFrames to avoid further errors
                st.session_state.branded_targets_df = pd.DataFrame()
                st.session_state.non_branded_targets_df = pd.DataFrame()
                st.session_state.all_targets_df = pd.DataFrame()

        # END Targeting Performance Section

        # Note: Search Term Performance is now fully integrated in the main dashboard tabs
        # and is no longer duplicated as a separate section

    # --- Performance by Product Group ---
# Only show on the Advertising Audit page
if st.session_state.current_page == "advertising_audit":

    # Moved to top level of file - see top of file for implementation



    def normalize_match_types(df):
        # Define match_type_col within the function to avoid global variable issues
        match_type_col = 'Match Type' if 'Match Type' in df.columns else 'Target Type'
    
        if match_type_col not in df.columns or df.empty:
            return df
        df = df.copy()
        # Replace 'Sponsored Brands Target' and 'Sponsored Display Target' with 'Product Target'
        # Normalize Category labels to 'Category Targeting' for consistency with downstream tables
        df[match_type_col] = df[match_type_col].apply(
            lambda x: (
                'Product Target' if x in ['Sponsored Brands Target', 'Sponsored Display Target']
                else ('Category Targeting' if str(x) in ['Category Target', 'Category'] else x)
            )
        )
        return df


    def kpi_agg(df, account_total_spend=0, account_total_sales=0):
        # Aggregates KPIs for a grouped df
        # Calculate total spend and sales for percentage calculations
    
        # Handle different possible sales column names
        sales_col = None
        # First check for Ad Sales, then Sales, then other variants
        if 'Ad Sales' in df.columns and df['Ad Sales'].notna().any():
            sales_col = 'Ad Sales'
        elif 'Sales' in df.columns and df['Sales'].notna().any():
            sales_col = 'Sales'
        elif 'Sales (Views & Clicks)' in df.columns and df['Sales (Views & Clicks)'].notna().any():
            sales_col = 'Sales (Views & Clicks)'
        else:
            # If no column with data is found, default to Ad Sales and it will be handled as 0
            sales_col = 'Ad Sales'
            if 'Ad Sales' not in df.columns:
                df['Ad Sales'] = 0
    
        # Ensure numeric values for calculations
        df_numeric = df.copy()
        for col in ['Spend', 'Impressions', 'Clicks', 'Orders']:
            if col in df_numeric.columns:
                df_numeric[col] = pd.to_numeric(df_numeric[col].astype(str).str.replace('[$,%]', '', regex=True), errors='coerce').fillna(0)
    
        if sales_col:
            df_numeric[sales_col] = pd.to_numeric(df_numeric[sales_col].astype(str).str.replace('[$,%]', '', regex=True), errors='coerce').fillna(0)
    
        # Calculate metrics
        group_spend = df_numeric['Spend'].sum() if 'Spend' in df_numeric.columns else 0
        group_sales = df_numeric[sales_col].sum() if sales_col else 0
    
        return pd.Series({
            'Spend': group_spend,
            'Ad Sales': group_sales,
            '% of Spend': (group_spend / account_total_spend) * 100 if account_total_spend > 0 else 0,
            '% of Ad Sales': (group_sales / account_total_sales) * 100 if account_total_sales > 0 else 0,
            'ACoS': (group_spend / group_sales) * 100 if group_sales > 0 else 0,
            'ROAS': (group_sales / group_spend) if group_spend > 0 else 0,
            'CPC': (group_spend / df_numeric['Clicks'].sum()) if 'Clicks' in df_numeric.columns and df_numeric['Clicks'].sum() > 0 else 0,
            'CVR': (df_numeric['Orders'].sum() / df_numeric['Clicks'].sum()) * 100 if 'Orders' in df_numeric.columns and 'Clicks' in df_numeric.columns and df_numeric['Clicks'].sum() > 0 else 0,
            'CTR': (df_numeric['Clicks'].sum() / df_numeric['Impressions'].sum()) * 100 if 'Clicks' in df_numeric.columns and 'Impressions' in df_numeric.columns and df_numeric['Impressions'].sum() > 0 else 0,
            'AOV': (group_sales / df_numeric['Orders'].sum()) if 'Orders' in df_numeric.columns and df_numeric['Orders'].sum() > 0 else 0,
            'CPA': (group_spend / df_numeric['Orders'].sum()) if 'Orders' in df_numeric.columns and df_numeric['Orders'].sum() > 0 else 0,
            'Impressions': df_numeric['Impressions'].sum() if 'Impressions' in df_numeric.columns else 0,
            'Clicks': df_numeric['Clicks'].sum() if 'Clicks' in df_numeric.columns else 0,
            'Orders': df_numeric['Orders'].sum() if 'Orders' in df_numeric.columns else 0,
            'Units Sold': df_numeric['Orders'].sum() if 'Orders' in df_numeric.columns else 0 # Alias for Orders
        })

    # Formatting helper for aggregation tables

    def ensure_acos_column(df):
        # Rename 'ACOS' to 'ACoS' if present and not already renamed
        if 'ACOS' in df.columns and 'ACoS' not in df.columns:
            df = df.rename(columns={'ACOS': 'ACoS'})
        # Ensure 'ACoS' is numeric before formatting
        if 'ACoS' in df.columns:
            df['ACoS'] = pd.to_numeric(df['ACoS'], errors='coerce')
        return df

    def format_table_with_styling(df, index_col=None, sort_by=None, sort_ascending=False):
        """
        Format a DataFrame with proper styling for currency, percentage, and numeric values.
        Also enables proper sorting capabilities.
    
        Args:
            df: DataFrame to format
            index_col: Column to use as the index/first column
            sort_by: Column to sort by
            sort_ascending: Whether to sort in ascending order
        
        Returns:
            Styled DataFrame ready for display
        """
        if df.empty:
            return df
    
        # Create a clean copy of the dataframe
        df_fmt = df.copy()
    
        # Ensure ACoS column is present and numeric before formatting
        df_fmt = ensure_acos_column(df_fmt)
    
        # Clean and convert percentage columns to numeric values
        percentage_cols = ['ACoS', 'CVR', 'CTR', '% of Spend', '% of Ad Sales', '% of Total Sales', 
                           'TACoS', 'Ad Sales % of Total']
        for col in percentage_cols:
            if col in df_fmt.columns:
                # Clean and convert to float
                def clean_percent(val):
                    if pd.isnull(val):
                        return np.nan
                    try:
                        # Remove percent, commas, whitespace, and any non-numeric prefix/suffix
                        if isinstance(val, str):
                            val = val.split(';')[0]  # Remove any CSS if present
                            val = val.replace('%', '').replace(',', '').replace('\n', '').strip()
                        return float(val)
                    except Exception:
                        return np.nan
                df_fmt[col] = df_fmt[col].apply(clean_percent)
    
        # Convert currency columns to numeric values
        currency_cols = ['Spend', 'Ad Sales', 'Sales', 'CPC', 'AOV', 'CPA', 'Total Sales']
        for col in currency_cols:
            if col in df_fmt.columns:
                df_fmt[col] = pd.to_numeric(df_fmt[col], errors='coerce')
    
        # Convert ratio columns to numeric
        ratio_cols = ['ROAS']
        for col in ratio_cols:
            if col in df_fmt.columns:
                df_fmt[col] = pd.to_numeric(df_fmt[col], errors='coerce')
    
        # Convert integer columns to numeric
        integer_cols = ['Impressions', 'Clicks', 'Orders', 'Units Sold', 'ASIN Count']
        for col in integer_cols:
            if col in df_fmt.columns:
                df_fmt[col] = pd.to_numeric(df_fmt[col], errors='coerce').fillna(0).astype(int)
    
        # Reorder columns if index_col is provided
        if index_col and index_col in df_fmt.columns:
            cols = [index_col] + [c for c in df_fmt.columns if c != index_col]
            df_fmt = df_fmt[cols]
    
        # Apply sorting if specified
        if sort_by and sort_by in df_fmt.columns:
            df_fmt = df_fmt.sort_values(by=sort_by, ascending=sort_ascending)
        elif 'Ad Sales' in df_fmt.columns:
            # Default sort by Ad Sales descending
            df_fmt = df_fmt.sort_values('Ad Sales', ascending=False)
    
        # Replace NaN values with 0 for display
        df_fmt = df_fmt.fillna(0)
    
        # Define formatting functions
        def currency_fmt(x):
            return f"${x:,.2f}" if pd.notnull(x) and x != 0 else "$0.00"
    
        def percent_fmt(x):
            return f"{x*100:.1f}%" if pd.notnull(x) and x != 0 else "0.0%"
    
        def ratio_fmt(x):
            return f"{x:.2f}" if pd.notnull(x) and x != 0 else "0.00"
    
        def count_fmt(x):
            return f"{int(x):,}" if pd.notnull(x) and x != 0 else "0"
    
        # Create formatting dictionary
        fmt_dict = {}
    
        # Add currency columns to formatting dictionary
        for col in currency_cols:
            if col in df_fmt.columns:
                fmt_dict[col] = currency_fmt
    
        # Add percentage columns to formatting dictionary
        for col in percentage_cols:
            if col in df_fmt.columns:
                fmt_dict[col] = percent_fmt
    
        # Add ratio columns to formatting dictionary
        for col in ratio_cols:
            if col in df_fmt.columns:
                fmt_dict[col] = ratio_fmt
    
        # Add integer columns to formatting dictionary
        for col in integer_cols:
            if col in df_fmt.columns:
                fmt_dict[col] = count_fmt
    
        # Apply formatting
        styled_df = df_fmt.style.format(fmt_dict)
    
        return styled_df

    def format_agg_table(df, index_col=None):
        # Ensure ACoS column is present and numeric before formatting
        df = ensure_acos_column(df)
        # Sort by Ad Sales in descending order
        if 'Ad Sales' in df.columns:
            df = df.sort_values('Ad Sales', ascending=False)
    
        # Create a clean copy of the dataframe with proper numeric types
        df_fmt = df.copy()
    
        # Clean and convert percentage columns to numeric values
        for col in ['ACoS', 'CVR', 'CTR', '% of Spend', '% of Ad Sales']:
            if col in df_fmt.columns:
                # Clean and convert to float
                def clean_percent(val):
                    if pd.isnull(val):
                        return np.nan
                    try:
                        # Remove percent, commas, whitespace, and any non-numeric prefix/suffix
                        if isinstance(val, str):
                            val = val.split(';')[0]  # Remove any CSS if present
                            val = val.replace('%', '').replace(',', '').replace('\n', '').strip()
                        return float(val)
                    except Exception:
                        return np.nan
                df_fmt[col] = df_fmt[col].apply(clean_percent)
    
        # Convert currency columns to numeric values
        for col in ['Spend', 'Ad Sales', 'CPC', 'AOV', 'CPA']:
            if col in df_fmt.columns:
                df_fmt[col] = pd.to_numeric(df_fmt[col], errors='coerce')
    
        # Convert ROAS to numeric
        if 'ROAS' in df_fmt.columns:
            df_fmt['ROAS'] = pd.to_numeric(df_fmt['ROAS'], errors='coerce')
    
        # Convert integer columns to numeric
        for col in ['Impressions', 'Clicks', 'Orders', 'Units Sold']:
            if col in df_fmt.columns:
                df_fmt[col] = pd.to_numeric(df_fmt[col], errors='coerce').fillna(0).astype(int)
    
        # Reorder columns if index_col is provided
        if index_col and index_col in df_fmt.columns:
            cols = [index_col] + [c for c in df_fmt.columns if c != index_col]
            df_fmt = df_fmt[cols]
    
        return df_fmt

    # Column config for aggregation tables (match main tables)
    agg_column_config = {
        # Currency columns with dollar formatting and comma separators
        "Spend": st.column_config.NumberColumn(
            "Spend",
            format="$,.2f",
            help="Total ad spend"
        ),
        "Ad Sales": st.column_config.NumberColumn(
            "Ad Sales",
            format="$,.2f",
            help="Sales attributed to advertising"
        ),
        "CPC": st.column_config.NumberColumn(
            "CPC",
            format="$,.2f",
            help="Cost per click"
        ),
        "AOV": st.column_config.NumberColumn(
            "AOV",
            format="$,.2f",
            help="Average order value"
        ),
        "CPA": st.column_config.NumberColumn(
            "CPA",
            format="$,.2f",
            help="Cost per acquisition"
        ),
    
        # Percentage columns
        "% of Total Spend": st.column_config.NumberColumn(
            "% of Total Spend",
            format="%.2f%%",
            help="Percentage of total advertising spend"
        ),
        "% of Total Ad Sales": st.column_config.NumberColumn(
            "% of Total Ad Sales",
            format="%.2f%%",
            help="Percentage of total advertising sales"
        ),
        "ACoS": st.column_config.NumberColumn(
            "ACoS",
            format="%.2f%%",
            help="Advertising cost of sales"
        ),
        "CVR": st.column_config.NumberColumn(
            "CVR",
            format="%.2f%%",
            help="Conversion rate"
        ),
        "CTR": st.column_config.NumberColumn(
            "CTR",
            format="%.2f%%",
            help="Click-through rate"
        ),
    
        # Decimal columns
        "ROAS": st.column_config.NumberColumn(
            "ROAS",
            format="%.2f",
            help="Return on ad spend"
        ),
    
        # Integer columns with comma formatting
        "Impressions": st.column_config.NumberColumn(
            "Impressions",
            format="%,d",
            help="Number of ad impressions"
        ),
        "Clicks": st.column_config.NumberColumn(
            "Clicks",
            format="%,d",
            help="Number of ad clicks"
        ),
        "Orders": st.column_config.NumberColumn(
            "Orders",
            format="%,d",
            help="Number of orders"
        ),
        "Units Sold": st.column_config.NumberColumn(
            "Units Sold",
            format="%,d",
            help="Number of units sold"
        ),
    }

    # Helper for Sponsored Display Product Target remarketing classification

    def split_sd_product_target_remarketing(df, branded_asins):
        """
        Categorize Sponsored Display rows based on Targeting Expression content.
        Categories:
        - Product Target: Contains ASIN but no views/purchases
        - Remarketing - Branded: Contains ASIN + views/purchases and ASIN is branded
        - Remarketing - Competitor: Contains ASIN + views/purchases and ASIN is not branded
        - Remarketing - Competitor: Contains views/purchases but no ASIN
        - Category Targeting: Contains neither ASIN nor views/purchases
        All checks are case-insensitive.
        """
        # Make a copy to avoid modifying the original dataframe
        df = df.copy()
    
        # Check if dataframe is empty
        if df.empty:
            if 'debug_messages' in st.session_state:
                st.session_state.debug_messages.append("[SD Categorization] Empty dataframe passed to function")
            return df
    
        # Add Product column if not present
        if 'Product' not in df.columns:
            df['Product'] = 'Sponsored Display'
            if 'debug_messages' in st.session_state:
                st.session_state.debug_messages.append("[SD Categorization] Added missing 'Product' column")
    
        # Add Match Type column if not present
        if 'Match Type' not in df.columns:
            df['Match Type'] = 'Product Target'
            if 'debug_messages' in st.session_state:
                st.session_state.debug_messages.append("[SD Categorization] Added missing 'Match Type' column")
    
        # Find the Targeting Expression column (case-insensitive)
        te_col = next((c for c in df.columns if c.strip().lower() == 'targeting expression'), None)
        if not te_col:
            if 'debug_messages' in st.session_state:
                st.session_state.debug_messages.append(f"[SD Categorization] No 'Targeting Expression' column found. Available columns: {list(df.columns)}")
            return df
    
        # Convert branded ASINs to uppercase for case-insensitive comparison
        branded_asins_upper = set(str(a).upper() for a in branded_asins) if branded_asins else set()
    
        # Debug information
        if 'debug_messages' in st.session_state:
            st.session_state.debug_messages.append(f"[SD Categorization] Found {len(branded_asins_upper)} branded ASINs for comparison")
    
        # Check for Entity column to identify audience targeting rows
        entity_col = None
        for possible_name in ['Entity', 'entity']:
            if possible_name in df.columns:
                entity_col = possible_name
                break
    
        # Count of rows processed in each category
        category_counts = {'Product Target': 0, 'Remarketing - Branded': 0, 'Remarketing - Competitor': 0, 'Category Targeting': 0}
    
        def classify_row(row):
            # Only process Sponsored Display rows
            if str(row['Product']).strip().lower() != 'sponsored display':
                return row['Match Type']
        
            # Get targeting expression and convert to lowercase for consistent checks
            expr = str(row[te_col]).lower() if pd.notna(row[te_col]) else ''
            expr_upper = str(row[te_col]).upper() if pd.notna(row[te_col]) else ''
        
            # HIGHEST PRIORITY: Check for exact-product targeting - this should always be Branded
            # Handle both direct 'exact-product' and nested formats like 'views=(exact-product lookback=7)'
            contains_exact_product = 'exact-product' in expr
        
            # Debug the expression to help diagnose issues
            if 'debug_messages' in st.session_state:
                st.session_state.debug_messages.append(f"[SD Row] Examining expression: '{expr}'")
            
            if contains_exact_product:
                category_counts['Remarketing - Branded'] += 1
                if 'debug_messages' in st.session_state:
                    st.session_state.debug_messages.append(f"[SD Row] HIGHEST PRIORITY: Classified as Remarketing - Branded due to exact-product in: {expr}")
                return 'Remarketing - Branded'
            
            # Check if this is an Audience Targeting row based on Entity column
            is_audience_targeting = False
            if entity_col and pd.notna(row.get(entity_col)):
                entity_value = str(row[entity_col]).lower()
                is_audience_targeting = 'audience' in entity_value or 'targeting' in entity_value
        
            # Check if expression contains an ASIN (B0...)
            contains_asin = bool(re.search(r'b0[0-9a-z]{8}', expr, re.IGNORECASE)) or 'asin=' in expr.lower()
            contains_views_purchases = 'views' in expr or 'purchases' in expr
            contains_category = 'category=' in expr
        
            # Extract ASIN from the expression if it exists
            # First try standard pattern
            asin_match = re.search(r'(b0[0-9a-z]{8})', expr_upper)
        
            # If not found, try the asin="BXXXXXXXX" format
            if not asin_match and 'asin=' in expr.lower():
                asin_match = re.search(r'asin="?(b0[0-9a-z]{8})"?', expr_upper, re.IGNORECASE)
        
            asin_value = asin_match.group(1) if asin_match else None
        
            # Skip debug message for ASIN extraction
            
            is_branded_asin = asin_value in branded_asins_upper if asin_value else False
        
            # Debug specific rows if needed
            if 'debug_messages' in st.session_state and (contains_asin or contains_views_purchases):
                st.session_state.debug_messages.append(f"[SD Row] ASIN: {asin_value}, Views/Purchases: {contains_views_purchases}, Branded: {is_branded_asin}, Expr: {expr[:50]}...")
        
            # Special handling for Audience Targeting rows
            if is_audience_targeting:
                if contains_views_purchases:
                    # If it contains views/purchases, it's remarketing
                    if contains_asin and is_branded_asin:
                        category_counts['Remarketing - Branded'] += 1
                        return 'Remarketing - Branded'
                    else:
                        category_counts['Remarketing - Competitor'] += 1
                        return 'Remarketing - Competitor'
                elif contains_category:
                    category_counts['Category Targeting'] += 1
                    return 'Category Targeting'
            # Enhanced detection for Auto targeting in SD campaigns
            if 'auto' in expr.lower() or 'targeting type' in str(row.get('Targeting Type', '')).lower():
                targeting_type_col = next((c for c in df.columns if c.strip().lower() == 'targeting type'), None)
                if targeting_type_col and pd.notna(row.get(targeting_type_col)) and str(row.get(targeting_type_col)).strip().lower() == 'auto':
                    category_counts['Product Target'] += 1
                    return 'Product Target'  # SD Auto campaigns are typically Product Targeting
        

        
            # Apply categorization logic for other rows
            if contains_asin:
                if contains_views_purchases:
                    if is_branded_asin:
                        category_counts['Remarketing - Branded'] += 1
                        return 'Remarketing - Branded'
                    else:
                        category_counts['Remarketing - Competitor'] += 1
                        return 'Remarketing - Competitor'
                else:
                    category_counts['Product Target'] += 1
                    return 'Product Target'
            elif contains_views_purchases:
                category_counts['Remarketing - Competitor'] += 1
                return 'Remarketing - Competitor'
            elif contains_category:
                category_counts['Category Targeting'] += 1
                return 'Category Targeting'
            else:
                category_counts['Product Target'] += 1
                return 'Product Target'  # Default to Product Target if no other criteria match
        df = df.copy()
        df['Match Type'] = df.apply(classify_row, axis=1)
        return df

    # (normalize_match_types is already at top-level, do not duplicate)

    # --- Targeting Performance Section logic continues ---
    # Get ACoS targets from config
    config = st.session_state.get('client_config', {})
    goals = config.get('goals', {}) if config else {}
    branded_target = goals.get('branded_acos')
    non_branded_target = goals.get('non_branded_acos')

    # Only show targeting tables on the Advertising Audit page
    if st.session_state.current_page == "advertising_audit":
        # Add debug message for this section if needed
        if 'debug_messages' in st.session_state:
            st.session_state.debug_messages.append("[INFO] Displaying Targeting by Match Type section")
    
        # --- Performance by Tactic Section ---
        st.markdown("<div id='performance-by-tactic-ad-type-match-type' class='section-anchor'></div>", unsafe_allow_html=True)
        st.markdown("<hr style='height:2px;border-width:0;color:gold;background-color:gold;margin-top:25px;margin-bottom:15px;margin-left:10px;margin-right:10px'>", unsafe_allow_html=True)
        st.markdown("<span class='main-section-header'>Performance by Tactic (Ad Type & Match Type)</span>", unsafe_allow_html=True)
    
        # --- 1. Match Type Aggregation ---
        st.markdown('#### Targeting by Match Type')

        # Use the targeting data stored in session state
        branded_targets_df = st.session_state.get('branded_targets_df', pd.DataFrame())
        non_branded_targets_df = st.session_state.get('non_branded_targets_df', pd.DataFrame())
        all_df = pd.concat([branded_targets_df, non_branded_targets_df], ignore_index=True)

        match_type_col = 'Match Type' if 'Match Type' in branded_targets_df.columns else 'Target Type'
        kpi_cols = ['Spend','Ad Sales','% of Spend','% of Ad Sales','ACoS','ROAS','CPC','CVR','CTR','AOV','CPA','Impressions','Clicks','Orders','Units Sold']

        # Get Branded ASINs from client config
        branded_asins = []
        config = st.session_state.get('client_config', {})
        goals = config.get('goals', {}) if config else {}
        branded_target = goals.get('branded_acos')
        non_branded_target = goals.get('non_branded_acos')
    
        if config:
            branded_asins = config.get('branded_asins_data', [])

        # Create the tabs
        match_tabs = st.tabs(["All", "Branded", "Non-Branded"])

        # All
        with match_tabs[0]:
            all_df = pd.concat([branded_targets_df, non_branded_targets_df], ignore_index=True)
            if not all_df.empty and match_type_col in all_df.columns:
                # Normalize match types before grouping
                normalized_df = normalize_match_types(all_df)
                # Split SD Product Target into Remarketing types
                normalized_df = split_sd_product_target_remarketing(normalized_df, branded_asins)
                # Calculate account totals for percentage calculations
                all_total_spend = normalized_df['Spend'].sum()
                all_total_sales = normalized_df['Sales'].sum()
                mt_all = normalized_df.groupby(match_type_col).apply(lambda x: kpi_agg(x, all_total_spend, all_total_sales), include_groups=False).reset_index()
                expected_match_types = [
                    "Exact", "Phrase", "Broad", "Auto", "Product Target", "Category Targeting",
                    "Remarketing - Branded", "Remarketing - Competitor"
                ]
                mt_all = mt_all.set_index(match_type_col)
                mt_all = mt_all.reindex(expected_match_types, fill_value=0).reset_index().rename(columns={"index": match_type_col})
                mt_all_fmt = format_agg_table(mt_all, index_col=match_type_col)
                # Apply ACoS styling to the All tab with account-wide target
                account_target = goals.get('account_wide_acos')
                # Calculate average ACoS if needed
                use_avg_acos = st.session_state.client_config.get('goals', {}).get('use_avg_acos_account', False)
                avg_acos = None
                if account_target is None and use_avg_acos:
                    # Calculate ACoS properly as (total spend / total sales) * 100
                    total_spend = mt_all['Spend'].sum() if 'Spend' in mt_all.columns else 0
                    total_sales = mt_all['Sales'].sum() if 'Sales' in mt_all.columns else 0
                
                    if total_sales > 0 and total_spend > 0:
                        avg_acos = (total_spend / total_sales) * 100
                        avg_acos = round(avg_acos, 1)
                    else:
                        avg_acos = None
            
                # Display the target being used
                target_to_use = None
                if account_target is not None:
                    target_to_use = float(account_target)
                elif avg_acos is not None and use_avg_acos:
                    target_to_use = float(avg_acos)
                else:
                    target_to_use = None
            
                try:
                    # Initialize sorting state if not exists
                    if 'all_targets_sort_by' not in st.session_state:
                        st.session_state.all_targets_sort_by = None
                    if 'all_targets_sort_ascending' not in st.session_state:
                        st.session_state.all_targets_sort_ascending = False
                
                    # Apply sorting if specified
                    if st.session_state.all_targets_sort_by is not None:
                        mt_all_fmt = mt_all_fmt.sort_values(
                            by=st.session_state.all_targets_sort_by,
                            ascending=st.session_state.all_targets_sort_ascending
                        )
                
                    # Display the table with ACoS styling if target is available
                    if target_to_use is not None:
                        # Log the target value for debugging
                        if 'debug_messages' in st.session_state:
                            st.session_state.debug_messages.append(f"[All Targets Display] Using ACoS target: {target_to_use}")
                        # Use the toggle value from client config for 'use_avg_as_fallback'
                        use_avg_fallback = st.session_state.client_config.get('goals', {}).get('use_avg_acos_account', False)
                    
                        # Handle sorting when clicked on column headers
                        clicked = style_acos(mt_all_fmt, target_to_use, column_config=agg_column_config, use_avg_as_fallback=use_avg_fallback)
                        if clicked and clicked.column:
                            # Toggle sorting direction if same column is clicked again
                            if st.session_state.all_targets_sort_by == clicked.column:
                                st.session_state.all_targets_sort_ascending = not st.session_state.all_targets_sort_ascending
                            else:
                                st.session_state.all_targets_sort_by = clicked.column
                                st.session_state.all_targets_sort_ascending = True
                            st.rerun()
                    else:  # target_to_use is None
                        if 'debug_messages' in st.session_state:
                            st.session_state.debug_messages.append(f"[All Targets Display - {match_type_col}] No ACoS target. Displaying table without ACoS-based colors.")
                    
                        # Create a custom column config based on the actual columns in the dataframe
                        custom_column_config = {}
                    
                        # Add Match Type column
                        custom_column_config[match_type_col] = st.column_config.TextColumn(label=match_type_col)
                    
                        # Add currency columns with dollar formatting
                        for col in ['Spend', 'Ad Sales', 'CPC', 'AOV', 'CPA']:
                            if col in mt_all_fmt.columns:
                                # Convert to numeric first
                                mt_all_fmt[col] = pd.to_numeric(mt_all_fmt[col].astype(str).str.replace(r'[^0-9.-]', '', regex=True), errors='coerce').fillna(0)
                                custom_column_config[col] = st.column_config.NumberColumn(label=col, format="dollar")
                    
                        # Add percentage columns
                        for col in ['ACoS', 'CVR', 'CTR', '% of Spend', '% of Ad Sales']:
                            if col in mt_all_fmt.columns:
                                # Convert to numeric first (as decimal, not percentage)
                                mt_all_fmt[col] = pd.to_numeric(mt_all_fmt[col].astype(str).str.replace(r'[^0-9.-]', '', regex=True), errors='coerce').fillna(0)
                                custom_column_config[col] = st.column_config.NumberColumn(label=col, format="%.2f%%")
                    
                        # Add ROAS as decimal
                        if 'ROAS' in mt_all_fmt.columns:
                            mt_all_fmt['ROAS'] = pd.to_numeric(mt_all_fmt['ROAS'].astype(str).str.replace(r'[^0-9.-]', '', regex=True), errors='coerce').fillna(0)
                            custom_column_config['ROAS'] = st.column_config.NumberColumn(label="ROAS", format="%.2f")
                    
                        # Add integer columns with comma formatting
                        for col in ['Impressions', 'Clicks', 'Orders', 'Units Sold']:
                            if col in mt_all_fmt.columns:
                                mt_all_fmt[col] = pd.to_numeric(mt_all_fmt[col].astype(str).str.replace(r'[^0-9.-]', '', regex=True), errors='coerce').fillna(0).astype(int)
                                custom_column_config[col] = st.column_config.NumberColumn(label=col, format="localized")
                    
                        # Apply sorting if specified
                        if st.session_state.get('all_targets_sort_by') is not None:
                            sort_by_col = st.session_state.all_targets_sort_by
                            sort_ascending = st.session_state.get('all_targets_sort_ascending', False)
                            if sort_by_col in mt_all_fmt.columns:
                                try:
                                    mt_all_fmt = mt_all_fmt.sort_values(by=sort_by_col, ascending=sort_ascending)
                                except Exception as e:
                                    if 'debug_messages' in st.session_state:
                                        st.session_state.debug_messages.append(f"[ERROR] Sorting 'All' tab failed for column '{sort_by_col}': {e}. Displaying unsorted.")
                    
                        # Create a copy for numeric processing for styling
                        numeric_df = mt_all_fmt.copy()
                    
                        # Convert to numeric for styling
                        for col in numeric_df.columns:
                            if col != match_type_col:
                                numeric_df[col] = pd.to_numeric(numeric_df[col].astype(str).str.replace('$', '').str.replace('%', '').str.replace(',', ''), errors='coerce')
                    
                        # Create formatting dictionary
                        fmt_dict = {}
                        for col in numeric_df.columns:
                            if col == match_type_col:
                                continue
                            elif col == 'ROAS':
                                fmt_dict[col] = lambda x: f"{x:.2f}"
                            elif col in ['ACoS', 'TACoS', 'CVR', 'CTR', '% of Spend', '% of Ad Sales']:
                                fmt_dict[col] = lambda x: f"{x:.2f}%"
                            elif col in ['CPC', 'AOV', 'CPA']:
                                fmt_dict[col] = lambda x: f"${x:.2f}"
                            elif col in ['Spend', 'Sales', 'Ad Sales']:
                                fmt_dict[col] = lambda x: f"${x:,.2f}"
                            else:
                                fmt_dict[col] = lambda x: f"{x:,.0f}" if pd.notnull(x) else "0"
                    
                        # Style the dataframe
                        styled_df = numeric_df.style.format(fmt_dict)
                    
                        # Apply color gradients to percentage columns
                        if '% of Spend' in numeric_df.columns:
                            styled_df = styled_df.apply(lambda x: [color_gradient_blue(v, 0, 100) 
                                                                if not pd.isna(v) else '' 
                                                                for v in x] if x.name == '% of Spend' else [''] * len(x), axis=0)
                        if '% of Ad Sales' in numeric_df.columns:
                            styled_df = styled_df.apply(lambda x: [color_gradient_green(v, 0, 100) 
                                                                if not pd.isna(v) else '' 
                                                                for v in x] if x.name == '% of Ad Sales' else [''] * len(x), axis=0)
                    
                        # Display the styled dataframe
                        st.dataframe(styled_df, use_container_width=True, hide_index=True)
                    
                        if clicked and clicked.column:
                            # Toggle sorting direction if same column is clicked again
                            if st.session_state.all_targets_sort_by == clicked.column:
                                st.session_state.all_targets_sort_ascending = not st.session_state.all_targets_sort_ascending
                            else:
                                st.session_state.all_targets_sort_by = clicked.column
                                st.session_state.all_targets_sort_ascending = True
                            st.rerun()
                
                    # Display row count
                    st.caption(f"Total Rows: {len(mt_all_fmt)}")
                    if 'debug_messages' in st.session_state:
                        st.session_state.debug_messages.append('[All Targets Display] Successfully displayed All Targeting table.')
                except Exception as e:
                    import traceback
                    tb = traceback.format_exc()
                    if 'debug_messages' in st.session_state:
                        st.session_state.debug_messages.append(f"[All Targets Display Error] {e}\n{tb}")
            else:
                st.info("No targeting data by Match Type.")

        # Branded
        with match_tabs[1]:
            if not branded_targets_df.empty and match_type_col in branded_targets_df.columns:
                # Normalize match types before grouping
                normalized_df = normalize_match_types(branded_targets_df)
                normalized_df = split_sd_product_target_remarketing(normalized_df, branded_asins)
                # Calculate account totals for percentage calculations
                b_total_spend = normalized_df['Spend'].sum()
                b_total_sales = normalized_df['Sales'].sum()
                mt_b = normalized_df.groupby(match_type_col).apply(lambda x: kpi_agg(x, b_total_spend, b_total_sales), include_groups=False).reset_index()
                expected_match_types = [
                    "Exact", "Phrase", "Broad", "Auto", "Product Target", "Category Targeting",
                    "Remarketing - Branded", "Remarketing - Competitor"
                ]
                mt_b = mt_b.set_index(match_type_col)
                mt_b = mt_b.reindex(expected_match_types, fill_value=0).reset_index().rename(columns={"index": match_type_col})
                mt_b_fmt = format_agg_table(mt_b, index_col=match_type_col)
                # Calculate average ACoS if needed
                use_avg_acos = st.session_state.client_config.get('goals', {}).get('use_avg_acos_branded', False)
                avg_acos = None
                if branded_target is None and use_avg_acos:
                    # Calculate ACoS properly as (total spend / total sales) * 100
                    total_spend = mt_b['Spend'].sum() if 'Spend' in mt_b.columns else 0
                    total_sales = mt_b['Sales'].sum() if 'Sales' in mt_b.columns else 0
                
                    if total_sales > 0 and total_spend > 0:
                        avg_acos = (total_spend / total_sales) * 100
                        avg_acos = round(avg_acos, 1)
                    else:
                        avg_acos = None
            
                # Display the target being used
                target_to_use = None
                if branded_target is not None:
                    target_to_use = float(branded_target)
                    pass
                elif avg_acos is not None and use_avg_acos:
                    target_to_use = float(avg_acos)
                    st.caption(f"Using Average ACoS as Target: {target_to_use}% (no explicit target set)")
                else:
                    pass
                
                # Initialize sorting state if not exists
                if 'branded_targets_sort_by' not in st.session_state:
                    st.session_state.branded_targets_sort_by = None
                if 'branded_targets_sort_ascending' not in st.session_state:
                    st.session_state.branded_targets_sort_ascending = False
            
                # Apply sorting if specified
                if st.session_state.branded_targets_sort_by is not None:
                    mt_b_fmt = mt_b_fmt.sort_values(
                        by=st.session_state.branded_targets_sort_by,
                        ascending=st.session_state.branded_targets_sort_ascending
                    )
            
                # Create a custom column config based on the actual columns in the dataframe
                custom_column_config = {}
            
                # Add Match Type column
                custom_column_config[match_type_col] = st.column_config.TextColumn(label=match_type_col)
            
                # Add currency columns with dollar formatting
                for col in ['Spend', 'Ad Sales', 'CPC', 'AOV', 'CPA']:
                    if col in mt_b_fmt.columns:
                        # Convert to numeric first
                        mt_b_fmt[col] = pd.to_numeric(mt_b_fmt[col].astype(str).str.replace(r'[^0-9.-]', '', regex=True), errors='coerce').fillna(0)
                        custom_column_config[col] = st.column_config.NumberColumn(label=col, format="dollar")
            
                # Add percentage columns
                for col in ['ACoS', 'CVR', 'CTR', '% of Spend', '% of Ad Sales']:
                    if col in mt_b_fmt.columns:
                        # Convert to numeric first (as decimal, not percentage)
                        mt_b_fmt[col] = pd.to_numeric(mt_b_fmt[col].astype(str).str.replace(r'[^0-9.-]', '', regex=True), errors='coerce').fillna(0)
                        custom_column_config[col] = st.column_config.NumberColumn(label=col, format="%.2f%%")
            
                # Add ROAS as decimal
                if 'ROAS' in mt_b_fmt.columns:
                    mt_b_fmt['ROAS'] = pd.to_numeric(mt_b_fmt['ROAS'].astype(str).str.replace(r'[^0-9.-]', '', regex=True), errors='coerce').fillna(0)
                    custom_column_config['ROAS'] = st.column_config.NumberColumn(label="ROAS", format="%.2f")
            
                # Add integer columns with comma formatting
                for col in ['Impressions', 'Clicks', 'Orders', 'Units Sold']:
                    if col in mt_b_fmt.columns:
                        mt_b_fmt[col] = pd.to_numeric(mt_b_fmt[col].astype(str).str.replace(r'[^0-9.-]', '', regex=True), errors='coerce').fillna(0).astype(int)
                        custom_column_config[col] = st.column_config.NumberColumn(label=col, format="localized")
            
                # Create a copy for numeric processing for styling
                numeric_df = mt_b_fmt.copy()
            
                # Convert to numeric for styling
                for col in numeric_df.columns:
                    if col != match_type_col:
                        numeric_df[col] = pd.to_numeric(numeric_df[col].astype(str).str.replace('$', '').str.replace('%', '').str.replace(',', ''), errors='coerce')
            
                # Create formatting dictionary
                fmt_dict = {}
                for col in numeric_df.columns:
                    if col == match_type_col:
                        continue
                    elif col == 'ROAS':
                        fmt_dict[col] = lambda x: f"{x:.2f}"
                    elif col in ['ACoS', 'TACoS', 'CVR', 'CTR', '% of Spend', '% of Ad Sales']:
                        fmt_dict[col] = lambda x: f"{x:.2f}%"
                    elif col in ['CPC', 'AOV', 'CPA']:
                        fmt_dict[col] = lambda x: f"${x:.2f}"
                    elif col in ['Spend', 'Sales', 'Ad Sales']:
                        fmt_dict[col] = lambda x: f"${x:,.2f}"
                    else:
                        fmt_dict[col] = lambda x: f"{x:,.0f}" if pd.notnull(x) else "0"
            
                # Style the dataframe
                styled_df = numeric_df.style.format(fmt_dict)
            
                # Apply color gradients to percentage columns
                if '% of Spend' in numeric_df.columns:
                    styled_df = styled_df.apply(lambda x: [color_gradient_blue(v, 0, 100) 
                                                        if not pd.isna(v) else '' 
                                                        for v in x] if x.name == '% of Spend' else [''] * len(x), axis=0)
                if '% of Ad Sales' in numeric_df.columns:
                    styled_df = styled_df.apply(lambda x: [color_gradient_green(v, 0, 100) 
                                                        if not pd.isna(v) else '' 
                                                        for v in x] if x.name == '% of Ad Sales' else [''] * len(x), axis=0)
            
                # Apply ACoS styling if target is available
                if target_to_use is not None:
                    # Log the target value for debugging
                    if 'debug_messages' in st.session_state:
                        st.session_state.debug_messages.append(f"[Branded Targets Display] Using ACoS target: {target_to_use}")
                    # Use the toggle value from client config for 'use_avg_as_fallback'
                    use_avg_fallback = st.session_state.client_config.get('goals', {}).get('use_avg_acos_branded', False)
                    styled_df = apply_acos_styling(styled_df, numeric_df, 'ACoS', target_to_use, use_avg_fallback)
            
                # Display the styled dataframe
                st.dataframe(styled_df, use_container_width=True, hide_index=True)
            
                # Display row count
                st.caption(f"Total Rows: {len(mt_b_fmt)}")

            else:
                st.info("No Branded targeting data by Match Type.")

        # Non-Branded
        with match_tabs[2]:
            if not non_branded_targets_df.empty and match_type_col in non_branded_targets_df.columns:
                # Normalize match types before grouping
                normalized_df = normalize_match_types(non_branded_targets_df)
                normalized_df = split_sd_product_target_remarketing(normalized_df, branded_asins)
                # Calculate account totals for percentage calculations
                nb_total_spend = normalized_df['Spend'].sum()
                nb_total_sales = normalized_df['Sales'].sum()
                mt_nb = normalized_df.groupby(match_type_col).apply(lambda x: kpi_agg(x, nb_total_spend, nb_total_sales), include_groups=False).reset_index()
                expected_match_types = [
                    "Exact", "Phrase", "Broad", "Auto", "Product Target", "Category Targeting",
                    "Remarketing - Branded", "Remarketing - Competitor"
                ]
                mt_nb = mt_nb.set_index(match_type_col)
                mt_nb = mt_nb.reindex(expected_match_types, fill_value=0).reset_index().rename(columns={"index": match_type_col})
                mt_nb_fmt = format_agg_table(mt_nb, index_col=match_type_col)
                # Calculate average ACoS if needed
                use_avg_acos = st.session_state.client_config.get('goals', {}).get('use_avg_acos_nonbranded', False)
                avg_acos = None
                if non_branded_target is None and use_avg_acos:
                    # Calculate ACoS properly as (total spend / total sales) * 100
                    total_spend = mt_nb['Spend'].sum() if 'Spend' in mt_nb.columns else 0
                    total_sales = mt_nb['Sales'].sum() if 'Sales' in mt_nb.columns else 0
                
                    if total_sales > 0 and total_spend > 0:
                        avg_acos = (total_spend / total_sales) * 100
                        avg_acos = round(avg_acos, 1)
                    else:
                        avg_acos = None
            
                # Display the target being used
                target_to_use = None
                if non_branded_target is not None:
                    target_to_use = float(non_branded_target)
                    pass
                elif avg_acos is not None and use_avg_acos:
                    target_to_use = float(avg_acos)
                    st.caption(f"Using Average ACoS as Target: {target_to_use}% (no explicit target set)")
                else:
                    pass
                
                # Initialize sorting state if not exists
                if 'non_branded_targets_sort_by' not in st.session_state:
                    st.session_state.non_branded_targets_sort_by = None
                if 'non_branded_targets_sort_ascending' not in st.session_state:
                    st.session_state.non_branded_targets_sort_ascending = False
            
                # Apply sorting if specified
                if st.session_state.non_branded_targets_sort_by is not None:
                    mt_nb_fmt = mt_nb_fmt.sort_values(
                        by=st.session_state.non_branded_targets_sort_by,
                        ascending=st.session_state.non_branded_targets_sort_ascending
                    )
                
                try:
                    # Create a custom column config based on the actual columns in the dataframe
                    custom_column_config = {}
                
                    # Add Match Type column
                    custom_column_config[match_type_col] = st.column_config.TextColumn(label=match_type_col)
                
                    # Add currency columns with dollar formatting
                    for col in ['Spend', 'Ad Sales', 'CPC', 'AOV', 'CPA']:
                        if col in mt_nb_fmt.columns:
                            # Convert to numeric first
                            mt_nb_fmt[col] = pd.to_numeric(mt_nb_fmt[col].astype(str).str.replace(r'[^0-9.-]', '', regex=True), errors='coerce').fillna(0)
                            custom_column_config[col] = st.column_config.NumberColumn(label=col, format="dollar")
                
                    # Add percentage columns
                    for col in ['ACoS', 'CVR', 'CTR', '% of Spend', '% of Ad Sales']:
                        if col in mt_nb_fmt.columns:
                            # Convert to numeric first (as decimal, not percentage)
                            mt_nb_fmt[col] = pd.to_numeric(mt_nb_fmt[col].astype(str).str.replace(r'[^0-9.-]', '', regex=True), errors='coerce').fillna(0)
                            custom_column_config[col] = st.column_config.NumberColumn(label=col, format="%.2f%%")
                
                    # Add ROAS as decimal
                    if 'ROAS' in mt_nb_fmt.columns:
                        mt_nb_fmt['ROAS'] = pd.to_numeric(mt_nb_fmt['ROAS'].astype(str).str.replace(r'[^0-9.-]', '', regex=True), errors='coerce').fillna(0)
                        custom_column_config['ROAS'] = st.column_config.NumberColumn(label="ROAS", format="%.2f")
                
                    # Add integer columns with comma formatting
                    for col in ['Impressions', 'Clicks', 'Orders', 'Units Sold']:
                        if col in mt_nb_fmt.columns:
                            mt_nb_fmt[col] = pd.to_numeric(mt_nb_fmt[col].astype(str).str.replace(r'[^0-9.-]', '', regex=True), errors='coerce').fillna(0).astype(int)
                            custom_column_config[col] = st.column_config.NumberColumn(label=col, format="localized")
                
                    # Create a copy for numeric processing for styling
                    numeric_df = mt_nb_fmt.copy()
                
                    # Convert to numeric for styling
                    for col in numeric_df.columns:
                        if col != match_type_col:
                            numeric_df[col] = pd.to_numeric(numeric_df[col].astype(str).str.replace('$', '').str.replace('%', '').str.replace(',', ''), errors='coerce')
                
                    # Create formatting dictionary
                    fmt_dict = {}
                    for col in numeric_df.columns:
                        if col == match_type_col:
                            continue
                        elif col == 'ROAS':
                            fmt_dict[col] = lambda x: f"{x:.2f}"
                        elif col in ['ACoS', 'TACoS', 'CVR', 'CTR', '% of Spend', '% of Ad Sales']:
                            fmt_dict[col] = lambda x: f"{x:.2f}%"
                        elif col in ['CPC', 'AOV', 'CPA']:
                            fmt_dict[col] = lambda x: f"${x:.2f}"
                        elif col in ['Spend', 'Sales', 'Ad Sales']:
                            fmt_dict[col] = lambda x: f"${x:,.2f}"
                        else:
                            fmt_dict[col] = lambda x: f"{x:,.0f}" if pd.notnull(x) else "0"
                
                    # Style the dataframe
                    styled_df = numeric_df.style.format(fmt_dict)
                
                    # Apply color gradients to percentage columns
                    if '% of Spend' in numeric_df.columns:
                        styled_df = styled_df.apply(lambda x: [color_gradient_blue(v, 0, 100) 
                                                            if not pd.isna(v) else '' 
                                                            for v in x] if x.name == '% of Spend' else [''] * len(x), axis=0)
                    if '% of Ad Sales' in numeric_df.columns:
                        styled_df = styled_df.apply(lambda x: [color_gradient_green(v, 0, 100) 
                                                            if not pd.isna(v) else '' 
                                                            for v in x] if x.name == '% of Ad Sales' else [''] * len(x), axis=0)
                
                    # Apply ACoS styling if target is available
                    if target_to_use is not None:
                        # Log the target value for debugging
                        if 'debug_messages' in st.session_state:
                            st.session_state.debug_messages.append(f"[Non-Branded Targets Display] Using ACoS target: {target_to_use}")
                        # Use the toggle value from client config for 'use_avg_as_fallback'
                        use_avg_fallback = st.session_state.client_config.get('goals', {}).get('use_avg_acos_nonbranded', False)
                        styled_df = apply_acos_styling(styled_df, numeric_df, 'ACoS', target_to_use, use_avg_fallback)
                
                    # Display the styled dataframe
                    st.dataframe(styled_df, use_container_width=True, hide_index=True)
                
                    # Display row count
                    st.caption(f"Total Rows: {len(mt_nb_fmt)}")
                    if 'debug_messages' in st.session_state:
                        st.session_state.debug_messages.append('[Non-Branded Targets Display] Successfully displayed Non-Branded Targeting table.')
                except Exception as e:
                    import traceback
                    tb = traceback.format_exc()
                    if 'debug_messages' in st.session_state:
                        st.session_state.debug_messages.append(f"[Non-Branded Targets Display Error] {e}\n{tb}")
            else:
                st.info("No Non-Branded targeting data by Match Type.")

            # --- Pie Charts for Targeting by Match Type (All, Non-Branded, Branded) ---
            def display_match_type_pies(df, label, debug_key):
                if not df.empty and match_type_col in df.columns:
                    # Removed header text as requested
                    total_spend = df['Spend'].sum()
                    total_sales = df['Ad Sales'].sum() if 'Ad Sales' in df.columns else df['Sales'].sum() if 'Sales' in df.columns else 0

                    # Remove match types with 0 value from pie charts for all tabs
                    filtered_df = df.copy()
                    # Remove rows where both Spend and Sales/Ad Sales are 0
                    sales_col = 'Ad Sales' if 'Ad Sales' in filtered_df.columns else 'Sales'
                    filtered_df = filtered_df[(filtered_df['Spend'] > 0) & (filtered_df[sales_col] > 0)]

                    # Create a consistent color mapping for match types
                    # This ensures the same match type always gets the same color across all charts
                    match_types = filtered_df[match_type_col].unique()

                    # Define a fixed color mapping for common match types
                    color_mapping = {
                        'Product Target': '#66C2A5',  # Teal
                        'Exact': '#FFCC66',          # Yellow
                        'Broad': '#FC8D62',          # Orange
                        'Auto': '#8DA0CB',           # Blue
                        'Phrase': '#E78AC3',         # Pink
                        'Category Targeting': '#A6D854',  # Green
                        'Remarketing - Branded': '#FFD92F',  # Gold
                        'Remarketing - Competitor': '#E5C494',  # Tan
                    }

                    # Create a color sequence based on the match types in this dataset
                    colors = []
                    for match_type in match_types:
                        if match_type in color_mapping:
                            colors.append(color_mapping[match_type])
                        else:
                            # Fallback to a default color if not in mapping
                            colors.append('#B3B3B3')  # Gray

                    col1, col2 = st.columns(2)
                    with col1:
                        fig_spend = px.pie(
                            filtered_df,
                            names=match_type_col,
                            values='Spend',
                            title='% of Total Spend by Match Type',
                            color=match_type_col,
                            color_discrete_map=color_mapping
                        )
                        fig_spend.update_traces(textinfo='percent+label', pull=[0.05]*len(filtered_df))
                        st.plotly_chart(fig_spend, use_container_width=True, key=f"{label}_spend_pie")
                    with col2:
                        fig_sales = px.pie(
                            filtered_df,
                            names=match_type_col,
                            values='Ad Sales' if 'Ad Sales' in filtered_df.columns else 'Sales',
                            title='% of Total Ad Sales by Match Type',
                            color=match_type_col,
                            color_discrete_map=color_mapping
                        )
                        fig_sales.update_traces(textinfo='percent+label', pull=[0.05]*len(filtered_df))
                        st.plotly_chart(fig_sales, use_container_width=True, key=f"{label}_sales_pie")
                    if 'debug_messages' in st.session_state:
                        st.session_state.debug_messages.append(f'[{debug_key}] Displayed pie charts for {label} Match Type.')

            # Pie charts for each tab
            with match_tabs[0]:  # All tab
                if 'mt_all' in locals():
                    display_match_type_pies(mt_all, 'All', 'All Match Type Charts')
            with match_tabs[1]:  # Branded tab
                if 'mt_b' in locals():
                    display_match_type_pies(mt_b, 'Branded', 'Branded Match Type Charts')
            with match_tabs[2]:  # Non-Branded tab
                if 'mt_nb' in locals():
                    display_match_type_pies(mt_nb, 'Non-Branded', 'Non-Branded Match Type Charts')

            # --- 2. Ad Type Aggregation ---
        st.markdown("<hr style='height:2px;border-width:0;color:gold;background-color:gold;margin-top:25px;margin-bottom:15px;margin-left:10px;margin-right:10px'>", unsafe_allow_html=True)
        st.markdown("<div style='margin-top:30px;'></div>", unsafe_allow_html=True)
        st.markdown('#### Targeting by Ad Type')
    
        # Extract product groups from campaign names
        product_groups_from_campaigns = set()
    
        # Initialize session state for product group filter if not exists
        if 'adtype_product_group_filter' not in st.session_state:
            st.session_state.adtype_product_group_filter = []
        if 'adtype_filter_active' not in st.session_state:
            st.session_state.adtype_filter_active = False
        
        # Extract product groups from campaign names in bulk data
        if 'bulk_data' in st.session_state and st.session_state.bulk_data is not None:
            for sheet_name, df in st.session_state.bulk_data.items():
                # Look for Campaign Name column in different formats
                campaign_col = None
                for col in df.columns:
                    if col in ['Campaign Name (Informational Only)', 'Campaign Name']:
                        campaign_col = col
                        break
            
                if campaign_col and not df.empty:
                    # Extract product groups from campaign names using client configuration
                    if st.session_state.get('client_config') and 'campaign_tags_data' in st.session_state.client_config:
                        for campaign_name in df[campaign_col].dropna().unique():
                            # Check if this campaign is in the client config
                            campaign_key = campaign_name.strip().upper()
                            if campaign_key in st.session_state.client_config['campaign_tags_data']:
                                product_group = st.session_state.client_config['campaign_tags_data'][campaign_key].get('tag_1', '') or 'Untagged Group'
                                if product_group and product_group.strip():
                                    product_groups_from_campaigns.add(product_group)
    
        # Add product groups from client configuration as well
        if st.session_state.get('client_config') and 'campaign_tags_data' in st.session_state.client_config:
            for campaign_info in st.session_state.client_config['campaign_tags_data'].values():
                product_group = campaign_info.get('tag_1', '') or 'Untagged Group'
                if product_group and product_group.strip():
                    product_groups_from_campaigns.add(product_group)
    
        # Sort product groups for consistent display
        product_groups_from_campaigns = sorted(list(product_groups_from_campaigns))
    
        # Create filter row with product group dropdown
        filter_col1, filter_col2 = st.columns([0.3, 0.7])
        with filter_col1:
            if product_groups_from_campaigns:
                selected_product_groups = st.multiselect(
                    "Filter by Product Group(s):",
                    options=product_groups_from_campaigns,
                    key="adtype_product_group_filter"
                )
                st.session_state.adtype_filter_active = len(selected_product_groups) > 0
    
        # Create tabs for All, Branded, Non-Branded
        adtype_tabs = st.tabs(["All", "Branded", "Non-Branded"])
    
        # Define the expected ad types for consistent display
        expected_ad_types = ["Sponsored Products", "Sponsored Brands", "Sponsored Display"]
    
        # Debug helper: count SD hint rows pre-inference and log sources
        def _sd_hint_counts(df: pd.DataFrame):
            hints = {}
            try:
                if df is None or df.empty:
                    return {'rows': 0}
                lower_map = {c.lower(): c for c in df.columns}
                hints['rows'] = len(df)
                if 'Product' in df.columns:
                    hints['product_sd_rows'] = int((df['Product'].astype(str) == 'Sponsored Display').sum())
                ss = lower_map.get('sheet source')
                if ss:
                    hints['sheet_source_display_rows'] = int(df[ss].astype(str).str.lower().str.contains('sponsored display').sum())
                ct = lower_map.get('campaign type')
                if ct:
                    hints['campaign_type_display_rows'] = int(df[ct].astype(str).str.lower().str.contains(r'display|\bsd\b').sum())
                # campaign columns
                camp = None
                for c in ['campaign', 'campaign name (informational only)', 'campaign name']:
                    if c in lower_map:
                        camp = lower_map[c]
                        break
                if camp:
                    s = df[camp].astype(str).str.lower()
                    hints['campaign_name_sd_hint_rows'] = int((s.str.contains(r'sponsored display|\bsd\b')).sum())
                # spend signal
                if 'Spend' in df.columns:
                    hints['total_spend'] = float(df['Spend'].fillna(0).sum())
            except Exception:
                pass
            return hints
    
        # Function to create and display the ad type table and charts for the selected tab
        def display_ad_type_tab(tab_index, tab_name, df):
            with adtype_tabs[tab_index]:
                if 'Product' in df.columns:
                    # Calculate totals for percentage calculations
                    total_spend = df['Spend'].sum() if 'Spend' in df.columns else 0
                    total_sales = df['Sales'].sum() if 'Sales' in df.columns else 0
                
                    # Aggregate data by Product
                    ad_df = df.groupby('Product').apply(lambda x: kpi_agg(x, total_spend, total_sales), include_groups=False).reset_index() if not df.empty else pd.DataFrame(columns=['Product'])
                
                    # Debug the dataframe structure
                    if 'debug_messages' in st.session_state:
                        st.session_state.debug_messages.append(f"[Ad Type Debug] DataFrame before reindexing: {ad_df.shape}, Columns: {list(ad_df.columns)}")
                        if not ad_df.empty and 'Product' in ad_df.columns:
                            st.session_state.debug_messages.append(f"[Ad Type Debug] Product values: {ad_df['Product'].tolist()}")
                            # SD presence and spend after aggregation for this tab
                            try:
                                sd_present = 'Sponsored Display' in ad_df['Product'].tolist()
                                sd_spend = float(ad_df.loc[ad_df['Product'] == 'Sponsored Display', 'Spend'].sum()) if 'Spend' in ad_df.columns else 0.0
                                st.session_state.debug_messages.append(f"[Ad Type Debug] {tab_name}: SD present after group? {sd_present}; SD aggregated spend=${sd_spend:,.2f}")
                            except Exception:
                                pass
                
                    # Create a fresh DataFrame with all expected ad types if empty
                    if ad_df.empty:
                        # Create a DataFrame with all expected ad types and zero values for all metrics
                        ad_df = pd.DataFrame({
                            'Product': expected_ad_types,
                            'Spend': [0, 0, 0],
                            'Ad Sales': [0, 0, 0],
                            '% of Spend': [0, 0, 0],
                            '% of Ad Sales': [0, 0, 0],
                            'ACoS': [0, 0, 0],
                            'ROAS': [0, 0, 0],
                            'CPC': [0, 0, 0],
                            'CVR': [0, 0, 0],
                            'CTR': [0, 0, 0],
                            'AOV': [0, 0, 0],
                            'CPA': [0, 0, 0],
                            'Impressions': [0, 0, 0],
                            'Clicks': [0, 0, 0],
                            'Orders': [0, 0, 0],
                            'Units Sold': [0, 0, 0]
                        })
                    else:
                        # Check if we have all expected ad types
                        existing_products = set(ad_df['Product'].tolist()) if 'Product' in ad_df.columns else set()
                        missing_products = set(expected_ad_types) - existing_products
                    
                        # If we're missing any ad types, add them with zero values
                        if missing_products:
                            # Get column names from existing DataFrame
                            columns = ad_df.columns.tolist()
                        
                            # Create rows for missing products
                            for product in missing_products:
                                new_row = {col: 0 for col in columns}
                                new_row['Product'] = product
                                ad_df = pd.concat([ad_df, pd.DataFrame([new_row])], ignore_index=True)
                    
                        # Sort to ensure consistent order
                        ad_df = ad_df.sort_values('Product', key=lambda x: pd.Categorical(
                            x, categories=expected_ad_types, ordered=True
                        )).reset_index(drop=True)
                    ad_df_fmt = format_agg_table(ad_df, index_col='Product')
                
                    # Create a custom column config based on the actual columns in the dataframe
                    custom_column_config = {}
                
                    # Add Product column
                    custom_column_config['Product'] = st.column_config.TextColumn(label="Product")
                
                    # Add currency columns with dollar formatting
                    for col in ['Spend', 'Ad Sales', 'CPC', 'AOV', 'CPA']:
                        if col in ad_df_fmt.columns:
                            # Convert to numeric first
                            ad_df_fmt[col] = pd.to_numeric(ad_df_fmt[col].astype(str).str.replace(r'[^0-9.-]', '', regex=True), errors='coerce').fillna(0)
                            custom_column_config[col] = st.column_config.NumberColumn(label=col, format="dollar")
                
                    # Add percentage columns
                    for col in ['ACoS', 'CVR', 'CTR', '% of Spend', '% of Ad Sales']:
                        if col in ad_df_fmt.columns:
                            # Convert to numeric first (as decimal, not percentage)
                            ad_df_fmt[col] = pd.to_numeric(ad_df_fmt[col].astype(str).str.replace(r'[^0-9.-]', '', regex=True), errors='coerce').fillna(0)
                            custom_column_config[col] = st.column_config.NumberColumn(label=col, format="%.2f%%")
                
                    # Add ROAS as decimal
                    if 'ROAS' in ad_df_fmt.columns:
                        ad_df_fmt['ROAS'] = pd.to_numeric(ad_df_fmt['ROAS'].astype(str).str.replace(r'[^0-9.-]', '', regex=True), errors='coerce').fillna(0)
                        custom_column_config['ROAS'] = st.column_config.NumberColumn(label="ROAS", format="%.2f")
                
                    # Add integer columns with comma formatting
                    for col in ['Impressions', 'Clicks', 'Orders', 'Units Sold']:
                        if col in ad_df_fmt.columns:
                            ad_df_fmt[col] = pd.to_numeric(ad_df_fmt[col].astype(str).str.replace(r'[^0-9.-]', '', regex=True), errors='coerce').fillna(0).astype(int)
                            custom_column_config[col] = st.column_config.NumberColumn(label=col, format="localized")
                
                    # ACoS target display
                    # Use the toggle value from client config for 'use_avg_as_fallback'
                    use_avg_fallback = goals.get('use_avg_acos_account', False)
                    account_target = goals.get('account_wide_acos')
                
                    # Create a copy for numeric processing
                    numeric_df = ad_df_fmt.copy()
                
                    # Convert formatted values to numeric for proper sorting
                    for col in numeric_df.columns:
                        if col not in ['Product']:
                            numeric_df[col] = pd.to_numeric(numeric_df[col].astype(str).str.replace('$', '').str.replace('%', '').str.replace(',', ''), errors='coerce')
                
                    # Create formatting dictionary
                    fmt_dict = {}
                    for col in numeric_df.columns:
                        if col == 'Product':
                            continue
                        elif col in ['ACoS', 'TACoS', 'CVR', 'CTR']:
                            fmt_dict[col] = lambda x: f"{x:.2f}%"
                        elif col == 'ROAS':
                            fmt_dict[col] = lambda x: f"{x:.2f}"
                        elif col in ['CPC', 'AOV', 'CPA']:
                            fmt_dict[col] = lambda x: f"${x:.2f}"
                        elif col in ['Spend', 'Sales', 'Ad Sales']:
                            fmt_dict[col] = lambda x: f"${x:,.2f}"
                        elif col in ['% of Spend', '% of Ad Sales']:
                            fmt_dict[col] = lambda x: f"{x:.2f}%"
                        else:
                            fmt_dict[col] = lambda x: f"{x:,.0f}" if pd.notnull(x) else "0"
                
                    # Style the dataframe
                    styled_df = numeric_df.style.format(fmt_dict)
                
                    # Apply color gradients to percentage columns
                    if '% of Spend' in numeric_df.columns:
                        styled_df = styled_df.apply(lambda x: [color_gradient_blue(v, 0, 100) 
                                                            if not pd.isna(v) else '' 
                                                            for v in x] if x.name == '% of Spend' else [''] * len(x), axis=0)
                    if '% of Ad Sales' in numeric_df.columns:
                        styled_df = styled_df.apply(lambda x: [color_gradient_green(v, 0, 100) 
                                                            if not pd.isna(v) else '' 
                                                            for v in x] if x.name == '% of Ad Sales' else [''] * len(x), axis=0)
                
                    # Apply ACoS styling if target is available
                    if account_target is not None:
                        # Apply ACoS styling to the ACoS column only
                        acos_col_idx = numeric_df.columns.get_loc('ACoS') if 'ACoS' in numeric_df.columns else None
                        if acos_col_idx is not None:
                            styled_df = apply_acos_styling(styled_df, numeric_df, 'ACoS', account_target, use_avg_fallback)
                
                    # Display the dataframe with sorting enabled
                    st.dataframe(styled_df, use_container_width=True, hide_index=True)
                
                    # Add bar charts
                    if not df.empty and 'Product' in df.columns:
                        # Add filter indicator if filter is active
                        if st.session_state.adtype_filter_active:
                            st.caption(f"Filtered by Product Group: {', '.join(st.session_state.adtype_product_group_filter)}")
                    
                        col1, col2 = st.columns(2)
                    
                        with col1:
                            # Spend chart
                            # Calculate percentages of total spend
                            total_spend = ad_df['Spend'].sum()
                            ad_df['Spend_Pct'] = ad_df['Spend'] / total_spend * 100 if total_spend > 0 else 0
                        
                            # Create custom text with dollar amount and percentage
                            ad_df['Spend_Text'] = ad_df.apply(
                                lambda row: f"${row['Spend']:,.2f} ({row['Spend_Pct']:.1f}%)", 
                                axis=1
                            )
                        
                            fig_spend = px.bar(
                                ad_df, 
                                x='Product', 
                                y='Spend',
                                title='Spend by Ad Type',
                                color_discrete_sequence=['#1f77b4'],  # Blue color for spend charts
                                text='Spend_Text'  # Use our custom text column
                            )
                            fig_spend.update_layout(
                                xaxis_title='Ad Type',
                                yaxis_title='Spend',
                                height=500,
                                margin=dict(l=50, r=50, t=80, b=80),
                                autosize=True
                            )
                            fig_spend.update_traces(textposition='outside')
                            # Ensure y-axis has enough room for labels
                            fig_spend.update_yaxes(automargin=True, tickformat='$,.0f')
                            st.plotly_chart(fig_spend, use_container_width=True, key=f"adtype_spend_chart_{tab_name}_{tab_index}")
                        
                        with col2:
                            # Ad Sales chart
                            # Calculate percentages of total sales
                            total_sales = ad_df['Ad Sales'].sum()
                            ad_df['Sales_Pct'] = ad_df['Ad Sales'] / total_sales * 100 if total_sales > 0 else 0
                        
                            # Create custom text with dollar amount and percentage
                            ad_df['Sales_Text'] = ad_df.apply(
                                lambda row: f"${row['Ad Sales']:,.2f} ({row['Sales_Pct']:.1f}%)", 
                                axis=1
                            )
                        
                            fig_sales = px.bar(
                                ad_df, 
                                x='Product', 
                                y='Ad Sales',
                                title='Ad Sales by Ad Type',
                                color_discrete_sequence=['#10b981'],  # Green color for sales charts
                                text='Sales_Text'  # Use our custom text column
                            )
                            fig_sales.update_layout(
                                xaxis_title='Ad Type',
                                yaxis_title='Sales ($)',
                                height=500,
                                margin=dict(l=50, r=50, t=80, b=80),
                                autosize=True
                            )
                            fig_sales.update_traces(textposition='outside')
                            fig_sales.update_yaxes(automargin=True, tickformat='$,.0f')
                            st.plotly_chart(fig_sales, use_container_width=True, key=f"adtype_sales_chart_{tab_name}_{tab_index}")
                else:
                    st.info(f"No {tab_name} targeting data by Ad Type (missing 'Product' column).")
    
        # Helper to infer ad type if not present
        def infer_ad_type(df):
            if 'Product' in df.columns and df['Product'].notna().any() and (df['Product'] != '').any():
                return df
            
            # Try to infer from Sheet Source or Campaign Type
            df = df.copy()
        
            # First, check if we can infer from Sheet Source
            if 'Sheet Source' in df.columns:
                def _adtype_from_sheet(row):
                    s = str(row['Sheet Source']).lower()
                    if 'sponsored products' in s:
                        return 'Sponsored Products'
                    elif 'sponsored brands' in s:
                        return 'Sponsored Brands'
                    elif 'sponsored display' in s:
                        return 'Sponsored Display'
                    return 'Unknown'
                df['Product'] = df.apply(_adtype_from_sheet, axis=1)
        
            # If we still don't have valid Product values, try to infer from Campaign Type
            if 'Product' not in df.columns or not df['Product'].notna().any() or (df['Product'] == '').all() or (df['Product'] == 'Unknown').all():
                if 'Campaign Type' in df.columns:
                    def _adtype_from_campaign_type(row):
                        ct = str(row['Campaign Type']).lower()
                        if 'product' in ct or 'sp' in ct or 'sponsored product' in ct:
                            return 'Sponsored Products'
                        elif 'brand' in ct or 'sb' in ct or 'sponsored brand' in ct:
                            return 'Sponsored Brands'
                        elif 'display' in ct or 'sd' in ct or 'sponsored display' in ct:
                            return 'Sponsored Display'
                        return 'Unknown'
                    df['Product'] = df.apply(_adtype_from_campaign_type, axis=1)
        
            # Last resort: try to infer from Campaign Name
            if 'Product' not in df.columns or not df['Product'].notna().any() or (df['Product'] == '').all() or (df['Product'] == 'Unknown').all():
                campaign_col = None
                for col in df.columns:
                    if col in ['Campaign Name (Informational Only)', 'Campaign Name', 'Campaign']:
                        campaign_col = col
                        break
                    
                if campaign_col:
                    def _adtype_from_campaign(row):
                        campaign = str(row[campaign_col]).lower()
                        if 'sp' in campaign.split() or 'sp|' in campaign or '| sp' in campaign or 'sponsored product' in campaign:
                            return 'Sponsored Products'
                        elif 'sb' in campaign.split() or 'sb|' in campaign or '| sb' in campaign or 'sponsored brand' in campaign:
                            return 'Sponsored Brands'
                        elif 'sd' in campaign.split() or 'sd|' in campaign or '| sd' in campaign or 'sponsored display' in campaign:
                            return 'Sponsored Display'
                        return 'Unknown'
                    df['Product'] = df.apply(_adtype_from_campaign, axis=1)
                
            if 'debug_messages' in st.session_state and not df.empty:
                product_counts = df['Product'].value_counts().to_dict()
                st.session_state.debug_messages.append(f"[Ad Type Inference] Product column distribution: {product_counts}")
            
            return df
        
        # Process data outside of tabs to avoid duplication
        # First normalize match types, then infer ad type
        if 'debug_messages' in st.session_state:
            try:
                st.session_state.debug_messages.append(f"[Ad Type Debug] Source sizes - branded_targets_df: {len(branded_targets_df)}; non_branded_targets_df: {len(non_branded_targets_df)}")
            except Exception:
                pass
        all_df = pd.concat([branded_targets_df, non_branded_targets_df], ignore_index=True)
        normalized_df = normalize_match_types(all_df)
        # Guarantee 'Product' column exists
        if 'Product' not in normalized_df.columns:
            normalized_df['Product'] = ''
        all_ad = infer_ad_type(normalized_df)
    
        # Create separate dataframes for branded and non-branded ads
        b_normalized_df = normalize_match_types(branded_targets_df)
        nb_normalized_df = normalize_match_types(non_branded_targets_df)
    
        # Ensure 'Product' column exists in both dataframes
        if 'Product' not in b_normalized_df.columns:
            b_normalized_df['Product'] = ''
        if 'Product' not in nb_normalized_df.columns:
            nb_normalized_df['Product'] = ''
        
        # Create ad type dataframes
        b_ad = infer_ad_type(b_normalized_df)
        nb_ad = infer_ad_type(nb_normalized_df)
    
        # Debug the product distribution
        if 'debug_messages' in st.session_state:
            st.session_state.debug_messages.append(f"[Ad Type Debug] All Ad Types: {all_ad['Product'].unique().tolist() if 'Product' in all_ad.columns else 'No Product column'}")
            st.session_state.debug_messages.append(f"[Ad Type Debug] Branded Ad Types: {b_ad['Product'].unique().tolist() if 'Product' in b_ad.columns else 'No Product column'}")
            st.session_state.debug_messages.append(f"[Ad Type Debug] Non-Branded Ad Types: {nb_ad['Product'].unique().tolist() if 'Product' in nb_ad.columns else 'No Product column'}")
            # SD hint counts pre-inference on normalized branded/non-branded
            try:
                st.session_state.debug_messages.append(f"[Ad Type Debug] SD hints (Branded pre-infer): {_sd_hint_counts(b_normalized_df)}")
                st.session_state.debug_messages.append(f"[Ad Type Debug] SD hints (Non-Branded pre-infer): {_sd_hint_counts(nb_normalized_df)}")
            except Exception:
                pass
            # SD rows and spend after inference
            try:
                all_sd_rows = int((all_ad['Product'] == 'Sponsored Display').sum()) if 'Product' in all_ad.columns else 0
                b_sd_rows = int((b_ad['Product'] == 'Sponsored Display').sum()) if 'Product' in b_ad.columns else 0
                nb_sd_rows = int((nb_ad['Product'] == 'Sponsored Display').sum()) if 'Product' in nb_ad.columns else 0
                st.session_state.debug_messages.append(f"[Ad Type Debug] SD rows post-infer -> All: {all_sd_rows}; Branded: {b_sd_rows}; Non-Branded: {nb_sd_rows}")
                if 'Spend' in all_ad.columns:
                    all_sd_spend = float(all_ad.loc[all_ad['Product'] == 'Sponsored Display', 'Spend'].fillna(0).sum())
                    b_sd_spend = float(b_ad.loc[b_ad['Product'] == 'Sponsored Display', 'Spend'].fillna(0).sum())
                    nb_sd_spend = float(nb_ad.loc[nb_ad['Product'] == 'Sponsored Display', 'Spend'].fillna(0).sum())
                    st.session_state.debug_messages.append(f"[Ad Type Debug] SD spend post-infer -> All: ${all_sd_spend:,.2f}; Branded: ${b_sd_spend:,.2f}; Non-Branded: ${nb_sd_spend:,.2f}")
            except Exception:
                pass
    
        # Force inclusion of all expected ad types in each dataframe
        # For all ads
        for ad_type in expected_ad_types:
            if 'Product' not in all_ad.columns or ad_type not in all_ad['Product'].unique():
                # Create a dummy row with the missing ad type
                dummy_row = {col: 0 for col in all_ad.columns} if not all_ad.empty else {'Spend': 0, 'Sales': 0}
                dummy_row['Product'] = ad_type
                all_ad = pd.concat([all_ad, pd.DataFrame([dummy_row])], ignore_index=True)
    
        # For branded ads
        for ad_type in expected_ad_types:
            if 'Product' not in b_ad.columns or ad_type not in b_ad['Product'].unique():
                # Create a dummy row with the missing ad type
                dummy_row = {col: 0 for col in b_ad.columns} if not b_ad.empty else {'Spend': 0, 'Sales': 0}
                dummy_row['Product'] = ad_type
                b_ad = pd.concat([b_ad, pd.DataFrame([dummy_row])], ignore_index=True)
    
        # For non-branded ads
        for ad_type in expected_ad_types:
            if 'Product' not in nb_ad.columns or ad_type not in nb_ad['Product'].unique():
                # Create a dummy row with the missing ad type
                dummy_row = {col: 0 for col in nb_ad.columns} if not nb_ad.empty else {'Spend': 0, 'Sales': 0}
                dummy_row['Product'] = ad_type
                nb_ad = pd.concat([nb_ad, pd.DataFrame([dummy_row])], ignore_index=True)
    
        # Apply product group filter if active
        if st.session_state.adtype_filter_active and len(st.session_state.adtype_product_group_filter) > 0:
            # Find campaign column
            campaign_col = None
            for col in all_ad.columns:
                if col in ['Campaign Name (Informational Only)', 'Campaign Name', 'Campaign']:
                    campaign_col = col
                    break
        
            if campaign_col:
                # Create a filtered version of the dataframe based on product groups
                filtered_all_ad = all_ad.copy()
                filtered_b_ad = b_ad.copy()
                filtered_nb_ad = nb_ad.copy()
            
                # Get list of campaigns that match the selected product groups
                matching_campaigns = set()
            
                for campaign_name, info in st.session_state.client_config.get('campaign_tags_data', {}).items():
                    product_group = info.get('tag_1', '') or 'Untagged Group'
                    if product_group in st.session_state.adtype_product_group_filter:
                        matching_campaigns.add(campaign_name.upper())
            
                # Apply filtering only if we found matching campaigns
                if matching_campaigns:
                    # Filter the All dataframe
                    if campaign_col in filtered_all_ad.columns:
                        filtered_all_ad = filtered_all_ad[filtered_all_ad[campaign_col].astype(str).str.upper().isin(matching_campaigns)]
                
                    # Filter the Branded dataframe
                    if campaign_col in filtered_b_ad.columns:
                        filtered_b_ad = filtered_b_ad[filtered_b_ad[campaign_col].astype(str).str.upper().isin(matching_campaigns)]
                
                    # Filter the Non-Branded dataframe
                    if campaign_col in filtered_nb_ad.columns:
                        filtered_nb_ad = filtered_nb_ad[filtered_nb_ad[campaign_col].astype(str).str.upper().isin(matching_campaigns)]
            
                # Use the filtered dataframes only if they're not empty
                all_ad = filtered_all_ad if not filtered_all_ad.empty else all_ad
                b_ad = filtered_b_ad if not filtered_b_ad.empty else b_ad
                nb_ad = filtered_nb_ad if not filtered_nb_ad.empty else nb_ad
            
                # Add debug message if no matching rows
                if all_ad.empty and 'debug_messages' in st.session_state:
                    st.session_state.debug_messages.append(f"[Product Group Filter] No campaigns found matching selected product groups: {', '.join(st.session_state.adtype_product_group_filter)}")
            
                # Additional debug: distribution and SD spend after PG filter
                if 'debug_messages' in st.session_state:
                    try:
                        st.session_state.debug_messages.append(f"[Ad Type Debug] Matching campaigns count: {len(matching_campaigns)}")
                        dist_all = all_ad['Product'].value_counts().to_dict() if 'Product' in all_ad.columns else {}
                        dist_b = b_ad['Product'].value_counts().to_dict() if 'Product' in b_ad.columns else {}
                        dist_nb = nb_ad['Product'].value_counts().to_dict() if 'Product' in nb_ad.columns else {}
                        st.session_state.debug_messages.append(f"[Ad Type Debug] Product distribution after PG filter - All: {dist_all}; Branded: {dist_b}; Non-Branded: {dist_nb}")
                        if 'Spend' in all_ad.columns:
                            all_sd_spend_f = float(all_ad.loc[all_ad['Product'] == 'Sponsored Display', 'Spend'].fillna(0).sum())
                            b_sd_spend_f = float(b_ad.loc[b_ad['Product'] == 'Sponsored Display', 'Spend'].fillna(0).sum())
                            nb_sd_spend_f = float(nb_ad.loc[nb_ad['Product'] == 'Sponsored Display', 'Spend'].fillna(0).sum())
                            st.session_state.debug_messages.append(f"[Ad Type Debug] SD spend after PG filter -> All: ${all_sd_spend_f:,.2f}; Branded: ${b_sd_spend_f:,.2f}; Non-Branded: ${nb_sd_spend_f:,.2f}")
                    except Exception:
                        pass
    
        # Display each tab with its corresponding data
        display_ad_type_tab(0, "All", all_ad)
        display_ad_type_tab(1, "Branded", b_ad)
        display_ad_type_tab(2, "Non-Branded", nb_ad)
    
        # Display filter indicator if filter is active
        if st.session_state.adtype_filter_active:
            st.caption(f"Filtered by Product Group: {', '.join(st.session_state.adtype_product_group_filter)}")
        # Function to create and display the ad type table and charts for the selected tab
        def display_ad_type_data(tab_name):
            # Get the appropriate dataframe based on the selected tab
            df = ad_type_data[tab_name]
        
            if 'Product' in df.columns:
                # Calculate totals for percentage calculations
                total_spend = df['Spend'].sum() if 'Spend' in df.columns else 0
                total_sales = df['Sales'].sum() if 'Sales' in df.columns else 0
            
                # Aggregate data by Product
                ad_df = df.groupby('Product').apply(lambda x: kpi_agg(x, total_spend, total_sales), include_groups=False).reset_index() if not df.empty else pd.DataFrame(columns=['Product'])
                ad_df = ad_df.set_index('Product') if not ad_df.empty else pd.DataFrame(index=expected_ad_types)
                ad_df = ad_df.reindex(expected_ad_types, fill_value=0).reset_index().rename(columns={"index": 'Product'})
                ad_df_fmt = format_agg_table(ad_df, index_col='Product')
            
                # Create a custom column config based on the actual columns in the dataframe
                custom_column_config = {}
            
                # Add Product column
                custom_column_config['Product'] = st.column_config.TextColumn(label="Product")
            
                # Add currency columns with dollar formatting
                for col in ['Spend', 'Ad Sales', 'CPC', 'AOV', 'CPA']:
                    if col in ad_all_fmt.columns:
                        # Convert to numeric first
                        ad_all_fmt[col] = pd.to_numeric(ad_all_fmt[col].astype(str).str.replace(r'[^0-9.-]', '', regex=True), errors='coerce').fillna(0)
                        custom_column_config[col] = st.column_config.NumberColumn(label=col, format="dollar")
            
                # Add percentage columns
                for col in ['ACoS', 'CVR', 'CTR', '% of Spend', '% of Ad Sales']:
                    if col in ad_all_fmt.columns:
                        # Convert to numeric first (as decimal, not percentage)
                        ad_all_fmt[col] = pd.to_numeric(ad_all_fmt[col].astype(str).str.replace(r'[^0-9.-]', '', regex=True), errors='coerce').fillna(0)
                        custom_column_config[col] = st.column_config.NumberColumn(label=col, format="%.2f%%")
            
                # Add ROAS as decimal
                if 'ROAS' in ad_all_fmt.columns:
                    ad_all_fmt['ROAS'] = pd.to_numeric(ad_all_fmt['ROAS'].astype(str).str.replace(r'[^0-9.-]', '', regex=True), errors='coerce').fillna(0)
                    custom_column_config['ROAS'] = st.column_config.NumberColumn(label="ROAS", format="%.2f")
            
                # Add integer columns with comma formatting
                for col in ['Impressions', 'Clicks', 'Orders', 'Units Sold']:
                    if col in ad_all_fmt.columns:
                        ad_all_fmt[col] = pd.to_numeric(ad_all_fmt[col].astype(str).str.replace(r'[^0-9.-]', '', regex=True), errors='coerce').fillna(0).astype(int)
                        custom_column_config[col] = st.column_config.NumberColumn(label=col, format="localized")
            
                # ACoS target display removed
                # Use the toggle value from client config for 'use_avg_as_fallback'
                use_avg_fallback = goals.get('use_avg_acos_account', False)
                account_target = goals.get('account_wide_acos')
            
                # Create a copy for numeric processing
                numeric_df = ad_all_fmt.copy()
            
                # Convert formatted values to numeric for proper sorting
                for col in numeric_df.columns:
                    if col not in ['Product']:
                        numeric_df[col] = pd.to_numeric(numeric_df[col].astype(str).str.replace('$', '').str.replace('%', '').str.replace(',', ''), errors='coerce')
            
                # Create formatting dictionary
                fmt_dict = {}
                for col in numeric_df.columns:
                    if col == 'Product':
                        continue
                    elif col in ['ACoS', 'TACoS', 'CVR', 'CTR']:
                        fmt_dict[col] = lambda x: f"{x:.2f}%"
                    elif col == 'ROAS':
                        fmt_dict[col] = lambda x: f"{x:.2f}"
                    elif col in ['CPC', 'AOV', 'CPA']:
                        fmt_dict[col] = lambda x: f"${x:.2f}"
                    elif col in ['Spend', 'Sales', 'Ad Sales']:
                        fmt_dict[col] = lambda x: f"${x:,.2f}"
                    elif col in ['% of Spend', '% of Ad Sales']:
                        fmt_dict[col] = lambda x: f"{x:.2f}%"
                    else:
                        fmt_dict[col] = lambda x: f"{x:,.0f}" if pd.notnull(x) else "0"
            
                # Style the dataframe
                styled_df = numeric_df.style.format(fmt_dict)
            
                # Apply color gradients to percentage columns
                if '% of Spend' in numeric_df.columns:
                    styled_df = styled_df.apply(lambda x: [color_gradient_blue(v, 0, 100) 
                                                        if not pd.isna(v) else '' 
                                                        for v in x] if x.name == '% of Spend' else [''] * len(x), axis=0)
                if '% of Ad Sales' in numeric_df.columns:
                    styled_df = styled_df.apply(lambda x: [color_gradient_green(v, 0, 100) 
                                                        if not pd.isna(v) else '' 
                                                        for v in x] if x.name == '% of Ad Sales' else [''] * len(x), axis=0)
            
                # Apply ACoS styling if target is available
                if account_target is not None:
                    # Apply ACoS styling to the ACoS column only
                    acos_col_idx = numeric_df.columns.get_loc('ACoS') if 'ACoS' in numeric_df.columns else None
                    if acos_col_idx is not None:
                        styled_df = apply_acos_styling(styled_df, numeric_df, 'ACoS', account_target, use_avg_fallback)
            
                # Display the dataframe with sorting enabled
                st.dataframe(styled_df, use_container_width=True, hide_index=True)
            else:
                st.info("No targeting data by Ad Type (missing 'Product' column).")
        
            # Add bar charts for All Ad Type
            if not all_ad.empty and 'Product' in all_ad.columns:
                # Add filter indicator if filter is active
                if st.session_state.adtype_filter_active:
                    st.caption(f"Filtered by Product Group: {st.session_state.adtype_product_group_filter}")
                # Removed header text as requested
                col1, col2 = st.columns(2)
            
                with col1:
                    # Spend chart
                    # Calculate percentages of total spend
                    total_spend = ad_all['Spend'].sum()
                    ad_all['Spend_Pct'] = ad_all['Spend'] / total_spend * 100 if total_spend > 0 else 0
                
                    # Create custom text with dollar amount and percentage
                    ad_all['Spend_Text'] = ad_all.apply(
                        lambda row: f"${row['Spend']:,.2f} ({row['Spend_Pct']:.1f}%)", 
                        axis=1
                    )
                
                    fig_spend = px.bar(
                        ad_all, 
                        x='Product', 
                        y='Spend',
                        title='Spend by Ad Type',
                        color_discrete_sequence=['#1f77b4'],  # Blue color for spend charts
                        text='Spend_Text'  # Use our custom text column
                    )
                    fig_spend.update_layout(
                        xaxis_title='Ad Type',
                        yaxis_title='Spend',
                        height=500,
                        margin=dict(l=50, r=50, t=80, b=80),
                        autosize=True
                    )
                    fig_spend.update_traces(textposition='outside')
                    # Ensure y-axis has enough room for labels
                    fig_spend.update_yaxes(automargin=True, tickformat='$,.0f')
                    st.plotly_chart(fig_spend, use_container_width=True)
                
                with col2:
                    # Ad Sales chart
                    # Calculate percentages of total sales
                    total_sales = ad_all['Ad Sales'].sum()
                    ad_all['Sales_Pct'] = ad_all['Ad Sales'] / total_sales * 100 if total_sales > 0 else 0
                
                    # Create custom text with dollar amount and percentage
                    ad_all['Sales_Text'] = ad_all.apply(
                        lambda row: f"${row['Ad Sales']:,.2f} ({row['Sales_Pct']:.1f}%)", 
                        axis=1
                    )
                
                    fig_sales = px.bar(
                        ad_all, 
                        x='Product', 
                        y='Ad Sales',
                        title='Ad Sales by Ad Type',
                        color_discrete_sequence=['#10b981'],  # Green color for sales charts
                        text='Sales_Text'  # Use our custom text column
                    )
                    fig_sales.update_layout(
                        xaxis_title='Ad Type',
                        yaxis_title='Sales ($)',
                        height=500,
                        margin=dict(l=50, r=50, t=80, b=80),
                        autosize=True
                    )
                    fig_sales.update_traces(textposition='outside')
                    fig_sales.update_yaxes(automargin=True, tickformat='$,.0f')
                    st.plotly_chart(fig_sales, use_container_width=True)
            
                # Calculate account totals for percentage calculations
                nb_ad_total_spend = nb_ad['Spend'].sum() if 'Spend' in nb_ad.columns else 0
                nb_ad_total_sales = nb_ad['Sales'].sum() if 'Sales' in nb_ad.columns else 0
                ad_nb = nb_ad.groupby('Product').apply(lambda x: kpi_agg(x, nb_ad_total_spend, nb_ad_total_sales), include_groups=False).reset_index() if not nb_ad.empty else pd.DataFrame(columns=['Product'])
                ad_nb = ad_nb.set_index('Product') if not ad_nb.empty else pd.DataFrame(index=expected_ad_types)
                ad_nb = ad_nb.reindex(expected_ad_types, fill_value=0).reset_index().rename(columns={"index": 'Product'})
                ad_nb_fmt = format_agg_table(ad_nb, index_col='Product')
            
                # Create a custom column config based on the actual columns in the dataframe
                custom_column_config = {}
            
                # Add Product column
                custom_column_config['Product'] = st.column_config.TextColumn(label="Product")
            
                # Add currency columns with dollar formatting
                for col in ['Spend', 'Ad Sales', 'CPC', 'AOV', 'CPA']:
                    if col in ad_nb_fmt.columns:
                        # Convert to numeric first
                        ad_nb_fmt[col] = pd.to_numeric(ad_nb_fmt[col].astype(str).str.replace(r'[^0-9.-]', '', regex=True), errors='coerce').fillna(0)
                        custom_column_config[col] = st.column_config.NumberColumn(label=col, format="dollar")
            
                # Add percentage columns
                for col in ['ACoS', 'CVR', 'CTR', '% of Spend', '% of Ad Sales']:
                    if col in ad_nb_fmt.columns:
                        # Convert to numeric first (as decimal, not percentage)
                        ad_nb_fmt[col] = pd.to_numeric(ad_nb_fmt[col].astype(str).str.replace(r'[^0-9.-]', '', regex=True), errors='coerce').fillna(0)
                        custom_column_config[col] = st.column_config.NumberColumn(label=col, format="%.2f%%")
            
                # Add ROAS as decimal
                if 'ROAS' in ad_nb_fmt.columns:
                    ad_nb_fmt['ROAS'] = pd.to_numeric(ad_nb_fmt['ROAS'].astype(str).str.replace(r'[^0-9.-]', '', regex=True), errors='coerce').fillna(0)
                    custom_column_config['ROAS'] = st.column_config.NumberColumn(label="ROAS", format="%.2f")
            
                # Add integer columns with comma formatting
                for col in ['Impressions', 'Clicks', 'Orders', 'Units Sold']:
                    if col in ad_nb_fmt.columns:
                        ad_nb_fmt[col] = pd.to_numeric(ad_nb_fmt[col].astype(str).str.replace(r'[^0-9.-]', '', regex=True), errors='coerce').fillna(0).astype(int)
                        custom_column_config[col] = st.column_config.NumberColumn(label=col, format="localized")
            
                # Create a copy for numeric processing
                numeric_df = ad_nb_fmt.copy()
            
                # Convert formatted values to numeric for proper sorting
                for col in numeric_df.columns:
                    if col not in ['Product']:
                        numeric_df[col] = pd.to_numeric(numeric_df[col].astype(str).str.replace('$', '').str.replace('%', '').str.replace(',', ''), errors='coerce')
            
                # Create formatting dictionary
                fmt_dict = {}
                for col in numeric_df.columns:
                    if col == 'Product':
                        continue
                    elif col == 'ROAS':
                        fmt_dict[col] = lambda x: f"{x:.2f}"
                    elif col in ['ACoS', 'TACoS', 'CVR', 'CTR', '% of Spend', '% of Ad Sales']:
                        fmt_dict[col] = lambda x: f"{x:.2f}%"
                    elif col in ['CPC', 'AOV', 'CPA']:
                        fmt_dict[col] = lambda x: f"${x:.2f}"
                    elif col in ['Spend', 'Sales', 'Ad Sales']:
                        fmt_dict[col] = lambda x: f"${x:,.2f}"
                    else:
                        fmt_dict[col] = lambda x: f"{x:,.0f}" if pd.notnull(x) else "0"
            
                # Style the dataframe
                styled_df = numeric_df.style.format(fmt_dict)
            
                # Apply color gradients to percentage columns
                if '% of Spend' in numeric_df.columns:
                    styled_df = styled_df.apply(lambda x: [color_gradient_blue(v, 0, 100) 
                                                        if not pd.isna(v) else '' 
                                                        for v in x] if x.name == '% of Spend' else [''] * len(x), axis=0)
                if '% of Ad Sales' in numeric_df.columns:
                    styled_df = styled_df.apply(lambda x: [color_gradient_green(v, 0, 100) 
                                                        if not pd.isna(v) else '' 
                                                        for v in x] if x.name == '% of Ad Sales' else [''] * len(x), axis=0)
            
                # Apply ACoS styling if target is available
                use_avg_fallback = goals.get('use_avg_acos_nonbranded', False)
                non_branded_target = goals.get('non_branded_acos')
            
                if non_branded_target is not None:
                    # Apply ACoS styling to the ACoS column only
                    acos_col_idx = numeric_df.columns.get_loc('ACoS') if 'ACoS' in numeric_df.columns else None
                    if acos_col_idx is not None:
                        styled_df = apply_acos_styling(styled_df, numeric_df, 'ACoS', non_branded_target, use_avg_fallback)
            
                # Display the dataframe with sorting enabled
                st.dataframe(styled_df, use_container_width=True, hide_index=True)
            else:
                st.info("No Non-Branded targeting data by Ad Type (missing 'Product' column).")
        
            # Add bar charts for Non-Branded Ad Type
            if not nb_ad.empty and 'Product' in nb_ad.columns:
                # Add filter indicator if filter is active
                if st.session_state.adtype_filter_active:
                    st.caption(f"Filtered by Product Group: {st.session_state.adtype_product_group_filter}")
                # Removed header text as requested
                col1, col2 = st.columns(2)
            
                with col1:
                    # Spend chart
                    # Calculate percentages of total spend
                    total_spend = ad_nb['Spend'].sum()
                    ad_nb['Spend_Pct'] = ad_nb['Spend'] / total_spend * 100 if total_spend > 0 else 0
                
                    # Create custom text with dollar amount and percentage
                    ad_nb['Spend_Text'] = ad_nb.apply(
                        lambda row: f"${row['Spend']:,.2f} ({row['Spend_Pct']:.1f}%)", 
                        axis=1
                    )
                
                    fig_spend = px.bar(
                        ad_nb, 
                        x='Product', 
                        y='Spend',
                        title='Spend by Ad Type',
                        color_discrete_sequence=['#1f77b4'],  # Blue color for spend charts
                        text='Spend_Text'  # Use our custom text column
                    )
                    fig_spend.update_layout(
                        xaxis_title='Ad Type',
                        yaxis_title='Spend ($)',
                        height=500,
                        margin=dict(l=50, r=50, t=80, b=80),
                        autosize=True
                    )
                    fig_spend.update_traces(textposition='outside')
                    # Ensure y-axis has enough room for labels
                    fig_spend.update_yaxes(automargin=True, tickformat='$,.0f')
                    st.plotly_chart(fig_spend, use_container_width=True)
                
                with col2:
                    # Ad Sales chart
                    # Calculate percentages of total sales
                    total_sales = ad_nb['Ad Sales'].sum()
                    ad_nb['Sales_Pct'] = ad_nb['Ad Sales'] / total_sales * 100 if total_sales > 0 else 0
                
                    # Create custom text with dollar amount and percentage
                    ad_nb['Sales_Text'] = ad_nb.apply(
                        lambda row: f"${row['Ad Sales']:,.2f} ({row['Sales_Pct']:.1f}%)", 
                        axis=1
                    )
                
                    fig_sales = px.bar(
                        ad_nb, 
                        x='Product', 
                        y='Ad Sales',
                        title='Ad Sales by Ad Type',
                        color_discrete_sequence=['#10b981'],  # Green color for sales charts
                        text='Sales_Text'  # Use our custom text column
                    )
                    fig_sales.update_layout(
                        xaxis_title='Ad Type',
                        yaxis_title='Sales ($)',
                        height=500,
                        margin=dict(l=50, r=50, t=80, b=80),
                        autosize=True
                    )
                    fig_sales.update_traces(textposition='outside')
                    # Ensure y-axis has enough room for labels
                    fig_sales.update_yaxes(automargin=True, tickformat='$,.0f')
                    st.plotly_chart(fig_sales, use_container_width=True)

        # --- 3. Combined Ad Type & Match Type Pivot Table ---
        st.markdown("<hr style='height:2px;border-width:0;color:gold;background-color:gold;margin-top:25px;margin-bottom:15px;margin-left:10px;margin-right:10px'>", unsafe_allow_html=True)
        st.markdown("<div style='margin-top:30px;'></div>", unsafe_allow_html=True)
        st.markdown('#### Targeting by Ad Type & Match Type')
    
        # Get product groups from client configuration for filtering
        product_groups = set()
        if st.session_state.get('client_config') and 'branded_asins_data' in st.session_state.client_config:
            for asin_info in st.session_state.client_config['branded_asins_data'].values():
                product_group = asin_info.get('product_group', '')
                if product_group and product_group.strip():
                    product_groups.add(product_group)
    
        # Add campaign product groups as well
        if st.session_state.get('client_config') and 'campaign_tags_data' in st.session_state.client_config:
            for campaign_info in st.session_state.client_config['campaign_tags_data'].values():
                product_group = campaign_info.get('tag_1', '') or 'Untagged Group'
                if product_group and product_group.strip():
                    product_groups.add(product_group)
    
        product_groups = sorted(list(product_groups))

        # Initialize session state variables if they don't exist
        if 'targeting_product_group_filter2' not in st.session_state:
            st.session_state.targeting_product_group_filter2 = []
        if 'targeting_filter_active2' not in st.session_state:
            st.session_state.targeting_filter_active2 = False
        
        # Initialize debug variables for product group filtering
        if 'debug_targeting_filter' not in st.session_state:
            st.session_state.debug_targeting_filter = {}
    
        # Store current filter state for debugging
        st.session_state.debug_targeting_filter['active'] = st.session_state.get('targeting_filter_active2', False)
        st.session_state.debug_targeting_filter['selected_groups'] = st.session_state.get('targeting_product_group_filter2', [])
    
        # Get product groups from campaign tagging data
        campaign_tagging_product_groups = set()
        if 'client_config' in st.session_state and 'campaign_tags_data' in st.session_state.client_config:
            for campaign_data in st.session_state.client_config['campaign_tags_data'].values():
                if 'tag_1' in campaign_data and campaign_data['tag_1']:  # tag_1 is the Product Group in Campaign Tagging
                    campaign_tagging_product_groups.add(campaign_data['tag_1'])
            # Add Untagged Group as a filter option for campaigns that could be untagged
            if st.session_state.client_config["campaign_tags_data"]:
                campaign_tagging_product_groups.add("Untagged Group")
    
        campaign_tagging_product_groups = sorted(list(campaign_tagging_product_groups))
    
        # Only show the filter if we have product groups from campaign tagging
        if campaign_tagging_product_groups:
            # Create the multiselect widget with a callback to update session state
            def update_product_group_filter():
                # Synchronize the widget value with the session state variable
                st.session_state.targeting_product_group_filter2 = st.session_state.targeting_product_group_filter2_widget
                # Update the filter active state
                st.session_state.targeting_filter_active2 = len(st.session_state.targeting_product_group_filter2) > 0
                # Add debug information
                if 'debug_targeting_filter' not in st.session_state:
                    st.session_state.debug_targeting_filter = {}
                st.session_state.debug_targeting_filter['widget_callback'] = {
                    'widget_value': st.session_state.targeting_product_group_filter2_widget,
                    'session_state_value': st.session_state.targeting_product_group_filter2,
                    'filter_active': st.session_state.targeting_filter_active2,
                    'timestamp': datetime.now().strftime('%H:%M:%S')
                }
        
            # Create the multiselect widget with the callback
            selected_groups = st.multiselect(
                "Filter by Product Group(s)",
                options=campaign_tagging_product_groups,
                key="targeting_product_group_filter2_widget",
                on_change=update_product_group_filter
            )
        
            # Ensure session state is updated immediately for first render
            if st.session_state.targeting_product_group_filter2 != st.session_state.targeting_product_group_filter2_widget:
                st.session_state.targeting_product_group_filter2 = st.session_state.targeting_product_group_filter2_widget
                st.session_state.targeting_filter_active2 = len(st.session_state.targeting_product_group_filter2) > 0
        else:
            # No product groups in campaign tagging, so hide the filter completely
            st.session_state.targeting_product_group_filter2 = []
            st.session_state.targeting_filter_active2 = False
    
        # Create tabs
        combined_tabs = st.tabs(["All", "Branded", "Non-Branded"])

        # Function to create combined Ad Type & Match Type column
        def create_combined_column(df):
            if df.empty:
                return df
            
            df = df.copy()
        
            # Define match_type_col within the function to avoid global variable issues
            match_type_col = 'Match Type' if 'Match Type' in df.columns else 'Target Type'
        
            # Ensure we have both columns
            if 'Product' not in df.columns:
                df = infer_ad_type(df)
        
            if match_type_col not in df.columns:
                # If we still don't have a match type column, add a default one
                df[match_type_col] = 'Unknown'
            
            # Add product group information if available
            # Only populate if there are actually product groups defined in Campaign Tagging
            if 'Campaign' in df.columns and st.session_state.get('client_config') and 'campaign_tags_data' in st.session_state.client_config:
                # Standardize campaign names for consistent matching
                def standardize_campaign_name(name):
                    return str(name).strip().lower()
                
                # Get campaign tags data with standardized keys
                campaign_tags_data = {standardize_campaign_name(k): v for k, v in st.session_state.client_config['campaign_tags_data'].items()}
            
                # Check if there are any product groups defined
                has_product_groups = any(v.get('tag_1', '') for v in st.session_state.client_config['campaign_tags_data'].values())
            
                # Apply product group mapping with standardized campaign names
                if has_product_groups:  # Only if there are product groups defined
                    df['Product Group'] = df['Campaign'].apply(
                        lambda x: campaign_tags_data.get(standardize_campaign_name(x), {}).get('tag_1', '') or 'Untagged Group'
                    )
                else:
                    df['Product Group'] = 'Untagged Group'  # Use 'Untagged Group' if no product groups defined
        
            # Create the combined column with special handling for different ad types
            def combine_adtype_matchtype(row):
                ad_type = row.get('Product', 'Unknown')
                match_type = row.get(match_type_col, 'Unknown')
            
                # Handle Sponsored Products
                if ad_type == 'Sponsored Products':
                    if match_type in ['Exact', 'Phrase', 'Broad']:
                        return f"{ad_type} - {match_type}"
                    elif match_type == 'Auto':
                        return f"{ad_type} - Auto"
                    elif match_type == 'Product Target':
                        return f"{ad_type} - Product Target"
                    elif match_type == 'Category Targeting':
                        return f"{ad_type} - Category Target"
            
                # Handle Sponsored Brands
                elif ad_type == 'Sponsored Brands':
                    if match_type in ['Exact', 'Phrase', 'Broad']:
                        return f"{ad_type} - {match_type}"
                    elif match_type == 'Product Target':
                        return f"{ad_type} - Product Target"
                    elif match_type == 'Category Targeting':
                        return f"{ad_type} - Category Target"
            
                # Handle Sponsored Display
                elif ad_type == 'Sponsored Display':
                    if match_type == 'Product Target':
                        return f"{ad_type} - Product Target"
                    elif match_type == 'Category Targeting':
                        return f"{ad_type} - Category Target"
                    elif match_type == 'Remarketing - Branded':
                        return f"{ad_type} - Remarketing - Branded"
                    elif match_type == 'Remarketing - Competitor':
                        return f"{ad_type} - Remarketing - Competitor"
            
                # Default fallback
                return f"{ad_type} - {match_type}"
        
            df['Ad Type & Match Type'] = df.apply(combine_adtype_matchtype, axis=1)
            return df
            
        # All Combined
        with combined_tabs[0]:
            # First, ensure we have the product group information in both dataframes
            b_df = branded_targets_df.copy()
            nb_df = non_branded_targets_df.copy()
        
            # Standardize campaign names for consistent matching
            def standardize_campaign_name(name):
                return str(name).strip().lower()
            
            # Get campaign tags data with standardized keys
            campaign_tags_data = {}
            has_product_groups = False
            if st.session_state.get('client_config') and 'campaign_tags_data' in st.session_state.client_config:
                campaign_tags_data = {standardize_campaign_name(k): v for k, v in st.session_state.client_config['campaign_tags_data'].items()}
                # Check if there are any product groups defined
                has_product_groups = any(v.get('tag_1', '') for v in st.session_state.client_config['campaign_tags_data'].values())
        
            # Add product group to branded dataframe
            # Only populate if there are actually product groups defined in Campaign Tagging
            if 'Campaign' in b_df.columns:
                if has_product_groups:  # Only if there are product groups defined
                    b_df['Product Group'] = b_df['Campaign'].apply(
                        lambda x: campaign_tags_data.get(standardize_campaign_name(x), {}).get('tag_1', '') or 'Untagged Group'
                    )
                else:
                    b_df['Product Group'] = 'Untagged Group'  # Use 'Untagged Group' if no product groups defined
            
            # Add product group to non-branded dataframe
            if 'Campaign' in nb_df.columns:
                if has_product_groups:  # Only if there are product groups defined
                    nb_df['Product Group'] = nb_df['Campaign'].apply(
                        lambda x: campaign_tags_data.get(standardize_campaign_name(x), {}).get('tag_1', '') or 'Untagged Group'
                    )
                else:
                    nb_df['Product Group'] = 'Untagged Group'  # Use 'Untagged Group' if no product groups defined
            
            # Apply product group filtering if active
            if st.session_state.get('targeting_filter_active2', False) and len(st.session_state.targeting_product_group_filter2) > 0:
                # Convert all product groups to strings for consistent comparison
                selected_groups = [str(pg) for pg in st.session_state.targeting_product_group_filter2]
            
                # Store original row counts for debugging
                b_df_original_count = len(b_df) if not b_df.empty else 0
                nb_df_original_count = len(nb_df) if not nb_df.empty else 0
            
                # Debug information before filtering
                st.session_state.debug_targeting_filter['before_filtering'] = {
                    'branded_rows': b_df_original_count,
                    'nonbranded_rows': nb_df_original_count,
                    'branded_product_groups': b_df['Product Group'].unique().tolist() if 'Product Group' in b_df.columns and not b_df.empty else [],
                    'nonbranded_product_groups': nb_df['Product Group'].unique().tolist() if 'Product Group' in nb_df.columns and not nb_df.empty else []
                }
            
                # Filter branded dataframe
                if 'Product Group' in b_df.columns:
                    b_df = b_df[b_df['Product Group'].astype(str).isin(selected_groups)]
                
                # Filter non-branded dataframe
                if 'Product Group' in nb_df.columns:
                    nb_df = nb_df[nb_df['Product Group'].astype(str).isin(selected_groups)]
            
                # Store filtered row counts for debugging
                b_df_filtered_count = len(b_df) if not b_df.empty else 0
                nb_df_filtered_count = len(nb_df) if not nb_df.empty else 0
            
                # Debug information after filtering
                st.session_state.debug_targeting_filter['after_filtering'] = {
                    'branded_rows': b_df_filtered_count,
                    'nonbranded_rows': nb_df_filtered_count,
                    'branded_product_groups': b_df['Product Group'].unique().tolist() if 'Product Group' in b_df.columns and not b_df.empty else [],
                    'nonbranded_product_groups': nb_df['Product Group'].unique().tolist() if 'Product Group' in nb_df.columns and not b_df.empty else [],
                    'rows_removed_branded': b_df_original_count - b_df_filtered_count,
                    'rows_removed_nonbranded': nb_df_original_count - nb_df_filtered_count
                }
                
                # Store filtered dataframes in session state for use in Sankey diagrams
                st.session_state.filtered_branded_targets_df = b_df
                st.session_state.filtered_non_branded_targets_df = nb_df
            
                # Combine filtered dataframes
                all_df = pd.concat([b_df, nb_df], ignore_index=True)
            
                # Add debug caption to show active filter
                st.caption(f"Filtered by Product Group(s): {', '.join(st.session_state.targeting_product_group_filter2)}")
            else:
                # Use original dataframes if no filter is active
                all_df = pd.concat([b_df, nb_df], ignore_index=True)
            
                # Reset filtered dataframes in session state
                st.session_state.filtered_branded_targets_df = b_df
                st.session_state.filtered_non_branded_targets_df = nb_df
        
            # Process the data for display
            normalized_df = normalize_match_types(all_df)
        
            # Ensure required columns exist
            if 'Product' not in normalized_df.columns:
                normalized_df['Product'] = ''
            if match_type_col not in normalized_df.columns:
                normalized_df[match_type_col] = ''
        
            # Create the combined column for display
            all_combined = create_combined_column(infer_ad_type(normalized_df))
        
            # Debug information if needed
            if 'debug' in st.session_state and st.session_state.debug:
                st.write(f"Selected product groups: {st.session_state.get('targeting_product_group_filter2', [])}")
                if 'Product Group' in all_combined.columns:
                    st.write(f"Available product groups in data: {all_combined['Product Group'].unique()}")
                    st.write(f"Data types - Selected: {type(st.session_state.targeting_product_group_filter2[0]) if st.session_state.targeting_product_group_filter2 else 'None'}, In DataFrame: {all_combined['Product Group'].dtype}")
                else:
                    st.warning("No 'Product Group' column found in the data")
        
            expected_adtype_matchtypes = [
                "Sponsored Products - Exact", "Sponsored Products - Phrase", "Sponsored Products - Broad", "Sponsored Products - Auto", "Sponsored Products - Product Target", "Sponsored Products - Category Target",
                "Sponsored Brands - Exact", "Sponsored Brands - Phrase", "Sponsored Brands - Broad", "Sponsored Brands - Product Target", "Sponsored Brands - Category Target",
                "Sponsored Display - Product Target", "Sponsored Display - Category Target",
                "Sponsored Display - Remarketing - Branded", "Sponsored Display - Remarketing - Competitor"
            ]
            if 'Ad Type & Match Type' in all_combined.columns:
                all_combined_total_spend = all_combined['Spend'].sum() if not all_combined.empty else 0
                all_combined_total_sales = all_combined['Sales'].sum() if not all_combined.empty else 0
                combined_all = all_combined.groupby('Ad Type & Match Type').apply(lambda x: kpi_agg(x, all_combined_total_spend, all_combined_total_sales), include_groups=False).reset_index() if not all_combined.empty else pd.DataFrame(columns=['Ad Type & Match Type'])
                combined_all = combined_all.set_index('Ad Type & Match Type') if not combined_all.empty else pd.DataFrame(index=expected_adtype_matchtypes)
                combined_all = combined_all.reindex(expected_adtype_matchtypes, fill_value=0).reset_index().rename(columns={"index": 'Ad Type & Match Type'})
                combined_all_fmt = format_agg_table(combined_all, index_col='Ad Type & Match Type')
            
                # Create a custom column config based on the actual columns in the dataframe
                custom_column_config = {}
            
                # Add Ad Type & Match Type column
                custom_column_config['Ad Type & Match Type'] = st.column_config.TextColumn(label="Ad Type & Match Type")
            
                # Add currency columns with dollar formatting
                for col in ['Spend', 'Ad Sales', 'CPC', 'AOV', 'CPA']:
                    if col in combined_all_fmt.columns:
                        # Convert to numeric first
                        combined_all_fmt[col] = pd.to_numeric(combined_all_fmt[col].astype(str).str.replace(r'[^0-9.-]', '', regex=True), errors='coerce').fillna(0)
                        custom_column_config[col] = st.column_config.NumberColumn(label=col, format="dollar")
            
                # Add percentage columns
                for col in ['ACoS', 'CVR', 'CTR', '% of Spend', '% of Ad Sales']:
                    if col in combined_all_fmt.columns:
                        # Convert to numeric first (as decimal, not percentage)
                        combined_all_fmt[col] = pd.to_numeric(combined_all_fmt[col].astype(str).str.replace(r'[^0-9.-]', '', regex=True), errors='coerce').fillna(0)
                        custom_column_config[col] = st.column_config.NumberColumn(label=col, format="%.2f%%")
            
                # Add ROAS as decimal
                if 'ROAS' in combined_all_fmt.columns:
                    combined_all_fmt['ROAS'] = pd.to_numeric(combined_all_fmt['ROAS'].astype(str).str.replace(r'[^0-9.-]', '', regex=True), errors='coerce').fillna(0)
                    custom_column_config['ROAS'] = st.column_config.NumberColumn(label="ROAS", format="%.2f")
            
                # Add integer columns with comma formatting
                for col in ['Impressions', 'Clicks', 'Orders', 'Units Sold']:
                    if col in combined_all_fmt.columns:
                        combined_all_fmt[col] = pd.to_numeric(combined_all_fmt[col].astype(str).str.replace(r'[^0-9.-]', '', regex=True), errors='coerce').fillna(0).astype(int)
                        custom_column_config[col] = st.column_config.NumberColumn(label=col, format="localized")
            
                # ACoS target display removed
                # Use the toggle value from client config for 'use_avg_as_fallback'
                use_avg_fallback = goals.get('use_avg_acos_account', False)
                account_target = goals.get('account_wide_acos')
            
                # Create a copy for numeric processing
                numeric_df = combined_all_fmt.copy()
            
                # Convert formatted values to numeric for proper sorting
                for col in numeric_df.columns:
                    if col not in ['Ad Type & Match Type']:
                        numeric_df[col] = pd.to_numeric(numeric_df[col].astype(str).str.replace('$', '').str.replace('%', '').str.replace(',', ''), errors='coerce')
            
                # Create formatting dictionary
                fmt_dict = {}
                for col in numeric_df.columns:
                    if col == 'Ad Type & Match Type':
                        continue
                    elif col == 'ROAS':
                        fmt_dict[col] = lambda x: f"{x:.2f}"
                    elif col in ['ACoS', 'TACoS', 'CVR', 'CTR', '% of Spend', '% of Ad Sales']:
                        fmt_dict[col] = lambda x: f"{x:.2f}%"
                    elif col in ['CPC', 'AOV', 'CPA']:
                        fmt_dict[col] = lambda x: f"${x:.2f}"
                    elif col in ['Spend', 'Sales', 'Ad Sales']:
                        fmt_dict[col] = lambda x: f"${x:,.2f}"
                    else:
                        fmt_dict[col] = lambda x: f"{x:,.0f}" if pd.notnull(x) else "0"
            
                # Style the dataframe
                styled_df = numeric_df.style.format(fmt_dict)
            
                # Apply color gradients to percentage columns
                if '% of Spend' in numeric_df.columns:
                    styled_df = styled_df.apply(lambda x: [color_gradient_blue(v, 0, 100) 
                                                        if not pd.isna(v) else '' 
                                                        for v in x] if x.name == '% of Spend' else [''] * len(x), axis=0)
                if '% of Ad Sales' in numeric_df.columns:
                    styled_df = styled_df.apply(lambda x: [color_gradient_green(v, 0, 100) 
                                                        if not pd.isna(v) else '' 
                                                        for v in x] if x.name == '% of Ad Sales' else [''] * len(x), axis=0)
            
                # Apply ACoS styling if target is available
                if account_target is not None:
                    # Apply ACoS styling to the ACoS column only
                    acos_col_idx = numeric_df.columns.get_loc('ACoS') if 'ACoS' in numeric_df.columns else None
                    if acos_col_idx is not None:
                        styled_df = apply_acos_styling(styled_df, numeric_df, 'ACoS', account_target, use_avg_fallback)
            
                # Display the dataframe with sorting enabled
                st.dataframe(styled_df, use_container_width=True, hide_index=True)
            else:
                st.info("No targeting data by Ad Type & Match Type (missing 'Ad Type & Match Type' column).")

        # Branded Combined
        with combined_tabs[1]:
            # Use the filtered branded dataframe from the "All" tab
            if 'filtered_branded_targets_df' in st.session_state:
                b_df = st.session_state.filtered_branded_targets_df
            else:
                b_df = branded_targets_df.copy()
            
            # Show filter status if active
            if st.session_state.get('targeting_filter_active2', False) and len(st.session_state.targeting_product_group_filter2) > 0:
                st.caption(f"Filtered by Product Group(s): {', '.join(st.session_state.targeting_product_group_filter2)}")
            
            # Process the data for display
            normalized_df = normalize_match_types(b_df)
        
            # Ensure required columns exist
            if 'Product' not in normalized_df.columns:
                normalized_df['Product'] = ''
            if match_type_col not in normalized_df.columns:
                normalized_df[match_type_col] = ''
            
            # Create the combined column for display
            b_combined = create_combined_column(infer_ad_type(normalized_df))
        
            expected_adtype_matchtypes = [
                "Sponsored Products - Exact", "Sponsored Products - Phrase", "Sponsored Products - Broad", "Sponsored Products - Auto", "Sponsored Products - Product Target", "Sponsored Products - Category Target",
                "Sponsored Brands - Exact", "Sponsored Brands - Phrase", "Sponsored Brands - Broad", "Sponsored Brands - Product Target", "Sponsored Brands - Category Target",
                "Sponsored Display - Product Target", "Sponsored Display - Category Target",
                "Sponsored Display - Remarketing - Branded", "Sponsored Display - Remarketing - Competitor"
            ]
            if 'Ad Type & Match Type' in b_combined.columns:
                b_combined_total_spend = b_combined['Spend'].sum() if not b_combined.empty else 0
                b_combined_total_sales = b_combined['Sales'].sum() if not b_combined.empty else 0
                combined_b = b_combined.groupby('Ad Type & Match Type').apply(lambda x: kpi_agg(x, b_combined_total_spend, b_combined_total_sales), include_groups=False).reset_index() if not b_combined.empty else pd.DataFrame(columns=['Ad Type & Match Type'])
                combined_b = combined_b.set_index('Ad Type & Match Type') if not combined_b.empty else pd.DataFrame(index=expected_adtype_matchtypes)
                combined_b = combined_b.reindex(expected_adtype_matchtypes, fill_value=0).reset_index().rename(columns={"index": 'Ad Type & Match Type'})
                combined_b_fmt = format_agg_table(combined_b, index_col='Ad Type & Match Type')
            
                # Create a custom column config based on the actual columns in the dataframe
                custom_column_config = {}
            
                # Add Ad Type & Match Type column
                custom_column_config['Ad Type & Match Type'] = st.column_config.TextColumn(label="Ad Type & Match Type")
            
                # Add currency columns with dollar formatting
                for col in ['Spend', 'Ad Sales', 'CPC', 'AOV', 'CPA']:
                    if col in combined_b_fmt.columns:
                        # Convert to numeric first
                        combined_b_fmt[col] = pd.to_numeric(combined_b_fmt[col].astype(str).str.replace(r'[^0-9.-]', '', regex=True), errors='coerce').fillna(0)
                        custom_column_config[col] = st.column_config.NumberColumn(label=col, format="dollar")
            
                # Add percentage columns
                for col in ['ACoS', 'CVR', 'CTR', '% of Spend', '% of Ad Sales']:
                    if col in combined_b_fmt.columns:
                        # Convert to numeric first (as decimal, not percentage)
                        combined_b_fmt[col] = pd.to_numeric(combined_b_fmt[col].astype(str).str.replace(r'[^0-9.-]', '', regex=True), errors='coerce').fillna(0)
                        custom_column_config[col] = st.column_config.NumberColumn(label=col, format="%.2f%%")
            
                # Add ROAS as decimal
                if 'ROAS' in combined_b_fmt.columns:
                    combined_b_fmt['ROAS'] = pd.to_numeric(combined_b_fmt['ROAS'].astype(str).str.replace(r'[^0-9.-]', '', regex=True), errors='coerce').fillna(0)
                    custom_column_config['ROAS'] = st.column_config.NumberColumn(label="ROAS", format="%.2f")
            
                # Add integer columns with comma formatting
                for col in ['Impressions', 'Clicks', 'Orders', 'Units Sold']:
                    if col in combined_b_fmt.columns:
                        combined_b_fmt[col] = pd.to_numeric(combined_b_fmt[col].astype(str).str.replace(r'[^0-9.-]', '', regex=True), errors='coerce').fillna(0).astype(int)
                        custom_column_config[col] = st.column_config.NumberColumn(label=col, format="localized")
            
                # ACoS target display removed
                # Use the toggle value from client config for 'use_avg_as_fallback'
                use_avg_fallback = goals.get('use_avg_acos_branded', False)
            
                # Create a copy for numeric processing
                numeric_df = combined_b_fmt.copy()
            
                # Convert formatted values to numeric for proper sorting
                for col in numeric_df.columns:
                    if col not in ['Ad Type & Match Type']:
                        numeric_df[col] = pd.to_numeric(numeric_df[col].astype(str).str.replace('$', '').str.replace('%', '').str.replace(',', ''), errors='coerce')
            
                # Create formatting dictionary
                fmt_dict = {}
                for col in numeric_df.columns:
                    if col == 'Ad Type & Match Type':
                        continue
                    elif col == 'ROAS':
                        fmt_dict[col] = lambda x: f"{x:.2f}"
                    elif col in ['ACoS', 'TACoS', 'CVR', 'CTR', '% of Spend', '% of Ad Sales']:
                        fmt_dict[col] = lambda x: f"{x:.2f}%"
                    elif col in ['CPC', 'AOV', 'CPA']:
                        fmt_dict[col] = lambda x: f"${x:.2f}"
                    elif col in ['Spend', 'Sales', 'Ad Sales']:
                        fmt_dict[col] = lambda x: f"${x:,.2f}"
                    else:
                        fmt_dict[col] = lambda x: f"{x:,.0f}" if pd.notnull(x) else "0"
            
                # Style the dataframe
                styled_df = numeric_df.style.format(fmt_dict)
            
                # Apply color gradients to percentage columns
                if '% of Spend' in numeric_df.columns:
                    styled_df = styled_df.apply(lambda x: [color_gradient_blue(v, 0, 100) 
                                                        if not pd.isna(v) else '' 
                                                        for v in x] if x.name == '% of Spend' else [''] * len(x), axis=0)
                if '% of Ad Sales' in numeric_df.columns:
                    styled_df = styled_df.apply(lambda x: [color_gradient_green(v, 0, 100) 
                                                        if not pd.isna(v) else '' 
                                                        for v in x] if x.name == '% of Ad Sales' else [''] * len(x), axis=0)
            
                # Apply ACoS styling if target is available
                if branded_target is not None:
                    # Apply ACoS styling to the ACoS column only
                    acos_col_idx = numeric_df.columns.get_loc('ACoS') if 'ACoS' in numeric_df.columns else None
                    if acos_col_idx is not None:
                        styled_df = apply_acos_styling(styled_df, numeric_df, 'ACoS', branded_target, use_avg_fallback)
            
                # Display the dataframe with sorting enabled
                st.dataframe(styled_df, use_container_width=True, hide_index=True)
            else:
                st.info("No Branded targeting data by Ad Type & Match Type (missing 'Ad Type & Match Type' column).")

        # Non-Branded Combined
        with combined_tabs[2]:
            # Use the filtered non-branded dataframe from the "All" tab
            if 'filtered_non_branded_targets_df' in st.session_state:
                nb_df = st.session_state.filtered_non_branded_targets_df
            else:
                nb_df = non_branded_targets_df.copy()
            
            # Show filter status if active
            if st.session_state.get('targeting_filter_active2', False) and len(st.session_state.targeting_product_group_filter2) > 0:
                st.caption(f"Filtered by Product Group(s): {', '.join(st.session_state.targeting_product_group_filter2)}")
            
            # Process the data for display
            normalized_df = normalize_match_types(nb_df)
        
            # Ensure required columns exist
            if 'Product' not in normalized_df.columns:
                normalized_df['Product'] = ''
            if match_type_col not in normalized_df.columns:
                normalized_df[match_type_col] = ''
            
            # Create the combined column for display
            nb_combined = create_combined_column(infer_ad_type(normalized_df))
        
            if 'Ad Type & Match Type' in nb_combined.columns:
                # Calculate account totals for percentage calculations
                nb_combined_total_spend = nb_combined['Spend'].sum() if not nb_combined.empty else 0
                nb_combined_total_sales = nb_combined['Sales'].sum() if not nb_combined.empty else 0
                combined_nb = nb_combined.groupby('Ad Type & Match Type').apply(lambda x: kpi_agg(x, nb_combined_total_spend, nb_combined_total_sales), include_groups=False).reset_index() if not nb_combined.empty else pd.DataFrame(columns=['Ad Type & Match Type'])
                expected_adtype_matchtypes = [
                    "Sponsored Products - Exact", "Sponsored Products - Phrase", "Sponsored Products - Broad", "Sponsored Products - Auto", "Sponsored Products - Product Target", "Sponsored Products - Category Target",
                    "Sponsored Brands - Exact", "Sponsored Brands - Phrase", "Sponsored Brands - Broad", "Sponsored Brands - Product Target", "Sponsored Brands - Category Target",
                    "Sponsored Display - Product Target", "Sponsored Display - Category Target",
                    "Sponsored Display - Remarketing - Branded", "Sponsored Display - Remarketing - Competitor"
                ]
                combined_nb = combined_nb.set_index('Ad Type & Match Type') if not combined_nb.empty else pd.DataFrame(index=expected_adtype_matchtypes)
                combined_nb = combined_nb.reindex(expected_adtype_matchtypes, fill_value=0).reset_index().rename(columns={"index": 'Ad Type & Match Type'})
                combined_nb_fmt = format_agg_table(combined_nb, index_col='Ad Type & Match Type')
            
                # Create a custom column config based on the actual columns in the dataframe
                custom_column_config = {}
            
                # Add Ad Type & Match Type column
                custom_column_config['Ad Type & Match Type'] = st.column_config.TextColumn(label="Ad Type & Match Type")
            
                # Add currency columns with dollar formatting
                for col in ['Spend', 'Ad Sales', 'CPC', 'AOV', 'CPA']:
                    if col in combined_nb_fmt.columns:
                        # Convert to numeric first
                        combined_nb_fmt[col] = pd.to_numeric(combined_nb_fmt[col].astype(str).str.replace(r'[^0-9.-]', '', regex=True), errors='coerce').fillna(0)
                        custom_column_config[col] = st.column_config.NumberColumn(label=col, format="dollar")
            
                # Add percentage columns
                for col in ['ACoS', 'CVR', 'CTR', '% of Spend', '% of Ad Sales']:
                    if col in combined_nb_fmt.columns:
                        # Convert to numeric first (as decimal, not percentage)
                        combined_nb_fmt[col] = pd.to_numeric(combined_nb_fmt[col].astype(str).str.replace(r'[^0-9.-]', '', regex=True), errors='coerce').fillna(0)
                        custom_column_config[col] = st.column_config.NumberColumn(label=col, format="%.2f%%")
            
                # Add ROAS as decimal
                if 'ROAS' in combined_nb_fmt.columns:
                    combined_nb_fmt['ROAS'] = pd.to_numeric(combined_nb_fmt['ROAS'].astype(str).str.replace(r'[^0-9.-]', '', regex=True), errors='coerce').fillna(0)
                    custom_column_config['ROAS'] = st.column_config.NumberColumn(label="ROAS", format="%.2f")
            
                # Add integer columns with comma formatting
                for col in ['Impressions', 'Clicks', 'Orders', 'Units Sold']:
                    if col in combined_nb_fmt.columns:
                        combined_nb_fmt[col] = pd.to_numeric(combined_nb_fmt[col].astype(str).str.replace(r'[^0-9.-]', '', regex=True), errors='coerce').fillna(0).astype(int)
                        custom_column_config[col] = st.column_config.NumberColumn(label=col, format="localized")
            
                # ACoS target display removed
                # Use the toggle value from client config for 'use_avg_as_fallback'
                use_avg_fallback = goals.get('use_avg_acos_nonbranded', False)
            
                # Create a copy for numeric processing
                numeric_df = combined_nb_fmt.copy()
            
                # Convert formatted values to numeric for proper sorting
                for col in numeric_df.columns:
                    if col not in ['Ad Type & Match Type']:
                        numeric_df[col] = pd.to_numeric(numeric_df[col].astype(str).str.replace('$', '').str.replace('%', '').str.replace(',', ''), errors='coerce')
            
                # Create formatting dictionary
                fmt_dict = {}
                for col in numeric_df.columns:
                    if col == 'Ad Type & Match Type':
                        continue
                    elif col == 'ROAS':
                        fmt_dict[col] = lambda x: f"{x:.2f}"
                    elif col in ['ACoS', 'TACoS', 'CVR', 'CTR', '% of Spend', '% of Ad Sales']:
                        fmt_dict[col] = lambda x: f"{x:.2f}%"
                    elif col in ['CPC', 'AOV', 'CPA']:
                        fmt_dict[col] = lambda x: f"${x:.2f}"
                    elif col in ['Spend', 'Sales', 'Ad Sales']:
                        fmt_dict[col] = lambda x: f"${x:,.2f}"
                    else:
                        fmt_dict[col] = lambda x: f"{x:,.0f}" if pd.notnull(x) else "0"
            
                # Style the dataframe
                styled_df = numeric_df.style.format(fmt_dict)
            
                # Apply color gradients to percentage columns
                if '% of Spend' in numeric_df.columns:
                    styled_df = styled_df.apply(lambda x: [color_gradient_blue(v, 0, 100) 
                                                        if not pd.isna(v) else '' 
                                                        for v in x] if x.name == '% of Spend' else [''] * len(x), axis=0)
                if '% of Ad Sales' in numeric_df.columns:
                    styled_df = styled_df.apply(lambda x: [color_gradient_green(v, 0, 100) 
                                                        if not pd.isna(v) else '' 
                                                        for v in x] if x.name == '% of Ad Sales' else [''] * len(x), axis=0)
            
                # Apply ACoS styling if target is available
                if non_branded_target is not None:
                    # Apply ACoS styling to the ACoS column only
                    acos_col_idx = numeric_df.columns.get_loc('ACoS') if 'ACoS' in numeric_df.columns else None
                    if acos_col_idx is not None:
                        styled_df = apply_acos_styling(styled_df, numeric_df, 'ACoS', non_branded_target, use_avg_fallback)
            
                # Display the dataframe with sorting enabled
                st.dataframe(styled_df, use_container_width=True, hide_index=True)
            else:
                st.info("No Non-Branded targeting data by Ad Type & Match Type (missing 'Ad Type & Match Type' column).")
        
        # --- Product Group Focus: Ad Type Ã— Match Type Matrix ---
        
        # Show only if product groups exist in Client Settings â†’ Campaign Tagging
        has_pg = False
        ct_data = {}
        if st.session_state.get('client_config') and 'campaign_tags_data' in st.session_state.client_config:
            ct_data = st.session_state.client_config['campaign_tags_data'] or {}
            has_pg = any((v or {}).get('tag_1', '').strip() for v in ct_data.values())
        
        # Always show this section, even if no product groups are set up
        st.markdown("<hr style='height:2px;border-width:0;color:gold;background-color:gold;margin-top:25px;margin-bottom:15px;margin-left:10px;margin-right:10px'>", unsafe_allow_html=True)
        st.markdown("<div style='margin-top:30px;'></div>", unsafe_allow_html=True)
        st.markdown("#### Product Group Focus: Ad Type Ã— Match Type")
        st.caption("Select a few product groups to see investment distribution and gaps by Ad Type and Match Type.")

        # Options sourced from Campaign Tagging product groups (tag_1)
        pg_options = sorted({(v or {}).get('tag_1', '').strip() for v in ct_data.values() if (v or {}).get('tag_1', '').strip()})
        
        # Add "Select All" functionality - centered
        col1a, col1b, col1c = st.columns([2, 1, 2])
        with col1b:
            if st.button("Select All", key="pg_focus_select_all"):
                st.session_state.pg_focus_select_all_clicked = True
                st.rerun()
        
        # Handle button clicks by setting appropriate default values
        if st.session_state.get('pg_focus_select_all_clicked', False):
            default_selection = pg_options
            st.session_state.pg_focus_select_all_clicked = False  # Reset flag
        else:
            # Default to empty selection (shows all data) on first load
            default_selection = st.session_state.get('pg_focus_groups', [])
        
        with col1a:
            sel_pgs = st.multiselect("Choose Product Group(s)", options=pg_options, default=default_selection, key="pg_focus_groups")
        # Now showing both spend (background) and ROAS (text/border) together

        # Get base data from session state (before any filtering)
        b_src = st.session_state.get('branded_targets_df', pd.DataFrame())
        nb_src = st.session_state.get('non_branded_targets_df', pd.DataFrame())

        def _std_name(x):
            return str(x).strip().lower()

        def add_product_group(df: pd.DataFrame) -> pd.DataFrame:
            if df is None or df.empty:
                return pd.DataFrame()
            df = df.copy()
            if 'Campaign' in df.columns:
                tag_map = { _std_name(k): (v or {}) for k, v in ct_data.items() }
                df['Product Group'] = df['Campaign'].apply(lambda n: tag_map.get(_std_name(n), {}).get('tag_1', ''))
            else:
                df['Product Group'] = ''
            return df

        def tidy(df: pd.DataFrame) -> pd.DataFrame:
            if df is None or df.empty:
                return pd.DataFrame(columns=['Product Group','Ad Type','Match Cat','Spend','Ad Sales','ROAS','ACoS','Clicks','Orders','Is Enabled'])
            df = add_product_group(df)
            if 'Product' not in df.columns:
                df = infer_ad_type(df)
            mt_col = 'Match Type' if 'Match Type' in df.columns else ('Target Type' if 'Target Type' in df.columns else None)
            if mt_col is None:
                df['Match Type'] = 'Unknown'
                mt_col = 'Match Type'
            df = normalize_match_types(df)

            # If Is Enabled already present, keep it; otherwise derive from status/entity columns
            if 'Is Enabled' in df.columns:
                # Coerce to boolean
                df['Is Enabled'] = df['Is Enabled'].astype(bool)
            else:
                # Derive enabled-state flag from available status/state column (case-insensitive)
                # and restrict counting to target/keyword entity rows when possible.
                try:
                    lower_map = {c.lower(): c for c in df.columns}
                    status_candidates = ['state', 'targeting state', 'target state', 'status', 'target status']
                    status_col = next((lower_map[c] for c in status_candidates if c in lower_map), None)
                    # Try multiple possible entity/record columns
                    entity_candidates = ['entity', 'record type', 'record_type', 'type', 'targeting type', 'targeting_type']
                    entity_col = next((lower_map[c] for c in entity_candidates if c in lower_map), None)

                    # Base enabled flag from status
                    if status_col is not None:
                        enabled_base = df[status_col].astype(str).str.strip().str.lower().eq('enabled')
                    else:
                        enabled_base = pd.Series(False, index=df.index)

                    # If we have entity info, only count enabled for target/keyword entities
                    if entity_col is not None:
                        # Accept common variants (e.g., 'Sponsored Products Keyword', etc.) and exclude negatives
                        ent_series = df[entity_col].astype(str).str.strip().str.lower()
                        has_keyword = ent_series.str.contains('keyword') & ~ent_series.str.contains('negative')
                        # Match both 'product targeting' and 'product target'
                        has_product_target = (ent_series.str.contains('product targeting') | ent_series.str.contains('product target')) & ~ent_series.str.contains('negative')
                        has_contextual = ent_series.str.contains('contextual')
                        has_audience = ent_series.str.contains('audience')
                        entity_ok = has_keyword | has_product_target | has_contextual | has_audience
                        df['Is Enabled'] = enabled_base & entity_ok
                    else:
                        df['Is Enabled'] = enabled_base
                except Exception:
                    df['Is Enabled'] = False

            def map_cat(row):
                ad = row.get('Product', 'Unknown')
                mt = row.get(mt_col, 'Unknown')
                # Normalize Category Targeting to Category Target for this visualization
                if mt == 'Category Targeting':
                    mt = 'Category Target'
                if ad == 'Sponsored Products' and mt in ['Exact','Phrase','Broad','Auto','Product Target','Category Target']:
                    return mt
                if ad == 'Sponsored Brands' and mt in ['Exact','Phrase','Broad','Product Target','Category Target']:
                    return mt
                if ad == 'Sponsored Display' and mt in ['Product Target','Category Target','Remarketing - Branded','Remarketing - Competitor']:
                    return mt
                return 'Other'

            out = df.copy()
            out['Ad Type'] = out['Product']
            out['Match Cat'] = out.apply(map_cat, axis=1)
            out = out[out['Match Cat'] != 'Other']
            
            # Ensure required columns exist
            if 'Spend' not in out.columns:
                out['Spend'] = 0
            if 'Ad Sales' not in out.columns:
                out['Ad Sales'] = 0
            if 'Clicks' not in out.columns:
                out['Clicks'] = 0
            if 'Orders' not in out.columns:
                out['Orders'] = 0
            
            # Calculate ROAS and ACoS
            out['ROAS'] = out.apply(lambda row: row['Ad Sales'] / row['Spend'] if row['Spend'] > 0 else 0, axis=1)
            out['ACoS'] = out.apply(lambda row: (row['Spend'] / row['Ad Sales']) * 100 if row['Ad Sales'] > 0 else 0, axis=1)
            
            return out[['Product Group','Ad Type','Match Cat','Spend','Ad Sales','ROAS','ACoS','Clicks','Orders','Is Enabled']]

        b_tidy = tidy(b_src)
        nb_tidy = tidy(nb_src)
        all_tidy = pd.concat([b_tidy, nb_tidy], ignore_index=True) if (not b_tidy.empty or not nb_tidy.empty) else pd.DataFrame(columns=['Product Group','Ad Type','Match Cat','Spend','Ad Sales','ROAS','ACoS','Clicks','Orders','Is Enabled'])
        


        # Always show the section, even if no product groups are configured
        if True:  # Show regardless of product group configuration
            tabs_pg = st.tabs(["All", "Branded", "Non-Branded"])
            x_cats = ['Exact','Phrase','Broad','Auto','Product Target','Category Target','Remarketing - Branded','Remarketing - Competitor']
            y_cats = ['Sponsored Products','Sponsored Brands','Sponsored Display']

            def render(tab, label: str, data: pd.DataFrame):
                with tab:
                    if data is None or data.empty:
                        st.info(f"No {label} data available.")
                        return
                    
                    # Filter data for selected product groups and aggregate
                    # If no product groups are selected, show all data
                    if sel_pgs:
                        filtered_data = data[data['Product Group'].isin(sel_pgs)]
                        if filtered_data.empty:
                            st.warning(f"No data for selected product groups in {label}.")
                            return
                    else:
                        # Show all data when no product groups are selected
                        filtered_data = data
                    
                    # Check if we have any data to work with
                    if filtered_data.empty:
                        st.info(f"No data available for {label}.")
                        return
                    
                    # Aggregate across all selected product groups (or all data if none selected)
                    agg_data = filtered_data.groupby(['Ad Type', 'Match Cat']).agg({
                        'Spend': 'sum',
                        'Ad Sales': 'sum',
                        'Clicks': 'sum',
                        'Orders': 'sum',
                        'Is Enabled': 'sum'
                    }).reset_index()
                    
                    # Calculate ROAS and ACoS after aggregation
                    agg_data['ROAS'] = agg_data.apply(lambda row: row['Ad Sales'] / row['Spend'] if row['Spend'] > 0 else 0, axis=1)
                    agg_data['ACoS'] = agg_data.apply(lambda row: (row['Spend'] / row['Ad Sales']) * 100 if row['Ad Sales'] > 0 else 0, axis=1)
                    
                    # Rename enabled sum for clarity
                    agg_data.rename(columns={'Is Enabled': 'Enabled Targets'}, inplace=True)
                    

                    # Check if we have any aggregated data
                    if agg_data.empty or agg_data['Spend'].sum() == 0:
                        st.info(f"No spend data available for {label} with current selection.")
                        return
                    
                    # Calculate CPC and CVR
                    agg_data['CPC'] = agg_data.apply(lambda row: row['Spend'] / row['Clicks'] if row['Clicks'] > 0 else 0, axis=1)
                    agg_data['CVR'] = agg_data.apply(lambda row: (row['Orders'] / row['Clicks']) * 100 if row['Clicks'] > 0 else 0, axis=1)
                    
                    # Create pivot tables for spend (background) and ROAS (text/border)
                    pv_spend = agg_data.pivot_table(index='Ad Type', columns='Match Cat', values='Spend', aggfunc='sum', fill_value=0)
                    pv_spend = pv_spend.reindex(index=y_cats, columns=x_cats).fillna(0)
                    
                    pv_roas = agg_data.pivot_table(index='Ad Type', columns='Match Cat', values='ROAS', aggfunc='mean', fill_value=0)
                    pv_roas = pv_roas.reindex(index=y_cats, columns=x_cats).fillna(0)
                    
                    # Calculate account average ROAS for the current tab using filtered_data (before grouping)
                    total_spend = filtered_data['Spend'].sum()
                    if total_spend > 0:
                        account_avg_roas = (filtered_data['Ad Sales'].sum()) / total_spend
                    else:
                        account_avg_roas = 0
                    
                    # Spend color scale: black to royal blue (no white)
                    spend_color_scale = [[0, '#000000'], [0.2, '#0a0a1a'], [0.4, '#1a1a3a'], [0.6, '#2a2a5a'], [0.8, '#3a3a7a'], [1, '#4169e1']]
                    
                    # ROAS color function based on account average
                    def get_roas_color(roas_value):
                        if roas_value == 0:
                            return '#666666'  # Gray for no data
                        elif roas_value < account_avg_roas * 0.7:  # Well below average
                            return '#dc2626'  # Red
                        elif roas_value < account_avg_roas * 0.9:  # Slightly below average
                            return '#eab308'  # Yellow
                        elif roas_value <= account_avg_roas * 1.1:  # Around average
                            return '#84cc16'  # Green-yellow
                        else:  # Above average
                            return '#16a34a'  # Green
                    
                    # Ad type abbreviations
                    ad_type_abbrev = {
                        'Sponsored Products': 'SP',
                        'Sponsored Brands': 'SB', 
                        'Sponsored Display': 'SD'
                    }
                    
                    # Match type full names (no abbreviations)
                    match_type_names = {
                        'Exact': 'Exact',
                        'Phrase': 'Phrase',
                        'Broad': 'Broad',
                        'Auto': 'Auto',
                        'Product Target': 'Product Target',
                        'Category Target': 'Category Target',
                        'Remarketing - Branded': 'Remarketing - Branded',
                        'Remarketing - Competitor': 'Remarketing - Competitor'
                    }
                    
                    # Create a blank figure and add custom rectangles for each cell
                    fig = go.Figure()
                    
                    # Get min/max spend values for color scaling
                    spend_values = pv_spend.values.flatten()
                    spend_values = spend_values[spend_values > 0]  # Only non-zero values
                    if len(spend_values) > 0:
                        min_spend = spend_values.min()
                        max_spend = spend_values.max()
                    else:
                        min_spend = max_spend = 0
                    
                    # Define impossible combinations based on tab type
                    def is_impossible_combination(ad_type, match_cat, tab_label):
                        # Sponsored Brands Auto and Sponsored Display Auto don't exist
                        if ad_type == 'Sponsored Brands' and match_cat == 'Auto':
                            return True
                        if ad_type == 'Sponsored Display' and match_cat == 'Auto':
                            return True
                        
                        # Sponsored Display doesn't support keyword match types
                        if ad_type == 'Sponsored Display' and match_cat in ['Exact', 'Phrase', 'Broad']:
                            return True
                        
                        # Sponsored Brands and Sponsored Products don't support remarketing
                        if ad_type in ['Sponsored Brands', 'Sponsored Products'] and match_cat in ['Remarketing - Branded', 'Remarketing - Competitor']:
                            return True
                        
                        # Tab-specific restrictions
                        if tab_label == 'Non-Branded':
                            # Non-Branded tab: block "Remarketing - Branded"
                            if match_cat == 'Remarketing - Branded':
                                return True
                        elif tab_label == 'Branded':
                            # Branded tab: block "Remarketing - Competitor"
                            if match_cat == 'Remarketing - Competitor':
                                return True
                        
                        return False
                    
                    # Create custom rectangles and hover data for each cell with gaps
                    gap = 0.08  # Reduced gap size for larger squares
                    hover_data = []
                    x_coords = []
                    y_coords = []
                    
                    # First, add grayed-out rectangles for impossible combinations
                    for i, ad_type in enumerate(pv_spend.index):
                        for j, match_cat in enumerate(pv_spend.columns):
                            if is_impossible_combination(ad_type, match_cat, label):
                                # Add grayed-out rectangle for impossible combination
                                fig.add_shape(
                                    type="rect",
                                    x0=j-0.5+gap, y0=i-0.5+gap,
                                    x1=j+0.5-gap, y1=i+0.5-gap,
                                    fillcolor='rgba(60, 60, 60, 0.5)',  # Dark gray with transparency
                                    line=dict(color='rgba(100, 100, 100, 0.8)', width=1),
                                    xref="x", yref="y"
                                )
                                
                                # Add "N/A" text for impossible combinations
                                fig.add_annotation(
                                    x=j, y=i,
                                    text="N/A",
                                    showarrow=False,
                                    font=dict(size=14, color='rgba(150, 150, 150, 0.8)', family="Arial Bold"),
                                    xref="x", yref="y"
                                )
                    
                    # Then add data rectangles for valid combinations
                    for i, ad_type in enumerate(pv_spend.index):
                        for j, match_cat in enumerate(pv_spend.columns):
                            spend_val = pv_spend.iloc[i, j]
                            roas_val = pv_roas.iloc[i, j]
                            
                            # Skip impossible combinations
                            if is_impossible_combination(ad_type, match_cat, label):
                                continue
                            
                            if spend_val > 0:
                                # Calculate spend color intensity (0-1 scale)
                                if max_spend > min_spend:
                                    intensity = (spend_val - min_spend) / (max_spend - min_spend)
                                else:
                                    intensity = 0.5
                                
                                # Create spend color (black to royal blue gradient)
                                spend_color = f'rgba({int(65*intensity)}, {int(105*intensity)}, {int(225*intensity)}, 1)'
                                
                                # Add rectangle for this cell
                                fig.add_shape(
                                    type="rect",
                                    x0=j-0.5+gap, y0=i-0.5+gap,
                                    x1=j+0.5-gap, y1=i+0.5-gap,
                                    fillcolor=spend_color,
                                    line=dict(width=0),
                                    xref="x", yref="y"
                                )
                                
                                # Prepare hover data
                                sales_val = agg_data[(agg_data['Ad Type'] == ad_type) & (agg_data['Match Cat'] == match_cat)]['Ad Sales'].iloc[0] if len(agg_data[(agg_data['Ad Type'] == ad_type) & (agg_data['Match Cat'] == match_cat)]) > 0 else 0
                                cpc_val = agg_data[(agg_data['Ad Type'] == ad_type) & (agg_data['Match Cat'] == match_cat)]['CPC'].iloc[0] if len(agg_data[(agg_data['Ad Type'] == ad_type) & (agg_data['Match Cat'] == match_cat)]) > 0 else 0
                                cvr_val = agg_data[(agg_data['Ad Type'] == ad_type) & (agg_data['Match Cat'] == match_cat)]['CVR'].iloc[0] if len(agg_data[(agg_data['Ad Type'] == ad_type) & (agg_data['Match Cat'] == match_cat)]) > 0 else 0
                                enabled_val = int(agg_data[(agg_data['Ad Type'] == ad_type) & (agg_data['Match Cat'] == match_cat)]['Enabled Targets'].iloc[0]) if len(agg_data[(agg_data['Ad Type'] == ad_type) & (agg_data['Match Cat'] == match_cat)]) > 0 else 0
                                
                                roas_color = get_roas_color(roas_val)
                                hover_text = f"Ad Type: {ad_type}<br>Match Type: {match_cat}<br>Spend: ${spend_val:,.2f}<br>Ad Sales: ${sales_val:,.2f}<br>Enabled Targets: {enabled_val}<br>ROAS: {roas_val:.2f} ({roas_color})<br>CPC: ${cpc_val:.2f}<br>CVR: {cvr_val:.1f}%<br>Account Avg ROAS: {account_avg_roas:.2f}"
                                
                                hover_data.append(hover_text)
                                x_coords.append(j)
                                y_coords.append(i)
                    
                    # Add invisible scatter trace for hover functionality
                    if hover_data:
                        fig.add_trace(go.Scatter(
                            x=x_coords,
                            y=y_coords,
                            mode='markers',
                            marker=dict(size=20, opacity=0),  # Invisible markers
                            text=hover_data,
                            hovertemplate='%{text}<extra></extra>',
                            showlegend=False
                        ))
                    
                    # Add a dummy heatmap trace for the spend colorbar (invisible)
                    if max_spend > 0:  # Show colorbar if there's any spend data
                        # Ensure we have a range for the colorbar even if all values are the same
                        colorbar_min = min_spend if max_spend > min_spend else 0
                        colorbar_max = max_spend if max_spend > min_spend else min_spend * 1.1 if min_spend > 0 else 1
                        
                        fig.add_trace(go.Heatmap(
                            z=[[colorbar_min, colorbar_max]],
                            x=[-10, -9],  # Position far off-screen so it's not visible
                            y=[-10],
                            colorscale=spend_color_scale,
                            showscale=True,
                            colorbar=dict(
                                title=dict(text='Spend ($)', side="right", font=dict(color='white', size=12)),
                                tickfont=dict(color='white', size=10),
                                bgcolor='rgba(0,0,0,0.5)',
                                bordercolor='rgba(255,255,255,0.3)',
                                borderwidth=1,
                                x=1.02,  # Position to the right of the chart
                                len=0.7,  # Make it slightly shorter
                                thickness=15,  # Make it thicker for better visibility
                                tickmode='linear',
                                tick0=colorbar_min,
                                dtick=(colorbar_max - colorbar_min) / 4 if colorbar_max > colorbar_min else colorbar_max / 4
                            ),
                            hoverinfo='skip',
                            showlegend=False
                        ))
                    
                    # Set up the figure layout
                    fig.update_layout(
                        xaxis=dict(
                            showgrid=False, 
                            dtick=1,
                            range=[-0.5, len(pv_spend.columns)-0.5],
                            tickvals=list(range(len(pv_spend.columns))),
                            ticktext=list(pv_spend.columns)
                        ),
                        yaxis=dict(
                            showgrid=False, 
                            dtick=1,
                            range=[-0.5, len(pv_spend.index)-0.5],
                            tickvals=list(range(len(pv_spend.index))),
                            ticktext=list(pv_spend.index)
                        )
                    )
                    
                    # Add text annotations and ROAS-colored borders for cells with data
                    for i, ad_type in enumerate(pv_spend.index):
                        for j, match_cat in enumerate(pv_spend.columns):
                            spend_val = pv_spend.iloc[i, j]
                            roas_val = pv_roas.iloc[i, j]
                            
                            # Skip impossible combinations
                            if is_impossible_combination(ad_type, match_cat, label):
                                continue
                            
                            if spend_val > 0:
                                # Get ROAS color for text and border
                                roas_color = get_roas_color(roas_val)
                                enabled_val = int(agg_data[(agg_data['Ad Type'] == ad_type) & (agg_data['Match Cat'] == match_cat)]['Enabled Targets'].iloc[0]) if len(agg_data[(agg_data['Ad Type'] == ad_type) & (agg_data['Match Cat'] == match_cat)]) > 0 else 0
                                
                                # Create multi-line text with spend and ROAS
                                ad_abbrev = ad_type_abbrev.get(ad_type, ad_type)
                                match_name = match_type_names.get(match_cat, match_cat)
                                
                                # Create text with descriptions at top, spend in middle, ROAS at bottom
                                # Special-case SD Remarketing labels to drop 'SD -' and simplify competitor hyphen
                                if ad_type == 'Sponsored Display' and match_cat == 'Remarketing - Branded':
                                    description_text = 'Remarketing - Branded'
                                elif ad_type == 'Sponsored Display' and match_cat == 'Remarketing - Competitor':
                                    description_text = 'Remarketing Competitor'
                                else:
                                    description_text = f"{ad_abbrev} - {match_name}"
                                spend_text = f"Spend: ${spend_val:,.0f}"
                                roas_text = f"ROAS: {roas_val:.2f}"
                                
                                # Add description at top (white text)
                                fig.add_annotation(
                                    x=j, y=i+0.25,  # Top of cell
                                    text=description_text,
                                    showarrow=False,
                                    font=dict(size=11, color='white'),
                                    bgcolor='rgba(0,0,0,0.3)',
                                    borderwidth=0,
                                    xref="x", yref="y"
                                )
                                
                                # Add spend in middle (white text)
                                fig.add_annotation(
                                    x=j, y=i,  # Center of cell
                                    text=spend_text,
                                    showarrow=False,
                                    font=dict(size=12, color='white'),
                                    bgcolor='rgba(0,0,0,0.3)',
                                    borderwidth=0,
                                    xref="x", yref="y"
                                )
                                
                                # Add ROAS-colored text annotation at bottom
                                fig.add_annotation(
                                    x=j, y=i-0.25,  # Bottom of cell
                                    text=roas_text,
                                    showarrow=False,
                                    font=dict(size=12, color=roas_color, family="Arial Black"),
                                    bgcolor='rgba(0,0,0,0.3)',
                                    borderwidth=0,
                                    xref="x", yref="y"
                                )
                                
                                # Add ROAS-colored border around the smaller square
                                fig.add_shape(
                                    type="rect",
                                    x0=j-0.5+gap, y0=i-0.5+gap,
                                    x1=j+0.5-gap, y1=i+0.5-gap,
                                    line=dict(color=roas_color, width=3),
                                    fillcolor="rgba(0,0,0,0)",  # Transparent fill
                                    xref="x", yref="y"
                                )
                    
                    # Text annotations are already added above with ROAS-colored borders
                    
                    # Update title to show "All Product Groups" when none are selected
                    if sel_pgs:
                        title_text = f"{label} - {len(sel_pgs)} Product Group(s) Selected | Account Avg ROAS: {account_avg_roas:.2f}"
                    else:
                        title_text = f"{label} - All Product Groups | Account Avg ROAS: {account_avg_roas:.2f}"
                    
                    fig.update_layout(
                        title=title_text,
                        template='plotly_dark',
                        paper_bgcolor='#0e1117',  # Match Streamlit dark theme
                        plot_bgcolor='#0e1117',   # Match Streamlit dark theme
                        margin=dict(l=10, r=10, t=80, b=10),
                        height=450,
                        font=dict(size=11, color='white'),
                        xaxis=dict(
                            side='top',
                            tickfont=dict(color='white'),
                            showgrid=False,  # Custom borders handle gaps
                            dtick=1
                        ),
                        yaxis=dict(
                            tickfont=dict(color='white'),
                            showgrid=False,  # Custom borders handle gaps
                            dtick=1
                        ),
                        coloraxis_colorbar=dict(
                            title=dict(text='Spend ($)', side="right", font=dict(color='white')),
                            tickfont=dict(color='white'),
                            bgcolor='rgba(0,0,0,0.3)',
                            bordercolor='rgba(255,255,255,0.2)',
                            borderwidth=1
                        )
                    )
                    
                    st.plotly_chart(fig, use_container_width=True)
                    
                    # Show summary stats
                    total_spend = agg_data['Spend'].sum()
                    total_sales = agg_data['Ad Sales'].sum()
                    overall_roas = total_sales / total_spend if total_spend > 0 else 0
                    
                    col1, col2, col3 = st.columns(3)
                    with col1:
                        st.metric("Total Spend", f"${total_spend:,.2f}")
                    with col2:
                        st.metric("Total Ad Sales", f"${total_sales:,.2f}")
                    with col3:
                        st.metric("Overall ROAS", f"{overall_roas:.2f}x")

            # Call render function for each tab
            render(tabs_pg[0], 'All', all_tidy)
            render(tabs_pg[1], 'Branded', b_tidy)
            render(tabs_pg[2], 'Non-Branded', nb_tidy)

            # --- Product Target Opportunities (Dropdown) ---
            # Show only when product groups exist in Client Settings (ASIN-based) and when we can compute opportunities
            # Ensure ASIN performance is computed before rendering opportunities
            if ('asin_perf_df' not in st.session_state) or (st.session_state.asin_perf_df is None):
                bulk_data = st.session_state.get('bulk_data')
                sales_df = st.session_state.get('sales_report_data')
                client_cfg = st.session_state.get('client_config') or {}
                if isinstance(bulk_data, dict) or isinstance(sales_df, pd.DataFrame):
                    try:
                        sd_attr_choice = st.session_state.get('sd_attribution_choice', 'Sales')
                        st.session_state.asin_perf_df = compute_asin_performance(
                            bulk_data=bulk_data or {},
                            sales_data=sales_df,
                            client_config=client_cfg,
                            sd_attribution_choice=sd_attr_choice
                        )
                        if 'debug_messages' in st.session_state:
                            st.session_state.debug_messages.append("[Product Analysis] asin_perf_df computed via helper before Opportunities section")
                    except Exception as e:
                        if 'debug_messages' in st.session_state:
                            st.session_state.debug_messages.append(f"[Product Analysis] Failed to compute asin_perf_df: {e}")
            try:
                # Determine if product groups are configured in Client Settings Center (ASINs)
                has_asin_pgs = False
                try:
                    cfg = st.session_state.get('client_config') or {}
                    bad = cfg.get('branded_asins_data') or {}
                    if isinstance(bad, dict) and len(bad) > 0:
                        has_asin_pgs = any((v or {}).get('product_group', '').strip() for v in bad.values())
                except Exception:
                    has_asin_pgs = False

                if has_asin_pgs and ('asin_perf_df' in st.session_state) and (st.session_state.asin_perf_df is not None) \
                   and ('Product Group' in st.session_state.asin_perf_df.columns):
                    # Build Product Group level totals from ASIN Performance (aligns with 'Performance by Product Group')
                    tmp_df = st.session_state.asin_perf_df.copy()
                    # Normalize Product Group values
                    tmp_df['Product Group'] = tmp_df['Product Group'].fillna('').astype(str)
                    tmp_df.loc[tmp_df['Product Group'].str.strip() == '', 'Product Group'] = 'Untagged Group'
                    tmp_df.loc[tmp_df['Product Group'] == 'Unassigned', 'Product Group'] = 'Untagged Group'

                    agg_cols = {}
                    if 'Spend' in tmp_df.columns:
                        agg_cols['Spend'] = 'sum'
                    if 'Total Sales' in tmp_df.columns:
                        agg_cols['Total Sales'] = 'sum'

                    # Require both Spend and Total Sales to compute opportunity shares
                    if ('Spend' in agg_cols) and ('Total Sales' in agg_cols):
                        pg_totals = tmp_df.groupby('Product Group').agg(agg_cols).reset_index()
                        total_spend_all = pg_totals['Spend'].sum() if 'Spend' in pg_totals.columns else 0
                        total_total_sales_all = pg_totals['Total Sales'].sum() if 'Total Sales' in pg_totals.columns else 0

                        # Compute percent shares
                        pg_totals['% of Spend'] = pg_totals['Spend'] / total_spend_all if total_spend_all > 0 else 0
                        pg_totals['% of Total Sales'] = pg_totals['Total Sales'] / total_total_sales_all if total_total_sales_all > 0 else 0

                        # Exclude untagged and keep only opportunities where Sales% > Spend%
                        opp_df = pg_totals[(pg_totals['Product Group'] != 'Untagged Group') &
                                           (pg_totals['% of Total Sales'] > pg_totals['% of Spend'])].copy()

                        if len(opp_df) > 0:
                            # Delta: (% of Spend) - (% of Total Sales), sort ascending (more negative first)
                            opp_df['Delta'] = opp_df['% of Spend'] - opp_df['% of Total Sales']
                            opp_df = opp_df.sort_values('Delta', ascending=True)

                            # Build labels for dropdown with delta in percentage points
                            def _opp_label(row):
                                spend_pct = row['% of Spend'] * 100
                                sales_pct = row['% of Total Sales'] * 100
                                delta_pct = row['Delta'] * 100
                                return f"{row['Product Group']} â€” Spend {spend_pct:.1f}% vs Total Sales {sales_pct:.1f}% ({delta_pct:.1f}%)"

                            opp_labels = [_opp_label(r) for _, r in opp_df.iterrows()]
                            label_to_pg = {label: pg for label, pg in zip(opp_labels, opp_df['Product Group'])}

                            with st.expander("Product Target Opportunities", expanded=False):
                                st.caption("Product groups where % of Total Sales exceeds % of Spend. Sorted by (Spend% - Total Sales%), most negative first.")

                                selected_label = st.selectbox(
                                    "Select Product Group",
                                    options=opp_labels,
                                    index=0 if len(opp_labels) > 0 else None,
                                    key='product_target_opportunities_select'
                                )
                                selected_pg_for_heatmap = label_to_pg.get(selected_label)

                                if selected_pg_for_heatmap:
                                    tabs_opp = st.tabs(["All", "Branded", "Non-Branded"])

                                    def render_opp(tab, label: str, data: pd.DataFrame):
                                        with tab:
                                            if data is None or data.empty:
                                                st.info(f"No {label} data available.")
                                                return

                                            filtered_data = data[data['Product Group'] == selected_pg_for_heatmap]
                                            if filtered_data.empty:
                                                st.warning(f"No data for {selected_pg_for_heatmap} in {label}.")
                                                return

                                            # Aggregate by Ad Type x Match Cat
                                            agg_data = filtered_data.groupby(['Ad Type', 'Match Cat']).agg({
                                                'Spend': 'sum',
                                                'Ad Sales': 'sum',
                                                'Clicks': 'sum',
                                                'Orders': 'sum',
                                                'Is Enabled': 'sum'
                                            }).reset_index()

                                            agg_data['ROAS'] = agg_data.apply(lambda row: row['Ad Sales'] / row['Spend'] if row['Spend'] > 0 else 0, axis=1)
                                            agg_data['ACoS'] = agg_data.apply(lambda row: (row['Spend'] / row['Ad Sales']) * 100 if row['Ad Sales'] > 0 else 0, axis=1)
                                            agg_data.rename(columns={'Is Enabled': 'Enabled Targets'}, inplace=True)

                                            if agg_data.empty or agg_data['Spend'].sum() == 0:
                                                st.info(f"No spend data available for {label} with current selection.")
                                                return

                                            agg_data['CPC'] = agg_data.apply(lambda row: row['Spend'] / row['Clicks'] if row['Clicks'] > 0 else 0, axis=1)
                                            agg_data['CVR'] = agg_data.apply(lambda row: (row['Orders'] / row['Clicks']) * 100 if row['Clicks'] > 0 else 0, axis=1)

                                            pv_spend = agg_data.pivot_table(index='Ad Type', columns='Match Cat', values='Spend', aggfunc='sum', fill_value=0)
                                            pv_spend = pv_spend.reindex(index=y_cats, columns=x_cats).fillna(0)
                                            pv_roas = agg_data.pivot_table(index='Ad Type', columns='Match Cat', values='ROAS', aggfunc='mean', fill_value=0)
                                            pv_roas = pv_roas.reindex(index=y_cats, columns=x_cats).fillna(0)

                                            total_spend = filtered_data['Spend'].sum()
                                            account_avg_roas = (filtered_data['Ad Sales'].sum()) / total_spend if total_spend > 0 else 0

                                            spend_color_scale = [[0, '#000000'], [0.2, '#0a0a1a'], [0.4, '#1a1a3a'], [0.6, '#2a2a5a'], [0.8, '#3a3a7a'], [1, '#4169e1']]

                                            def get_roas_color(roas_value):
                                                if roas_value == 0:
                                                    return '#666666'
                                                elif roas_value < account_avg_roas * 0.7:
                                                    return '#dc2626'
                                                elif roas_value < account_avg_roas * 0.9:
                                                    return '#eab308'
                                                elif roas_value <= account_avg_roas * 1.1:
                                                    return '#84cc16'
                                                else:
                                                    return '#16a34a'

                                            ad_type_abbrev = {
                                                'Sponsored Products': 'SP',
                                                'Sponsored Brands': 'SB',
                                                'Sponsored Display': 'SD'
                                            }
                                            match_type_names = {
                                                'Exact': 'Exact',
                                                'Phrase': 'Phrase',
                                                'Broad': 'Broad',
                                                'Auto': 'Auto',
                                                'Product Target': 'Product Target',
                                                'Category Target': 'Category Target',
                                                'Remarketing - Branded': 'Remarketing - Branded',
                                                'Remarketing - Competitor': 'Remarketing - Competitor'
                                            }

                                            fig = go.Figure()

                                            spend_values = pv_spend.values.flatten()
                                            spend_values = spend_values[spend_values > 0]
                                            if len(spend_values) > 0:
                                                min_spend = spend_values.min()
                                                max_spend = spend_values.max()
                                            else:
                                                min_spend = max_spend = 0

                                            def is_impossible_combination(ad_type, match_cat, tab_label):
                                                if ad_type == 'Sponsored Brands' and match_cat == 'Auto':
                                                    return True
                                                if ad_type == 'Sponsored Display' and match_cat == 'Auto':
                                                    return True
                                                if ad_type == 'Sponsored Display' and match_cat in ['Exact', 'Phrase', 'Broad']:
                                                    return True
                                                if ad_type in ['Sponsored Brands', 'Sponsored Products'] and match_cat in ['Remarketing - Branded', 'Remarketing - Competitor']:
                                                    return True
                                                if tab_label == 'Non-Branded' and match_cat == 'Remarketing - Branded':
                                                    return True
                                                if tab_label == 'Branded' and match_cat == 'Remarketing - Competitor':
                                                    return True
                                                return False

                                            gap = 0.08
                                            hover_data = []
                                            x_coords = []
                                            y_coords = []

                                            for i, ad_type in enumerate(pv_spend.index):
                                                for j, match_cat in enumerate(pv_spend.columns):
                                                    if is_impossible_combination(ad_type, match_cat, label):
                                                        fig.add_shape(
                                                            type="rect",
                                                            x0=j-0.5+gap, y0=i-0.5+gap,
                                                            x1=j+0.5-gap, y1=i+0.5+gap,
                                                            fillcolor='rgba(60, 60, 60, 0.5)',
                                                            line=dict(color='rgba(100, 100, 100, 0.8)', width=1),
                                                            xref="x", yref="y"
                                                        )
                                                        fig.add_annotation(
                                                            x=j, y=i,
                                                            text="N/A",
                                                            showarrow=False,
                                                            font=dict(size=14, color='rgba(150, 150, 150, 0.8)', family="Arial Bold"),
                                                            xref="x", yref="y"
                                                        )

                                            for i, ad_type in enumerate(pv_spend.index):
                                                for j, match_cat in enumerate(pv_spend.columns):
                                                    spend_val = pv_spend.iloc[i, j]
                                                    roas_val = pv_roas.iloc[i, j]

                                                    if is_impossible_combination(ad_type, match_cat, label):
                                                        continue

                                                    if spend_val > 0:
                                                        if max_spend > min_spend:
                                                            intensity = (spend_val - min_spend) / (max_spend - min_spend)
                                                        else:
                                                            intensity = 0.5
                                                        spend_color = f'rgba({int(65*intensity)}, {int(105*intensity)}, {int(225*intensity)}, 1)'
                                                        fig.add_shape(
                                                            type="rect",
                                                            x0=j-0.5+gap, y0=i-0.5+gap,
                                                            x1=j+0.5-gap, y1=i+0.5+gap,
                                                            fillcolor=spend_color,
                                                            line=dict(width=0),
                                                            xref="x", yref="y"
                                                        )

                                                        sales_val = agg_data[(agg_data['Ad Type'] == ad_type) & (agg_data['Match Cat'] == match_cat)]['Ad Sales'].iloc[0] if len(agg_data[(agg_data['Ad Type'] == ad_type) & (agg_data['Match Cat'] == match_cat)]) > 0 else 0
                                                        cpc_val = agg_data[(agg_data['Ad Type'] == ad_type) & (agg_data['Match Cat'] == match_cat)]['CPC'].iloc[0] if len(agg_data[(agg_data['Ad Type'] == ad_type) & (agg_data['Match Cat'] == match_cat)]) > 0 else 0
                                                        cvr_val = agg_data[(agg_data['Ad Type'] == ad_type) & (agg_data['Match Cat'] == match_cat)]['CVR'].iloc[0] if len(agg_data[(agg_data['Ad Type'] == ad_type) & (agg_data['Match Cat'] == match_cat)]) > 0 else 0
                                                        enabled_val = int(agg_data[(agg_data['Ad Type'] == ad_type) & (agg_data['Match Cat'] == match_cat)]['Enabled Targets'].iloc[0]) if len(agg_data[(agg_data['Ad Type'] == ad_type) & (agg_data['Match Cat'] == match_cat)]) > 0 else 0

                                                        roas_color = get_roas_color(roas_val)
                                                        hover_text = f"Ad Type: {ad_type}<br>Match Type: {match_cat}<br>Spend: ${spend_val:,.2f}<br>Ad Sales: ${sales_val:,.2f}<br>Enabled Targets: {enabled_val}<br>ROAS: {roas_val:.2f} ({roas_color})<br>CPC: ${cpc_val:.2f}<br>CVR: {cvr_val:.1f}%<br>Account Avg ROAS: {account_avg_roas:.2f}"
                                                        hover_data.append(hover_text)
                                                        x_coords.append(j)
                                                        y_coords.append(i)

                                            if hover_data:
                                                fig.add_trace(go.Scatter(
                                                    x=x_coords,
                                                    y=y_coords,
                                                    mode='markers',
                                                    marker=dict(size=20, opacity=0),
                                                    text=hover_data,
                                                    hovertemplate='%{text}<extra></extra>',
                                                    showlegend=False
                                                ))

                                            if max_spend > 0:
                                                colorbar_min = min_spend if max_spend > min_spend else 0
                                                colorbar_max = max_spend if max_spend > min_spend else min_spend * 1.1 if min_spend > 0 else 1
                                                fig.add_trace(go.Heatmap(
                                                    z=[[colorbar_min, colorbar_max]],
                                                    x=[-10, -9],
                                                    y=[-10],
                                                    colorscale=spend_color_scale,
                                                    showscale=True,
                                                    colorbar=dict(
                                                        title=dict(text='Spend ($)', side="right", font=dict(color='white', size=12)),
                                                        tickfont=dict(color='white', size=10),
                                                        bgcolor='rgba(0,0,0,0.5)',
                                                        bordercolor='rgba(255,255,255,0.3)',
                                                        borderwidth=1,
                                                        x=1.02,
                                                        len=0.7,
                                                        thickness=15,
                                                        tickmode='linear',
                                                        tick0=colorbar_min,
                                                        dtick=(colorbar_max - colorbar_min) / 4 if colorbar_max > min_spend else colorbar_max / 4
                                                    ),
                                                    hoverinfo='skip',
                                                    showlegend=False
                                                ))

                                            fig.update_layout(
                                                xaxis=dict(
                                                    showgrid=False,
                                                    dtick=1,
                                                    range=[-0.5, len(pv_spend.columns)-0.5],
                                                    tickvals=list(range(len(pv_spend.columns))),
                                                    ticktext=list(pv_spend.columns)
                                                ),
                                                yaxis=dict(
                                                    showgrid=False,
                                                    dtick=1,
                                                    range=[-0.5, len(pv_spend.index)-0.5],
                                                    tickvals=list(range(len(pv_spend.index))),
                                                    ticktext=list(pv_spend.index)
                                                )
                                            )

                                            # Annotate text and borders
                                            for i, ad_type in enumerate(pv_spend.index):
                                                for j, match_cat in enumerate(pv_spend.columns):
                                                    spend_val = pv_spend.iloc[i, j]
                                                    roas_val = pv_roas.iloc[i, j]

                                                    if is_impossible_combination(ad_type, match_cat, label):
                                                        continue

                                                    if spend_val > 0:
                                                        roas_color = get_roas_color(roas_val)
                                                        enabled_val = int(agg_data[(agg_data['Ad Type'] == ad_type) & (agg_data['Match Cat'] == match_cat)]['Enabled Targets'].iloc[0]) if len(agg_data[(agg_data['Ad Type'] == ad_type) & (agg_data['Match Cat'] == match_cat)]) > 0 else 0
                                                        ad_abbrev = ad_type_abbrev.get(ad_type, ad_type)
                                                        match_name = match_type_names.get(match_cat, match_cat)
                                                        # Special-case SD Remarketing labels to drop 'SD -' and simplify competitor hyphen
                                                        if ad_type == 'Sponsored Display' and match_cat == 'Remarketing - Branded':
                                                            description_text = 'Remarketing - Branded'
                                                        elif ad_type == 'Sponsored Display' and match_cat == 'Remarketing - Competitor':
                                                            description_text = 'Remarketing Competitor'
                                                        else:
                                                            description_text = f"{ad_abbrev} - {match_name}"
                                                        spend_text = f"Spend: ${spend_val:,.0f}"
                                                        roas_text = f"ROAS: {roas_val:.2f}"

                                                        fig.add_annotation(x=j, y=i+0.25, text=description_text, showarrow=False,
                                                                           font=dict(size=11, color='white'), bgcolor='rgba(0,0,0,0.3)',
                                                                           borderwidth=0, xref="x", yref="y")
                                                        fig.add_annotation(x=j, y=i, text=spend_text, showarrow=False,
                                                                           font=dict(size=12, color='white'), bgcolor='rgba(0,0,0,0.3)',
                                                                           borderwidth=0, xref="x", yref="y")
                                                        fig.add_annotation(x=j, y=i-0.25, text=roas_text, showarrow=False,
                                                                           font=dict(size=12, color=roas_color, family="Arial Black"), bgcolor='rgba(0,0,0,0.3)',
                                                                           borderwidth=0, xref="x", yref="y")
                                                        fig.add_shape(type="rect", x0=j-0.5+gap, y0=i-0.5+gap, x1=j+0.5-gap, y1=i+0.5+gap,
                                                                      line=dict(color=roas_color, width=3), fillcolor="rgba(0,0,0,0)",
                                                                      xref="x", yref="y")

                                            title_text = f"{label} - {selected_pg_for_heatmap} | Account Avg ROAS: {account_avg_roas:.2f}"
                                            fig.update_layout(
                                                title=title_text,
                                                template='plotly_dark',
                                                paper_bgcolor='#0e1117',
                                                plot_bgcolor='#0e1117',
                                                margin=dict(l=10, r=10, t=80, b=10),
                                                height=450,
                                                font=dict(size=11, color='white'),
                                                xaxis=dict(side='top', tickfont=dict(color='white'), showgrid=False, dtick=1),
                                                yaxis=dict(tickfont=dict(color='white'), showgrid=False, dtick=1),
                                                coloraxis_colorbar=dict(title=dict(text='Spend ($)', side="right", font=dict(color='white')),
                                                                        tickfont=dict(color='white'), bgcolor='rgba(0,0,0,0.3)',
                                                                        bordercolor='rgba(255,255,255,0.2)', borderwidth=1)
                                            )

                                            st.plotly_chart(fig, use_container_width=True)

                                            # Summary metrics
                                            total_spend = agg_data['Spend'].sum()
                                            total_sales = agg_data['Ad Sales'].sum()
                                            overall_roas = total_sales / total_spend if total_spend > 0 else 0
                                            col1, col2, col3 = st.columns(3)
                                            with col1:
                                                st.metric("Total Spend", f"${total_spend:,.2f}")
                                            with col2:
                                                st.metric("Total Ad Sales", f"${total_sales:,.2f}")
                                            with col3:
                                                st.metric("Overall ROAS", f"{overall_roas:.2f}x")

                                    render_opp(tabs_opp[0], 'All', all_tidy)
                                    render_opp(tabs_opp[1], 'Branded', b_tidy)
                                    render_opp(tabs_opp[2], 'Non-Branded', nb_tidy)
                        else:
                            # No opportunities found: still render the expander with guidance
                            with st.expander("Product Target Opportunities", expanded=False):
                                st.caption("Product groups where % of Total Sales exceeds % of Spend. Sorted by (Spend% - Total Sales%), most negative first.")
                                st.info("No opportunities found based on current data. Ensure Sales data is loaded and product groups are assigned.")
                                if st.session_state.get('global_debug_mode', False):
                                    try:
                                        st.write(pg_totals[['Product Group','Spend','Total Sales','% of Spend','% of Total Sales']]
                                                 .sort_values('% of Total Sales', ascending=False)
                                                 .head(10))
                                    except Exception:
                                        pass
                    else:
                        # Missing Spend or Total Sales metric: render expander with reason
                        with st.expander("Product Target Opportunities", expanded=False):
                            st.caption("Product groups where % of Total Sales exceeds % of Spend. Sorted by (Spend% - Total Sales%), most negative first.")
                            st.info("Cannot compute opportunities because either Spend or Total Sales is missing. Load both ad bulk file and sales report.")
                            if st.session_state.get('global_debug_mode', False):
                                st.write(f"Debug: Spend present={ 'Spend' in tmp_df.columns }, Total Sales present={ 'Total Sales' in tmp_df.columns }")
                else:
                    # Prerequisites missing: still render the expander with diagnostics
                    with st.expander("Product Target Opportunities", expanded=False):
                        st.caption("Product groups where % of Total Sales exceeds % of Spend. Sorted by (Spend% - Total Sales%), most negative first.")
                        if not has_asin_pgs:
                            st.info("Client Settings lacks Product Groups under Branded ASINs.")
                        if ('asin_perf_df' not in st.session_state) or (st.session_state.asin_perf_df is None):
                            st.info("ASIN performance data not available yet.")
                        elif 'Product Group' not in st.session_state.asin_perf_df.columns:
                            st.info("ASIN performance data is missing the 'Product Group' column.")
                        if st.session_state.get('global_debug_mode', False):
                            try:
                                asin_df_present = ('asin_perf_df' in st.session_state) and (st.session_state.asin_perf_df is not None)
                                st.write(f"Debug: has_asin_pgs={has_asin_pgs}, asin_perf_df_present={asin_df_present}")
                            except Exception:
                                pass
            except Exception as e:
                # Fail-safe: keep dashboard running even if something goes wrong here
                if st.session_state.get('debug_mode'):
                    st.warning(f"[Debug] Product Target Opportunities error: {e}")

        # --- Ad Spend/Sales Flow Sankey Diagram ---
    
        st.markdown("<hr style='height:2px;border-width:0;color:gold;background-color:gold;margin-top:25px;margin-bottom:15px;margin-left:10px;margin-right:10px'>", unsafe_allow_html=True)
        st.markdown("<div style='margin-top:30px;'></div>", unsafe_allow_html=True)
    
        # Create tabs for Ad Spend and Ad Sales flows
        flow_tabs = st.tabs(["Ad Spend Flow", "Ad Sales Flow"])
    
        def create_ad_spend_sankey_from_targeting_data():
            """
            Create a Sankey diagram showing the flow of ad spend from Branded/Non-Branded to Ad Type to Match Type
            using the existing targeting data from the 'Targeting by Ad Type & Match Type' table.
        
            Returns:
                Plotly figure object for the Sankey diagram
            """
            # Use the filtered dataframes from the 'Targeting by Ad Type & Match Type' section above
            if 'filtered_branded_targets_df' in st.session_state and 'filtered_non_branded_targets_df' in st.session_state:
                branded_targets_df = st.session_state.filtered_branded_targets_df
                non_branded_targets_df = st.session_state.filtered_non_branded_targets_df
            else:
                # Fallback to original dataframes if filtered ones aren't available
                if not ('branded_targets_df' in locals() or 'branded_targets_df' in globals()) or \
                   not ('non_branded_targets_df' in locals() or 'non_branded_targets_df' in globals()):
                    return None
            
            # Product group filtering is already applied in the filtered dataframes above
            product_group_filter_active = st.session_state.get('targeting_filter_active2', False)
            selected_product_groups = st.session_state.get('targeting_product_group_filter2', [])
            
            # Extract ad types and match types from the combined data
            ad_types = ['Sponsored Products', 'Sponsored Brands', 'Sponsored Display']
        
            # Simplify match types for better visualization - consolidate keyword match types
            match_types = ['Keywords', 'Auto', 'Product Target', 'Category Target', 'Remarketing']
        
            # Create nodes for both branded and non-branded flows
            # Format: "[Branded/Non-Branded] [Ad Type/Match Type]"
            nodes = []
        
            # Add Branded and Non-Branded nodes
            nodes.append('Branded')
            nodes.append('Non-Branded')
        
            # Add Ad Type nodes for both Branded and Non-Branded
            branded_ad_types = [f'Branded {ad_type}' for ad_type in ad_types]
            non_branded_ad_types = [f'Non-Branded {ad_type}' for ad_type in ad_types]
            nodes.extend(branded_ad_types)
            nodes.extend(non_branded_ad_types)
        
            # Add Match Type nodes for both Branded and Non-Branded
            branded_match_types = [f'Branded {match_type}' for match_type in match_types]
            non_branded_match_types = [f'Non-Branded {match_type}' for match_type in match_types]
            nodes.extend(branded_match_types)
            nodes.extend(non_branded_match_types)
        
            # Initialize data structures for the Sankey diagram
            sources = []
            targets = []
            values = []
        
            # Define a function to consolidate match types
            def consolidate_match_type(match_type):
                if match_type in ['Exact', 'Phrase', 'Broad']:  # Consolidate keyword match types
                    return 'Keywords'
                elif match_type == 'Auto':
                    return 'Auto'
                elif 'Product' in match_type:
                    return 'Product Target'
                elif 'Category' in match_type:
                    return 'Category Target'
                elif 'Remarketing' in match_type:
                    return 'Remarketing'
                else:
                    return 'Other'
        
            # Process branded targets (filtering already applied in the dataframe)
            if not branded_targets_df.empty:
                b_df = branded_targets_df.copy()
            
                # Extract ad type from each row
                if 'Product' not in b_df.columns:
                    b_df = infer_ad_type(b_df)
                
                # Extract match type from each row
                match_type_col = 'Match Type' if 'Match Type' in b_df.columns else 'Target Type'
                if match_type_col not in b_df.columns:
                    b_df[match_type_col] = 'Unknown'
                
                # Apply the match type consolidation
                b_df['Consolidated Match Type'] = b_df[match_type_col].apply(consolidate_match_type)
    # Layer 1: Branded -> Branded Ad Types
                for ad_type in ad_types:
                    branded_ad_type = f'Branded {ad_type}'
                    ad_type_spend = b_df[b_df['Product'] == ad_type]['Spend'].sum()
                    if ad_type_spend > 0:
                        sources.append(nodes.index('Branded'))
                        targets.append(nodes.index(branded_ad_type))
                        values.append(ad_type_spend)
                    
                # Layer 2: Branded Ad Types -> Branded Match Types
                for ad_type in ad_types:
                    branded_ad_type = f'Branded {ad_type}'
                    ad_type_df = b_df[b_df['Product'] == ad_type]
                
                    for match_type in match_types:
                        branded_match_type = f'Branded {match_type}'
                        match_spend = ad_type_df[ad_type_df['Consolidated Match Type'] == match_type]['Spend'].sum()
                        if match_spend > 0:
                            sources.append(nodes.index(branded_ad_type))
                            targets.append(nodes.index(branded_match_type))
                            values.append(match_spend)
        
            # Process non-branded targets (filtering already applied in the dataframe)
            if not non_branded_targets_df.empty:
                nb_df = non_branded_targets_df.copy()
            
                # Extract ad type from each row
                if 'Product' not in nb_df.columns:
                    nb_df = infer_ad_type(nb_df)
                
                # Extract match type from each row
                match_type_col = 'Match Type' if 'Match Type' in nb_df.columns else 'Target Type'
                if match_type_col not in nb_df.columns:
                    nb_df[match_type_col] = 'Unknown'
                
                # Apply the match type consolidation
                nb_df['Consolidated Match Type'] = nb_df[match_type_col].apply(consolidate_match_type)
    # Layer 1: Non-Branded -> Non-Branded Ad Types
                for ad_type in ad_types:
                    non_branded_ad_type = f'Non-Branded {ad_type}'
                    ad_type_spend = nb_df[nb_df['Product'] == ad_type]['Spend'].sum()
                    if ad_type_spend > 0:
                        sources.append(nodes.index('Non-Branded'))
                        targets.append(nodes.index(non_branded_ad_type))
                        values.append(ad_type_spend)
                    
                # Layer 2: Non-Branded Ad Types -> Non-Branded Match Types
                for ad_type in ad_types:
                    non_branded_ad_type = f'Non-Branded {ad_type}'
                    ad_type_df = nb_df[nb_df['Product'] == ad_type]
                
                    for match_type in match_types:
                        non_branded_match_type = f'Non-Branded {match_type}'
                        match_spend = ad_type_df[ad_type_df['Consolidated Match Type'] == match_type]['Spend'].sum()
                        if match_spend > 0:
                            sources.append(nodes.index(non_branded_ad_type))
                            targets.append(nodes.index(non_branded_match_type))
                            values.append(match_spend)
        
            # Create the Sankey diagram
            if not values:  # If no values, return None
                return None
            
            # Define color scheme for nodes
            node_colors = []
            for node in nodes:
                if node == 'Branded':
                    node_colors.append('#1f77b4')  # Blue
                elif node == 'Non-Branded':
                    node_colors.append('#ff7f0e')  # Orange
                elif node.startswith('Branded Sponsored Products'):
                    node_colors.append('#2ca02c')  # Green
                elif node.startswith('Branded Sponsored Brands'):
                    node_colors.append('#9467bd')  # Purple
                elif node.startswith('Branded Sponsored Display'):
                    node_colors.append('#8c564b')  # Brown
                elif node.startswith('Non-Branded Sponsored Products'):
                    node_colors.append('#2ca02c')  # Green (lighter shade)
                elif node.startswith('Non-Branded Sponsored Brands'):
                    node_colors.append('#9467bd')  # Purple (lighter shade)
                elif node.startswith('Non-Branded Sponsored Display'):
                    node_colors.append('#8c564b')  # Brown (lighter shade)
                elif 'Keywords' in node:
                    node_colors.append('#e377c2')  # Pink
                elif 'Auto' in node:
                    node_colors.append('#17becf')  # Cyan
                elif 'Product Target' in node:
                    node_colors.append('#d62728')  # Red
                elif 'Category Target' in node:
                    node_colors.append('#ff9896')  # Light red
                elif 'Remarketing' in node:
                    node_colors.append('#aec7e8')  # Light blue
                else:
                    node_colors.append('#7f7f7f')  # Gray
            
            fig = go.Figure(data=[go.Sankey(
                arrangement='snap',  # Better node arrangement
                node=dict(
                    pad=20,  # More padding between nodes
                    thickness=25,  # Thicker nodes
                    line=dict(color='black', width=0.5),
                    label=nodes,
                    color=node_colors
                ),
                link=dict(
                    source=sources,
                    target=targets,
                    value=values,
                    hovertemplate='<b>%{source.label} â†’ %{target.label}</b><br>Spend: $%{value:,.2f}<extra></extra>',
                    color='rgba(100, 100, 100, 0.2)'  # Slightly visible link color
                )
            )])
        
            # Update layout for better appearance
            fig.update_layout(
                title=dict(
                    text='Ad Spend Flow by Branded/Non-Branded, Ad Type, and Match Type',
                    font=dict(size=16, color='#333333'),
                    x=0.5,  # Center the title
                    y=0.98  # Position at the top
                ),
                font=dict(family='Arial, sans-serif', size=13),
                height=700,  # Taller for better visibility
                margin=dict(l=20, r=20, t=60, b=20),
                paper_bgcolor='rgba(0,0,0,0)',  # Transparent background
                plot_bgcolor='rgba(0,0,0,0)'  # Transparent plot area
            )
        
            return fig
    
        # Create the Ad Sales Flow Sankey function
        def create_ad_sales_sankey_from_targeting_data():
            """
            Create a Sankey diagram showing the flow of ad sales from Branded/Non-Branded to Ad Type to Match Type
            using the existing targeting data from the 'Targeting by Ad Type & Match Type' table.
        
            Returns:
                Plotly figure object for the Sankey diagram
            """
            # Use the filtered dataframes from the 'Targeting by Ad Type & Match Type' section above
            if 'filtered_branded_targets_df' in st.session_state and 'filtered_non_branded_targets_df' in st.session_state:
                branded_targets_df = st.session_state.filtered_branded_targets_df
                non_branded_targets_df = st.session_state.filtered_non_branded_targets_df
            else:
                # Fallback to original dataframes if filtered ones aren't available
                if not ('branded_targets_df' in locals() or 'branded_targets_df' in globals()) or \
                   not ('non_branded_targets_df' in locals() or 'non_branded_targets_df' in globals()):
                    return None
            
            # Product group filtering is already applied in the filtered dataframes above
            product_group_filter_active = st.session_state.get('targeting_filter_active2', False)
            selected_product_groups = st.session_state.get('targeting_product_group_filter2', [])
        
            # Store filter state for debugging
            if 'debug_targeting_filter' not in st.session_state:
                st.session_state.debug_targeting_filter = {}
            st.session_state.debug_targeting_filter['ad_sales_sankey'] = {
                'filter_active': product_group_filter_active,
                'selected_groups': selected_product_groups
            }
            
            # Extract ad types and match types from the combined data
            ad_types = ['Sponsored Products', 'Sponsored Brands', 'Sponsored Display']
        
            # Simplify match types for better visualization - consolidate keyword match types
            match_types = ['Keywords', 'Auto', 'Product Target', 'Category Target', 'Remarketing']
        
            # Create nodes for both branded and non-branded flows
            # Format: "[Branded/Non-Branded] [Ad Type/Match Type]"
            nodes = []
        
            # Add Branded and Non-Branded nodes
            nodes.append('Branded')
            nodes.append('Non-Branded')
        
            # Add Ad Type nodes for both Branded and Non-Branded
            branded_ad_types = [f'Branded {ad_type}' for ad_type in ad_types]
            non_branded_ad_types = [f'Non-Branded {ad_type}' for ad_type in ad_types]
            nodes.extend(branded_ad_types)
            nodes.extend(non_branded_ad_types)
        
            # Add Match Type nodes for both Branded and Non-Branded
            branded_match_types = [f'Branded {match_type}' for match_type in match_types]
            non_branded_match_types = [f'Non-Branded {match_type}' for match_type in match_types]
            nodes.extend(branded_match_types)
            nodes.extend(non_branded_match_types)
        
            # Initialize data structures for the Sankey diagram
            sources = []
            targets = []
            values = []
        
            # Define a function to consolidate match types
            def consolidate_match_type(match_type):
                if match_type in ['Exact', 'Phrase', 'Broad']:  # Consolidate keyword match types
                    return 'Keywords'
                elif match_type == 'Auto':
                    return 'Auto'
                elif 'Product' in match_type:
                    return 'Product Target'
                elif 'Category' in match_type:
                    return 'Category Target'
                elif 'Remarketing' in match_type:
                    return 'Remarketing'
                else:
                    return 'Other'
        
            # Get the appropriate sales column based on attribution setting for Sponsored Display
            def get_sales_column(df):
                """Helper function to determine which sales column to use based on data availability and attribution setting"""
                # Check if we're dealing with Sponsored Display data and user has selected Views & Clicks attribution
                is_sponsored_display = False
                if 'Product' in df.columns:
                    is_sponsored_display = df['Product'].astype(str).str.contains('Sponsored Display', case=False).any()
                elif 'Campaign Type' in df.columns:
                    is_sponsored_display = df['Campaign Type'].astype(str).str.contains('Sponsored Display', case=False).any()
            
                # --- SALES ATTRIBUTION CONSISTENCY: Always respect st.session_state.sd_attribution_choice for sales columns ---
                if is_sponsored_display and 'sd_attribution_choice' in st.session_state and \
                   st.session_state.sd_attribution_choice == 'Sales (Views & Clicks)':
                    # For Sponsored Display with Views & Clicks attribution
                    for col in ['Sales (Views & Clicks)', 'Total Sales (Views & Clicks)']:
                        if col in df.columns:
                            return col
            
                # Default sales columns in order of preference
                for col in ['Sales', 'Total Sales', '7 Day Total Sales']:
                    if col in df.columns:
                        return col
            
                # If no sales column found
                return None
        
            # Process branded targets (filtering already applied in the dataframe)
            if not branded_targets_df.empty:
                b_df = branded_targets_df.copy()
            
                # Extract ad type from each row
                if 'Product' not in b_df.columns:
                    b_df = infer_ad_type(b_df)
                
                # Extract match type from each row
                match_type_col = 'Match Type' if 'Match Type' in b_df.columns else 'Target Type'
                if match_type_col not in b_df.columns:
                    b_df[match_type_col] = 'Unknown'
                
                # Apply the match type consolidation
                b_df['Consolidated Match Type'] = b_df[match_type_col].apply(consolidate_match_type)
            
                # Get the appropriate sales column
                sales_column = get_sales_column(b_df)
                
                # Layer 1: Branded -> Branded Ad Types
                for ad_type in ad_types:
                    branded_ad_type = f'Branded {ad_type}'
                    ad_type_df = b_df[b_df['Product'] == ad_type]
                    ad_type_sales = ad_type_df[sales_column].sum() if sales_column in ad_type_df.columns else 0
                    if ad_type_sales > 0:
                        sources.append(nodes.index('Branded'))
                        targets.append(nodes.index(branded_ad_type))
                        values.append(ad_type_sales)
                    
                # Layer 2: Branded Ad Types -> Branded Match Types
                for ad_type in ad_types:
                    branded_ad_type = f'Branded {ad_type}'
                    ad_type_df = b_df[b_df['Product'] == ad_type]
                
                    for match_type in match_types:
                        branded_match_type = f'Branded {match_type}'
                        match_df = ad_type_df[ad_type_df['Consolidated Match Type'] == match_type]
                        match_sales = match_df[sales_column].sum() if sales_column in match_df.columns else 0
                        if match_sales > 0:
                            sources.append(nodes.index(branded_ad_type))
                            targets.append(nodes.index(branded_match_type))
                            values.append(match_sales)
        
            # Process non-branded targets (filtering already applied in the dataframe)
            if not non_branded_targets_df.empty:
                nb_df = non_branded_targets_df.copy()
            
                # Extract ad type from each row
                if 'Product' not in nb_df.columns:
                    nb_df = infer_ad_type(nb_df)
                
                # Extract match type from each row
                match_type_col = 'Match Type' if 'Match Type' in nb_df.columns else 'Target Type'
                if match_type_col not in nb_df.columns:
                    nb_df[match_type_col] = 'Unknown'
                
                # Apply the match type consolidation
                nb_df['Consolidated Match Type'] = nb_df[match_type_col].apply(consolidate_match_type)
            
                # Get the appropriate sales column
                sales_column = get_sales_column(nb_df)
                
                # Layer 1: Non-Branded -> Non-Branded Ad Types
                for ad_type in ad_types:
                    non_branded_ad_type = f'Non-Branded {ad_type}'
                    ad_type_df = nb_df[nb_df['Product'] == ad_type]
                    ad_type_sales = ad_type_df[sales_column].sum() if sales_column in ad_type_df.columns else 0
                    if ad_type_sales > 0:
                        sources.append(nodes.index('Non-Branded'))
                        targets.append(nodes.index(non_branded_ad_type))
                        values.append(ad_type_sales)
                    
                # Layer 2: Non-Branded Ad Types -> Non-Branded Match Types
                for ad_type in ad_types:
                    non_branded_ad_type = f'Non-Branded {ad_type}'
                    ad_type_df = nb_df[nb_df['Product'] == ad_type]
                
                    for match_type in match_types:
                        non_branded_match_type = f'Non-Branded {match_type}'
                        match_df = ad_type_df[ad_type_df['Consolidated Match Type'] == match_type]
                        match_sales = match_df[sales_column].sum() if sales_column in match_df.columns else 0
                        if match_sales > 0:
                            sources.append(nodes.index(non_branded_ad_type))
                            targets.append(nodes.index(non_branded_match_type))
                            values.append(match_sales)
        
            # Create the Sankey diagram
            if not values:  # If no values, return None
                return None
            
            # Define color scheme for nodes
            node_colors = []
            for node in nodes:
                if node == 'Branded':
                    node_colors.append('#1f77b4')  # Blue
                elif node == 'Non-Branded':
                    node_colors.append('#ff7f0e')  # Orange
                elif node.startswith('Branded Sponsored Products'):
                    node_colors.append('#2ca02c')  # Green
                elif node.startswith('Branded Sponsored Brands'):
                    node_colors.append('#9467bd')  # Purple
                elif node.startswith('Branded Sponsored Display'):
                    node_colors.append('#8c564b')  # Brown
                elif node.startswith('Non-Branded Sponsored Products'):
                    node_colors.append('#2ca02c')  # Green (lighter shade)
                elif node.startswith('Non-Branded Sponsored Brands'):
                    node_colors.append('#9467bd')  # Purple (lighter shade)
                elif node.startswith('Non-Branded Sponsored Display'):
                    node_colors.append('#8c564b')  # Brown (lighter shade)
                elif 'Keywords' in node:
                    node_colors.append('#e377c2')  # Pink
                elif 'Auto' in node:
                    node_colors.append('#17becf')  # Cyan
                elif 'Product Target' in node:
                    node_colors.append('#d62728')  # Red
                elif 'Category Target' in node:
                    node_colors.append('#ff9896')  # Light red
                elif 'Remarketing' in node:
                    node_colors.append('#aec7e8')  # Light blue
                else:
                    node_colors.append('#7f7f7f')  # Gray
            
            fig = go.Figure(data=[go.Sankey(
                arrangement='snap',  # Better node arrangement
                node=dict(
                    pad=20,  # More padding between nodes
                    thickness=25,  # Thicker nodes
                    line=dict(color='black', width=0.5),
                    label=nodes,
                    color=node_colors
                ),
                link=dict(
                    source=sources,
                    target=targets,
                    value=values,
                    hovertemplate='<b>%{source.label} â†’ %{target.label}</b><br>Sales: $%{value:,.2f}<extra></extra>',
                    color='rgba(100, 100, 100, 0.2)'  # Slightly visible link color
                )
            )])
        
            # Update layout for better appearance
            fig.update_layout(
                title=dict(
                    text='Ad Sales Flow by Branded/Non-Branded, Ad Type, and Match Type',
                    font=dict(size=16, color='#333333'),
                    x=0.5,  # Center the title
                    y=0.98  # Position at the top
                ),
                font=dict(family='Arial, sans-serif', size=13),
                height=700,  # Taller for better visibility
                margin=dict(l=20, r=20, t=60, b=20),
                paper_bgcolor='rgba(0,0,0,0)',  # Transparent background
                plot_bgcolor='rgba(0,0,0,0)'  # Transparent plot area
            )
        
            return fig
    
        # Create product group filter for Sankey diagrams
        # Removed duplicate filter
    
        # Create and display the Sankey diagrams in their respective tabs
        with flow_tabs[0]:  # Ad Spend Flow tab
            sankey_spend_fig = create_ad_spend_sankey_from_targeting_data()
            if sankey_spend_fig:
                st.plotly_chart(sankey_spend_fig, use_container_width=True)
            else:
                st.info('Not enough data to create the Ad Spend Flow diagram.')
    
        with flow_tabs[1]:  # Ad Sales Flow tab
            sankey_sales_fig = create_ad_sales_sankey_from_targeting_data()
            if sankey_sales_fig:
                st.plotly_chart(sankey_sales_fig, use_container_width=True)
            else:
                st.info('Not enough data to create the Ad Sales Flow diagram.')
    
        # --- Product Analysis Section ---
        st.markdown("<div id='product-analysis' class='section-anchor'></div>", unsafe_allow_html=True)
        st.markdown("<hr style='height:2px;border-width:0;color:gold;background-color:gold;margin-top:25px;margin-bottom:15px;margin-left:10px;margin-right:10px'>", unsafe_allow_html=True)
        st.markdown("<span class='main-section-header dashboard-section'>Product Analysis</span>", unsafe_allow_html=True)
        st.markdown("<div style='margin-bottom:1.2rem;'></div>", unsafe_allow_html=True)

        # Add a spinner while ASIN data is being processed
        with st.spinner("Processing data..."):
            # Prepare and cache column lookups
            bulk_data = st.session_state.get('bulk_data', {})
            if bulk_data is None:
                bulk_data = {}
            bulk_dfs = [df for df in bulk_data.values() if isinstance(df, pd.DataFrame)]
            combined_bulk_df = pd.concat(bulk_dfs, ignore_index=True) if bulk_dfs else None
            sales_df = st.session_state.get('sales_report_data')
            branded_asins_data = st.session_state.client_config.get('branded_asins_data', {}) if st.session_state.get('client_config') else {}

            # Caching possible columns for efficiency
            if isinstance(combined_bulk_df, pd.DataFrame):
                asin_columns_cache = [col for col in combined_bulk_df.columns if 'asin' in col.lower()]
                sales_columns_cache = [col for col in combined_bulk_df.columns if col.lower() == 'sales']
                sd_sales_cols_cache = [col for col in combined_bulk_df.columns if 'sales (views & clicks)' in col.lower()]
            else:
                asin_columns_cache, sales_columns_cache, sd_sales_cols_cache = [], [], []

            # Caching sales report columns
            if isinstance(sales_df, pd.DataFrame):
                sales_report_asin_cols = [col for col in sales_df.columns if 'asin' in col.lower()]
                sales_report_title_cols = [col for col in sales_df.columns if 'title' in col.lower() or 'product name' in col.lower() or 'item name' in col.lower()]
            else:
                sales_report_asin_cols, sales_report_title_cols = [], []

            # Initialize variables
            asin_perf_records = []
            
            # Create ASIN-to-SKU mapping for Seller Central accounts
            asin_to_sku_mapping = {}
            if isinstance(combined_bulk_df, pd.DataFrame):
                # Check if we have SKU column (indicates Seller Central) - case insensitive
                sku_col = None
                asin_col = None
                
                # Find SKU column (case insensitive)
                for col in combined_bulk_df.columns:
                    if col.upper() == 'SKU':
                        sku_col = col
                        break
                
                # Find ASIN column (case insensitive)
                for col in combined_bulk_df.columns:
                    if col.upper() == 'ASIN (INFORMATIONAL ONLY)':
                        asin_col = col
                        break
                    elif col.upper() == 'ASIN':
                        asin_col = col
                
                if sku_col:
                    
                    if asin_col:
                        # Find Entity column (case insensitive)
                        entity_col = None
                        for col in combined_bulk_df.columns:
                            if col.upper() == 'ENTITY':
                                entity_col = col
                                break
                        
                        # More flexible Entity check - look for any rows with both SKU and ASIN data
                        if entity_col:
                            # Try the strict Entity filter first (case insensitive)
                            product_ad_rows = combined_bulk_df[
                                (combined_bulk_df[entity_col].astype(str).str.strip().str.lower().isin(['product ad', 'product ads', 'productad', 'productads'])) &
                                (combined_bulk_df[sku_col].notna()) &
                                (combined_bulk_df[asin_col].notna())
                            ]
                            
                            # If no results with Entity filter, try without it
                            if product_ad_rows.empty:
                                product_ad_rows = combined_bulk_df[
                                    (combined_bulk_df[sku_col].notna()) &
                                    (combined_bulk_df[asin_col].notna())
                                ]
                        else:
                            # No Entity column, just filter by SKU and ASIN presence
                            product_ad_rows = combined_bulk_df[
                                (combined_bulk_df[sku_col].notna()) &
                                (combined_bulk_df[asin_col].notna())
                            ]
                    else:
                        product_ad_rows = pd.DataFrame()
                    
                    # Create the mapping using the detected column names
                    for _, row in product_ad_rows.iterrows():
                        asin = str(row[asin_col]).strip() if asin_col else ''
                        sku = str(row[sku_col]).strip() if sku_col else ''
                        if asin and sku and asin != 'nan' and sku != 'nan':
                            asin_to_sku_mapping[asin] = sku
                    
                    st.session_state.debug_messages.append(f"[Product Analysis] Found {len(product_ad_rows)} rows with SKU data")
                    st.session_state.debug_messages.append(f"[Product Analysis] Created SKU mapping for {len(asin_to_sku_mapping)} ASINs")
                    if len(asin_to_sku_mapping) > 0:
                        sample_mappings = list(asin_to_sku_mapping.items())[:3]
                        st.session_state.debug_messages.append(f"[Product Analysis] Sample SKU mappings: {sample_mappings}")
            
            # Get bulk file data to extract ad metrics
            bulk_dfs = []
            for sheet_name, df in bulk_data.items():
                if isinstance(df, pd.DataFrame):
                    bulk_dfs.append(df)
                    st.session_state.debug_messages.append(f"[Product Analysis] Including sheet '{sheet_name}' with {len(df)} rows")
        
            # Combine all bulk data sheets
            combined_bulk_df = pd.concat(bulk_dfs, ignore_index=True) if bulk_dfs else None
        
            if combined_bulk_df is not None:
                st.session_state.debug_messages.append(f"[Product Analysis] Combined bulk data has {len(combined_bulk_df)} total rows")
                if 'ASIN' in combined_bulk_df.columns:
                    asin_count = combined_bulk_df['ASIN'].dropna().nunique()
                    st.session_state.debug_messages.append(f"[Product Analysis] Found {asin_count} unique ASINs in combined data")
                if 'Entity' in combined_bulk_df.columns:
                    entity_counts = combined_bulk_df['Entity'].value_counts().to_dict()
                    st.session_state.debug_messages.append(f"[Product Analysis] Entity distribution: {entity_counts}")
            else:
                st.session_state.debug_messages.append("[Product Analysis] No combined bulk data available")
        
            # Get sales report data for Total Sales
            sales_df = st.session_state.get('sales_report_data')
        
            # Find total sales column in sales report
            total_sales_col = None
            if isinstance(sales_df, pd.DataFrame):
                # Define the helper function for finding columns
                def find_col_pattern(df_cols, patterns, case_sensitive=False):
                    """Finds the first matching column name from a list."""
                    # First try exact matches
                    for pattern in patterns:
                        if pattern in df_cols:
                            return pattern
                
                    # Then try case-insensitive if allowed
                    if not case_sensitive:
                        df_cols_lower = [col.lower() for col in df_cols]
                        for pattern in patterns:
                            pattern_lower = pattern.lower()
                            if pattern_lower in df_cols_lower:
                                idx = df_cols_lower.index(pattern_lower)
                                return df_cols[idx]  # Return original case
                    
                    # Finally try partial matches
                    for pattern in patterns:
                        pattern_lower = pattern.lower()
                        for col in df_cols:
                            if pattern_lower in col.lower():
                                return col
                
                    # If no match found
                    return None
            
                # Use user-selected Total Sales metric if available, otherwise fall back to default search
                if 'selected_total_sales_metric' in st.session_state and st.session_state.selected_total_sales_metric in sales_df.columns:
                    total_sales_col = st.session_state.selected_total_sales_metric
                else:
                    total_sales_col = find_col_pattern(sales_df.columns, ['Total Sales'])

                # Also detect Sessions/Glance Views column for later use
                sessions_col = find_col_pattern(
                    sales_df.columns,
                    ['Sessions', 'Glance Views', 'Detail Page Views']
                )
            else:
                total_sales_col = None
                sessions_col = None
        
            # Calculate total spend and sales for percentage calculations
            total_spend = 0
            total_ad_sales = 0
            total_sales = 0
        
            # Track all ASINs we'll process
            all_asins_to_process = set()
        
            # First, collect all ASINs from the sales report
            if isinstance(sales_df, pd.DataFrame) and 'ASIN' in sales_df.columns:
                sales_asins = sales_df['ASIN'].astype(str).str.strip().unique()
                all_asins_to_process.update(sales_asins)
        
            # Also add ASINs from branded_asins_data configuration
            if branded_asins_data:
                all_asins_to_process.update(branded_asins_data.keys())
        
            if isinstance(combined_bulk_df, pd.DataFrame):

                # We'll handle sales column selection at the row level based on Product type and attribution choice
                # Here we just need to calculate the totals for percentage calculations
            
                # Add debug info about bulk file
                if isinstance(combined_bulk_df, pd.DataFrame):
                    asin_columns_in_bulk = [col for col in combined_bulk_df.columns if 'asin' in col.lower()]
                
                    # Show sample of ASINs in bulk file
                    for col in asin_columns_in_bulk:
                        unique_asins = combined_bulk_df[col].dropna().astype(str).str.upper().unique()
                        sample_asins = list(unique_asins)[:10]  # Show first 10 for brevity
            # First, identify which ASINs actually appear in the bulk file
            asins_in_bulk_file = set()
            asin_columns = []  # Initialize to prevent NameError
            if isinstance(combined_bulk_df, pd.DataFrame):
                # Check all possible ASIN columns
                asin_columns = [col for col in combined_bulk_df.columns if 'asin' in col.lower()]
                for col in asin_columns:
                    # Get all unique ASINs from this column
                    asins = combined_bulk_df[col].astype(str).str.lower().unique()
                    asins_in_bulk_file.update(asins)
        
            # Log the count of ASINs found in bulk file
        
        # Vectorized, cached computation of ASIN performance
            @st.cache_data(show_spinner=False)
            def compute_asin_perf_cached(_bulk_df, _sales_df, _branded_json, sd_choice, _total_sales_col, _sessions_col, _sku_map):
                try:
                    # Prepare bulk metrics per ASIN
                    ad_agg = pd.DataFrame()
                    if isinstance(_bulk_df, pd.DataFrame) and not _bulk_df.empty:
                        dfb = _bulk_df.copy()
                        cols = list(dfb.columns)
                        # Find ASIN column (prefer exact 'ASIN', then 'ASIN (Informational Only)', else first contains 'asin')
                        asin_col = next((c for c in cols if c.lower() == 'asin'), None)
                        if asin_col is None:
                            asin_col = next((c for c in cols if c.lower() == 'asin (informational only)'), None)
                        if asin_col is None:
                            asin_like = [c for c in cols if 'asin' in c.lower()]
                            asin_col = asin_like[0] if asin_like else None
                        if asin_col is None:
                            dfb['ASIN_STD'] = ''
                        else:
                            dfb['ASIN_STD'] = dfb[asin_col].astype(str).str.strip().str.upper()

                        # Normalize selector columns
                        prod_col = next((c for c in cols if c.lower() == 'product'), None)
                        entity_col = next((c for c in cols if c.lower() == 'entity'), None)
                        spend_col = next((c for c in cols if c.lower() == 'spend'), None)
                        clicks_col = next((c for c in cols if c.lower() == 'clicks'), None)
                        orders_col = next((c for c in cols if c.lower() == 'orders'), None)
                        imps_col = next((c for c in cols if c.lower() == 'impressions'), None)

                        # Sales columns
                        sales_col = next((c for c in cols if c.lower() == 'sales'), None)
                        sd_vc_col = next((c for c in cols if 'sales (views & clicks)' in c.lower()), None)

                        mask = (dfb['ASIN_STD'].astype(str).str.len() > 0)
                        if entity_col is not None:
                            mask &= dfb[entity_col].astype(str).str.strip().str.lower().isin(['product ad', 'product ads'])
                        if prod_col is not None:
                            mask &= dfb[prod_col].astype(str).isin(['Sponsored Products', 'Sponsored Display'])
                        dfb = dfb[mask].copy()

                        # Build Ad Sales column honoring SD attribution choice
                        dfb['Ad Sales'] = 0.0
                        if prod_col is not None and sales_col is not None:
                            non_sd_mask = dfb[prod_col] != 'Sponsored Display'
                            dfb.loc[non_sd_mask, 'Ad Sales'] = pd.to_numeric(dfb.loc[non_sd_mask, sales_col], errors='coerce').fillna(0)
                        if prod_col is not None:
                            sd_mask = dfb[prod_col] == 'Sponsored Display'
                            if sd_choice == 'Sales (Views & Clicks)' and sd_vc_col is not None:
                                dfb.loc[sd_mask, 'Ad Sales'] = pd.to_numeric(dfb.loc[sd_mask, sd_vc_col], errors='coerce').fillna(0)
                            elif sales_col is not None:
                                dfb.loc[sd_mask, 'Ad Sales'] = pd.to_numeric(dfb.loc[sd_mask, sales_col], errors='coerce').fillna(0)

                        # Numeric conversions
                        for c_name, default in [(spend_col, 0), (clicks_col, 0), (orders_col, 0), (imps_col, 0)]:
                            if c_name is not None and c_name in dfb.columns:
                                dfb[c_name] = pd.to_numeric(dfb[c_name], errors='coerce').fillna(default)

                        # Aggregate
                        agg_map = {}
                        if spend_col is not None: agg_map[spend_col] = 'sum'
                        if clicks_col is not None: agg_map[clicks_col] = 'sum'
                        if orders_col is not None: agg_map[orders_col] = 'sum'
                        if imps_col is not None: agg_map[imps_col] = 'sum'
                        agg_map['Ad Sales'] = 'sum'
                        if agg_map:
                            ad_agg = dfb.groupby('ASIN_STD', as_index=False).agg(agg_map)
                            # Rename to standardized names
                            rename_map = {}
                            if spend_col is not None: rename_map[spend_col] = 'Spend'
                            if clicks_col is not None: rename_map[clicks_col] = 'Clicks'
                            if orders_col is not None: rename_map[orders_col] = 'Orders'
                            if imps_col is not None: rename_map[imps_col] = 'Impressions'
                            ad_agg = ad_agg.rename(columns=rename_map)

                    # Prepare sales totals per ASIN
                    sales_agg = pd.DataFrame()
                    if isinstance(_sales_df, pd.DataFrame) and not _sales_df.empty and 'ASIN' in _sales_df.columns and _total_sales_col:
                        s = _sales_df.copy()
                        s['ASIN_STD'] = s['ASIN'].astype(str).str.strip().str.upper()
                        if _total_sales_col in s.columns:
                            s['Total Sales'] = pd.to_numeric(s[_total_sales_col], errors='coerce').fillna(0)
                            if _sessions_col and _sessions_col in s.columns:
                                s['Sessions'] = pd.to_numeric(s[_sessions_col], errors='coerce').fillna(0)
                            else:
                                s['Sessions'] = 0
                            sales_agg = s.groupby('ASIN_STD', as_index=False)[['Total Sales', 'Sessions']].sum()

                    # Merge ad and sales
                    if not ad_agg.empty and not sales_agg.empty:
                        df = pd.merge(ad_agg, sales_agg, on='ASIN_STD', how='outer')
                    elif not ad_agg.empty:
                        df = ad_agg.copy()
                        df['Total Sales'] = 0.0
                        df['Sessions'] = 0.0
                    elif not sales_agg.empty:
                        df = sales_agg.copy()
                        df[['Spend', 'Clicks', 'Orders', 'Impressions', 'Ad Sales']] = 0.0
                    else:
                        df = pd.DataFrame(columns=['ASIN_STD', 'Spend', 'Clicks', 'Orders', 'Impressions', 'Ad Sales', 'Total Sales', 'Sessions'])

                    # Attach metadata from branded_asins_data and SKU mapping
                    if _branded_json:
                        meta = pd.DataFrame([
                            {
                                'ASIN_STD': str(k).strip().upper(),
                                'Product Group': (v or {}).get('product_group', '') or 'Untagged Group',
                                'Product Title': (v or {}).get('product_title', ''),
                                'SKU': (v or {}).get('sku', '')
                            }
                            for k, v in _branded_json.items()
                        ])
                    else:
                        meta = pd.DataFrame(columns=['ASIN_STD', 'Product Group', 'Product Title', 'SKU'])

                    df = pd.merge(df, meta, on='ASIN_STD', how='left')

                    # Prefer SKU from bulk SKU map if present
                    if _sku_map:
                        sku_series = pd.Series(_sku_map, dtype=object)
                        sku_series.index = sku_series.index.map(lambda x: str(x).strip().upper())
                        df['SKU'] = df['SKU'].astype(str).where(df['SKU'].astype(str).str.strip().ne(''), df['ASIN_STD'].map(sku_series))

                    # Finalize columns and compute ACoS/TACoS
                    df['ASIN'] = df['ASIN_STD']
                    # Ensure numeric columns exist
                    for col in ['Spend', 'Ad Sales', 'Total Sales', 'Clicks', 'Orders', 'Impressions', 'Sessions']:
                        if col not in df.columns:
                            df[col] = 0.0
                        df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0)

                    df['ACoS'] = np.where(df['Ad Sales'] > 0, (df['Spend'] / df['Ad Sales']) * 100.0, 0.0)
                    df['TACoS'] = np.where(df['Total Sales'] > 0, (df['Spend'] / df['Total Sales']) * 100.0, 0.0)

                    # Order columns
                    preferred_cols = ['Product Group', 'ASIN', 'Product Title', 'SKU', 'Spend', 'Ad Sales', 'Total Sales', 'Impressions', 'Sessions', '% of Spend', '% of Ad Sales', '% of Total Sales', 'ACoS', 'TACoS', 'Clicks', 'Orders']
                    for pc in preferred_cols:
                        if pc not in df.columns:
                            df[pc] = df.get(pc, 0)
                    df = df[preferred_cols]

                    return df
                except Exception as e:
                    if 'debug_messages' in st.session_state:
                        st.session_state.debug_messages.append(f"[Product Analysis] Vectorized compute error: {e}")
                    return pd.DataFrame(columns=['Product Group', 'ASIN', 'Product Title', 'SKU', 'Spend', 'Ad Sales', 'Total Sales', 'Impressions', 'Sessions', '% of Spend', '% of Ad Sales', '% of Total Sales', 'ACoS', 'TACoS', 'Clicks', 'Orders'])

            # Compute with caching
            sd_choice = st.session_state.get('sd_attribution_choice', 'Sales')
            branded_json = branded_asins_data if isinstance(branded_asins_data, dict) else {}
            try:
                asin_perf_df = compute_asin_perf_cached(
                    combined_bulk_df,
                    sales_df,
                    branded_json,
                    sd_choice,
                    total_sales_col,
                    sessions_col,
                    asin_to_sku_mapping
                )
            except Exception:
                # Fallback to empty df if cache fails
                asin_perf_df = pd.DataFrame()
        
            # Log how many ASINs were processed
            if not asin_perf_df.empty:
                branded_count = len(asin_perf_df[asin_perf_df['Product Group'] != 'Untagged Group'])
                untagged_count = len(asin_perf_df[asin_perf_df['Product Group'] == 'Untagged Group'])
                st.session_state.debug_messages.append(f"[Product Analysis] Processed {len(asin_perf_df)} total ASINs ({branded_count} branded, {untagged_count} untagged)")
                
                # Calculate totals for debug info
                total_spend_processed = asin_perf_df['Spend'].sum()
                total_ad_sales_processed = asin_perf_df['Ad Sales'].sum()
                total_sales_processed = asin_perf_df['Total Sales'].sum()
                st.session_state.debug_messages.append(f"[Product Analysis] Total metrics: Spend ${total_spend_processed:,.2f}, Ad Sales ${total_ad_sales_processed:,.2f}, Total Sales ${total_sales_processed:,.2f}")
            else:
                st.session_state.debug_messages.append("[Product Analysis] No ASIN performance data created")
        
            # Populate missing SKUs from Branded ASINs data for Seller Central clients
            if not asin_perf_df.empty and 'SKU' in asin_perf_df.columns:
                branded_asins_data = st.session_state.client_config.get('branded_asins_data', {})
                if branded_asins_data:
                    populated_count = 0
                    for idx, row in asin_perf_df.iterrows():
                        asin = row['ASIN']
                        current_sku = str(row.get('SKU', '')).strip()
                        
                        # If SKU is missing or empty, try to populate from Branded ASINs data
                        if not current_sku or current_sku in ['', 'nan', 'NaN']:
                            if asin in branded_asins_data:
                                persisted_sku = branded_asins_data[asin].get('sku', '')
                                if persisted_sku and persisted_sku.strip():
                                    asin_perf_df.at[idx, 'SKU'] = persisted_sku.strip()
                                    populated_count += 1
                    
                    if populated_count > 0:
                        st.session_state.debug_messages.append(f"[Product Analysis] Populated {populated_count} missing SKUs from Branded ASINs data")

            # Store the ASIN performance data in the session state so it can be accessed by other sections
            st.session_state.asin_perf_df = asin_perf_df
            
            # Always-on inclusion filter: Keep only ASINs with any activity
            # Activity is defined as any of Spend, Ad Sales, Clicks, Impressions, Sessions/Glance Views, or Total Sales > 0
            if not asin_perf_df.empty:
                try:
                    _sp  = pd.to_numeric(asin_perf_df.get('Spend', 0), errors='coerce').fillna(0)
                    _ads = pd.to_numeric(asin_perf_df.get('Ad Sales', 0), errors='coerce').fillna(0)
                    _clk = pd.to_numeric(asin_perf_df.get('Clicks', 0), errors='coerce').fillna(0)
                    _imp = pd.to_numeric(asin_perf_df.get('Impressions', 0), errors='coerce').fillna(0)
                    _tot = pd.to_numeric(asin_perf_df.get('Total Sales', 0), errors='coerce').fillna(0)
                    _ses = pd.to_numeric(asin_perf_df.get('Sessions', 0), errors='coerce').fillna(0) if 'Sessions' in asin_perf_df.columns else pd.Series([0]*len(asin_perf_df), index=asin_perf_df.index)
                    _before = len(asin_perf_df)
                    asin_perf_df = asin_perf_df[(_sp > 0) | (_ads > 0) | (_clk > 0) | (_imp > 0) | (_tot > 0) | (_ses > 0)].copy()
                    _after = len(asin_perf_df)
                    if _after != _before and 'debug_messages' in st.session_state:
                        st.session_state.debug_messages.append(
                            f"[Product Analysis] Inclusion filter removed {_before - _after} inactive ASINs; remaining {_after}"
                        )
                except Exception as _e:
                    if 'debug_messages' in st.session_state:
                        st.session_state.debug_messages.append(f"[Product Analysis] Inclusion filter error: {_e}")
                # Update session state with filtered data
                st.session_state.asin_perf_df = asin_perf_df
            
            # Smart filtering for very large datasets (>10,000 ASINs)
            # Only show ASINs with any performance data (any of Spend, Ad Sales, Clicks, Impressions, Sessions, Total Sales > 0)
            if not asin_perf_df.empty and len(asin_perf_df) > 10000:
                # Count original ASINs
                original_count = len(asin_perf_df)
                
                # Filter to ASINs with meaningful data
                imp_series = pd.to_numeric(asin_perf_df.get('Impressions', 0), errors='coerce').fillna(0)
                ses_series = pd.to_numeric(asin_perf_df.get('Sessions', 0), errors='coerce').fillna(0)
                active_asins_mask = (
                    (asin_perf_df['Spend'] > 0) |
                    (asin_perf_df['Ad Sales'] > 0) |
                    (asin_perf_df['Clicks'] > 0) |
                    (asin_perf_df['Total Sales'] > 0) |
                    (imp_series > 0) |
                    (ses_series > 0)
                )
                asin_perf_df = asin_perf_df[active_asins_mask].copy()
                
                # Update the session state with filtered data
                st.session_state.asin_perf_df = asin_perf_df
                
                # Show filtering info
                filtered_count = len(asin_perf_df)
                inactive_count = original_count - filtered_count
                
                if inactive_count > 0:
                    st.info(
                        f"ðŸŽ¯ **Smart Filtering Applied:** Showing {filtered_count:,} active ASINs "
                        f"(filtered out {inactive_count:,} inactive ASINs with no spend, no ad sales, no clicks, no impressions, no sessions, and no total sales)"
                    )
                    st.session_state.debug_messages.append(
                        f"[Smart Filter] Reduced dataset from {original_count:,} to {filtered_count:,} ASINs "
                        f"by removing inactive ASINs"
                    )
            
            if not asin_perf_df.empty:
                # Debug: Show SKU mapping status (only if debug mode is enabled)
                if st.session_state.get('global_debug_mode', False):
                    st.write(f"Debug: SKU mapping has {len(asin_to_sku_mapping)} entries")
                    if len(asin_to_sku_mapping) > 0:
                        sample_mappings = list(asin_to_sku_mapping.items())[:3]
                        st.write(f"Debug: Sample mappings: {sample_mappings}")
                
                # --- SKU Column Option ---
                show_sku_column = False
                if asin_to_sku_mapping:  # Only show option if we have SKU data
                    st.markdown("**Display Options**")
                    show_sku_column = st.checkbox(
                        "Include SKU column (Seller Central)", 
                        value=False, 
                        key="show_sku_column",
                        help="Show SKU column with data from Seller Central bulk file"
                    )
                    st.markdown("---")
                
                # --- Basic Filters ---
                if show_sku_column:
                    col1, col2, col3, col4 = st.columns(4)
                    with col4:
                        filter_sku = st.text_input("Filter by SKU", "", key="asin_filter_sku")
                else:
                    col1, col2, col3 = st.columns(3)
                    filter_sku = ""
                
                with col1:
                    # --- Product Group Multiselect (moved to leftmost position) ---
                    # Gather all unique product groups from the DataFrame
                    product_groups = sorted([g for g in asin_perf_df['Product Group'].dropna().unique() if g])
                
                    # Initialize all session state variables if they don't exist
                    if 'asin_product_group_filter' not in st.session_state:
                        st.session_state.asin_product_group_filter = []
                    if 'asin_filter_active' not in st.session_state:
                        st.session_state.asin_filter_active = False
                
                    # Define a callback function to update related session state variables
                    def update_asin_filter_state():
                        # Update filter active state based on current selection
                        st.session_state.asin_filter_active = len(st.session_state.asin_product_group_filter) > 0
                
                    # Always show the multiselect, but disable it if no product groups
                    if product_groups:
                        # Avoid Streamlit warning by only passing 'default' if session state is not already set
                        if 'asin_product_group_filter' in st.session_state and st.session_state.asin_product_group_filter:
                            st.multiselect(
                                "Filter by Product Group(s)",
                                options=product_groups,
                                key="asin_product_group_filter",
                                on_change=update_asin_filter_state
                            )
                        else:
                            st.multiselect(
                                "Filter by Product Group(s)",
                                options=product_groups,
                                key="asin_product_group_filter",
                                on_change=update_asin_filter_state
                            )
                        # The callback will handle updating session state
                        filter_group = st.session_state.asin_product_group_filter
                    else:
                        st.multiselect(
                            "Filter by Product Group(s) (Add product groups in Client Settings)",
                            options=[],
                            disabled=True,
                            key="asin_product_group_filter_disabled"
                        )
                        # Reset filter state safely
                        if st.session_state.asin_product_group_filter:
                            st.session_state.asin_product_group_filter = []
                            st.session_state.asin_filter_active = False
                        filter_group = []
                with col2:
                    filter_title = st.text_input("Filter by Product Title", "", key="asin_filter_title")
                with col3:
                    filter_asin = st.text_input("Filter by ASIN", "", key="asin_filter_asin")
                
                # Advanced Filters Section
                st.markdown("---")
                with st.expander("ðŸ”§ Advanced Filters", expanded=False):
                    # Initialize advanced filter state
                    if 'asin_filter_groups' not in st.session_state:
                        st.session_state.asin_filter_groups = []
                    st.markdown("**Advanced Filtering with Filter Groups**")
                    st.caption("Create multiple filter groups and combine them with AND/OR logic for complex filtering.")
                    
                    # Build filterable columns based on visible/relevant columns
                    # Keep 'ASIN', 'Product Title', and 'SKU' available for advanced text filters
                    # Only exclude 'Product Group' (already covered by basic filter)
                    excluded_cols = {'Product Group'}
                    filterable_columns = [col for col in asin_perf_df.columns if col not in excluded_cols]
                    
                    # Dynamically classify numeric vs text columns
                    numeric_columns = []
                    for col in filterable_columns:
                        try:
                            test_series = pd.to_numeric(
                                asin_perf_df[col].astype(str).str.replace('$', '').str.replace('%', '').str.replace(',', ''),
                                errors='coerce'
                            )
                            if not test_series.isna().all():
                                numeric_columns.append(col)
                        except Exception:
                            pass
                    text_columns = [c for c in filterable_columns if c not in numeric_columns]
                    
                    # Operator sets
                    numeric_ops = ['>', '>=', '=', '<=', '<']
                    text_ops = ['contains', "doesn't contain", 'equals', "doesn't equal"]
                    
                    # Control buttons
                    col_btn1, col_btn2, col_btn3, col_spacer = st.columns([1.5, 1.5, 1.5, 5.5])
                    
                    with col_btn1:
                        add_group = st.button("âž• Add Filter Group", key="asin_add_filter_group")
                    
                    with col_btn2:
                        clear_all = False
                        if st.session_state.asin_filter_groups:
                            clear_all = st.button("ðŸ—‘ï¸ Clear All", key="asin_clear_all_filter_groups")
                    
                    # Handle button actions without immediate rerun
                    if add_group:
                        # Default to first available column
                        default_col = filterable_columns[0] if filterable_columns else 'Spend'
                        default_is_numeric = default_col in numeric_columns
                        st.session_state.asin_filter_groups.append({
                            'filters': [{
                                'column': default_col,
                                'operator': (numeric_ops[0] if default_is_numeric else text_ops[0]),
                                'value': '',
                                'logic': 'AND'
                            }],
                            'group_logic': 'AND'
                        })
                    
                    if clear_all:
                        st.session_state.asin_filter_groups = []
                        # Immediate refresh so UI reflects cleared groups without extra click
                        st.rerun()
                    
                    # Display filter groups
                    groups_to_remove = []
                    for group_idx, group in enumerate(st.session_state.asin_filter_groups):
                        with st.container():
                            # Group header
                            group_col1, group_col2, group_col3 = st.columns([6, 2, 1])
                            
                            with group_col1:
                                st.markdown(f"**Filter Group {group_idx + 1}**")
                            
                            with group_col2:
                                if group_idx > 0:  # Only show group logic for groups after the first
                                    group['group_logic'] = st.selectbox(
                                        "Group Logic",
                                        options=['AND', 'OR'],
                                        index=['AND', 'OR'].index(group['group_logic']),
                                        key=f"asin_group_logic_{group_idx}",
                                        help="How this group combines with previous groups"
                                    )
                            
                            with group_col3:
                                if st.button("ðŸ—‘ï¸", key=f"asin_remove_group_{group_idx}", help="Remove entire group"):
                                    groups_to_remove.append(group_idx)
                            
                            # Display filters within this group
                            filters_to_remove = []
                            for filter_idx, filter_config in enumerate(group['filters']):
                                col1, col2, col3, col4, col5, col6 = st.columns([2, 1.2, 1.2, 1, 0.8, 0.8])
                                
                                with col1:
                                    # Stabilize Column widget
                                    k_col = f"asin_filter_column_{group_idx}_{filter_idx}"
                                    if k_col not in st.session_state:
                                        st.session_state[k_col] = filter_config.get('column', (filterable_columns[0] if filterable_columns else 'Spend'))
                                    st.selectbox(
                                        "Column" if filter_idx == 0 else "",
                                        options=filterable_columns,
                                        key=k_col,
                                        label_visibility="visible" if filter_idx == 0 else "collapsed"
                                    )
                                    filter_config['column'] = st.session_state[k_col]
                                
                                with col2:
                                    # Stabilize Operator widget based on current column type
                                    is_numeric = filter_config['column'] in numeric_columns
                                    valid_ops = numeric_ops if is_numeric else text_ops
                                    k_op = f"asin_filter_operator_{group_idx}_{filter_idx}"
                                    if k_op not in st.session_state:
                                        st.session_state[k_op] = filter_config.get('operator', valid_ops[0])
                                    # If current operator in state is invalid for new column type, reset to default
                                    if st.session_state[k_op] not in valid_ops:
                                        st.session_state[k_op] = valid_ops[0]
                                    st.selectbox(
                                        "Operator" if filter_idx == 0 else "",
                                        options=valid_ops,
                                        key=k_op,
                                        label_visibility="visible" if filter_idx == 0 else "collapsed"
                                    )
                                    filter_config['operator'] = st.session_state[k_op]
                                
                                with col3:
                                    # Stabilize Value widget
                                    k_val = f"asin_filter_value_{group_idx}_{filter_idx}"
                                    if k_val not in st.session_state:
                                        st.session_state[k_val] = str(filter_config.get('value', ''))
                                    st.text_input(
                                        "Value" if filter_idx == 0 else "",
                                        key=k_val,
                                        label_visibility="visible" if filter_idx == 0 else "collapsed"
                                    )
                                    filter_config['value'] = st.session_state[k_val]
                                
                                with col4:
                                    if filter_idx > 0:  # Only show logic for filters after the first in each group
                                        # Stabilize Logic widget
                                        k_logic = f"asin_filter_logic_{group_idx}_{filter_idx}"
                                        if k_logic not in st.session_state:
                                            st.session_state[k_logic] = filter_config.get('logic', 'AND')
                                        st.selectbox(
                                            "Logic" if filter_idx == 1 else "",
                                            options=['AND', 'OR'],
                                            key=k_logic,
                                            label_visibility="visible" if filter_idx == 1 else "collapsed"
                                        )
                                        filter_config['logic'] = st.session_state[k_logic]
                                    else:
                                        st.write("")  # Empty space for first filter in group
                                
                                with col5:
                                    if st.button("âž•", key=f"asin_add_filter_{group_idx}_{filter_idx}", help="Add filter to this group"):
                                        next_col = filterable_columns[0] if filterable_columns else 'Spend'
                                        next_is_numeric = next_col in numeric_columns
                                        group['filters'].append({
                                            'column': next_col,
                                            'operator': (numeric_ops[0] if next_is_numeric else text_ops[0]),
                                            'value': '',
                                            'logic': 'AND'
                                        })
                                
                                with col6:
                                    if len(group['filters']) > 1:  # Don't allow removing the last filter in a group
                                        if st.button("ðŸ—‘ï¸", key=f"asin_remove_filter_{group_idx}_{filter_idx}", help="Remove filter"):
                                            filters_to_remove.append(filter_idx)
                            
                            # Remove filters marked for removal (process after all buttons to avoid key conflicts)
                            if filters_to_remove:
                                for filter_idx in reversed(filters_to_remove):
                                    group['filters'].pop(filter_idx)
                                # Immediate refresh to reflect deletion without extra click
                                st.rerun()
                            
                            st.markdown("---")
                    
                    # Remove groups marked for removal (process after all buttons to avoid key conflicts)  
                    if groups_to_remove:
                        for group_idx in reversed(groups_to_remove):
                            st.session_state.asin_filter_groups.pop(group_idx)
                        # Immediate refresh to reflect deletion without extra click
                        st.rerun()
                    
                    # Show active filter summary
                    if st.session_state.asin_filter_groups:
                        active_filters = sum(len([f for f in group['filters'] if f['value'].strip()]) for group in st.session_state.asin_filter_groups)
                        if active_filters > 0:
                            st.info(f"ðŸŽ¯ **{active_filters} advanced filter(s) active** - Results are filtered based on your criteria")
                
                # Toggle to show only rows where % of Total Sales > % of Spend
                show_sales_gt_spend = st.checkbox(
                    "Only rows where % of Total Sales > % of Spend",
                    key="asin_sales_gt_spend"
                )

                # Create a row with a subtle help icon and tooltip for the header
                col1, col2 = st.columns([0.98, 0.02])
                with col1:
                    st.markdown('#### Performance by ASIN')
                with col2:
                    st.markdown("""<div title="Product-Level metrics do not include SB data due to ambiguous attribution. Some products may have higher Ad Sales than Total Sales if a click on the ad led to a Brand Halo purchase on a variated product.">â„¹ï¸</div>""", unsafe_allow_html=True)
                
                filtered_df = asin_perf_df.copy()
                if filter_title:
                    filtered_df = filtered_df[filtered_df['Product Title'].str.contains(filter_title, case=False, na=False)]
                if filter_group:
                    filtered_df = filtered_df[filtered_df['Product Group'].isin(filter_group)]
                if filter_asin:
                    filtered_df = filtered_df[filtered_df['ASIN'].str.contains(filter_asin, case=False, na=False)]
                if filter_sku:
                    filtered_df = filtered_df[filtered_df['SKU'].str.contains(filter_sku, case=False, na=False)]
                
                # Apply advanced filters
                if st.session_state.asin_filter_groups:
                    try:
                        group_masks = []
                        
                        # Process each filter group
                        for group in st.session_state.asin_filter_groups:
                            group_mask = None
                            
                            # Process filters within the group
                            for filter_idx, filter_config in enumerate(group['filters']):
                                if filter_config['value'].strip():  # Only apply if value is provided
                                    column = filter_config['column']
                                    operator = filter_config['operator']
                                    value_str = filter_config['value'].strip()
                                    logic = filter_config['logic']
                                    
                                    if column in filtered_df.columns:
                                        is_numeric = column in numeric_columns
                                        if is_numeric:
                                            # Numeric comparison after cleaning
                                            numeric_series = pd.to_numeric(
                                                filtered_df[column].astype(str).str.replace('$', '').str.replace('%', '').str.replace(',', ''),
                                                errors='coerce'
                                            )
                                            try:
                                                value = float(value_str.replace('$', '').replace('%', '').replace(',', ''))
                                                # Create condition mask based on operator
                                                if operator == '>':
                                                    condition_mask = numeric_series > value
                                                elif operator == '>=':
                                                    condition_mask = numeric_series >= value
                                                elif operator == '=':
                                                    condition_mask = numeric_series == value
                                                elif operator == '<=':
                                                    condition_mask = numeric_series <= value
                                                elif operator == '<':
                                                    condition_mask = numeric_series < value
                                                else:
                                                    condition_mask = pd.Series([True] * len(numeric_series), index=numeric_series.index)
                                                condition_mask = condition_mask.fillna(False)
                                            except ValueError:
                                                st.error(f"Invalid numeric value in Group {len(group_masks)+1}, Filter {filter_idx+1}: '{value_str}'")
                                                continue
                                        else:
                                            # Text comparisons (case-insensitive)
                                            series_str = filtered_df[column].astype(str).str.strip().str.lower()
                                            value_norm = value_str.strip().lower()
                                            if operator == 'contains':
                                                condition_mask = series_str.str.contains(re.escape(value_norm), na=False)
                                            elif operator == "doesn't contain":
                                                condition_mask = ~series_str.str.contains(re.escape(value_norm), na=False)
                                            elif operator == 'equals':
                                                condition_mask = series_str == value_norm
                                            elif operator == "doesn't equal":
                                                condition_mask = series_str != value_norm
                                            else:
                                                condition_mask = pd.Series([True] * len(series_str), index=series_str.index)
                                        
                                        # Combine with previous conditions in this group
                                        if filter_idx == 0 or group_mask is None:
                                            group_mask = condition_mask
                                        else:
                                            if logic == 'AND':
                                                group_mask = group_mask & condition_mask
                                            else:  # OR
                                                group_mask = group_mask | condition_mask
                            
                            # Add this group's mask to the list
                            if group_mask is not None:
                                group_masks.append((group_mask, group.get('group_logic', 'AND')))
                        
                        # Combine all group masks
                        if group_masks:
                            final_mask = group_masks[0][0]  # Start with first group
                            
                            for i in range(1, len(group_masks)):
                                mask, logic = group_masks[i]
                                if logic == 'AND':
                                    final_mask = final_mask & mask
                                else:  # OR
                                    final_mask = final_mask | mask
                            
                            # Apply the final combined mask
                            filtered_df = filtered_df[final_mask]
                            # Enforce ROAS > 0 if any ROAS '>' filter exists in ASIN/Product Analysis filters
                            try:
                                if any(
                                    (isinstance(g, dict) and isinstance(g.get('filters', []), list) and any(
                                        (isinstance(f, dict) and f.get('column') == 'ROAS' and f.get('operator') == '>' and str(f.get('value', '')).strip() != '')
                                        for f in g.get('filters', [])
                                    ))
                                    for g in st.session_state.get('asin_filter_groups', [])
                                ):
                                    if 'ROAS' in filtered_df.columns:
                                        _roas = pd.to_numeric(
                                            filtered_df['ROAS'].astype(str).str.replace('$', '').str.replace('%', '').str.replace(',', ''),
                                            errors='coerce'
                                        )
                                        filtered_df = filtered_df[_roas > 0]
                            except Exception:
                                pass
                            
                    except Exception as e:
                        st.error(f"Error applying advanced filters: {str(e)}")
            
                # --- Totals Row ---
                # Attach Sessions or Glance Views from Business Report (Sales Report) if not already present
                try:
                    if 'Sessions' not in filtered_df.columns:
                        _sales_df = st.session_state.get('sales_report_data')
                        if isinstance(_sales_df, pd.DataFrame) and ('ASIN' in _sales_df.columns):
                            # Detect a sessions-like column (Sessions or Glance Views/Detail Page Views)
                            _cols = list(_sales_df.columns)
                            _sess_col = None
                            # Priority: Sessions > Glance Views > Detail Page Views > any column containing 'glance'
                            for _cand in ['Sessions', 'Glance Views', 'Detail Page Views']:
                                if any(c.lower() == _cand.lower() for c in _cols):
                                    _sess_col = next(c for c in _cols if c.lower() == _cand.lower())
                                    break
                            if _sess_col is None:
                                _glance_matches = [c for c in _cols if 'glance' in c.lower()]
                                if _glance_matches:
                                    _sess_col = _glance_matches[0]
                            if _sess_col is not None:
                                tmp_sessions = _sales_df[['ASIN', _sess_col]].copy()
                                tmp_sessions = tmp_sessions.rename(columns={_sess_col: 'Sessions'})
                                tmp_sessions['ASIN'] = tmp_sessions['ASIN'].astype(str).str.upper().str.strip()
                                filtered_df['ASIN'] = filtered_df['ASIN'].astype(str).str.upper().str.strip()
                                filtered_df = filtered_df.merge(tmp_sessions, on='ASIN', how='left', suffixes=('', '_sales'))
                                # If a Sessions column already existed from other sources, prefer the merged one and fill NaN with 0
                                if 'Sessions_sales' in filtered_df.columns:
                                    filtered_df['Sessions'] = filtered_df['Sessions_sales'].fillna(filtered_df.get('Sessions', 0)).fillna(0)
                                    filtered_df = filtered_df.drop(columns=['Sessions_sales'])
                                else:
                                    filtered_df['Sessions'] = filtered_df['Sessions'].fillna(0) if 'Sessions' in filtered_df.columns else 0
                except Exception as e:
                    if 'debug_messages' in st.session_state:
                        st.session_state.debug_messages.append(f"[ASIN Table] Error merging Sessions/Glance Views: {e}")
                global_total_spend = asin_perf_df['Spend'].sum() if 'Spend' in asin_perf_df.columns else 0
                global_total_ad_sales = asin_perf_df['Ad Sales'].sum() if 'Ad Sales' in asin_perf_df.columns else 0
                global_total_sales = asin_perf_df['Total Sales'].sum() if 'Total Sales' in asin_perf_df.columns else 0
            
                # Calculate percentages for each row
                if global_total_spend > 0:
                    filtered_df['% of Spend'] = (filtered_df['Spend'] / global_total_spend * 100).round(2)
                if global_total_ad_sales > 0:
                    filtered_df['% of Ad Sales'] = (filtered_df['Ad Sales'] / global_total_ad_sales * 100).round(2)
                if global_total_sales > 0:
                    filtered_df['% of Total Sales'] = (filtered_df['Total Sales'] / global_total_sales * 100).round(2)
            
                # Add debug info about totals
            
                # --- Totals Row ---
                if not filtered_df.empty:
                    # Removed 'Found In Bulk' column as per user request
                
                    # Default sort will be applied when displaying the dataframe
                
                    # Add debug info about the dataframe
                    top_asins = filtered_df.head(5)['ASIN'].tolist()
                
                                # Add info about untagged ASINs in the filtered dataframe
                untagged_in_filtered = filtered_df[filtered_df['Product Group'] == 'Untagged Group']
                if True:  # Always display table, regardless of untagged ASINs
                    untagged_count = len(untagged_in_filtered)
                    untagged_spend = untagged_in_filtered['Spend'].sum()
                    untagged_sales = untagged_in_filtered['Ad Sales'].sum()
                
                    # Calculate filtered totals for numeric values before formatting
                    filtered_total_spend = filtered_df['Spend'].sum() if 'Spend' in filtered_df.columns else 0
                    filtered_total_ad_sales = filtered_df['Ad Sales'].sum() if 'Ad Sales' in filtered_df.columns else 0
                    filtered_total_sales = filtered_df['Total Sales'].sum() if 'Total Sales' in filtered_df.columns else 0
                    filtered_total_clicks = filtered_df['Clicks'].sum() if 'Clicks' in filtered_df.columns else 0
                    filtered_total_sessions = filtered_df['Sessions'].sum() if 'Sessions' in filtered_df.columns else 0
                
                    # Calculate global totals from the original dataframe (before filtering)
                    global_total_spend = asin_perf_df['Spend'].sum() if 'Spend' in asin_perf_df.columns else 0
                    global_total_ad_sales = asin_perf_df['Ad Sales'].sum() if 'Ad Sales' in asin_perf_df.columns else 0
                    global_total_sales = asin_perf_df['Total Sales'].sum() if 'Total Sales' in asin_perf_df.columns else 0
                
                    # Now calculate percentages for each row based on filtered totals
                    if filtered_total_spend > 0:
                        filtered_df['% of Spend'] = filtered_df['Spend'].apply(lambda x: (x / filtered_total_spend * 100) if filtered_total_spend > 0 else 0)
                    if filtered_total_ad_sales > 0:
                        filtered_df['% of Ad Sales'] = filtered_df['Ad Sales'].apply(lambda x: (x / filtered_total_ad_sales * 100) if filtered_total_ad_sales > 0 else 0)
                    if filtered_total_sales > 0:
                        filtered_df['% of Total Sales'] = filtered_df['Total Sales'].apply(lambda x: (x / filtered_total_sales * 100) if filtered_total_sales > 0 else 0)
                
                    # Apply sales > spend percentage filter if enabled
                    if show_sales_gt_spend:
                        try:
                            if ('% of Total Sales' in filtered_df.columns) and ('% of Spend' in filtered_df.columns):
                                filtered_df = filtered_df[filtered_df['% of Total Sales'] > filtered_df['% of Spend']]
                            else:
                                # Fallback: compute using raw values and totals
                                spend_series = pd.to_numeric(filtered_df['Spend'], errors='coerce').fillna(0)
                                sales_series = pd.to_numeric(filtered_df['Total Sales'], errors='coerce').fillna(0)
                                if filtered_total_sales > 0 and filtered_total_spend > 0:
                                    cond = (sales_series / filtered_total_sales * 100) > (spend_series / filtered_total_spend * 100)
                                    filtered_df = filtered_df[cond]
                        except Exception:
                            pass

                    # No pagination - display all rows
                    paged_df = filtered_df.copy()

                    # Format numeric columns for display (on paged_df only)
                    display_df = paged_df.copy()

                    # Remove 'Orders' (keep Clicks visible), add CVR and AOV
                    for col in ['Orders']:
                        if col in display_df.columns:
                            display_df = display_df.drop(columns=[col])

                    # Calculate CVR (Orders/Clicks) and AOV (Ad Sales/Orders)
                    clicks = paged_df['Clicks'] if 'Clicks' in paged_df.columns else 0
                    orders = paged_df['Orders'] if 'Orders' in paged_df.columns else 0
                    ad_sales = paged_df['Ad Sales'] if 'Ad Sales' in paged_df.columns else 0

                    # Make sure all are numeric
                    clicks = pd.to_numeric(clicks, errors='coerce').fillna(0)
                    orders = pd.to_numeric(orders, errors='coerce').fillna(0)
                    ad_sales = pd.to_numeric(ad_sales, errors='coerce').fillna(0)

                    # Add CVR and AOV columns
                    display_df['CVR'] = (orders / clicks * 100).replace([np.inf, -np.inf], 0).fillna(0)
                    display_df['AOV'] = (ad_sales / orders).replace([np.inf, -np.inf], 0).fillna(0)

                    # Add Ad Sales % of Total (row-level) and Ad Traffic % of Total (Clicks/Sessions)
                    try:
                        if ('Ad Sales' in display_df.columns) and ('Total Sales' in display_df.columns):
                            display_df['Ad Sales % of Total'] = (pd.to_numeric(display_df['Ad Sales'], errors='coerce').fillna(0) /
                                                                 pd.to_numeric(display_df['Total Sales'], errors='coerce').replace({0: np.nan})) * 100
                            display_df['Ad Sales % of Total'] = display_df['Ad Sales % of Total'].replace([np.inf, -np.inf], 0).fillna(0)
                        else:
                            display_df['Ad Sales % of Total'] = 0
                    except Exception:
                        display_df['Ad Sales % of Total'] = 0

                    try:
                        if ('Clicks' in display_df.columns) and ('Sessions' in display_df.columns):
                            display_df['Ad Traffic % of Total'] = (pd.to_numeric(display_df['Clicks'], errors='coerce').fillna(0) /
                                                                   pd.to_numeric(display_df['Sessions'], errors='coerce').replace({0: np.nan})) * 100
                            display_df['Ad Traffic % of Total'] = display_df['Ad Traffic % of Total'].replace([np.inf, -np.inf], 0).fillna(0)
                        else:
                            display_df['Ad Traffic % of Total'] = 0
                    except Exception:
                        display_df['Ad Traffic % of Total'] = 0

                    # Reorder columns: place Clicks, Sessions, Ad Traffic % of Total, Ad Sales % of Total immediately after AOV
                    try:
                        desired_after_aov = ['Clicks', 'Sessions', 'Ad Traffic % of Total', 'Ad Sales % of Total']
                        if 'AOV' in display_df.columns:
                            cols = list(display_df.columns)
                            # Remove duplicates of desired columns first
                            cols = [c for c in cols if c not in desired_after_aov]
                            insert_idx = cols.index('AOV') + 1
                            for i, c in enumerate(desired_after_aov):
                                if c in display_df.columns or c in desired_after_aov:  # ensure presence check
                                    # Only insert if exists now (it will exist due to prior creation)
                                    cols.insert(insert_idx + i, c)
                            display_df = display_df[cols]
                        # Move Impressions to far right if present
                        if 'Impressions' in display_df.columns:
                            cols = [c for c in display_df.columns if c != 'Impressions'] + ['Impressions']
                            display_df = display_df[cols]
                    except Exception:
                        pass

                    money_cols = ['Spend', 'Ad Sales', 'Total Sales', 'AOV']
                    pct_cols = ['% of Spend', '% of Ad Sales', '% of Total Sales', 'ACoS', 'TACoS', 'CVR', 'Ad Sales % of Total', 'Ad Traffic % of Total']

                    # Remove 'Found in Bulk' column if it exists
                    if 'Found In Bulk' in display_df.columns:
                        display_df = display_df.drop(columns=['Found In Bulk'])
                
                    # Remove Product Group column if no product groups are defined in Branded ASINs
                    has_product_groups = False
                    if 'client_config' in st.session_state and 'branded_asins_data' in st.session_state.client_config:
                        # Check if any ASIN has a product group defined
                        has_product_groups = any(info.get('product_group', '') for info in st.session_state.client_config['branded_asins_data'].values())
                
                    if not has_product_groups and 'Product Group' in display_df.columns:
                        display_df = display_df.drop(columns=['Product Group'])
                    
                    # Remove SKU column if not requested or no SKU data available
                    if not show_sku_column and 'SKU' in display_df.columns:
                        display_df = display_df.drop(columns=['SKU'])
                    elif show_sku_column and 'SKU' not in display_df.columns:
                        # Add empty SKU column if requested but not present
                        display_df['SKU'] = ''

                    # Reorder columns to put SKU before ASIN (if both exist)
                    if show_sku_column and 'SKU' in display_df.columns and 'ASIN' in display_df.columns:
                        cols = list(display_df.columns)
                        # Remove SKU from its current position
                        cols.remove('SKU')
                        # Find ASIN position and insert SKU before it
                        asin_idx = cols.index('ASIN')
                        cols.insert(asin_idx, 'SKU')
                        # Reorder the DataFrame
                        display_df = display_df[cols]

                    # Ensure all columns are numeric for proper sorting
                    for col in money_cols:
                        if col in display_df.columns:
                            display_df[col] = pd.to_numeric(display_df[col], errors='coerce').fillna(0)
                    for col in pct_cols:
                        if col in display_df.columns:
                            display_df[col] = pd.to_numeric(display_df[col], errors='coerce').fillna(0)

                    # Calculate ACoS and TACoS for filtered totals
                    filtered_acos = (filtered_total_spend / filtered_total_ad_sales * 100) if filtered_total_ad_sales > 0 else 0
                    filtered_tacos = (filtered_total_spend / filtered_total_sales * 100) if filtered_total_sales > 0 else 0

                    # Create totals row with proper formatting
                    # Calculate totals for CVR and AOV (global, not just current page)
                    global_clicks = asin_perf_df['Clicks'] if 'Clicks' in asin_perf_df.columns else 0
                    global_orders = asin_perf_df['Orders'] if 'Orders' in asin_perf_df.columns else 0
                    global_ad_sales = asin_perf_df['Ad Sales'] if 'Ad Sales' in asin_perf_df.columns else 0
                    global_clicks = pd.to_numeric(global_clicks, errors='coerce').fillna(0)
                    global_orders = pd.to_numeric(global_orders, errors='coerce').fillna(0)
                    global_ad_sales = pd.to_numeric(global_ad_sales, errors='coerce').fillna(0)
                    global_cvr = (global_orders.sum() / global_clicks.sum() * 100) if global_clicks.sum() > 0 else 0
                    global_aov = (global_ad_sales.sum() / global_orders.sum()) if global_orders.sum() > 0 else 0

                    # Calculate CPC (Cost Per Click)
                    filtered_cpc = (filtered_total_spend / filtered_total_clicks) if filtered_total_clicks > 0 else 0

                    # Calculate percentage of filtered vs total for the totals row
                    filtered_spend_pct = (filtered_total_spend / global_total_spend * 100) if global_total_spend > 0 else 0
                    filtered_ad_sales_pct = (filtered_total_ad_sales / global_total_ad_sales * 100) if global_total_ad_sales > 0 else 0
                    filtered_total_sales_pct = (filtered_total_sales / global_total_sales * 100) if global_total_sales > 0 else 0

                    totals = {
                        'ASIN': 'Total',
                        'Spend': f"${filtered_total_spend:,.2f}",
                        'Ad Sales': f"${filtered_total_ad_sales:,.2f}",
                        'Total Sales': f"${filtered_total_sales:,.2f}",
                        'Clicks': f"{int(filtered_total_clicks):,}",
                        'Sessions': f"{int(filtered_total_sessions):,}",
                        '% of Spend': f"{filtered_spend_pct:.2f}%",
                        '% of Ad Sales': f"{filtered_ad_sales_pct:.2f}%",
                        '% of Total Sales': f"{filtered_total_sales_pct:.2f}%",
                        'ACoS': f"{filtered_acos:.2f}%",
                        'TACoS': f"{filtered_tacos:.2f}%",
                        'CVR': f"{global_cvr:.2f}%",
                        'AOV': f"${global_aov:,.2f}",
                        'CPC': f"${filtered_cpc:.2f}",
                        'Ad Sales % of Total': f"{((filtered_total_ad_sales/filtered_total_sales)*100) if filtered_total_sales>0 else 0:.2f}%",
                        'Ad Traffic % of Total': f"{((filtered_total_clicks/filtered_total_sessions)*100) if filtered_total_sessions>0 else 0:.2f}%"
                    }
                    
                    # SKU column is not included in totals row (only in data table)
                
                
                    # Create the total row as a separate DataFrame
                    total_row = pd.DataFrame([totals])
                
                    # Display the totals row in a separate table with ACoS conditional formatting
                    account_wide_acos = None
                    if 'client_config' in st.session_state:
                        goals = st.session_state.client_config.get('goals', {})
                        account_wide_acos = goals.get('account_wide_acos', None)
                    style_acos(total_row, account_wide_acos)

                    # Using global color gradient functions defined at the top of the file

                    # Create a dictionary for formatting
                    fmt_dict = {
                        'Spend': lambda x: f"${x:,.2f}",
                        'Ad Sales': lambda x: f"${x:,.2f}",
                        'Total Sales': lambda x: f"${x:,.2f}",
                        'AOV': lambda x: f"${x:,.2f}",
                        'CPC': lambda x: f"${x:,.2f}",
                        '% of Spend': lambda x: f"{x:.2f}%",
                        '% of Ad Sales': lambda x: f"{x:.2f}%",
                        '% of Total Sales': lambda x: f"{x:.2f}%",
                        'ACoS': lambda x: f"{x:.2f}%",
                        'TACoS': lambda x: f"{x:.2f}%",
                        'CVR': lambda x: f"{x:.2f}%",
                        'Ad Sales % of Total': lambda x: f"{x:.2f}%",
                        'Ad Traffic % of Total': lambda x: f"{x:.2f}%",
                        'Clicks': lambda x: f"{int(x):,}",
                        'Sessions': lambda x: f"{int(x):,}",
                        'Impressions': lambda x: f"{int(x):,}"
                    }
                
                    # Convert string-formatted values back to numeric for proper sorting
                    numeric_df = display_df.copy()
                    for col in money_cols:
                        if col in numeric_df.columns:
                            numeric_df[col] = pd.to_numeric(numeric_df[col].astype(str).str.replace('$', '').str.replace(',', ''), errors='coerce')
                    for col in pct_cols:
                        if col in numeric_df.columns:
                            numeric_df[col] = pd.to_numeric(numeric_df[col].astype(str).str.replace('%', ''), errors='coerce')
                
                    # Sort by Total Sales in descending order
                    numeric_df = numeric_df.sort_values(by='Total Sales', ascending=False)
                
                    # Smart handling for large datasets
                    total_cells = len(numeric_df) * len(numeric_df.columns)
                    max_cells_for_styling = 100000  # Lower threshold to reduce heavy Styler rendering on large tables
                    
                    # Initialize pagination variables
                    if 'asin_table_page' not in st.session_state:
                        st.session_state.asin_table_page = 0
                    if 'asin_table_page_size' not in st.session_state:
                        st.session_state.asin_table_page_size = 50
                    
                    # Add pagination controls if dataset is large
                    if total_cells > max_cells_for_styling:
                        st.warning(f"âš ï¸ Large dataset detected: {len(numeric_df):,} ASINs ({total_cells:,} cells). Using pagination for better performance.")
                        
                        # Pagination controls
                        col1, col2, col3, col4 = st.columns([2, 2, 2, 4])
                        
                        with col1:
                            page_size_options = [50, 100, 250, 500, 1000]
                            if st.session_state.asin_table_page_size not in page_size_options:
                                page_size_options.append(st.session_state.asin_table_page_size)
                                page_size_options.sort()
                            
                            new_page_size = st.selectbox(
                                "Rows per page:",
                                options=page_size_options,
                                index=page_size_options.index(st.session_state.asin_table_page_size),
                                key="asin_page_size_selector"
                            )
                            if new_page_size != st.session_state.asin_table_page_size:
                                st.session_state.asin_table_page_size = new_page_size
                                st.session_state.asin_table_page = 0  # Reset to first page
                        
                        with col2:
                            max_pages = max(0, (len(numeric_df) - 1) // st.session_state.asin_table_page_size)
                            current_page = st.number_input(
                                "Page:",
                                min_value=0,
                                max_value=max_pages,
                                value=st.session_state.asin_table_page,
                                key="asin_page_selector"
                            )
                            st.session_state.asin_table_page = int(current_page)
                        
                        with col3:
                            st.write(f"**Page {st.session_state.asin_table_page + 1} of {max_pages + 1}**")
                        
                        with col4:
                            # Navigation buttons
                            nav_col1, nav_col2, nav_col3, nav_col4 = st.columns(4)
                            with nav_col1:
                                if st.button("â®ï¸ First", disabled=st.session_state.asin_table_page == 0):
                                    st.session_state.asin_table_page = 0
                                    st.rerun()
                            with nav_col2:
                                if st.button("âª Prev", disabled=st.session_state.asin_table_page == 0):
                                    st.session_state.asin_table_page -= 1
                                    st.rerun()
                            with nav_col3:
                                if st.button("Next â©", disabled=st.session_state.asin_table_page >= max_pages):
                                    st.session_state.asin_table_page += 1
                                    st.rerun()
                            with nav_col4:
                                if st.button("Last â­ï¸", disabled=st.session_state.asin_table_page >= max_pages):
                                    st.session_state.asin_table_page = max_pages
                                    st.rerun()
                        
                        # Calculate pagination slice
                        start_idx = st.session_state.asin_table_page * st.session_state.asin_table_page_size
                        end_idx = start_idx + st.session_state.asin_table_page_size
                        paged_df = numeric_df.iloc[start_idx:end_idx].copy()
                        
                        # Show range info
                        actual_end = min(end_idx, len(numeric_df))
                        st.caption(f"Showing ASINs {start_idx + 1:,} to {actual_end:,} of {len(numeric_df):,} total")
                        
                    else:
                        # Small dataset - show all rows
                        paged_df = numeric_df.copy()
                    
                    # Check if we can apply styling to the current page
                    paged_cells = len(paged_df) * len(paged_df.columns)
                    
                    if paged_cells <= max_cells_for_styling and len(paged_df) > 0:
                        # Apply styling for small datasets (use the current page slice)
                        styled_df = paged_df.style.format(fmt_dict)
                        
                        # Apply color gradients
                        if '% of Spend' in paged_df.columns:
                            styled_df = styled_df.apply(lambda x: [color_gradient_blue(v, 0, 100, scale_max=40) 
                                                                if not pd.isna(v) else '' 
                                                                for v in x] if x.name == '% of Spend' else [''] * len(x), axis=0)
                        if '% of Ad Sales' in paged_df.columns:
                            styled_df = styled_df.apply(lambda x: [color_gradient_green(v, 0, 100, scale_max=40) 
                                                                if not pd.isna(v) else '' 
                                                                for v in x] if x.name == '% of Ad Sales' else [''] * len(x), axis=0)
                        if '% of Total Sales' in paged_df.columns:
                            styled_df = styled_df.apply(lambda x: [color_gradient_green(v, 0, 100, scale_max=40) 
                                                                if not pd.isna(v) else '' 
                                                                for v in x] if x.name == '% of Total Sales' else [''] * len(x), axis=0)
                        
                        # Display styled dataframe
                        st.dataframe(styled_df, use_container_width=True, hide_index=True)
                        
                    else:
                        # Too large even for current page - show unstyled with formatting
                        if len(paged_df) > 0:
                            # Format the dataframe manually without styling
                            display_df_formatted = paged_df.copy()
                            
                            # Apply formatting without styling
                            for col in ['Spend', 'Ad Sales', 'Total Sales', 'AOV', 'CPC']:
                                if col in display_df_formatted.columns:
                                    display_df_formatted[col] = display_df_formatted[col].apply(lambda x: f"${x:,.2f}" if pd.notna(x) else "$0.00")
                            
                            for col in ['% of Spend', '% of Ad Sales', '% of Total Sales', 'ACoS', 'TACoS', 'CVR', 'Ad Sales % of Total', 'Ad Traffic % of Total']:
                                if col in display_df_formatted.columns:
                                    display_df_formatted[col] = display_df_formatted[col].apply(lambda x: f"{x:.2f}%" if pd.notna(x) else "0.00%")
                            
                            # Format Clicks, Sessions, Impressions as integers with commas
                            for col in ['Clicks', 'Sessions', 'Impressions']:
                                if col in display_df_formatted.columns:
                                    display_df_formatted[col] = display_df_formatted[col].apply(lambda x: f"{int(x):,}" if pd.notna(x) else '0')
                            # Move Impressions to far right if present
                            if 'Impressions' in display_df_formatted.columns:
                                cols = [c for c in display_df_formatted.columns if c != 'Impressions'] + ['Impressions']
                                display_df_formatted = display_df_formatted[cols]
                            
                            st.info("ðŸ“Š Large dataset - displaying without color styling for better performance")
                            st.dataframe(display_df_formatted, use_container_width=True, hide_index=True)
                        else:
                            st.info("No data to display on this page")
                    
                    # Add download button for full dataset
                    if total_cells > max_cells_for_styling:
                        st.markdown("---")
                        col1, col2 = st.columns([3, 1])
                        with col1:
                            st.caption("ðŸ’¡ **Tip:** Use filters above to narrow down the data for better performance, or download the full dataset.")
                        with col2:
                            # Prepare full dataset for download
                            download_df = numeric_df.copy()
                            for col in ['Spend', 'Ad Sales', 'Total Sales', 'AOV', 'CPC']:
                                if col in download_df.columns:
                                    download_df[col] = download_df[col].apply(lambda x: f"${x:,.2f}" if pd.notna(x) else "$0.00")
                            for col in ['% of Spend', '% of Ad Sales', '% of Total Sales', 'ACoS', 'TACoS', 'CVR', 'Ad Sales % of Total', 'Ad Traffic % of Total']:
                                if col in download_df.columns:
                                    download_df[col] = download_df[col].apply(lambda x: f"{x:.2f}%" if pd.notna(x) else "0.00%")
                            # Integer formatting for counts
                            for col in ['Clicks', 'Sessions', 'Impressions']:
                                if col in download_df.columns:
                                    download_df[col] = download_df[col].apply(lambda x: f"{int(pd.to_numeric(x, errors='coerce').fillna(0)):,}" if pd.notna(x) else '0')
                            # Move Impressions to far right if present
                            if 'Impressions' in download_df.columns:
                                cols = [c for c in download_df.columns if c != 'Impressions'] + ['Impressions']
                                download_df = download_df[cols]
                            
                            csv = download_df.to_csv(index=False)
                            st.download_button(
                                label="ðŸ“¥ Download Full Data",
                                data=csv,
                                file_name="asin_performance_full.csv",
                                mime="text/csv"
                            )

                    # Display row count at the bottom
                    if total_cells > max_cells_for_styling:
                        st.caption(f"**Total ASINs:** {len(filtered_df):,} | **Showing:** {len(paged_df):,} on this page")
                    else:
                        st.caption(f"Total Rows: {len(filtered_df):,}")

                    # Add Export Table button (positioned closer to table)
                    col1, col2 = st.columns([4, 1])
                    with col2:
                        if st.button("ðŸ“Š Export Table", key="export_asin_table", help="Export the Performance by ASIN table with conditional formatting to Excel"):
                            try:
                                from openpyxl import Workbook
                                from openpyxl.styles import PatternFill, Font, Alignment
                                from openpyxl.formatting.rule import ColorScaleRule
                                from openpyxl.utils.dataframe import dataframe_to_rows
                                import io
                                
                                # Prepare data for export (use the full filtered dataset, not just the paged view)
                                export_df = numeric_df.copy()
                                
                                # Format the data for Excel
                                for col in ['Spend', 'Ad Sales', 'Total Sales', 'AOV', 'CPC']:
                                    if col in export_df.columns:
                                        # Keep numeric values for conditional formatting, but ensure proper formatting
                                        export_df[col] = pd.to_numeric(export_df[col], errors='coerce').fillna(0)
                                
                                for col in ['% of Spend', '% of Ad Sales', '% of Total Sales', 'ACoS', 'TACoS', 'CVR', 'Ad Sales % of Total', 'Ad Traffic % of Total']:
                                    if col in export_df.columns:
                                        # Keep as percentage values (0-100 range)
                                        export_df[col] = pd.to_numeric(export_df[col], errors='coerce').fillna(0)
                                
                                # Create workbook and worksheet
                                wb = Workbook()
                                ws = wb.active
                                ws.title = "Performance by ASIN"
                                
                                # Add data to worksheet
                                for r in dataframe_to_rows(export_df, index=False, header=True):
                                    ws.append(r)
                                
                                # Style the header row
                                header_fill = PatternFill(start_color="366092", end_color="366092", fill_type="solid")
                                header_font = Font(color="FFFFFF", bold=True)
                                
                                for cell in ws[1]:
                                    cell.fill = header_fill
                                    cell.font = header_font
                                    cell.alignment = Alignment(horizontal="center")
                                
                                # Apply conditional formatting
                                # Get column letters for each metric
                                col_mapping = {}
                                for idx, col_name in enumerate(export_df.columns, 1):
                                    col_mapping[col_name] = chr(64 + idx) if idx <= 26 else f"A{chr(64 + idx - 26)}"
                                
                                # Apply color scales for percentage columns
                                percentage_cols = ['% of Spend', '% of Ad Sales', '% of Total Sales', 'Ad Sales % of Total', 'Ad Traffic % of Total']
                                for col in percentage_cols:
                                    if col in col_mapping and col in export_df.columns:
                                        col_letter = col_mapping[col]
                                        range_str = f"{col_letter}2:{col_letter}{len(export_df) + 1}"
                                        
                                        if col == '% of Spend':
                                            # Blue gradient for spend
                                            rule = ColorScaleRule(start_type='num', start_value=0, start_color='FFFFFF',
                                                                mid_type='num', mid_value=20, mid_color='ADD8E6',
                                                                end_type='num', end_value=40, end_color='0066CC')
                                        else:
                                            # Green gradient for sales
                                            rule = ColorScaleRule(start_type='num', start_value=0, start_color='FFFFFF',
                                                                mid_type='num', mid_value=20, mid_color='90EE90',
                                                                end_type='num', end_value=40, end_color='006400')
                                        
                                        ws.conditional_formatting.add(range_str, rule)
                                
                                # Apply ACoS conditional formatting (red for high ACoS)
                                if 'ACoS' in col_mapping and 'ACoS' in export_df.columns:
                                    col_letter = col_mapping['ACoS']
                                    range_str = f"{col_letter}2:{col_letter}{len(export_df) + 1}"
                                    
                                    # Get account-wide ACoS target if available
                                    acos_target = 25  # Default target
                                    if 'client_config' in st.session_state:
                                        goals = st.session_state.client_config.get('goals', {})
                                        target_value = goals.get('account_wide_acos')
                                        if target_value is not None:
                                            acos_target = target_value
                                    
                                    # Ensure acos_target is a valid number
                                    if acos_target is None or not isinstance(acos_target, (int, float)):
                                        acos_target = 25
                                    
                                    # Red gradient for ACoS (higher is worse)
                                    rule = ColorScaleRule(start_type='num', start_value=0, start_color='90EE90',
                                                        mid_type='num', mid_value=acos_target, mid_color='FFFF00',
                                                        end_type='num', end_value=acos_target * 2, end_color='FF0000')
                                    ws.conditional_formatting.add(range_str, rule)
                                
                                # Format currency columns
                                from openpyxl.styles import NamedStyle
                                currency_style = NamedStyle(name="currency")
                                currency_style.number_format = '"$"#,##0.00'
                                
                                percentage_style = NamedStyle(name="percentage")
                                percentage_style.number_format = '0.00"%"'
                                
                                # Apply number formatting
                                for col_name, col_letter in col_mapping.items():
                                    if col_name in ['Spend', 'Ad Sales', 'Total Sales', 'AOV', 'CPC']:
                                        for row in range(2, len(export_df) + 2):
                                            ws[f"{col_letter}{row}"].number_format = '"$"#,##0.00'
                                    elif col_name in ['% of Spend', '% of Ad Sales', '% of Total Sales', 'ACoS', 'TACoS', 'CVR', 'Ad Sales % of Total', 'Ad Traffic % of Total']:
                                        for row in range(2, len(export_df) + 2):
                                            ws[f"{col_letter}{row}"].number_format = '0.00"%"'
                                
                                # Auto-adjust column widths
                                for column in ws.columns:
                                    max_length = 0
                                    column_letter = column[0].column_letter
                                    for cell in column:
                                        try:
                                            if len(str(cell.value)) > max_length:
                                                max_length = len(str(cell.value))
                                        except:
                                            pass
                                    adjusted_width = min(max_length + 2, 50)
                                    ws.column_dimensions[column_letter].width = adjusted_width
                                
                                # Save to BytesIO
                                excel_buffer = io.BytesIO()
                                wb.save(excel_buffer)
                                excel_buffer.seek(0)
                                
                                # Generate filename with timestamp
                                from datetime import datetime
                                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                                filename = f"Performance_by_ASIN_{timestamp}.xlsx"
                                
                                # Provide download button
                                st.download_button(
                                    label="ðŸ’¾ Download Excel File",
                                    data=excel_buffer.getvalue(),
                                    file_name=filename,
                                    mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                                    key="download_asin_excel"
                                )
                                
                                st.success(f"âœ… Excel file prepared! Click 'Download Excel File' to save {len(export_df):,} ASINs with conditional formatting.")
                                
                            except Exception as e:
                                st.error(f"Error creating Excel export: {str(e)}")
                                # Fallback to CSV export
                                csv_data = numeric_df.copy()
                                for col in ['Spend', 'Ad Sales', 'Total Sales', 'AOV', 'CPC']:
                                    if col in csv_data.columns:
                                        csv_data[col] = csv_data[col].apply(lambda x: f"${x:,.2f}" if pd.notna(x) else "$0.00")
                                for col in ['% of Spend', '% of Ad Sales', '% of Total Sales', 'ACoS', 'TACoS', 'CVR', 'Ad Sales % of Total', 'Ad Traffic % of Total']:
                                    if col in csv_data.columns:
                                        csv_data[col] = csv_data[col].apply(lambda x: f"{x:.2f}%" if pd.notna(x) else "0.00%")
                                
                                csv = csv_data.to_csv(index=False)
                                st.download_button(
                                    label="ðŸ“¥ Download CSV (Fallback)",
                                    data=csv,
                                    file_name="Performance_by_ASIN_fallback.csv",
                                    mime="text/csv"
                                )

                    # Add extra spacing above the ASIN Allocation Chart
                    st.markdown("<br><br>", unsafe_allow_html=True)
                    # Create a stacked bar chart to visualize Spend, Ad Sales, and Total Sales by ASIN
                    st.markdown("##### ASIN Allocation Chart")
                
                    # Create tabs for different visualization types
                    asin_viz_tabs = st.tabs(["% Values", "$ Values", "Bubble Chart", "Donut Chart"])
                
                    # Initialize common variables with default values
                    if 'asin_viz_sort_by' not in st.session_state:
                        st.session_state.asin_viz_sort_by = "Total Sales"
                    if 'asin_viz_count' not in st.session_state:
                        st.session_state.asin_viz_count = 10
                    if 'asin_title_length' not in st.session_state:
                        st.session_state.asin_title_length = 45
                
                    # Prepare base data for visualization
                    base_chart_df = filtered_df.copy()
                
                    # Ensure all values are numeric
                    for col in ['Spend', 'Ad Sales', 'Total Sales', 'ACoS', 'TACoS']:
                        if col in base_chart_df.columns:
                            base_chart_df[col] = pd.to_numeric(base_chart_df[col].astype(str).str.replace('$', '').str.replace('%', '').str.replace(',', ''), errors='coerce').fillna(0)
                
                    # Create shortened labels for ASINs with customizable length
                    def format_product_title(asin, title, max_length):
                        # If title is too long, truncate and add ellipsis
                        if len(title) > max_length:
                            truncated_title = title[:max_length] + '...'
                        else:
                            truncated_title = title
                    
                        # Format with ASIN in bold
                        return f"{asin} - {truncated_title}"
                
                    # Always use stacked mode
                    barmode = 'stack'
                
                    # Custom color scheme that works well in dark mode
                    custom_colors = {
                        'Spend': '#EA4335',      # Red for Spend
                        'Ad Sales': '#4285F4',  # Blue for Ad Sales
                        'Total Sales': '#34A853' # Green for Total Sales
                    }
                
                    # Percentage Values Tab (now first tab)
                    with asin_viz_tabs[0]:
                        # Add chart controls specific to % Values tab
                        with st.expander("Chart Display Options", expanded=False):
                            col1, col2 = st.columns(2)
                            with col1:
                                sort_by_pct = st.selectbox(
                                    "Sort ASINs by:",
                                    options=["Total Sales", "Spend", "Ad Sales"],
                                    index=0,
                                    key="asin_viz_sort_by_pct"
                                )
                                # Update the session state for other tabs to use
                                st.session_state.asin_viz_sort_by = sort_by_pct
                            with col2:
                                display_count_pct = st.selectbox(
                                    "Number of ASINs to display:",
                                    options=[5, 10, 15, 20],
                                    index=1,
                                    key="asin_viz_count_pct"
                                )
                                # Update the session state for other tabs to use
                                st.session_state.asin_viz_count = display_count_pct
                        
                            # Title length control
                            title_length_pct = st.slider(
                                "Product title length:",
                                min_value=20,
                                max_value=150,
                                value=45,
                                step=5,
                                key="asin_title_length_pct"
                            )
                            # Update the session state for other tabs to use
                            st.session_state.asin_title_length = title_length_pct
                    
                        # Use the values from this tab's controls
                        sort_by = sort_by_pct
                        display_count = display_count_pct
                        title_length = title_length_pct
                    
                        # Create a copy of the data for this tab
                        chart_df = base_chart_df.copy()
                    
                        # Sort based on user selection
                        if sort_by in chart_df.columns:
                            # Always sort descending for Spend, Total Sales, Ad Sales
                            ascending = False
                            chart_df = chart_df.sort_values(by=sort_by, ascending=ascending).head(display_count)
                        else:
                            # Default sort by Total Sales
                            chart_df = chart_df.sort_values(by='Total Sales', ascending=False).head(display_count)
                    
                        # Apply formatting with user-selected length
                        chart_df['label'] = chart_df.apply(
                            lambda row: format_product_title(
                                row['ASIN'], 
                                row['Product Title'], 
                                title_length
                            ),
                            axis=1
                        )
                        # Create percentage data
                        pct_df = chart_df.copy()
                    
                        # Calculate percentages based on ALL ASINs, not just displayed ones
                        # Use the global totals from the original filtered_df (before limiting to display_count)
                        total_spend_all = filtered_df['Spend'].sum()
                        total_ad_sales_all = filtered_df['Ad Sales'].sum()
                        total_sales_all = filtered_df['Total Sales'].sum()
                    
                        pct_df['Spend %'] = np.where(total_spend_all > 0, (pct_df['Spend'] / total_spend_all * 100).round(1), 0.0)
                        pct_df['Ad Sales %'] = np.where(total_ad_sales_all > 0, (pct_df['Ad Sales'] / total_ad_sales_all * 100).round(1), 0.0)
                        pct_df['Total Sales %'] = np.where(total_sales_all > 0, (pct_df['Total Sales'] / total_sales_all * 100).round(1), 0.0)
                        # Fill any remaining NaNs in % columns just in case
                        pct_df[['Spend %', 'Ad Sales %', 'Total Sales %']] = pct_df[['Spend %', 'Ad Sales %', 'Total Sales %']].fillna(0.0)
                    
                        # Set fixed label position
                        if 'label_position' not in st.session_state:
                            st.session_state.label_position = 'outside'
                    
                        # Create the % values chart - horizontal orientation with enhanced styling
                        fig_pct = px.bar(
                            pct_df,
                            y='label',  # Now y-axis has the labels
                            x=['Spend %', 'Ad Sales %', 'Total Sales %'],  # Now x-axis has the values
                            title=f'Top {display_count} ASINs by {sort_by} (% of All ASINs)',
                            labels={'label': 'ASIN', 'value': 'Percentage (%)', 'variable': 'Metric'},
                            height=600,  # Increased height for better readability
                            barmode=barmode,
                            orientation='h',  # Horizontal bars
                            color_discrete_map={
                                'Spend %': '#EA4335',  # Red for Spend
                                'Ad Sales %': '#4285F4',  # Blue for Ad Sales
                                'Total Sales %': '#34A853'  # Green for Total Sales
                            },
                            text_auto=False,  # We will set text manually to avoid NaN%
                            template='plotly_dark'  # Use a dark theme for better visual appeal
                        )
                        # Ensure bar text always shows a value, never NaN
                        for metric in ['Spend %', 'Ad Sales %', 'Total Sales %']:
                            fig_pct.for_each_trace(lambda trace: trace.update(text=[f'{v:.1f}%' if not pd.isna(v) and v > 0 else '0.0%' for v in trace.x]) if trace.name == metric else None)
                    
                        # Determine font size based on number of ASINs displayed
                        y_axis_font_size = 14 if display_count <= 10 else 11
                    
                        # Improve layout for horizontal chart with enhanced styling
                        fig_pct.update_layout(
                            xaxis=dict(
                                title=dict(text='Percentage (%)', font=dict(size=14, family="Arial, sans-serif")),
                                tickformat='.1f',
                                ticksuffix='%',
                                gridcolor='rgba(255,255,255,0.1)',
                                showgrid=True,
                                zeroline=True,
                                tickfont=dict(family="Arial, sans-serif")
                            ),
                            yaxis=dict(
                                title='',
                                automargin=True,  # Ensure there's enough space for labels
                                tickfont=dict(size=y_axis_font_size, family="Arial, sans-serif"),
                                gridcolor='rgba(0,0,0,0)'  # No horizontal grid lines
                            ),
                            legend=dict(
                                title='',
                                orientation='h',
                                yanchor='bottom',
                                y=1.02,
                                xanchor='right',
                                x=1,
                                bgcolor='rgba(0,0,0,0.5)',
                                bordercolor='rgba(255,255,255,0.3)',
                                font=dict(family="Arial, sans-serif", size=12)
                            ),
                            margin=dict(l=40, r=40, t=70, b=100),
                            hovermode='closest',
                            plot_bgcolor='rgba(0,0,0,0)',  # Transparent background
                            paper_bgcolor='rgba(0,0,0,0)',  # Transparent paper
                            title=dict(
                                text=f'Top {display_count} ASINs by {sort_by} (% of All ASINs)',
                                font=dict(size=16, family="Arial, sans-serif")
                            ),
                            hoverlabel=dict(
                                bgcolor='rgba(0,0,0,0.8)',
                                font_size=12,
                                font=dict(family="Arial, sans-serif", color='white')
                            )
                        )
                    
                        # Determine text size based on number of ASINs displayed
                        bar_text_size = 14 if display_count <= 10 else 11
                    
                        # Improve text display on bars with fixed position
                        fig_pct.update_traces(
                            textposition='outside',  # Fixed to outside position
                            textfont=dict(size=bar_text_size, family="Arial, sans-serif", color="white"),
                            hovertemplate='<b>%{y}</b><br>' +
                                         '%{x:.1f}% of total<br>' +
                                         '<extra></extra>',
                            texttemplate='%{text}',  # Use the custom text we defined above
                            selector=dict(type='bar'),
                            opacity=0.9,  # Slight transparency for better aesthetics
                            marker=dict(line=dict(width=0.5, color='rgba(255,255,255,0.2)'))  # Add subtle borders
                        )
                    
                        # No reference lines needed for stacked mode
                    
                        # Display the chart
                        st.plotly_chart(fig_pct, use_container_width=True)
                
                    # Dollar Values Tab (now second tab)
                    with asin_viz_tabs[1]:
                        # Add chart controls specific to $ Values tab
                        with st.expander("Chart Display Options", expanded=False):
                            col1, col2 = st.columns(2)
                            with col1:
                                # Get index of current sort_by value in options list
                                sort_options = ["Total Sales", "Spend", "Ad Sales"]
                                current_sort_index = sort_options.index(st.session_state.asin_viz_sort_by) if st.session_state.asin_viz_sort_by in sort_options else 0
                            
                                sort_by_dollar = st.selectbox(
                                    "Sort ASINs by:",
                                    options=sort_options,
                                    index=current_sort_index,
                                    key="asin_viz_sort_by_dollar"
                                )
                                # Update the session state for other tabs to use
                                st.session_state.asin_viz_sort_by = sort_by_dollar
                            with col2:
                                # Get index of current display count in options list
                                count_options = [5, 10, 15, 20]
                                current_count_index = count_options.index(st.session_state.asin_viz_count) if st.session_state.asin_viz_count in count_options else 1
                            
                                display_count_dollar = st.selectbox(
                                    "Number of ASINs to display:",
                                    options=count_options,
                                    index=current_count_index,
                                    key="asin_viz_count_dollar"
                                )
                                # Update the session state for other tabs to use
                                st.session_state.asin_viz_count = display_count_dollar
                        
                            # Title length control
                            title_length_dollar = st.slider(
                                "Product title length:",
                                min_value=20,
                                max_value=150,
                                value=st.session_state.asin_title_length,
                                step=5,
                                key="asin_title_length_dollar"
                            )
                            # Update the session state for other tabs to use
                            st.session_state.asin_title_length = title_length_dollar
                    
                        # Use the values from this tab's controls
                        sort_by = sort_by_dollar
                        display_count = display_count_dollar
                        title_length = title_length_dollar
                    
                        # Create a copy of the data for this tab
                        chart_df = base_chart_df.copy()
                    
                        # Sort based on user selection
                        if sort_by in chart_df.columns:
                            # Always sort descending for Spend, Total Sales, Ad Sales
                            ascending = False
                            chart_df = chart_df.sort_values(by=sort_by, ascending=ascending).head(display_count)
                        else:
                            # Default sort by Total Sales
                            chart_df = chart_df.sort_values(by='Total Sales', ascending=False).head(display_count)
                    
                        # Apply formatting with user-selected length
                        chart_df['label'] = chart_df.apply(
                            lambda row: format_product_title(
                                row['ASIN'], 
                                row['Product Title'], 
                                title_length
                            ),
                            axis=1
                        )
                        # Create the $ values chart - horizontal orientation
                        fig_dollars = px.bar(
                            chart_df,
                            y='label',  # Now y-axis has the labels
                            x=['Spend', 'Ad Sales', 'Total Sales'],  # Now x-axis has the values
                            title=f'Top {display_count} ASINs by {sort_by} ($)',
                            labels={'label': 'ASIN', 'value': 'Amount ($)', 'variable': 'Metric'},
                            height=600,  # Increased height for better readability
                            barmode=barmode,
                            orientation='h',  # Horizontal bars
                            color_discrete_map=custom_colors,
                            text_auto=False  # We'll set custom text formatting
                        )
                    
                        # Determine font size based on number of ASINs displayed
                        y_axis_font_size = 14 if display_count <= 10 else 10
                    
                        # Improve layout for horizontal chart
                        fig_dollars.update_layout(
                            xaxis=dict(
                                title=dict(text='Amount ($)', font=dict(size=14, family="Arial, sans-serif")),
                                tickformat='$,.0f',
                                gridcolor='rgba(255,255,255,0.1)',
                                showgrid=True,
                                zeroline=True,
                                zerolinecolor='rgba(255,255,255,0.3)'
                            ),
                            yaxis=dict(
                                title='',
                                automargin=True,  # Ensure there's enough space for labels
                                tickfont=dict(size=y_axis_font_size, family="Arial, sans-serif"),
                                gridcolor='rgba(0,0,0,0)'  # No horizontal grid lines
                            ),
                            legend=dict(
                                title='',
                                orientation='h',
                                yanchor='bottom',
                                y=1.02,
                                xanchor='right',
                                x=1,
                                bgcolor='rgba(0,0,0,0.5)',
                                bordercolor='rgba(255,255,255,0.3)',
                                font=dict(family="Arial, sans-serif", size=12)
                            ),
                            margin=dict(l=40, r=40, t=60, b=100),
                            hovermode='closest',
                            plot_bgcolor='rgba(0,0,0,0)',  # Transparent background
                            paper_bgcolor='rgba(0,0,0,0)',  # Transparent paper
                            template='plotly_dark',  # Use a dark theme for better visual appeal
                            hoverlabel=dict(
                                bgcolor='rgba(0,0,0,0.8)',
                                font_size=12,
                                font=dict(family="Arial, sans-serif", color='white')
                            )
                        )
                    
                        # Determine text size based on number of ASINs displayed
                        bar_text_size = 13 if display_count <= 10 else 10
                    
                        # Custom text formatting for dollar values
                        for i, metric in enumerate(['Spend', 'Ad Sales', 'Total Sales']):
                            # Format dollar values with appropriate scaling
                            def format_currency(value):
                                if value >= 1000000:  # $1M+
                                    return f'${value/1000000:.1f}M'
                                elif value >= 1000:   # $1K+
                                    return f'${value/1000:.0f}K'
                                else:
                                    return f'${value:.0f}'
                        
                            # Apply custom text to each trace
                            fig_dollars.data[i].text = [format_currency(val) if val > 0 else '' for val in fig_dollars.data[i].x]
                    
                        # Improve text display on bars
                        fig_dollars.update_traces(
                            textposition='outside',
                            textfont=dict(size=bar_text_size, family="Arial, sans-serif", color="white"),
                            hovertemplate='<b>%{y}</b><br>' +
                                         '%{fullData.name}: $%{x:,.2f}<br>' +
                                         '<extra></extra>',
                            selector=dict(type='bar'),
                            opacity=0.9,  # Slight transparency for better aesthetics
                            marker=dict(line=dict(width=0.5, color='rgba(255,255,255,0.2)'))  # Add subtle borders
                        )
                    
                        # No reference lines needed for stacked mode
                    
                        # Display the chart
                        st.plotly_chart(fig_dollars, use_container_width=True)
                
                    # Bubble Chart Visualization Tab
                    with asin_viz_tabs[2]:
                        # Add chart controls specific to Bubble Chart tab
                        with st.expander("Chart Display Options", expanded=False):
                            # Only show Bubble Chart specific controls
                            st.markdown("### Bubble Chart Axes")
                            col1, col2 = st.columns(2)
                            with col1:
                                x_metric = st.selectbox(
                                    "X-Axis Metric:",
                                    options=["Spend", "ACoS", "ROAS", "CTR", "CVR", "Ad Sales"],
                                    index=0,  # Spend as default
                                    key="bubble_x_metric"
                                )
                            with col2:
                                y_metric = st.selectbox(
                                    "Y-Axis Metric:",
                                    options=["Total Sales", "Ad Sales", "ACoS", "ROAS", "CTR", "CVR"],
                                    index=0,  # Total Sales as default
                                    key="bubble_y_metric"
                                )
                        
                            # Best-fit line options
                            st.markdown("### Best-Fit Line Options")
                            col3, col4 = st.columns(2)
                            with col3:
                                show_best_fit = st.checkbox(
                                    "Show Best-Fit Line",
                                    value=True,
                                    key="show_best_fit_line",
                                    help="Add a trend line to show the relationship between the selected metrics"
                                )
                            with col4:
                                if show_best_fit:
                                    best_fit_scope = st.selectbox(
                                        "Best-Fit Data Scope:",
                                        options=["Current filtered data", "All account data"],
                                        index=0,  # Current filtered data as default
                                        key="best_fit_scope",
                                        help="Choose whether to calculate the trend line based on current filters/pivot or all data in the account"
                                    )
                                else:
                                    # Set default when not showing best fit line
                                    best_fit_scope = "Current filtered data"
                    
                        # Use the values from the session state
                        sort_by = st.session_state.asin_viz_sort_by
                        display_count = st.session_state.asin_viz_count
                        title_length = st.session_state.asin_title_length
                    
                        # Values are now taken from session state
                    
                        # Create a copy of the data for this tab
                        chart_df = base_chart_df.copy()
                    
                        # Sort based on user selection
                        if sort_by in chart_df.columns:
                            # Always sort descending for Spend, Total Sales, Ad Sales
                            ascending = False
                            chart_df = chart_df.sort_values(by=sort_by, ascending=ascending).head(display_count)
                        else:
                            # Default sort by Total Sales
                            chart_df = chart_df.sort_values(by='Total Sales', ascending=False).head(display_count)
                    
                        # Apply formatting with user-selected length
                        chart_df['label'] = chart_df.apply(
                            lambda row: format_product_title(
                                row['ASIN'], 
                                row['Product Title'], 
                                title_length
                            ),
                            axis=1
                        )
                        # Create a copy of the data for bubble chart
                        bubble_df = chart_df.copy()
                    
                        # Add percentage columns for hover information
                        bubble_df['Spend %'] = np.where(total_spend_all > 0, (bubble_df['Spend'] / total_spend_all * 100).round(1), 0.0)
                        bubble_df['Ad Sales %'] = np.where(total_ad_sales_all > 0, (bubble_df['Ad Sales'] / total_ad_sales_all * 100).round(1), 0.0)
                        bubble_df['Total Sales %'] = np.where(total_sales_all > 0, (bubble_df['Total Sales'] / total_sales_all * 100).round(1), 0.0)
                    
                        # Ensure numeric values for all metrics
                        for col in ['ACoS', 'ROAS', 'CTR', 'CVR']:
                            if col in bubble_df.columns:
                                bubble_df[col] = pd.to_numeric(bubble_df[col].astype(str).str.replace('%', '').str.replace('x', '').str.replace(',', ''), errors='coerce').fillna(0)
                    
                        # Axis metrics are now selected in the Chart Display Options expander
                    
                        # Create hover text with comprehensive metrics
                        def create_hover_text(row):
                            hover_text = f"<b>{row['ASIN']}</b><br>"
                            hover_text += f"{row['Product Title'][:50]}{'...' if len(row['Product Title']) > 50 else ''}<br><br>"
                            hover_text += f"<b>Total Sales:</b> ${row['Total Sales']:,.2f} ({row['Total Sales %']:.1f}%)<br>"
                            hover_text += f"<b>Ad Sales:</b> ${row['Ad Sales']:,.2f} ({row['Ad Sales %']:.1f}%)<br>"
                            hover_text += f"<b>Spend:</b> ${row['Spend']:,.2f} ({row['Spend %']:.1f}%)<br>"
                        
                            if 'ACoS' in row and not pd.isna(row['ACoS']):
                                hover_text += f"<b>ACoS:</b> {row['ACoS']:.1f}%<br>"
                            if 'ROAS' in row and not pd.isna(row['ROAS']):
                                hover_text += f"<b>ROAS:</b> {row['ROAS']:.1f}x<br>"
                            if 'CTR' in row and not pd.isna(row['CTR']):
                                hover_text += f"<b>CTR:</b> {row['CTR']:.2f}%<br>"
                            if 'CVR' in row and not pd.isna(row['CVR']):
                                hover_text += f"<b>CVR:</b> {row['CVR']:.2f}%<br>"
                        
                            return hover_text
                    
                        bubble_df['hover_text'] = bubble_df.apply(create_hover_text, axis=1)
                    
                        # Calculate and add best-fit line if requested
                        best_fit_line_data = None
                        if show_best_fit and len(bubble_df) > 2:
                            try:
                                # Determine which dataset to use for best-fit calculation
                                if best_fit_scope == "All account data":
                                    # Use all base_chart_df data (before limiting to display_count)
                                    bestfit_df = base_chart_df.copy()
                                else:
                                    # Use only the currently filtered/displayed data
                                    bestfit_df = bubble_df.copy()
                            
                                # Ensure numeric values for best-fit calculation
                                for col in ['ACoS', 'ROAS', 'CTR', 'CVR']:
                                    if col in bestfit_df.columns:
                                        bestfit_df[col] = pd.to_numeric(bestfit_df[col].astype(str).str.replace('%', '').str.replace('x', '').str.replace(',', ''), errors='coerce').fillna(0)
                            
                                # Clean data for best-fit calculation (remove NaN and infinite values)
                                bestfit_clean = bestfit_df[[x_metric, y_metric]].dropna()
                                bestfit_clean = bestfit_clean[np.isfinite(bestfit_clean[x_metric]) & np.isfinite(bestfit_clean[y_metric])]
                            
                                if len(bestfit_clean) > 2:
                                    # Calculate linear regression
                                    X = bestfit_clean[x_metric].values.reshape(-1, 1)
                                    y = bestfit_clean[y_metric].values
                                
                                    LinearRegression = get_linear_regression()
                                    model = LinearRegression()
                                    model.fit(X, y)
                                
                                    # Generate line points
                                    x_min, x_max = bestfit_clean[x_metric].min(), bestfit_clean[x_metric].max()
                                    x_line = np.linspace(x_min, x_max, 100)
                                    y_line = model.predict(x_line.reshape(-1, 1))
                                
                                    # Calculate R-squared
                                    r_squared = model.score(X, y)
                                
                                    # Store best-fit line data
                                    best_fit_line_data = {
                                        'x': x_line,
                                        'y': y_line,
                                        'r_squared': r_squared,
                                        'slope': model.coef_[0],
                                        'intercept': model.intercept_,
                                        'data_scope': best_fit_scope,
                                        'n_points': len(bestfit_clean)
                                    }
                            except Exception as e:
                                st.warning(f"Could not calculate best-fit line: {str(e)}")
                                best_fit_line_data = None
                    
                        # Create the bubble chart
                        fig_bubble = px.scatter(
                            bubble_df,
                            x=x_metric,
                            y=y_metric,
                            size="Ad Sales",  # Bubble size represents Ad Sales
                            color="ACoS" if "ACoS" in bubble_df.columns else "Total Sales",  # Color represents ACoS if available
                            hover_name="ASIN",
                            text="ASIN",
                            size_max=60,
                            color_continuous_scale="RdYlGn_r" if "ACoS" in bubble_df.columns else "Viridis",  # Red-Yellow-Green reversed for ACoS
                            title=f'ASIN Performance: {y_metric} vs {x_metric}',
                            height=600,
                            template='plotly_dark'
                        )
                    
                        # Format hover text
                        fig_bubble.update_traces(
                            hovertemplate='%{customdata}<extra></extra>',
                            customdata=bubble_df['hover_text'],
                            textposition='top center',
                            textfont=dict(family="Arial, sans-serif", size=10, color="white"),
                            marker=dict(opacity=0.8, line=dict(width=1, color='white'))
                        )
                    
                        # Update layout
                        x_suffix = '%' if x_metric in ['ACoS', 'CTR', 'CVR'] else 'x' if x_metric == 'ROAS' else ''
                        y_suffix = '%' if y_metric in ['ACoS', 'CTR', 'CVR'] else 'x' if y_metric == 'ROAS' else ''
                    
                        fig_bubble.update_layout(
                            xaxis=dict(
                                title=dict(text=f"{x_metric} {x_suffix}", font=dict(size=14, family="Arial, sans-serif")),
                                ticksuffix=x_suffix,
                                gridcolor='rgba(255,255,255,0.1)',
                                zeroline=True,
                                zerolinecolor='rgba(255,255,255,0.3)'
                            ),
                            yaxis=dict(
                                title=dict(text=f"{y_metric} {y_suffix}", font=dict(size=14, family="Arial, sans-serif")),
                                ticksuffix=y_suffix if y_suffix else '',
                                tickprefix='$' if y_metric in ['Total Sales', 'Ad Sales', 'Spend'] else '',
                                gridcolor='rgba(255,255,255,0.1)',
                                zeroline=True,
                                zerolinecolor='rgba(255,255,255,0.3)'
                            ),
                            coloraxis_colorbar=dict(
                                title="ACoS" if "ACoS" in bubble_df.columns else "Total Sales",
                                ticksuffix="%" if "ACoS" in bubble_df.columns else "",
                                tickprefix="" if "ACoS" in bubble_df.columns else "$"
                            ),
                            margin=dict(l=40, r=40, t=60, b=40),
                            legend=dict(orientation='h'),
                            hoverlabel=dict(
                                bgcolor='rgba(0,0,0,0.8)',
                                font_size=12,
                                font=dict(family="Arial, sans-serif", color='white')
                            )
                        )
                    
                        # Add best-fit line to the chart if calculated
                        if best_fit_line_data is not None:
                            # Add the best-fit line to the figure
                            fig_bubble.add_trace(go.Scatter(
                                x=best_fit_line_data['x'],
                                y=best_fit_line_data['y'],
                                mode='lines',
                                name=f'Best-Fit Line (RÂ² = {best_fit_line_data["r_squared"]:.3f})',
                                line=dict(
                                    color='rgba(255, 255, 0, 0.8)',  # Yellow line
                                    width=2,
                                    dash='dash'
                                ),
                                hovertemplate=f'<b>Best-Fit Line</b><br>' +
                                             f'RÂ² = {best_fit_line_data["r_squared"]:.3f}<br>' +
                                             f'Slope = {best_fit_line_data["slope"]:.2e}<br>' +
                                             f'Data: {best_fit_line_data["data_scope"]}<br>' +
                                             f'Points: {best_fit_line_data["n_points"]}<br>' +
                                             f'{x_metric}: %{{x}}<br>' +
                                             f'{y_metric}: %{{y}}<extra></extra>',
                                showlegend=True
                            ))
                        
                            # Update layout to show legend if best-fit line is displayed
                            fig_bubble.update_layout(
                                showlegend=True,
                                legend=dict(
                                    orientation='h',
                                    yanchor='bottom',
                                    y=1.02,
                                    xanchor='right',
                                    x=1,
                                    bgcolor='rgba(0,0,0,0.5)',
                                    bordercolor='rgba(255,255,255,0.2)',
                                    borderwidth=1
                                )
                            )
                    
                        # Display the bubble chart
                        st.plotly_chart(fig_bubble, use_container_width=True)
                    
                        # Display best-fit line statistics if available
                        if best_fit_line_data is not None:
                            with st.expander("ðŸ“Š Best-Fit Line Statistics", expanded=False):
                                col1, col2, col3, col4 = st.columns(4)
                                with col1:
                                    st.metric("R-squared", f"{best_fit_line_data['r_squared']:.3f}")
                                with col2:
                                    st.metric("Slope", f"{best_fit_line_data['slope']:.2e}")
                                with col3:
                                    st.metric("Data Points", f"{best_fit_line_data['n_points']:,}")
                                with col4:
                                    st.metric("Data Scope", best_fit_line_data['data_scope'])
                            
                                # Interpretation text
                                r_squared = best_fit_line_data['r_squared']
                                if r_squared >= 0.7:
                                    interpretation = "Strong positive correlation"
                                elif r_squared >= 0.5:
                                    interpretation = "Moderate positive correlation"
                                elif r_squared >= 0.3:
                                    interpretation = "Weak positive correlation"
                                else:
                                    interpretation = "Very weak or no correlation"
                            
                                st.info(f"**Interpretation:** {interpretation} between {x_metric} and {y_metric}")
                    
                        # Add some spacing after the chart
                        st.markdown("<div style='margin-top:1rem;'></div>", unsafe_allow_html=True)
                
                    # Donut Chart Visualization Tab
                    with asin_viz_tabs[3]:
                        # Add chart controls specific to Donut Chart tab
                        with st.expander("Chart Display Options", expanded=False):
                            # Only show Donut Chart specific options
                            st.markdown("### Donut Chart Options")
                            donut_size = st.slider(
                                "Donut hole size:",
                                min_value=0.3,
                                max_value=0.8,
                                value=0.6,
                                step=0.05,
                                key="donut_hole_size"
                            )
                        
                            text_position = st.radio(
                                "Label position:",
                                options=["outside", "inside", "auto"],
                                index=0,
                                key="donut_text_position",
                                horizontal=True
                            )
                    
                        # Use the values from the session state
                        sort_by = st.session_state.asin_viz_sort_by
                        display_count = st.session_state.asin_viz_count
                        title_length = st.session_state.asin_title_length
                    
                        # Values are now taken from session state
                    
                        # Create a copy of the data for this tab
                        chart_df = base_chart_df.copy()
                    
                        # Sort based on user selection
                        if sort_by in chart_df.columns:
                            # Always sort descending for Spend, Total Sales, Ad Sales
                            ascending = False
                            chart_df = chart_df.sort_values(by=sort_by, ascending=ascending).head(display_count)
                        else:
                            # Default sort by Total Sales
                            chart_df = chart_df.sort_values(by='Total Sales', ascending=False).head(display_count)
                    
                        # Apply formatting with user-selected length
                        chart_df['label'] = chart_df.apply(
                            lambda row: format_product_title(
                                row['ASIN'], 
                                row['Product Title'], 
                                title_length
                            ),
                            axis=1
                        )
                        # Create a copy of the data for donut charts
                        donut_df = chart_df.copy()
                    
                        # Calculate percentages for each metric
                        donut_df['Spend %'] = np.where(total_spend_all > 0, (donut_df['Spend'] / total_spend_all * 100).round(1), 0.0)
                        donut_df['Ad Sales %'] = np.where(total_ad_sales_all > 0, (donut_df['Ad Sales'] / total_ad_sales_all * 100).round(1), 0.0)
                        donut_df['Total Sales %'] = np.where(total_sales_all > 0, (donut_df['Total Sales'] / total_sales_all * 100).round(1), 0.0)
                    
                        # No 'Others' category as requested
                    
                        # Create three columns for the three donut charts
                        col1, col2, col3 = st.columns(3)
                    
                        # Helper function to create donut charts
                        def create_donut_chart(df, values, title, color_sequence):
                            # Filter out zero values
                            df_filtered = df[df[values] > 0].copy()
                        
                            if len(df_filtered) == 0:
                                return None
                        
                            # Create donut chart
                            fig = go.Figure()
                        
                            # Create custom hover text with more information
                            hover_texts = []
                            for idx, row in df_filtered.iterrows():
                                asin = row['ASIN']
                                product_title = row['Product Title'] if 'Product Title' in row else ''
                                value = row[values]
                                spend = row['Spend'] if 'Spend' in row else 0
                                ad_sales = row['Ad Sales'] if 'Ad Sales' in row else 0
                                total_sales = row['Total Sales'] if 'Total Sales' in row else 0
                                acos = round((spend / ad_sales * 100), 1) if ad_sales > 0 else 0
                            
                                hover_text = f"<b>{asin}</b><br>"
                                if product_title:
                                    hover_text += f"{product_title[:50]}{'...' if len(product_title) > 50 else ''}<br>"
                            
                                # Only add metrics that aren't the current chart's value metric
                                if values != 'Spend':
                                    hover_text += f"<b>Spend:</b> ${spend:,.2f}<br>"
                                if values != 'Ad Sales':
                                    hover_text += f"<b>Ad Sales:</b> ${ad_sales:,.2f}<br>"
                                if values != 'Total Sales':
                                    hover_text += f"<b>Total Sales:</b> ${total_sales:,.2f}<br>"
                                
                                hover_text += f"<b>ACoS:</b> {acos}%<br>"
                            
                                hover_texts.append(hover_text)
                        
                            # Add the donut trace
                            fig.add_trace(go.Pie(
                                labels=df_filtered['ASIN'],
                                values=df_filtered[values],
                                hole=st.session_state.get('donut_hole_size', 0.6),
                                textinfo='label+percent',
                                textposition=st.session_state.get('donut_text_position', 'outside'),
                                texttemplate='%{label}<br>%{percent}',
                                hovertemplate='%{label}<br>%{percent}<br>%{value:$,.2f}<br>%{customdata}<extra></extra>',
                                customdata=hover_texts,
                                marker=dict(
                                    colors=color_sequence,
                                    line=dict(color='rgba(255,255,255,0.2)', width=1)
                                ),
                                sort=False,
                                direction='clockwise',
                                rotation=90
                            ))
                        
                            # Update layout with significantly increased top margin
                            fig.update_layout(
                                title=dict(
                                    text=title,
                                    font=dict(size=16, family="Arial, sans-serif"),
                                    y=0.98  # Move title even higher up
                                ),
                                margin=dict(t=120, b=20, l=20, r=20),  # Significantly increased top margin
                                showlegend=False,
                                height=450,  # Increased height to accommodate labels
                                template='plotly_dark',
                                plot_bgcolor='rgba(0,0,0,0)',
                                paper_bgcolor='rgba(0,0,0,0)',
                                hoverlabel=dict(
                                    bgcolor='rgba(0,0,0,0.8)',
                                    font_size=12,
                                    font=dict(family="Arial, sans-serif", color='white')
                                )
                            )
                        
                            # Add center text with total value
                            total_value = df_filtered[values].sum()
                            value_text = f'${total_value:,.0f}'
                        
                            # Add annotation in the center
                            fig.add_annotation(
                                text=f"<b>{value_text}</b>",
                                font=dict(size=16, family="Arial, sans-serif"),
                                showarrow=False,
                                x=0.5,
                                y=0.5
                            )
                        
                            return fig
                    
                        # Create color sequences for each chart - Consistent themed gradients (Red, Blue, Green)
                        spend_colors = ['#D32F2F', '#E53935', '#F44336', '#EF5350', '#E57373', '#FFCDD2', '#FFEBEE', '#FFF5F5'][:display_count+1]
                        ad_sales_colors = ['#1976D2', '#1E88E5', '#2196F3', '#42A5F5', '#64B5F6', '#BBDEFB', '#E3F2FD', '#F3F9FF'][:display_count+1]
                        total_sales_colors = ['#388E3C', '#43A047', '#4CAF50', '#66BB6A', '#81C784', '#C8E6C9', '#E8F5E8', '#F1F8E9'][:display_count+1]
                    
                        # Create and display the three donut charts
                        with col1:
                            spend_donut = create_donut_chart(donut_df, 'Spend', 'Ad Spend Distribution', spend_colors)
                            if spend_donut:
                                st.plotly_chart(spend_donut, use_container_width=True)
                    
                        with col2:
                            ad_sales_donut = create_donut_chart(donut_df, 'Ad Sales', 'Ad Sales Distribution', ad_sales_colors)
                            if ad_sales_donut:
                                st.plotly_chart(ad_sales_donut, use_container_width=True)
                    
                        with col3:
                            total_sales_donut = create_donut_chart(donut_df, 'Total Sales', 'Total Sales Distribution', total_sales_colors)
                            if total_sales_donut:
                                st.plotly_chart(total_sales_donut, use_container_width=True)
                    
                        # Add some spacing after the charts
                        st.markdown("<div style='margin-top:1rem;'></div>", unsafe_allow_html=True)
                
                    # Space for better visual separation between sections
                    st.markdown("<div style='margin-top:2rem;'></div>", unsafe_allow_html=True)

        # --- Performance by Parent ASIN Section ---
        # Check if Parent ASIN data exists in the sales report
        has_parent_asin_data = False
        parent_asin_relationships = {}
        
        if isinstance(sales_df, pd.DataFrame) and 'Parent ASIN' in sales_df.columns:
            # Filter out empty/null Parent ASIN values
            parent_asin_data = sales_df[sales_df['Parent ASIN'].notna() & 
                                      (sales_df['Parent ASIN'].astype(str).str.strip() != '') &
                                      (sales_df['Parent ASIN'].astype(str).str.strip() != 'nan')].copy()
            
            if not parent_asin_data.empty:
                has_parent_asin_data = True
                
                # Build parent-child relationships
                for _, row in parent_asin_data.iterrows():
                    parent_asin = str(row['Parent ASIN']).strip()
                    child_asin = str(row['ASIN']).strip()
                    
                    if parent_asin not in parent_asin_relationships:
                        parent_asin_relationships[parent_asin] = {
                            'children': [],
                            'parent_title': parent_asin  # Default to ASIN if no title
                        }
                    
                    parent_asin_relationships[parent_asin]['children'].append({
                        'asin': child_asin,
                        'title': str(row.get('Title', child_asin))
                    })
                    
                st.session_state.debug_messages.append(f"[Parent ASIN] Found {len(parent_asin_relationships)} Parent ASINs with children")

        if has_parent_asin_data and len(filtered_df) > 0:
            st.markdown("##### Performance by Parent ASIN")
            
            # Create Parent ASIN performance data for table display
            parent_asin_table_data = []
            
            for parent_asin, relationship_data in parent_asin_relationships.items():
                # Initialize parent ASIN record
                parent_summary = {
                    'Type': 'Parent',
                    'ASIN': parent_asin,
                    'Parent ASIN': parent_asin,
                    'Product Group': '',  # Will be set based on children
                    'Product Title': relationship_data['parent_title'],
                    'Child Count': len(relationship_data['children']),
                    'Spend': 0,
                    'Ad Sales': 0,
                    'Total Sales': 0,
                    'Clicks': 0,
                    'Sessions': 0,
                    'Orders': 0,
                    'ACoS': 0,
                    'TACoS': 0,
                    '% of Spend': 0,
                    '% of Ad Sales': 0,
                    '% of Total Sales': 0
                }
                
                # Aggregate metrics from all child ASINs
                child_records = []
                for child_info in relationship_data['children']:
                    child_asin = child_info['asin']
                    child_title = child_info['title']
                    
                    # Find this child ASIN in the filtered performance data
                    child_perf = filtered_df[filtered_df['ASIN'] == child_asin]
                    
                    if not child_perf.empty:
                        child_data = child_perf.iloc[0]
                        
                        # Add to parent totals
                        parent_summary['Spend'] += child_data.get('Spend', 0)
                        parent_summary['Ad Sales'] += child_data.get('Ad Sales', 0)
                        parent_summary['Total Sales'] += child_data.get('Total Sales', 0)
                        parent_summary['Clicks'] += child_data.get('Clicks', 0)
                        parent_summary['Sessions'] += child_data.get('Sessions', 0)
                        parent_summary['Orders'] += child_data.get('Orders', 0)
                        
                        # Get product group for this child ASIN
                        child_product_group = 'Untagged Group'  # Default
                        if (st.session_state.client_config and 
                            'branded_asins_data' in st.session_state.client_config and 
                            child_asin in st.session_state.client_config['branded_asins_data']):
                            asin_info = st.session_state.client_config['branded_asins_data'][child_asin]
                            if asin_info and str(asin_info.get('product_group', '')).strip():
                                child_product_group = str(asin_info.get('product_group', '')).strip()
                        
                        # Create child record
                        child_record = {
                            'Type': 'Child',
                            'ASIN': child_asin,
                            'Parent ASIN': parent_asin,
                            'Product Group': child_product_group,
                            'Product Title': child_title,
                            'Child Count': 0,  # Use 0 instead of empty string to fix Arrow serialization
                            'Spend': child_data.get('Spend', 0),
                            'Ad Sales': child_data.get('Ad Sales', 0),
                            'Total Sales': child_data.get('Total Sales', 0),
                            'Clicks': child_data.get('Clicks', 0),
                            'Sessions': child_data.get('Sessions', 0),
                            'Orders': child_data.get('Orders', 0)
                        }
                        
                        # Calculate child-level metrics
                        if child_record['Ad Sales'] > 0:
                            child_record['ACoS'] = (child_record['Spend'] / child_record['Ad Sales'] * 100)
                        else:
                            child_record['ACoS'] = 0
                            
                        if child_record['Total Sales'] > 0:
                            child_record['TACoS'] = (child_record['Spend'] / child_record['Total Sales'] * 100)
                        else:
                            child_record['TACoS'] = 0
                        
                        # Calculate CPC, CVR, and AOV for child
                        if child_record['Clicks'] > 0:
                            child_record['CPC'] = child_record['Spend'] / child_record['Clicks']
                            child_record['CVR'] = (child_record['Orders'] / child_record['Clicks'] * 100)
                        else:
                            child_record['CPC'] = 0
                            child_record['CVR'] = 0
                            
                        if child_record['Orders'] > 0:
                            child_record['AOV'] = child_record['Ad Sales'] / child_record['Orders']
                        else:
                            child_record['AOV'] = 0
                        
                        # Additional child-level ratios
                        try:
                            if child_record['Total Sales'] > 0:
                                child_record['Ad Sales % of Total'] = (child_record['Ad Sales'] / child_record['Total Sales']) * 100
                            else:
                                child_record['Ad Sales % of Total'] = 0
                        except Exception:
                            child_record['Ad Sales % of Total'] = 0
                        try:
                            if child_record.get('Sessions', 0) > 0:
                                child_record['Ad Traffic % of Total'] = (child_record.get('Clicks', 0) / child_record.get('Sessions', 0)) * 100
                            else:
                                child_record['Ad Traffic % of Total'] = 0
                        except Exception:
                            child_record['Ad Traffic % of Total'] = 0
                        
                        child_records.append(child_record)
                
                # Determine parent product group based on children
                child_product_groups = [record['Product Group'] for record in child_records if record['Product Group'] != 'Untagged Group']
                unique_groups = list(set(child_product_groups))
                
                if len(unique_groups) == 1:
                    # All children have the same product group
                    parent_summary['Product Group'] = unique_groups[0]
                elif len(unique_groups) > 1:
                    # Children have different product groups - leave parent blank
                    parent_summary['Product Group'] = ''
                else:
                    # All children are untagged
                    parent_summary['Product Group'] = 'Untagged Group'
                
                # Calculate parent-level metrics
                if parent_summary['Ad Sales'] > 0:
                    parent_summary['ACoS'] = (parent_summary['Spend'] / parent_summary['Ad Sales'] * 100)
                else:
                    parent_summary['ACoS'] = 0
                    
                if parent_summary['Total Sales'] > 0:
                    parent_summary['TACoS'] = (parent_summary['Spend'] / parent_summary['Total Sales'] * 100)
                else:
                    parent_summary['TACoS'] = 0
                
                # Calculate CPC, CVR, and AOV for parent
                if parent_summary['Clicks'] > 0:
                    parent_summary['CPC'] = parent_summary['Spend'] / parent_summary['Clicks']
                    parent_summary['CVR'] = (parent_summary['Orders'] / parent_summary['Clicks'] * 100)
                else:
                    parent_summary['CPC'] = 0
                    parent_summary['CVR'] = 0
                    
                if parent_summary['Orders'] > 0:
                    parent_summary['AOV'] = parent_summary['Ad Sales'] / parent_summary['Orders']
                else:
                    parent_summary['AOV'] = 0
                
                # Parent-level additional ratios
                try:
                    if parent_summary['Total Sales'] > 0:
                        parent_summary['Ad Sales % of Total'] = (parent_summary['Ad Sales'] / parent_summary['Total Sales']) * 100
                    else:
                        parent_summary['Ad Sales % of Total'] = 0
                except Exception:
                    parent_summary['Ad Sales % of Total'] = 0
                try:
                    if parent_summary.get('Sessions', 0) > 0:
                        parent_summary['Ad Traffic % of Total'] = (parent_summary.get('Clicks', 0) / parent_summary.get('Sessions', 0)) * 100
                    else:
                        parent_summary['Ad Traffic % of Total'] = 0
                except Exception:
                    parent_summary['Ad Traffic % of Total'] = 0
                
                # Add parent record followed by its children
                parent_asin_table_data.append(parent_summary)
                parent_asin_table_data.extend(child_records)
            
            # Create DataFrame for the table
            parent_asin_df = pd.DataFrame(parent_asin_table_data)
            
            # Check if there are any meaningful product groups defined (excluding 'Untagged Group')
            has_product_groups = False
            if (st.session_state.client_config and 
                'branded_asins_data' in st.session_state.client_config):
                for asin_info in st.session_state.client_config['branded_asins_data'].values():
                    if ('product_group' in asin_info and 
                        asin_info['product_group'].strip() and 
                        asin_info['product_group'].strip() != 'Untagged Group'):
                        has_product_groups = True
                        break
            
            # Calculate percentage metrics after creating the full dataframe
            col1, col2, col3 = st.columns([0.4, 0.3, 0.3])
            with col1:
                sort_options = {
                    'Parent ASIN (A-Z)': ('Parent ASIN', True),
                    'Spend (High to Low)': ('Spend', False),
                    'Spend (Low to High)': ('Spend', True),
                    'Ad Sales (High to Low)': ('Ad Sales', False),
                    'Ad Sales (Low to High)': ('Ad Sales', True),
                    'ACoS (High to Low)': ('ACoS', False),
                    'ACoS (Low to High)': ('ACoS', True),
                    'Total Sales (High to Low)': ('Total Sales', False),
                    'Total Sales (Low to High)': ('Total Sales', True)
                }
                
                selected_sort = st.selectbox(
                    "Sort By:",
                    options=list(sort_options.keys()),
                    index=1  # Default to Spend (High to Low)
                )
                
                sort_column, ascending = sort_options[selected_sort]
            
            with col2:
                show_detailed_columns = st.checkbox("Show detailed columns", value=True, key="parent_table_detailed")
                show_only_parents = st.checkbox("Show only Parent ASIN", value=False, key="parent_table_parents_only")
                
            with col3:
                # Conditional formatting scope toggle
                conditional_scope = st.radio(
                    "Conditional formatting scope:",
                    options=["Account-wide", "Parent ASIN only"],
                    index=0,
                    key="parent_conditional_scope",
                    help="Choose whether percentage conditional formatting is relative to the entire account or just the current Parent ASIN group"
                )
            
            # Calculate percentage metrics based on conditional formatting scope
            all_parents_df = parent_asin_df[parent_asin_df['Type'] == 'Parent']
            all_children_df = parent_asin_df[parent_asin_df['Type'] == 'Child']
            
            # Get account-wide totals from filtered_df for account-wide conditional formatting
            account_total_spend = filtered_df['Spend'].sum() if 'Spend' in filtered_df.columns else 0
            account_total_ad_sales = filtered_df['Ad Sales'].sum() if 'Ad Sales' in filtered_df.columns else 0
            account_total_sales = filtered_df['Total Sales'].sum() if 'Total Sales' in filtered_df.columns else 0
            
            all_parents_spend = all_parents_df['Spend'].sum()
            all_parents_ad_sales = all_parents_df['Ad Sales'].sum()
            all_parents_total_sales = all_parents_df['Total Sales'].sum()
            
            all_children_spend = all_children_df['Spend'].sum()
            all_children_ad_sales = all_children_df['Ad Sales'].sum()
            all_children_total_sales = all_children_df['Total Sales'].sum()
            
            # Determine which totals to use based on conditional scope and show_only_parents setting
            use_account_wide = (conditional_scope == "Account-wide" or show_only_parents)
            
            for idx, row in parent_asin_df.iterrows():
                if use_account_wide:
                    # Use account-wide totals for all calculations
                    if account_total_spend > 0:
                        parent_asin_df.at[idx, '% of Spend'] = (row['Spend'] / account_total_spend * 100)
                    if account_total_ad_sales > 0:
                        parent_asin_df.at[idx, '% of Ad Sales'] = (row['Ad Sales'] / account_total_ad_sales * 100)
                    if account_total_sales > 0:
                        parent_asin_df.at[idx, '% of Total Sales'] = (row['Total Sales'] / account_total_sales * 100)
                elif conditional_scope == "Parent ASIN only":
                    # Parent ASIN only scope: Children relative to their specific parent, Parents relative to all parents
                    if row['Type'] == 'Parent':
                        # Parents relative to all other parents
                        if all_parents_spend > 0:
                            parent_asin_df.at[idx, '% of Spend'] = (row['Spend'] / all_parents_spend * 100)
                        if all_parents_ad_sales > 0:
                            parent_asin_df.at[idx, '% of Ad Sales'] = (row['Ad Sales'] / all_parents_ad_sales * 100)
                        if all_parents_total_sales > 0:
                            parent_asin_df.at[idx, '% of Total Sales'] = (row['Total Sales'] / all_parents_total_sales * 100)
                    else:
                        # Children relative to their specific parent ASIN totals
                        parent_asin = row['Parent ASIN']
                        parent_data = parent_asin_df[(parent_asin_df['Parent ASIN'] == parent_asin) & (parent_asin_df['Type'] == 'Parent')]
                        
                        if not parent_data.empty:
                            parent_spend = parent_data.iloc[0]['Spend']
                            parent_ad_sales = parent_data.iloc[0]['Ad Sales']
                            parent_total_sales = parent_data.iloc[0]['Total Sales']
                            
                            if parent_spend > 0:
                                parent_asin_df.at[idx, '% of Spend'] = (row['Spend'] / parent_spend * 100)
                            if parent_ad_sales > 0:
                                parent_asin_df.at[idx, '% of Ad Sales'] = (row['Ad Sales'] / parent_ad_sales * 100)
                            if parent_total_sales > 0:
                                parent_asin_df.at[idx, '% of Total Sales'] = (row['Total Sales'] / parent_total_sales * 100)
                else:
                    # Account-wide scope: Parents relative to Parents, Children relative to Children
                    if row['Type'] == 'Parent':
                        # Parents relative to all other parents
                        if all_parents_spend > 0:
                            parent_asin_df.at[idx, '% of Spend'] = (row['Spend'] / all_parents_spend * 100)
                        if all_parents_ad_sales > 0:
                            parent_asin_df.at[idx, '% of Ad Sales'] = (row['Ad Sales'] / all_parents_ad_sales * 100)
                        if all_parents_total_sales > 0:
                            parent_asin_df.at[idx, '% of Total Sales'] = (row['Total Sales'] / all_parents_total_sales * 100)
                    else:
                        # Children relative to all other children
                        if all_children_spend > 0:
                            parent_asin_df.at[idx, '% of Spend'] = (row['Spend'] / all_children_spend * 100)
                        if all_children_ad_sales > 0:
                            parent_asin_df.at[idx, '% of Ad Sales'] = (row['Ad Sales'] / all_children_ad_sales * 100)
                        if all_children_total_sales > 0:
                            parent_asin_df.at[idx, '% of Total Sales'] = (row['Total Sales'] / all_children_total_sales * 100)
            
            # Sort the dataframe while maintaining parent-child grouping
            if sort_column == 'Parent ASIN':
                # For Parent ASIN sorting, sort by the parent ASIN column
                parent_asin_df_sorted = parent_asin_df.sort_values(['Parent ASIN', 'Type'], ascending=[ascending, False])
            else:
                # For other metrics, sort parent groups by the metric, but keep children with their parents
                parent_groups = []
                
                # Group by Parent ASIN
                for parent_asin in parent_asin_df[parent_asin_df['Type'] == 'Parent']['Parent ASIN'].unique():
                    parent_group = parent_asin_df[parent_asin_df['Parent ASIN'] == parent_asin].copy()
                    
                    # Sort children within each parent group by Total Sales (descending)
                    parent_row = parent_group[parent_group['Type'] == 'Parent']
                    child_rows = parent_group[parent_group['Type'] == 'Child'].sort_values('Total Sales', ascending=False)
                    
                    # Recombine parent and sorted children
                    parent_group = pd.concat([parent_row, child_rows], ignore_index=True)
                    parent_groups.append(parent_group)
                
                # Sort parent groups by the selected metric (using parent row values)
                parent_groups.sort(
                    key=lambda x: x[x['Type'] == 'Parent'][sort_column].iloc[0] if not x[x['Type'] == 'Parent'].empty else 0,
                    reverse=not ascending
                )
                
                # Concatenate sorted groups
                parent_asin_df_sorted = pd.concat(parent_groups, ignore_index=True)
            
            # Display columns based on detailed view setting
            if show_detailed_columns:
                if has_product_groups:
                    display_columns = ['Type', 'ASIN', 'Product Group', 'Product Title', '% of Spend', '% of Ad Sales', '% of Total Sales', 
                                     'Spend', 'Ad Sales', 'Total Sales', 'ACoS', 'TACoS', 'CPC', 'CVR', 'AOV',
                                     'Clicks', 'Sessions', 'Ad Traffic % of Total', 'Ad Sales % of Total', 'Child Count']
                else:
                    display_columns = ['Type', 'ASIN', 'Product Title', '% of Spend', '% of Ad Sales', '% of Total Sales', 
                                     'Spend', 'Ad Sales', 'Total Sales', 'ACoS', 'TACoS', 'CPC', 'CVR', 'AOV',
                                     'Clicks', 'Sessions', 'Ad Traffic % of Total', 'Ad Sales % of Total', 'Child Count']
            else:
                if has_product_groups:
                    display_columns = ['Type', 'ASIN', 'Product Group', 'Product Title', 'Spend', 'Ad Sales', 'Total Sales', 'ACoS', 'Child Count']
                else:
                    display_columns = ['Type', 'ASIN', 'Product Title', 'Spend', 'Ad Sales', 'Total Sales', 'ACoS', 'Child Count']
            
            # Filter to show only parents if requested
            if show_only_parents:
                parent_asin_df_sorted = parent_asin_df_sorted[parent_asin_df_sorted['Type'] == 'Parent'].copy()
            
            # Format the display dataframe
            display_df = parent_asin_df_sorted[display_columns].copy()

            # Reorder columns to place new fields after AOV (if present)
            try:
                desired_after_aov = ['Clicks', 'Sessions', 'Ad Traffic % of Total', 'Ad Sales % of Total']
                if 'AOV' in display_df.columns:
                    cols = list(display_df.columns)
                    cols = [c for c in cols if c not in desired_after_aov]
                    insert_idx = cols.index('AOV') + 1
                    for i, c in enumerate(desired_after_aov):
                        if c in parent_asin_df_sorted.columns:
                            cols.insert(insert_idx + i, c)
                    display_df = display_df[cols]
            except Exception:
                pass
            
            # Format currency columns
            currency_cols = ['Spend', 'Ad Sales', 'Total Sales', 'CPC', 'AOV']
            for col in currency_cols:
                if col in display_df.columns:
                    display_df[col] = display_df[col].apply(lambda x: f'${x:,.2f}' if pd.notna(x) and x != 0 else '$0.00')
            
            # Format percentage columns
            percentage_cols = ['ACoS', 'TACoS', '% of Spend', '% of Ad Sales', '% of Total Sales', 'CVR', 'Ad Sales % of Total', 'Ad Traffic % of Total']
            for col in percentage_cols:
                if col in display_df.columns:
                    display_df[col] = display_df[col].apply(lambda x: f'{x:.1f}%' if pd.notna(x) else '0.0%')

            # Format Clicks/Sessions as integer counts with commas
            for col in ['Clicks', 'Sessions']:
                if col in display_df.columns:
                    display_df[col] = display_df[col].apply(lambda x: f"{int(x):,}" if pd.notna(x) else '0')
            
            # Format Child Count column to show empty for child rows
            if 'Child Count' in display_df.columns:
                # First ensure the column is properly typed, then format for display
                display_df['Child Count'] = display_df.apply(
                    lambda row: str(int(row['Child Count'])) if row['Type'] == 'Parent' and row['Child Count'] > 0 else '',
                    axis=1
                )
                # Convert to string type to avoid Arrow serialization issues
                display_df['Child Count'] = display_df['Child Count'].astype(str)
            
            # Don't truncate product titles - let column width handle it
            
            # Style the dataframe with conditional formatting
            def highlight_parent_child_rows(row):
                styles = []
                for i, col in enumerate(row.index):
                    base_style = ''
                    
                    # Apply parent/child row styling
                    if row['Type'] == 'Parent':
                        base_style = 'background-color: rgba(70, 70, 70, 0.8); font-weight: bold; color: white'
                    else:
                        base_style = 'background-color: rgba(40, 40, 40, 0.4); color: #e0e0e0'
                    
                    # Apply conditional formatting for percentage columns
                    # Determine the appropriate scale based on row type and conditional scope
                    if col in ['% of Spend', '% of Ad Sales', '% of Total Sales'] and col in numeric_df.columns:
                        val = numeric_df.loc[row.name, col] if not pd.isna(numeric_df.loc[row.name, col]) else 0
                        
                        # Determine max scale based on conditional scope and row type
                        if use_account_wide or show_only_parents:
                            # Use a fixed scale for account-wide or parent-only view
                            max_scale = 100
                        elif conditional_scope == "Parent ASIN only":
                            # Parent ASIN only scope: Different scaling for parents vs children
                            if row['Type'] == 'Parent':
                                # Scale based on max percentage among parents
                                parent_vals = numeric_df[numeric_df['Type'] == 'Parent'][col].dropna()
                                max_scale = parent_vals.max() if len(parent_vals) > 0 else 100
                            else:
                                # For children in Parent ASIN only scope, scale to 100% since they're relative to their parent
                                max_scale = 100
                        else:
                            # Account-wide scope: Use dynamic scale based on row type
                            if row['Type'] == 'Parent':
                                # Scale based on max percentage among parents
                                parent_vals = numeric_df[numeric_df['Type'] == 'Parent'][col].dropna()
                                max_scale = parent_vals.max() if len(parent_vals) > 0 else 100
                            else:
                                # Scale based on max percentage among children
                                child_vals = numeric_df[numeric_df['Type'] == 'Child'][col].dropna()
                                max_scale = child_vals.max() if len(child_vals) > 0 else 100
                        
                        # Apply appropriate color gradient
                        if col == '% of Spend':
                            gradient_style = color_gradient_blue(val, 0, max_scale)
                            if gradient_style:
                                base_style = gradient_style
                        elif col == '% of Ad Sales':
                            gradient_style = color_gradient_green(val, 0, max_scale)
                            if gradient_style:
                                base_style = gradient_style
                        elif col == '% of Total Sales':
                            gradient_style = color_gradient_green(val, 0, max_scale)  # Changed to green to match Ad Sales
                            if gradient_style:
                                base_style = gradient_style
                    
                    styles.append(base_style)
                return styles
            
            # Create a numeric version for conditional formatting
            numeric_df = parent_asin_df_sorted[display_columns].copy()
            
            # Convert percentage columns to numeric for conditional formatting
            for col in ['% of Spend', '% of Ad Sales', '% of Total Sales']:
                if col in numeric_df.columns:
                    numeric_df[col] = pd.to_numeric(numeric_df[col], errors='coerce').fillna(0)
            
            # Apply styling and create the table
            styled_df = display_df.style.apply(highlight_parent_child_rows, axis=1)
            
            # Display the scrollable table
            st.dataframe(
                styled_df,
                use_container_width=True,
                hide_index=True,
                height=600,  # Fixed height to make it scrollable
                column_config={
                    "Type": st.column_config.TextColumn("Type", width="small"),
                    "ASIN": st.column_config.TextColumn("ASIN", width="medium"),
                    "Product Group": st.column_config.TextColumn("Product Group", width="medium"),
                    "Product Title": st.column_config.TextColumn("Product Title", width="large"),
                    "% of Spend": st.column_config.TextColumn("% of Spend", width="small"),
                    "% of Ad Sales": st.column_config.TextColumn("% of Ad Sales", width="small"),
                    "% of Total Sales": st.column_config.TextColumn("% of Total Sales", width="small"),
                    "Spend": st.column_config.TextColumn("Spend", width="small"),
                    "Ad Sales": st.column_config.TextColumn("Ad Sales", width="small"),
                    "Total Sales": st.column_config.TextColumn("Total Sales", width="small"),
                    "ACoS": st.column_config.TextColumn("ACoS", width="small"),
                    "Child Count": st.column_config.TextColumn("Children", width="small"),
                }
            )
            
            # Add Export Table button (mirrors Performance by ASIN export)
            exp_col1, exp_col2 = st.columns([4, 1])
            with exp_col2:
                if st.button("ðŸ“Š Export Table", key="export_parent_asin_table", help="Export the Performance by Parent ASIN table with conditional formatting to Excel"):
                    try:
                        from openpyxl import Workbook
                        from openpyxl.styles import PatternFill, Font, Alignment, NamedStyle
                        from openpyxl.formatting.rule import ColorScaleRule
                        from openpyxl.utils.dataframe import dataframe_to_rows
                        import io
                        
                        # Prepare data for export using the numeric version (respects current table setup)
                        export_df = numeric_df.copy()
                        
                        # Ensure numeric types for currency columns
                        for col in ['Spend', 'Ad Sales', 'Total Sales', 'AOV', 'CPC']:
                            if col in export_df.columns:
                                export_df[col] = pd.to_numeric(export_df[col], errors='coerce').fillna(0)
                        
                        # Ensure numeric types for percentage columns
                        for col in ['% of Spend', '% of Ad Sales', '% of Total Sales', 'ACoS', 'TACoS', 'CVR']:
                            if col in export_df.columns:
                                export_df[col] = pd.to_numeric(export_df[col], errors='coerce').fillna(0)
                        
                        # Create workbook and worksheet
                        wb = Workbook()
                        ws = wb.active
                        ws.title = "Performance by Parent ASIN"
                        
                        # Add data to worksheet
                        for r in dataframe_to_rows(export_df, index=False, header=True):
                            ws.append(r)
                        
                        # Style the header row
                        header_fill = PatternFill(start_color="366092", end_color="366092", fill_type="solid")
                        header_font = Font(color="FFFFFF", bold=True)
                        for cell in ws[1]:
                            cell.fill = header_fill
                            cell.font = header_font
                            cell.alignment = Alignment(horizontal="center")
                        
                        # Helper to convert index -> Excel column letter
                        def _col_letter(n: int) -> str:
                            s = ""
                            while n > 0:
                                n, r = divmod(n - 1, 26)
                                s = chr(65 + r) + s
                            return s
                        
                        # Map column names to letters
                        col_mapping = {col_name: _col_letter(idx) for idx, col_name in enumerate(export_df.columns, start=1)}
                        
                        # Determine row indices for Parent and Child (Excel rows start at 2)
                        parent_rows = []
                        child_rows = []
                        if 'Type' in export_df.columns:
                            for i, t in enumerate(export_df['Type'].astype(str).tolist(), start=2):
                                if t.strip().lower() == 'parent':
                                    parent_rows.append(i)
                                elif t.strip().lower() == 'child':
                                    child_rows.append(i)
                        
                        # Utility to build contiguous ranges from a list of row numbers
                        def _row_ranges(rows):
                            if not rows:
                                return []
                            ranges = []
                            start = prev = rows[0]
                            for r in rows[1:]:
                                if r == prev + 1:
                                    prev = r
                                else:
                                    ranges.append((start, prev))
                                    start = prev = r
                            ranges.append((start, prev))
                            return ranges
                        
                        # Apply conditional formatting to percentage columns respecting current scope
                        percentage_cols = ['% of Spend', '% of Ad Sales', '% of Total Sales']
                        use_account = (conditional_scope == "Account-wide") or show_only_parents
                        
                        for col in percentage_cols:
                            if col in export_df.columns and col in col_mapping:
                                letter = col_mapping[col]
                                # Determine end value(s) for scaling
                                if use_account:
                                    # One rule across all data rows
                                    data_range = f"{letter}2:{letter}{len(export_df) + 1}"
                                    rule = ColorScaleRule(
                                        start_type='num', start_value=0, start_color='FFFFFF',
                                        mid_type='num', mid_value=20, mid_color=('ADD8E6' if col == '% of Spend' else '90EE90'),
                                        end_type='num', end_value=100, end_color=('0066CC' if col == '% of Spend' else '006400')
                                    )
                                    ws.conditional_formatting.add(data_range, rule)
                                else:
                                    # Parent ASIN only: Parents relative to parents, Children relative to 100%
                                    # Parents
                                    if parent_rows:
                                        parent_vals = pd.to_numeric(export_df.loc[export_df['Type'] == 'Parent', col], errors='coerce').dropna()
                                        parent_end = float(parent_vals.max()) if len(parent_vals) > 0 else 100.0
                                        for start, end in _row_ranges(parent_rows):
                                            rng = f"{letter}{start}:{letter}{end}"
                                            rule = ColorScaleRule(
                                                start_type='num', start_value=0, start_color='FFFFFF',
                                                mid_type='num', mid_value=20, mid_color=('ADD8E6' if col == '% of Spend' else '90EE90'),
                                                end_type='num', end_value=parent_end, end_color=('0066CC' if col == '% of Spend' else '006400')
                                            )
                                            ws.conditional_formatting.add(rng, rule)
                                    # Children
                                    if child_rows:
                                        for start, end in _row_ranges(child_rows):
                                            rng = f"{letter}{start}:{letter}{end}"
                                            rule = ColorScaleRule(
                                                start_type='num', start_value=0, start_color='FFFFFF',
                                                mid_type='num', mid_value=20, mid_color=('ADD8E6' if col == '% of Spend' else '90EE90'),
                                                end_type='num', end_value=100, end_color=('0066CC' if col == '% of Spend' else '006400')
                                            )
                                            ws.conditional_formatting.add(rng, rule)
                        
                        # Apply ACoS color scale (green->yellow->red based on account target)
                        if 'ACoS' in export_df.columns and 'ACoS' in col_mapping:
                            acos_letter = col_mapping['ACoS']
                            acos_range = f"{acos_letter}2:{acos_letter}{len(export_df) + 1}"
                            acos_target = 25
                            if 'client_config' in st.session_state:
                                goals = st.session_state.client_config.get('goals', {})
                                target_value = goals.get('account_wide_acos')
                                if target_value is not None and isinstance(target_value, (int, float)):
                                    acos_target = target_value
                            acos_rule = ColorScaleRule(
                                start_type='num', start_value=0, start_color='90EE90',
                                mid_type='num', mid_value=acos_target, mid_color='FFFF00',
                                end_type='num', end_value=acos_target * 2, end_color='FF0000'
                            )
                            ws.conditional_formatting.add(acos_range, acos_rule)
                        
                        # Apply number formats
                        for col_name, letter in col_mapping.items():
                            if col_name in ['Spend', 'Ad Sales', 'Total Sales', 'AOV', 'CPC']:
                                for row in range(2, len(export_df) + 2):
                                    ws[f"{letter}{row}"].number_format = '"$"#,##0.00'
                            elif col_name in ['% of Spend', '% of Ad Sales', '% of Total Sales', 'ACoS', 'TACoS', 'CVR']:
                                for row in range(2, len(export_df) + 2):
                                    ws[f"{letter}{row}"].number_format = '0.00"%"'
                        
                        # Auto-fit column widths
                        for column in ws.columns:
                            max_len = 0
                            col_letter = column[0].column_letter
                            for cell in column:
                                try:
                                    max_len = max(max_len, len(str(cell.value)) if cell.value is not None else 0)
                                except Exception:
                                    pass
                            ws.column_dimensions[col_letter].width = min(max_len + 2, 50)
                        
                        # Save to BytesIO and provide download
                        excel_buffer = io.BytesIO()
                        wb.save(excel_buffer)
                        excel_buffer.seek(0)
                        from datetime import datetime
                        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                        filename = f"Performance_by_Parent_ASIN_{timestamp}.xlsx"
                        st.download_button(
                            label="ðŸ’¾ Download Excel File",
                            data=excel_buffer.getvalue(),
                            file_name=filename,
                            mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                            key="download_parent_asin_excel"
                        )
                        st.success(f"âœ… Excel file prepared! Click 'Download Excel File' to save {len(export_df):,} rows with conditional formatting.")
                    except Exception as e:
                        st.error(f"Error creating Excel export: {str(e)}")
                        # Fallback to CSV export
                        csv_data = numeric_df.copy()
                        for col in ['Spend', 'Ad Sales', 'Total Sales', 'AOV', 'CPC']:
                            if col in csv_data.columns:
                                csv_data[col] = csv_data[col].apply(lambda x: f"${x:,.2f}" if pd.notna(x) else "$0.00")
                        for col in ['% of Spend', '% of Ad Sales', '% of Total Sales', 'ACoS', 'TACoS', 'CVR']:
                            if col in csv_data.columns:
                                csv_data[col] = csv_data[col].apply(lambda x: f"{x:.2f}%" if pd.notna(x) else "0.00%")
                        csv = csv_data.to_csv(index=False)
                        st.download_button(
                            label="ðŸ“¥ Download CSV (Fallback)",
                            data=csv,
                            file_name="Performance_by_Parent_ASIN_fallback.csv",
                            mime="text/csv"
                        )
            
            # Summary statistics
            st.markdown("---")
            parent_df = parent_asin_df[parent_asin_df['Type'] == 'Parent'].copy()
            child_df = parent_asin_df[parent_asin_df['Type'] == 'Child'].copy()
            total_parents = len(parent_df)
            total_children = len(child_df)
            
            # Display metrics in a 2x1 grid
            summary_col1, summary_col2 = st.columns(2)
            with summary_col1:
                st.metric("Total Parent ASINs", total_parents)
            with summary_col2:
                st.metric("Total Child ASINs", total_children)
                
            # Add spacing
            st.markdown("<div style='margin-top:2rem;'></div>", unsafe_allow_html=True)
            
            # --- Parent ASIN Allocation Chart Section ---
            # Add extra spacing above the Parent ASIN Allocation Chart
            st.markdown("<br><br>", unsafe_allow_html=True)
            # Create a stacked bar chart to visualize Spend, Ad Sales, and Total Sales by Parent ASIN
            st.markdown("##### Parent ASIN Allocation Chart")
        
            # Create tabs for different visualization types
            parent_asin_viz_tabs = st.tabs(["% Values", "$ Values", "Bubble Chart", "Donut Chart"])
        
            # Initialize common variables with default values
            if 'parent_asin_viz_sort_by' not in st.session_state:
                st.session_state.parent_asin_viz_sort_by = "Total Sales"
            if 'parent_asin_viz_count' not in st.session_state:
                st.session_state.parent_asin_viz_count = 10
            if 'parent_asin_title_length' not in st.session_state:
                st.session_state.parent_asin_title_length = 45
        
            # Prepare base data for visualization - use only Parent rows
            base_parent_chart_df = parent_df.copy()
        
            # Ensure all values are numeric
            for col in ['Spend', 'Ad Sales', 'Total Sales', 'ACoS', 'TACoS']:
                if col in base_parent_chart_df.columns:
                    base_parent_chart_df[col] = pd.to_numeric(base_parent_chart_df[col].astype(str).str.replace('$', '').str.replace('%', '').str.replace(',', ''), errors='coerce').fillna(0)
        
            # Create shortened labels for Parent ASINs with customizable length
            def format_parent_product_title(asin, product_group, max_length, display_mode='asin_only'):
                # User can choose between 'asin_only' or 'asin_and_group'
                if display_mode == 'asin_only':
                    return asin
                elif display_mode == 'asin_and_group':
                    # Show ASIN - Product Group when there's a unanimous product group
                    if not product_group or product_group.strip() == '' or product_group == 'Untagged Group':
                        return asin  # Fall back to ASIN only if no valid product group
                    else:
                        return f"{asin} - {product_group}"
                else:
                    # Default fallback
                    return asin
        
            # Always use stacked mode
            barmode = 'stack'
        
            # Custom color scheme that works well in dark mode
            custom_colors = {
                'Spend': '#EA4335',      # Red for Spend
                'Ad Sales': '#4285F4',  # Blue for Ad Sales
                'Total Sales': '#34A853' # Green for Total Sales
            }
        
            # Percentage Values Tab (first tab)
            with parent_asin_viz_tabs[0]:
                # Add chart controls specific to % Values tab
                with st.expander("Chart Display Options", expanded=False):
                    col1, col2 = st.columns(2)
                    with col1:
                        sort_by_pct = st.selectbox(
                            "Sort Parent ASINs by:",
                            options=["Total Sales", "Spend", "Ad Sales"],
                            index=0,
                            key="parent_asin_viz_sort_by_pct"
                        )
                        # Update the session state for other tabs to use
                        st.session_state.parent_asin_viz_sort_by = sort_by_pct
                    with col2:
                        display_count_pct = st.selectbox(
                            "Number of Parent ASINs to display:",
                            options=[5, 10, 15, 20],
                            index=1,
                            key="parent_asin_viz_count_pct"
                        )
                        # Update the session state for other tabs to use
                        st.session_state.parent_asin_viz_count = display_count_pct
                
                    # Title length control
                    title_length_pct = st.slider(
                        "Product title length:",
                        min_value=20,
                        max_value=150,
                        value=45,
                        step=5,
                        key="parent_asin_title_length_pct"
                    )
                    # Update the session state for other tabs to use
                    st.session_state.parent_asin_title_length = title_length_pct
                
                    # Label display mode control
                    label_display_mode_pct = st.radio(
                        "Label display:",
                        options=["ASIN only", "ASIN + Product Group"],
                        index=1,  # Default to ASIN + Product Group
                        key="parent_asin_label_display_pct",
                        horizontal=True,
                        help="Choose whether to show just the ASIN or include the Product Group when available"
                    )
                    # Convert to internal format and update session state
                    label_mode = 'asin_only' if label_display_mode_pct == "ASIN only" else 'asin_and_group'
                    st.session_state.parent_asin_label_display_mode = label_mode
            
                # Use the values from this tab's controls
                sort_by = sort_by_pct
                display_count = display_count_pct
                title_length = title_length_pct
            
                # Create a copy of the data for this tab
                chart_df = base_parent_chart_df.copy()
            
                # Sort based on user selection
                if sort_by in chart_df.columns:
                    # Always sort descending for Spend, Total Sales, Ad Sales
                    ascending = False
                    chart_df = chart_df.sort_values(by=sort_by, ascending=ascending).head(display_count)
                else:
                    # Default sort by Total Sales
                    chart_df = chart_df.sort_values(by='Total Sales', ascending=False).head(display_count)
            
                # Apply formatting with user-selected length and display mode
                chart_df['label'] = chart_df.apply(
                    lambda row: format_parent_product_title(
                        row['ASIN'], 
                        row['Product Group'], 
                        title_length,
                        label_mode
                    ),
                    axis=1
                )
                # Create percentage data
                pct_df = chart_df.copy()
            
                # Calculate percentages based on ALL Parent ASINs, not just displayed ones
                # Use the global totals from the original parent_df (before limiting to display_count)
                total_spend_all = parent_df['Spend'].sum()
                total_ad_sales_all = parent_df['Ad Sales'].sum()
                total_sales_all = parent_df['Total Sales'].sum()
            
                pct_df['Spend %'] = np.where(total_spend_all > 0, (pct_df['Spend'] / total_spend_all * 100).round(1), 0.0)
                pct_df['Ad Sales %'] = np.where(total_ad_sales_all > 0, (pct_df['Ad Sales'] / total_ad_sales_all * 100).round(1), 0.0)
                pct_df['Total Sales %'] = np.where(total_sales_all > 0, (pct_df['Total Sales'] / total_sales_all * 100).round(1), 0.0)
                # Fill any remaining NaNs in % columns just in case
                pct_df[['Spend %', 'Ad Sales %', 'Total Sales %']] = pct_df[['Spend %', 'Ad Sales %', 'Total Sales %']].fillna(0.0)
            
                # Create the % values chart - horizontal orientation with enhanced styling
                fig_pct = px.bar(
                    pct_df,
                    y='label',  # Now y-axis has the labels
                    x=['Spend %', 'Ad Sales %', 'Total Sales %'],  # Now x-axis has the values
                    title=f'Top {display_count} Parent ASINs by {sort_by} (% of All Parent ASINs)',
                    labels={'label': 'Parent ASIN', 'value': 'Percentage (%)', 'variable': 'Metric'},
                    height=600,  # Increased height for better readability
                    barmode=barmode,
                    orientation='h',  # Horizontal bars
                    color_discrete_map={
                        'Spend %': '#EA4335',  # Red for Spend
                        'Ad Sales %': '#4285F4',  # Blue for Ad Sales
                        'Total Sales %': '#34A853'  # Green for Total Sales
                    },
                    text_auto=False,  # We will set text manually to avoid NaN%
                    template='plotly_dark'  # Use a dark theme for better visual appeal
                )
                # Ensure bar text always shows a value, never NaN
                for metric in ['Spend %', 'Ad Sales %', 'Total Sales %']:
                    fig_pct.for_each_trace(lambda trace: trace.update(text=[f'{v:.1f}%' if not pd.isna(v) and v > 0 else '0.0%' for v in trace.x]) if trace.name == metric else None)
            
                # Determine font size based on number of Parent ASINs displayed
                y_axis_font_size = 14 if display_count <= 10 else 11
            
                # Improve layout for horizontal chart with enhanced styling
                fig_pct.update_layout(
                    xaxis=dict(
                        title=dict(text='Percentage (%)', font=dict(size=14, family="Arial, sans-serif")),
                        tickformat='.1f',
                        ticksuffix='%',
                        gridcolor='rgba(255,255,255,0.1)',
                        showgrid=True,
                        zeroline=True,
                        tickfont=dict(family="Arial, sans-serif")
                    ),
                    yaxis=dict(
                        title='',
                        automargin=True,  # Ensure there's enough space for labels
                        tickfont=dict(size=y_axis_font_size, family="Arial, sans-serif"),
                        gridcolor='rgba(0,0,0,0)'  # No horizontal grid lines
                    ),
                    legend=dict(
                        title='',
                        orientation='h',
                        yanchor='bottom',
                        y=1.02,
                        xanchor='right',
                        x=1,
                        bgcolor='rgba(0,0,0,0.5)',
                        bordercolor='rgba(255,255,255,0.3)',
                        font=dict(family="Arial, sans-serif", size=12)
                    ),
                    margin=dict(l=40, r=40, t=70, b=100),
                    hovermode='closest',
                    plot_bgcolor='rgba(0,0,0,0)',  # Transparent background
                    paper_bgcolor='rgba(0,0,0,0)',  # Transparent paper
                    title=dict(
                        text=f'Top {display_count} Parent ASINs by {sort_by} (% of All Parent ASINs)',
                        font=dict(size=16, family="Arial, sans-serif")
                    ),
                    hoverlabel=dict(
                        bgcolor='rgba(0,0,0,0.8)',
                        font_size=12,
                        font=dict(family="Arial, sans-serif", color='white')
                    )
                )
            
                # Determine text size based on number of Parent ASINs displayed
                bar_text_size = 14 if display_count <= 10 else 11
            
                # Improve text display on bars with fixed position
                fig_pct.update_traces(
                    textposition='outside',  # Fixed to outside position
                    textfont=dict(size=bar_text_size, family="Arial, sans-serif", color="white"),
                    hovertemplate='<b>%{y}</b><br>' +
                                 '%{x:.1f}% of total<br>' +
                                 '<extra></extra>',
                    texttemplate='%{text}',  # Use the custom text we defined above
                    selector=dict(type='bar'),
                    opacity=0.9,  # Slight transparency for better aesthetics
                    marker=dict(line=dict(width=0.5, color='rgba(255,255,255,0.2)'))  # Add subtle borders
                )
            
                # Display the chart
                st.plotly_chart(fig_pct, use_container_width=True)
        
            # Dollar Values Tab (second tab)
            with parent_asin_viz_tabs[1]:
                # Add chart controls specific to $ Values tab
                with st.expander("Chart Display Options", expanded=False):
                    col1, col2 = st.columns(2)
                    with col1:
                        # Get index of current sort_by value in options list
                        sort_options = ["Total Sales", "Spend", "Ad Sales"]
                        current_sort_index = sort_options.index(st.session_state.parent_asin_viz_sort_by) if st.session_state.parent_asin_viz_sort_by in sort_options else 0
                    
                        sort_by_dollar = st.selectbox(
                            "Sort Parent ASINs by:",
                            options=sort_options,
                            index=current_sort_index,
                            key="parent_asin_viz_sort_by_dollar"
                        )
                        # Update the session state for other tabs to use
                        st.session_state.parent_asin_viz_sort_by = sort_by_dollar
                    with col2:
                        # Get index of current display count in options list
                        count_options = [5, 10, 15, 20]
                        current_count_index = count_options.index(st.session_state.parent_asin_viz_count) if st.session_state.parent_asin_viz_count in count_options else 1
                    
                        display_count_dollar = st.selectbox(
                            "Number of Parent ASINs to display:",
                            options=count_options,
                            index=current_count_index,
                            key="parent_asin_viz_count_dollar"
                        )
                        # Update the session state for other tabs to use
                        st.session_state.parent_asin_viz_count = display_count_dollar
                
                    # Title length control
                    title_length_dollar = st.slider(
                        "Product title length:",
                        min_value=20,
                        max_value=150,
                        value=st.session_state.parent_asin_title_length,
                        step=5,
                        key="parent_asin_title_length_dollar"
                    )
                    # Update the session state for other tabs to use
                    st.session_state.parent_asin_title_length = title_length_dollar
                
                    # Label display mode control
                    current_mode = st.session_state.get('parent_asin_label_display_mode', 'asin_and_group')
                    current_display = "ASIN only" if current_mode == 'asin_only' else "ASIN + Product Group"
                    label_display_mode_dollar = st.radio(
                        "Label display:",
                        options=["ASIN only", "ASIN + Product Group"],
                        index=0 if current_display == "ASIN only" else 1,
                        key="parent_asin_label_display_dollar",
                        horizontal=True,
                        help="Choose whether to show just the ASIN or include the Product Group when available"
                    )
                    # Convert to internal format and update session state
                    label_mode_dollar = 'asin_only' if label_display_mode_dollar == "ASIN only" else 'asin_and_group'
                    st.session_state.parent_asin_label_display_mode = label_mode_dollar
            
                # Use the values from this tab's controls
                sort_by = sort_by_dollar
                display_count = display_count_dollar
                title_length = title_length_dollar
            
                # Create a copy of the data for this tab
                chart_df = base_parent_chart_df.copy()
            
                # Sort based on user selection
                if sort_by in chart_df.columns:
                    # Always sort descending for Spend, Total Sales, Ad Sales
                    ascending = False
                    chart_df = chart_df.sort_values(by=sort_by, ascending=ascending).head(display_count)
                else:
                    # Default sort by Total Sales
                    chart_df = chart_df.sort_values(by='Total Sales', ascending=False).head(display_count)
            
                # Apply formatting with user-selected length and display mode
                chart_df['label'] = chart_df.apply(
                    lambda row: format_parent_product_title(
                        row['ASIN'], 
                        row['Product Group'], 
                        title_length,
                        label_mode_dollar
                    ),
                    axis=1
                )
                # Create the $ values chart - horizontal orientation
                fig_dollars = px.bar(
                    chart_df,
                    y='label',  # Now y-axis has the labels
                    x=['Spend', 'Ad Sales', 'Total Sales'],  # Now x-axis has the values
                    title=f'Top {display_count} Parent ASINs by {sort_by} ($)',
                    labels={'label': 'Parent ASIN', 'value': 'Amount ($)', 'variable': 'Metric'},
                    height=600,  # Increased height for better readability
                    barmode=barmode,
                    orientation='h',  # Horizontal bars
                    color_discrete_map={
                        'Spend': '#EA4335',  # Red for Spend
                        'Ad Sales': '#4285F4',  # Blue for Ad Sales
                        'Total Sales': '#34A853'  # Green for Total Sales
                    },
                    text_auto=False,  # We will set text manually
                    template='plotly_dark'  # Use a dark theme for better visual appeal
                )
            
                # Format text to show dollar amounts
                for metric in ['Spend', 'Ad Sales', 'Total Sales']:
                    fig_dollars.for_each_trace(lambda trace: trace.update(text=[f'${v:,.0f}' if not pd.isna(v) and v > 0 else '$0' for v in trace.x]) if trace.name == metric else None)
            
                # Determine font size based on number of Parent ASINs displayed
                y_axis_font_size = 14 if display_count <= 10 else 11
            
                # Improve layout for horizontal chart with enhanced styling
                fig_dollars.update_layout(
                    xaxis=dict(
                        title=dict(text='Amount ($)', font=dict(size=14, family="Arial, sans-serif")),
                        tickformat='$,.0f',
                        gridcolor='rgba(255,255,255,0.1)',
                        showgrid=True,
                        zeroline=True,
                        tickfont=dict(family="Arial, sans-serif")
                    ),
                    yaxis=dict(
                        title='',
                        automargin=True,  # Ensure there's enough space for labels
                        tickfont=dict(size=y_axis_font_size, family="Arial, sans-serif"),
                        gridcolor='rgba(0,0,0,0)'  # No horizontal grid lines
                    ),
                    legend=dict(
                        title='',
                        orientation='h',
                        yanchor='bottom',
                        y=1.02,
                        xanchor='right',
                        x=1,
                        bgcolor='rgba(0,0,0,0.5)',
                        bordercolor='rgba(255,255,255,0.3)',
                        font=dict(family="Arial, sans-serif", size=12)
                    ),
                    margin=dict(l=40, r=40, t=70, b=100),
                    hovermode='closest',
                    plot_bgcolor='rgba(0,0,0,0)',  # Transparent background
                    paper_bgcolor='rgba(0,0,0,0)',  # Transparent paper
                    title=dict(
                        text=f'Top {display_count} Parent ASINs by {sort_by} ($)',
                        font=dict(size=16, family="Arial, sans-serif")
                    ),
                    hoverlabel=dict(
                        bgcolor='rgba(0,0,0,0.8)',
                        font_size=12,
                        font=dict(family="Arial, sans-serif", color='white')
                    )
                )
            
                # Determine text size based on number of Parent ASINs displayed
                bar_text_size = 14 if display_count <= 10 else 11
            
                # Improve text display on bars with fixed position
                fig_dollars.update_traces(
                    textposition='outside',  # Fixed to outside position
                    textfont=dict(size=bar_text_size, family="Arial, sans-serif", color="white"),
                    hovertemplate='<b>%{y}</b><br>' +
                                 '$%{x:,.2f}<br>' +
                                 '<extra></extra>',
                    texttemplate='%{text}',  # Use the custom text we defined above
                    selector=dict(type='bar'),
                    opacity=0.9,  # Slight transparency for better aesthetics
                    marker=dict(line=dict(width=0.5, color='rgba(255,255,255,0.2)'))  # Add subtle borders
                )
            
                # Display the chart
                st.plotly_chart(fig_dollars, use_container_width=True)
        
            # Bubble Chart Tab (third tab)
            with parent_asin_viz_tabs[2]:
                # Add chart controls specific to Bubble Chart tab
                with st.expander("Chart Display Options", expanded=False):
                    # Axis selection for bubble chart
                    st.markdown("### Axis Selection")
                    col1, col2 = st.columns(2)
                    with col1:
                        x_metric = st.selectbox(
                            "X-Axis Metric:",
                            options=["Total Sales", "Ad Sales", "Spend", "ACoS", "ROAS", "CPC", "CVR", "AOV"],
                            index=0,  # Default to Total Sales
                            key="parent_bubble_x_metric"
                        )
                    with col2:
                        y_metric = st.selectbox(
                            "Y-Axis Metric:",
                            options=["ACoS", "ROAS", "Total Sales", "Ad Sales", "Spend", "CPC", "CVR", "AOV"],
                            index=0,  # Default to ACoS
                            key="parent_bubble_y_metric"
                        )
                
                    # Best-fit line options
                    st.markdown("### Best-Fit Line Options")
                    col3, col4 = st.columns(2)
                    with col3:
                        show_best_fit = st.checkbox(
                            "Show Best-Fit Line",
                            value=True,
                            key="parent_show_best_fit_line",
                            help="Add a trend line to show the relationship between the selected metrics"
                        )
                    with col4:
                        if show_best_fit:
                            best_fit_scope = st.selectbox(
                                "Best-Fit Data Scope:",
                                options=["Current filtered data", "All account data"],
                                index=0,  # Current filtered data as default
                                key="parent_best_fit_scope",
                                help="Choose whether to calculate the trend line based on current filters/pivot or all data in the account"
                            )
                        else:
                            # Set default when not showing best fit line
                            best_fit_scope = "Current filtered data"
            
                    # Label display mode control
                    st.markdown("### Label Display Options")
                    current_mode = st.session_state.get('parent_asin_label_display_mode', 'asin_and_group')
                    current_display = "ASIN only" if current_mode == 'asin_only' else "ASIN + Product Group"
                    label_display_mode_bubble = st.radio(
                        "Label display:",
                        options=["ASIN only", "ASIN + Product Group"],
                        index=0 if current_display == "ASIN only" else 1,
                        key="parent_asin_label_display_bubble",
                        horizontal=True,
                        help="Choose whether to show just the ASIN or include the Product Group when available"
                    )
                    # Convert to internal format and update session state
                    label_mode_bubble = 'asin_only' if label_display_mode_bubble == "ASIN only" else 'asin_and_group'
                    st.session_state.parent_asin_label_display_mode = label_mode_bubble
            
                # Use the values from the session state
                sort_by = st.session_state.parent_asin_viz_sort_by
                display_count = st.session_state.parent_asin_viz_count
                title_length = st.session_state.parent_asin_title_length
            
                # Create a copy of the data for this tab
                chart_df = base_parent_chart_df.copy()
            
                # Sort based on user selection
                if sort_by in chart_df.columns:
                    # Always sort descending for Spend, Total Sales, Ad Sales
                    ascending = False
                    chart_df = chart_df.sort_values(by=sort_by, ascending=ascending).head(display_count)
                else:
                    # Default sort by Total Sales
                    chart_df = chart_df.sort_values(by='Total Sales', ascending=False).head(display_count)
            
                # Apply formatting with user-selected length and display mode
                chart_df['label'] = chart_df.apply(
                    lambda row: format_parent_product_title(
                        row['ASIN'], 
                        row['Product Group'], 
                        title_length,
                        label_mode_bubble
                    ),
                    axis=1
                )
                # Create a copy of the data for bubble chart
                bubble_df = chart_df.copy()
            
                # Add percentage columns for hover information
                bubble_df['Spend %'] = np.where(total_spend_all > 0, (bubble_df['Spend'] / total_spend_all * 100).round(1), 0.0)
                bubble_df['Ad Sales %'] = np.where(total_ad_sales_all > 0, (bubble_df['Ad Sales'] / total_ad_sales_all * 100).round(1), 0.0)
                bubble_df['Total Sales %'] = np.where(total_sales_all > 0, (bubble_df['Total Sales'] / total_sales_all * 100).round(1), 0.0)
            
                # Ensure numeric values for all metrics
                for col in ['ACoS', 'ROAS', 'CPC', 'CVR', 'AOV']:
                    if col in bubble_df.columns:
                        bubble_df[col] = pd.to_numeric(bubble_df[col].astype(str).str.replace('%', '').str.replace('x', '').str.replace(',', ''), errors='coerce').fillna(0)
            
                # Create hover text with comprehensive metrics
                def create_parent_hover_text(row):
                    hover_text = f"<b>{row['ASIN']}</b><br>"
                    hover_text += f"{row['Product Title'][:50]}{'...' if len(row['Product Title']) > 50 else ''}<br><br>"
                    hover_text += f"<b>Total Sales:</b> ${row['Total Sales']:,.2f} ({row['Total Sales %']:.1f}%)<br>"
                    hover_text += f"<b>Ad Sales:</b> ${row['Ad Sales']:,.2f} ({row['Ad Sales %']:.1f}%)<br>"
                    hover_text += f"<b>Spend:</b> ${row['Spend']:,.2f} ({row['Spend %']:.1f}%)<br>"
                    hover_text += f"<b>Child ASINs:</b> {row['Child Count']}<br>"
                
                    if 'ACoS' in row and not pd.isna(row['ACoS']):
                        hover_text += f"<b>ACoS:</b> {row['ACoS']:.1f}%<br>"
                    if 'ROAS' in row and not pd.isna(row['ROAS']):
                        hover_text += f"<b>ROAS:</b> {row['ROAS']:.1f}x<br>"
                    if 'CPC' in row and not pd.isna(row['CPC']):
                        hover_text += f"<b>CPC:</b> ${row['CPC']:.2f}<br>"
                    if 'CVR' in row and not pd.isna(row['CVR']):
                        hover_text += f"<b>CVR:</b> {row['CVR']:.2f}%<br>"
                    if 'AOV' in row and not pd.isna(row['AOV']):
                        hover_text += f"<b>AOV:</b> ${row['AOV']:.2f}<br>"
                
                    return hover_text
            
                bubble_df['hover_text'] = bubble_df.apply(create_parent_hover_text, axis=1)
            
                # Calculate and add best-fit line if requested
                best_fit_line_data = None
                if show_best_fit and len(bubble_df) > 2:
                    try:
                        # Determine which dataset to use for best-fit calculation
                        if best_fit_scope == "All account data":
                            # Use all base_parent_chart_df data (before limiting to display_count)
                            bestfit_df = base_parent_chart_df.copy()
                        else:
                            # Use only the currently filtered/displayed data
                            bestfit_df = bubble_df.copy()
                    
                        # Ensure numeric values for best-fit calculation
                        for col in ['ACoS', 'ROAS', 'CPC', 'CVR', 'AOV']:
                            if col in bestfit_df.columns:
                                bestfit_df[col] = pd.to_numeric(bestfit_df[col].astype(str).str.replace('%', '').str.replace('x', '').str.replace(',', ''), errors='coerce').fillna(0)
                    
                        # Clean data for best-fit calculation (remove NaN and infinite values)
                        bestfit_clean = bestfit_df[[x_metric, y_metric]].dropna()
                        bestfit_clean = bestfit_clean[np.isfinite(bestfit_clean[x_metric]) & np.isfinite(bestfit_clean[y_metric])]
                    
                        if len(bestfit_clean) > 2:
                            # Calculate linear regression
                            from sklearn.linear_model import LinearRegression
                            X = bestfit_clean[x_metric].values.reshape(-1, 1)
                            y = bestfit_clean[y_metric].values
                        
                            model = LinearRegression()
                            model.fit(X, y)
                        
                            # Generate line points
                            x_min, x_max = bestfit_clean[x_metric].min(), bestfit_clean[x_metric].max()
                            x_line = np.linspace(x_min, x_max, 100)
                            y_line = model.predict(x_line.reshape(-1, 1))
                        
                            # Calculate R-squared
                            r_squared = model.score(X, y)
                        
                            # Store best-fit line data
                            best_fit_line_data = {
                                'x': x_line,
                                'y': y_line,
                                'r_squared': r_squared,
                                'slope': model.coef_[0],
                                'intercept': model.intercept_,
                                'data_scope': best_fit_scope,
                                'n_points': len(bestfit_clean)
                            }
                    except Exception as e:
                        st.warning(f"Could not calculate best-fit line: {str(e)}")
                        best_fit_line_data = None
            
                # Create the bubble chart
                fig_bubble = px.scatter(
                    bubble_df,
                    x=x_metric,
                    y=y_metric,
                    size="Ad Sales",  # Bubble size represents Ad Sales
                    color="ACoS" if "ACoS" in bubble_df.columns else "Total Sales",  # Color represents ACoS if available
                    hover_name="ASIN",
                    text="ASIN",
                    size_max=60,
                    color_continuous_scale="RdYlGn_r" if "ACoS" in bubble_df.columns else "Viridis",  # Red-Yellow-Green reversed for ACoS
                    title=f'Parent ASIN Performance: {y_metric} vs {x_metric}',
                    height=600,
                    template='plotly_dark'
                )
            
                # Format hover text
                fig_bubble.update_traces(
                    hovertemplate='%{customdata}<extra></extra>',
                    customdata=bubble_df['hover_text'],
                    textposition='top center',
                    textfont=dict(family="Arial, sans-serif", size=10, color="white"),
                    marker=dict(opacity=0.8, line=dict(width=1, color='white'))
                )
            
                # Update layout
                x_suffix = '%' if x_metric in ['ACoS', 'CVR'] else 'x' if x_metric == 'ROAS' else ''
                y_suffix = '%' if y_metric in ['ACoS', 'CVR'] else 'x' if y_metric == 'ROAS' else ''
            
                fig_bubble.update_layout(
                    xaxis=dict(
                        title=dict(text=f"{x_metric} {x_suffix}", font=dict(size=14, family="Arial, sans-serif")),
                        ticksuffix=x_suffix,
                        gridcolor='rgba(255,255,255,0.1)',
                        zeroline=True,
                        zerolinecolor='rgba(255,255,255,0.3)'
                    ),
                    yaxis=dict(
                        title=dict(text=f"{y_metric} {y_suffix}", font=dict(size=14, family="Arial, sans-serif")),
                        ticksuffix=y_suffix if y_suffix else '',
                        tickprefix='$' if y_metric in ['Total Sales', 'Ad Sales', 'Spend'] else '',
                        gridcolor='rgba(255,255,255,0.1)',
                        zeroline=True,
                        zerolinecolor='rgba(255,255,255,0.3)'
                    ),
                    coloraxis_colorbar=dict(
                        title="ACoS" if "ACoS" in bubble_df.columns else "Total Sales",
                        ticksuffix="%" if "ACoS" in bubble_df.columns else "",
                        tickprefix="" if "ACoS" in bubble_df.columns else "$"
                    ),
                    margin=dict(l=40, r=40, t=60, b=40),
                    legend=dict(orientation='h'),
                    hoverlabel=dict(
                        bgcolor='rgba(0,0,0,0.8)',
                        font_size=12,
                        font=dict(family="Arial, sans-serif", color='white')
                    )
                )
            
                # Add best-fit line to the chart if calculated
                if best_fit_line_data is not None:
                    # Add the best-fit line to the figure
                    fig_bubble.add_trace(go.Scatter(
                        x=best_fit_line_data['x'],
                        y=best_fit_line_data['y'],
                        mode='lines',
                        name=f'Best-Fit Line (RÂ² = {best_fit_line_data["r_squared"]:.3f})',
                        line=dict(
                            color='rgba(255, 255, 0, 0.8)',  # Yellow line
                            width=2,
                            dash='dash'
                        ),
                        hovertemplate=f'<b>Best-Fit Line</b><br>' +
                                     f'RÂ² = {best_fit_line_data["r_squared"]:.3f}<br>' +
                                     f'Slope = {best_fit_line_data["slope"]:.2e}<br>' +
                                     f'Data: {best_fit_line_data["data_scope"]}<br>' +
                                     f'Points: {best_fit_line_data["n_points"]}<br>' +
                                     f'{x_metric}: %{{x}}<br>' +
                                     f'{y_metric}: %{{y}}<extra></extra>',
                        showlegend=True
                    ))
                
                    # Update layout to show legend if best-fit line is displayed
                    fig_bubble.update_layout(
                        showlegend=True,
                        legend=dict(
                            orientation='h',
                            yanchor='bottom',
                            y=1.02,
                            xanchor='right',
                            x=1,
                            bgcolor='rgba(0,0,0,0.5)',
                            bordercolor='rgba(255,255,255,0.2)',
                            borderwidth=1
                        )
                    )
            
                # Display the bubble chart
                st.plotly_chart(fig_bubble, use_container_width=True)
            
                # Display best-fit line statistics if available
                if best_fit_line_data is not None:
                    with st.expander("ðŸ“Š Best-Fit Line Statistics", expanded=False):
                        col1, col2, col3, col4 = st.columns(4)
                        with col1:
                            st.metric("R-squared", f"{best_fit_line_data['r_squared']:.3f}")
                        with col2:
                            st.metric("Slope", f"{best_fit_line_data['slope']:.2e}")
                        with col3:
                            st.metric("Data Points", f"{best_fit_line_data['n_points']:,}")
                        with col4:
                            st.metric("Data Scope", best_fit_line_data['data_scope'])
                    
                        # Interpretation text
                        r_squared = best_fit_line_data['r_squared']
                        if r_squared >= 0.7:
                            interpretation = "Strong positive correlation"
                        elif r_squared >= 0.5:
                            interpretation = "Moderate positive correlation"
                        elif r_squared >= 0.3:
                            interpretation = "Weak positive correlation"
                        else:
                            interpretation = "Very weak or no correlation"
                    
                        st.info(f"**Interpretation:** {interpretation} between {x_metric} and {y_metric}")
        
            # Donut Chart Visualization Tab (fourth tab)
            with parent_asin_viz_tabs[3]:
                # Add chart controls specific to Donut Chart tab
                with st.expander("Chart Display Options", expanded=False):
                    # Only show Donut Chart specific options
                    st.markdown("### Donut Chart Options")
                    donut_size = st.slider(
                        "Donut hole size:",
                        min_value=0.3,
                        max_value=0.8,
                        value=0.6,
                        step=0.05,
                        key="parent_donut_hole_size"
                    )
                
                    text_position = st.radio(
                        "Label position:",
                        options=["outside", "inside", "auto"],
                        index=0,
                        key="parent_donut_text_position",
                        horizontal=True
                    )
                
                    # Label display mode control
                    current_mode = st.session_state.get('parent_asin_label_display_mode', 'asin_and_group')
                    current_display = "ASIN only" if current_mode == 'asin_only' else "ASIN + Product Group"
                    label_display_mode_donut = st.radio(
                        "Label display:",
                        options=["ASIN only", "ASIN + Product Group"],
                        index=0 if current_display == "ASIN only" else 1,
                        key="parent_asin_label_display_donut",
                        horizontal=True,
                        help="Choose whether to show just the ASIN or include the Product Group when available"
                    )
                    # Convert to internal format and update session state
                    label_mode_donut = 'asin_only' if label_display_mode_donut == "ASIN only" else 'asin_and_group'
                    st.session_state.parent_asin_label_display_mode = label_mode_donut
            
                # Use the values from the session state
                sort_by = st.session_state.parent_asin_viz_sort_by
                display_count = st.session_state.parent_asin_viz_count
                title_length = st.session_state.parent_asin_title_length
            
                # Create a copy of the data for this tab
                chart_df = base_parent_chart_df.copy()
            
                # Sort based on user selection
                if sort_by in chart_df.columns:
                    # Always sort descending for Spend, Total Sales, Ad Sales
                    ascending = False
                    chart_df = chart_df.sort_values(by=sort_by, ascending=ascending).head(display_count)
                else:
                    # Default sort by Total Sales
                    chart_df = chart_df.sort_values(by='Total Sales', ascending=False).head(display_count)
            
                # Apply formatting with user-selected length and display mode
                chart_df['label'] = chart_df.apply(
                    lambda row: format_parent_product_title(
                        row['ASIN'], 
                        row['Product Group'], 
                        title_length,
                        label_mode_donut
                    ),
                    axis=1
                )
                # Create donut charts for each metric
                col1, col2, col3 = st.columns(3)
            
                # Calculate totals for percentage calculations
                total_spend_all = parent_df['Spend'].sum()
                total_ad_sales_all = parent_df['Ad Sales'].sum()
                total_sales_all = parent_df['Total Sales'].sum()
            
                # Ad Spend Distribution
                with col1:
                    fig_spend = px.pie(
                        chart_df,
                        values='Spend',
                        names='label',
                        title='Ad Spend Distribution',
                        color_discrete_sequence=px.colors.qualitative.Set3,
                        template='plotly_dark'
                    )
                    
                    # Update to donut chart
                    fig_spend.update_traces(
                        hole=donut_size,
                        textposition=text_position,
                        textinfo='label+percent',
                        textfont=dict(size=10, family="Arial, sans-serif"),
                        hovertemplate='<b>%{label}</b><br>' +
                                     'Spend: $%{value:,.2f}<br>' +
                                     'Percentage: %{percent}<br>' +
                                     '<extra></extra>',
                        marker=dict(line=dict(color='#000000', width=1))
                    )
                    
                    # Add center text showing total
                    fig_spend.add_annotation(
                        text=f'${total_spend_all:,.0f}',
                        x=0.5, y=0.5,
                        font_size=16,
                        font_color='white',
                        showarrow=False
                    )
                    
                    fig_spend.update_layout(
                        showlegend=False,
                        margin=dict(t=50, b=0, l=0, r=0),
                        height=400,
                        title=dict(
                            text='Ad Spend Distribution',
                            font=dict(size=14, family="Arial, sans-serif"),
                            x=0.5
                        )
                    )
                    
                    st.plotly_chart(fig_spend, use_container_width=True)
            
                # Ad Sales Distribution
                with col2:
                    fig_ad_sales = px.pie(
                        chart_df,
                        values='Ad Sales',
                        names='label',
                        title='Ad Sales Distribution',
                        color_discrete_sequence=px.colors.qualitative.Pastel,
                        template='plotly_dark'
                    )
                    
                    # Update to donut chart
                    fig_ad_sales.update_traces(
                        hole=donut_size,
                        textposition=text_position,
                        textinfo='label+percent',
                        textfont=dict(size=10, family="Arial, sans-serif"),
                        hovertemplate='<b>%{label}</b><br>' +
                                     'Ad Sales: $%{value:,.2f}<br>' +
                                     'Percentage: %{percent}<br>' +
                                     '<extra></extra>',
                        marker=dict(line=dict(color='#000000', width=1))
                    )
                    
                    # Add center text showing total
                    fig_ad_sales.add_annotation(
                        text=f'${total_ad_sales_all:,.0f}',
                        x=0.5, y=0.5,
                        font_size=16,
                        font_color='white',
                        showarrow=False
                    )
                    
                    fig_ad_sales.update_layout(
                        showlegend=False,
                        margin=dict(t=50, b=0, l=0, r=0),
                        height=400,
                        title=dict(
                            text='Ad Sales Distribution',
                            font=dict(size=14, family="Arial, sans-serif"),
                            x=0.5
                        )
                    )
                    
                    st.plotly_chart(fig_ad_sales, use_container_width=True)
            
                # Total Sales Distribution
                with col3:
                    fig_total_sales = px.pie(
                        chart_df,
                        values='Total Sales',
                        names='label',
                        title='Total Sales Distribution',
                        color_discrete_sequence=px.colors.qualitative.Dark2,
                        template='plotly_dark'
                    )
                    
                    # Update to donut chart
                    fig_total_sales.update_traces(
                        hole=donut_size,
                        textposition=text_position,
                        textinfo='label+percent',
                        textfont=dict(size=10, family="Arial, sans-serif"),
                        hovertemplate='<b>%{label}</b><br>' +
                                     'Total Sales: $%{value:,.2f}<br>' +
                                     'Percentage: %{percent}<br>' +
                                     '<extra></extra>',
                        marker=dict(line=dict(color='#000000', width=1))
                    )
                    
                    # Add center text showing total
                    fig_total_sales.add_annotation(
                        text=f'${total_sales_all:,.0f}',
                        x=0.5, y=0.5,
                        font_size=16,
                        font_color='white',
                        showarrow=False
                    )
                    
                    fig_total_sales.update_layout(
                        showlegend=False,
                        margin=dict(t=50, b=0, l=0, r=0),
                        height=400,
                        title=dict(
                            text='Total Sales Distribution',
                            font=dict(size=14, family="Arial, sans-serif"),
                            x=0.5
                        )
                    )
                    
                    st.plotly_chart(fig_total_sales, use_container_width=True)
            
                # Add some spacing after the charts
                st.markdown("<div style='margin-top:1rem;'></div>", unsafe_allow_html=True)

        else:
            st.info("No Parent ASIN data in Sales Report")
        # --- Performance by Product Group ---
    # Only show on the Advertising Audit page
    if st.session_state.current_page == "advertising_audit":
        # Only show if there are product groups defined in the client settings
        show_product_group = False

        # Check if there are any product groups defined in client settings
        if st.session_state.client_config and 'branded_asins_data' in st.session_state.client_config:
            # Check if any ASINs have a non-empty product_group value in the client settings
            has_product_groups = any(info.get('product_group', '').strip() != '' 
                                   for info in st.session_state.client_config['branded_asins_data'].values())
    
            # Only proceed if product groups are defined AND we have data to display
            if has_product_groups and 'asin_perf_df' in st.session_state and st.session_state.asin_perf_df is not None and 'Product Group' in st.session_state.asin_perf_df.columns:
                # Verify we have actual product group values in the data (not just empty strings)
                show_product_group = st.session_state.asin_perf_df['Product Group'].astype(str).str.strip().replace('', pd.NA).dropna().any()
        
                # Add debug message about product groups
                if 'debug_messages' in st.session_state:
                    st.session_state.debug_messages.append(f"[Product Groups] Found defined product groups in client settings: {has_product_groups}")
                    st.session_state.debug_messages.append(f"[Product Groups] Found non-empty product groups in data: {show_product_group}")

        if show_product_group:
            st.markdown("<hr style='height:2px;border-width:0;color:gold;background-color:gold;margin-top:25px;margin-bottom:15px;margin-left:10px;margin-right:10px'>", unsafe_allow_html=True)
            st.markdown("<span class='main-section-header dashboard-section'>Performance by Product Group</span>", unsafe_allow_html=True)
            st.markdown("<div style='margin-bottom:1.2rem;'></div>", unsafe_allow_html=True)

            # Debug information about available columns
            # Aggregate required columns
            agg_dict = {}
            if 'ASIN' in st.session_state.asin_perf_df.columns:
                agg_dict['ASIN'] = 'count'
            for col in ['Spend', 'Ad Sales', 'Total Sales', 'Clicks', 'Orders']:
                if col in st.session_state.asin_perf_df.columns:
                    agg_dict[col] = 'sum'

            if agg_dict:
                # First, create a copy of the dataframe with consistent handling of untagged items
                temp_df = st.session_state.asin_perf_df.copy()
        
                # Replace empty or NaN product group values with 'Untagged Group'
                # Also replace 'Unassigned' with 'Untagged Group' for consistency
                temp_df['Product Group'] = temp_df['Product Group'].fillna('').astype(str)
                temp_df.loc[temp_df['Product Group'].str.strip() == '', 'Product Group'] = 'Untagged Group'
                temp_df.loc[temp_df['Product Group'] == 'Unassigned', 'Product Group'] = 'Untagged Group'
        
                # Now group by the updated Product Group column
                group_perf_df = temp_df.groupby('Product Group').agg(agg_dict)
                if 'ASIN' in group_perf_df.columns:
                    group_perf_df = group_perf_df.rename(columns={'ASIN': 'ASIN Count'})
                group_perf_df = group_perf_df.reset_index()

                # Product Group Filter Section - Collect ALL available product groups
                # Start with ASIN-level product groups
                available_product_groups = set(group_perf_df['Product Group'].unique())
                
                # Add campaign-level product groups from campaign tagging
                if st.session_state.client_config and 'campaign_tags_data' in st.session_state.client_config:
                    for campaign_info in st.session_state.client_config['campaign_tags_data'].values():
                        campaign_product_group = campaign_info.get('tag_1', '').strip()
                        if campaign_product_group and campaign_product_group != 'Untagged Group':
                            available_product_groups.add(campaign_product_group)
                
                # Convert to sorted list for consistent display
                available_product_groups = sorted(list(available_product_groups))
                
                # Count campaign vs ASIN product groups for user info
                asin_product_groups = set(group_perf_df['Product Group'].unique())
                campaign_product_groups = set(available_product_groups) - asin_product_groups if st.session_state.client_config and 'campaign_tags_data' in st.session_state.client_config else set()
        
                if 'selected_product_groups' not in st.session_state:
                    st.session_state.selected_product_groups = []
                # Initialize and sanitize the widget's own session state to avoid first-change reversion
                if 'product_group_multiselect' not in st.session_state:
                    # Seed from any previously stored selection if present
                    st.session_state.product_group_multiselect = list(st.session_state.selected_product_groups)
                else:
                    # Sanitize current selection against available options in case the option set changed
                    st.session_state.product_group_multiselect = [
                        g for g in st.session_state.product_group_multiselect if g in available_product_groups
                    ]
        
                # Info about available product groups (hidden per user request)
                # if campaign_product_groups:
                #     st.info(f"ðŸ“Š **Available Product Groups:** {len(asin_product_groups)} from ASIN data, {len(campaign_product_groups)} from Campaign Tagging")
        
                # Create two-column layout for filters
                filter_col1, filter_col2 = st.columns([1, 2])
        
                with filter_col1:
                    # Add checkbox for combining SB Campaign data with ASIN data
                    combine_sb_data = st.checkbox(
                        "Combine SB Campaign-Level Product Groups with ASIN Product Groups?",
                        value=True,
                        help="When enabled, aggregates Sponsored Brands campaign data with ASIN data for matching Product Groups. Enable this to see Campaign-tagged product groups (like 'Girdles SB') combined with ASIN product groups (like 'Stage 1 Girdles')."
                    )
        
                with filter_col2:
                    # Product group filter with both ASIN and Campaign product groups
                    selected_groups = st.multiselect(
                        "Filter by Product Group(s):",
                        options=available_product_groups,
                        key="product_group_multiselect",
                        placeholder="Choose an option",
                        help="Includes both ASIN-level product groups and Campaign-level product groups from Campaign Tagging"
                    )
                    st.session_state.selected_product_groups = selected_groups
        
                # --- Combine SB Campaign Data with ASIN Data if enabled ---
                if combine_sb_data:
                    # Get SB campaign performance data
                    try:
                        sb_campaign_df = get_campaign_performance_data(st.session_state.bulk_data, st.session_state.client_config)
                
                        if not sb_campaign_df.empty:
                            # Filter for Sponsored Brands campaigns only
                            sb_data = sb_campaign_df[sb_campaign_df['Ad Type'] == 'SB'].copy()
                    
                            if not sb_data.empty:
                                # Group SB data by Product Group and aggregate
                                sb_agg_dict = {
                                    'Spend': 'sum',
                                    'Ad Sales': 'sum',
                                    'Clicks': 'sum',
                                    'Orders': 'sum'
                                }
                        
                                sb_grouped = sb_data.groupby('Product Group').agg(sb_agg_dict).reset_index()
                        
                                # Combine with existing ASIN data
                                combined_data = []
                        
                                # Process each Product Group
                                all_product_groups = set(group_perf_df['Product Group'].unique()) | set(sb_grouped['Product Group'].unique())
                        
                                for pg in all_product_groups:
                                    # Get ASIN data for this product group
                                    asin_data = group_perf_df[group_perf_df['Product Group'] == pg]
                                    sb_data_pg = sb_grouped[sb_grouped['Product Group'] == pg]
                            
                                    # Combine the data
                                    combined_row = {'Product Group': pg}
                            
                                    # Add metrics from ASIN data
                                    if not asin_data.empty:
                                        asin_row = asin_data.iloc[0]
                                        combined_row['Spend'] = asin_row.get('Spend', 0)
                                        combined_row['Ad Sales'] = asin_row.get('Ad Sales', 0)
                                        combined_row['Total Sales'] = asin_row.get('Total Sales', 0)
                                        combined_row['Clicks'] = asin_row.get('Clicks', 0)
                                        combined_row['Orders'] = asin_row.get('Orders', 0)
                                        combined_row['ASIN Count'] = asin_row.get('ASIN Count', 0)
                                    else:
                                        # Initialize with zeros
                                        combined_row.update({
                                            'Spend': 0, 'Ad Sales': 0, 'Total Sales': 0,
                                            'Clicks': 0, 'Orders': 0, 'ASIN Count': 0
                                        })
                            
                                    # Add SB campaign data
                                    if not sb_data_pg.empty:
                                        sb_row = sb_data_pg.iloc[0]
                                        combined_row['Spend'] += sb_row.get('Spend', 0)
                                        combined_row['Ad Sales'] += sb_row.get('Ad Sales', 0)
                                        # Note: SB campaigns don't have Total Sales in campaign reports
                                        combined_row['Clicks'] += sb_row.get('Clicks', 0)
                                        combined_row['Orders'] += sb_row.get('Orders', 0)
                            
                                    combined_data.append(combined_row)
                        
                                # Create new combined DataFrame
                                group_perf_df = pd.DataFrame(combined_data)
                        
                                # Note: available_product_groups was already set above to include campaign product groups
                        
                        
                    except Exception as e:
                        st.warning(f"Could not combine SB campaign data: {str(e)}")
        
                # Attach Sessions (aka Glance Views/Page Views) from Business Report if available
                try:
                    sales_report_df = st.session_state.get('sales_report_data')
                    if isinstance(sales_report_df, pd.DataFrame) and \
                       ('ASIN' in sales_report_df.columns) and ('Sessions' in sales_report_df.columns):
                        # Build ASIN -> Product Group map from temp_df (already normalized PG values)
                        if ('ASIN' in temp_df.columns) and ('Product Group' in temp_df.columns):
                            asin_pg_map = temp_df[['ASIN', 'Product Group']].dropna().copy()
                            asin_pg_map['ASIN'] = asin_pg_map['ASIN'].astype(str).str.upper().str.strip()

                            sessions_tmp = sales_report_df[['ASIN', 'Sessions']].copy()
                            sessions_tmp['ASIN'] = sessions_tmp['ASIN'].astype(str).str.upper().str.strip()

                            merged_sessions = pd.merge(sessions_tmp, asin_pg_map, on='ASIN', how='inner')
                            sessions_by_pg = merged_sessions.groupby('Product Group', as_index=False)['Sessions'].sum()

                            # Merge onto group_perf_df; fill missing with 0
                            group_perf_df = group_perf_df.merge(sessions_by_pg, on='Product Group', how='left')
                            group_perf_df['Sessions'] = group_perf_df['Sessions'].fillna(0)
                except Exception as e:
                    if 'debug_messages' in st.session_state:
                        st.session_state.debug_messages.append(f"[Product Groups] Error attaching Sessions from Business Report: {e}")

                # Calculate totals for percent columns (from the entire unfiltered dataset)
                total_spend_all = group_perf_df['Spend'].sum() if 'Spend' in group_perf_df.columns else 0
                total_ad_sales_all = group_perf_df['Ad Sales'].sum() if 'Ad Sales' in group_perf_df.columns else 0
                total_total_sales_all = group_perf_df['Total Sales'].sum() if 'Total Sales' in group_perf_df.columns else 0
        
                # Filter data based on selection
                if selected_groups:
                    filtered_group_perf_df = group_perf_df[group_perf_df['Product Group'].isin(selected_groups)].copy()
                    filtered_temp_df = temp_df[temp_df['Product Group'].isin(selected_groups)].copy()
                    st.caption(f"Filtered by Product Group(s): {', '.join(selected_groups)}")
                else:
                    # No filter applied - show all data
                    filtered_group_perf_df = group_perf_df.copy()
                    filtered_temp_df = temp_df.copy()
        
                # Calculate metrics for the filtered data
                if len(filtered_group_perf_df) > 0:
                    # Calculate totals for filtered data
                    filtered_spend = filtered_group_perf_df['Spend'].sum() if 'Spend' in filtered_group_perf_df.columns else 0
                    filtered_ad_sales = filtered_group_perf_df['Ad Sales'].sum() if 'Ad Sales' in filtered_group_perf_df.columns else 0
                    filtered_total_sales = filtered_group_perf_df['Total Sales'].sum() if 'Total Sales' in filtered_group_perf_df.columns else 0
                    filtered_clicks = filtered_group_perf_df['Clicks'].sum() if 'Clicks' in filtered_group_perf_df.columns else 0
                    filtered_orders = filtered_group_perf_df['Orders'].sum() if 'Orders' in filtered_group_perf_df.columns else 0
                    filtered_sessions = filtered_group_perf_df['Sessions'].sum() if 'Sessions' in filtered_group_perf_df.columns else 0
            
                    # Calculate calculated columns for filtered data
                    filtered_group_perf_df['% of Spend'] = filtered_group_perf_df['Spend'] / total_spend_all if total_spend_all else 0
                    filtered_group_perf_df['% of Ad Sales'] = filtered_group_perf_df['Ad Sales'] / total_ad_sales_all if total_ad_sales_all else 0
                    filtered_group_perf_df['% of Total Sales'] = filtered_group_perf_df['Total Sales'] / total_total_sales_all if total_total_sales_all else 0
                    filtered_group_perf_df['ACoS'] = filtered_group_perf_df['Spend'] / filtered_group_perf_df['Ad Sales'] if 'Ad Sales' in filtered_group_perf_df.columns else np.nan
                    filtered_group_perf_df['ROAS'] = filtered_group_perf_df['Ad Sales'] / filtered_group_perf_df['Spend'] if 'Spend' in filtered_group_perf_df.columns else np.nan
                    filtered_group_perf_df['TACoS'] = filtered_group_perf_df['Spend'] / filtered_group_perf_df['Total Sales'] if 'Total Sales' in filtered_group_perf_df.columns else np.nan
                    filtered_group_perf_df['Ad Sales % of Total'] = filtered_group_perf_df['Ad Sales'] / filtered_group_perf_df['Total Sales'] if 'Total Sales' in filtered_group_perf_df.columns else np.nan
                    filtered_group_perf_df['CPC'] = filtered_group_perf_df['Spend'] / filtered_group_perf_df['Clicks'] if 'Clicks' in filtered_group_perf_df.columns else np.nan
                    filtered_group_perf_df['CVR'] = filtered_group_perf_df['Orders'] / filtered_group_perf_df['Clicks'] if ('Orders' in filtered_group_perf_df.columns and 'Clicks' in filtered_group_perf_df.columns) else np.nan
                    filtered_group_perf_df['AOV'] = filtered_group_perf_df['Total Sales'] / filtered_group_perf_df['Orders'] if 'Orders' in filtered_group_perf_df.columns else np.nan
                    # Ad Traffic % of Total = Clicks / Sessions (if Sessions available)
                    if ('Clicks' in filtered_group_perf_df.columns) and ('Sessions' in filtered_group_perf_df.columns):
                        with np.errstate(divide='ignore', invalid='ignore'):
                            filtered_group_perf_df['Ad Traffic % of Total'] = np.where(
                                filtered_group_perf_df['Sessions'] > 0,
                                filtered_group_perf_df['Clicks'] / filtered_group_perf_df['Sessions'],
                                np.nan
                            )
                    else:
                        filtered_group_perf_df['Ad Traffic % of Total'] = np.nan

                    # Calculate metrics for totals row
                    total_spend_pct = filtered_spend / total_spend_all if total_spend_all > 0 else 0
                    total_ad_sales_pct = filtered_ad_sales / total_ad_sales_all if total_ad_sales_all > 0 else 0
                    total_total_sales_pct = filtered_total_sales / total_total_sales_all if total_total_sales_all > 0 else 0
                    total_acos = filtered_spend / filtered_ad_sales if filtered_ad_sales > 0 else 0
                    total_roas = filtered_ad_sales / filtered_spend if filtered_spend > 0 else 0
                    total_tacos = filtered_spend / filtered_total_sales if filtered_total_sales > 0 else 0
                    total_cpc = filtered_spend / filtered_clicks if filtered_clicks > 0 else 0
                    total_cvr = filtered_orders / filtered_clicks if filtered_clicks > 0 else 0
                    total_aov = filtered_total_sales / filtered_orders if filtered_orders > 0 else 0

                    # Create totals row
                    totals_data = {
                        'Product Group': 'Total',
                        'Spend': filtered_spend,
                        'Ad Sales': filtered_ad_sales,
                        'Total Sales': filtered_total_sales,
                        'Sessions': filtered_sessions,
                        'Clicks': filtered_clicks,
                        '% of Spend': total_spend_pct,
                        '% of Ad Sales': total_ad_sales_pct,
                        '% of Total Sales': total_total_sales_pct,
                        'ACoS': total_acos,
                        'ROAS': total_roas,
                        'TACoS': total_tacos,
                        'CPC': total_cpc,
                        'CVR': total_cvr,
                        'AOV': total_aov,
                        'Ad Sales % of Total': filtered_ad_sales / filtered_total_sales if filtered_total_sales > 0 else 0,
                        'Ad Traffic % of Total': (filtered_clicks / filtered_sessions) if filtered_sessions > 0 else 0
                    }
            
                    # Add ASIN Count if available
                    if 'ASIN Count' in filtered_group_perf_df.columns:
                        totals_data['ASIN Count'] = filtered_group_perf_df['ASIN Count'].sum()

                    # Create totals row as DataFrame (separate from main data)
                    totals_df = pd.DataFrame([totals_data])
            
                    # Use filtered data for the main table (no totals row)
                    display_df = filtered_group_perf_df.copy()
                else:
                    # No data to display
                    display_df = filtered_group_perf_df.copy()
                    totals_df = pd.DataFrame()

                # Reorder columns as requested
                columns_order = [
                    'Product Group', 'Spend', 'Ad Sales', 'Total Sales',
                    '% of Spend', '% of Ad Sales', '% of Total Sales',
                    'ACoS', 'ROAS', 'TACoS', 'Ad Sales % of Total', 'Ad Traffic % of Total',
                    'Sessions', 'Clicks', 'CPC', 'CVR', 'AOV', 'ASIN Count'
                ]
                # Only include columns that exist
                columns_order = [col for col in columns_order if col in display_df.columns]
                display_df = display_df[columns_order]
        
                if not totals_df.empty:
                    totals_columns_order = [col for col in columns_order if col in totals_df.columns]
                    totals_df = totals_df[totals_columns_order]

                # Sort by 'Ad Sales' in descending order by default (if column exists)
                if len(display_df) > 0 and 'Ad Sales' in display_df.columns:
                    display_df = display_df.sort_values(by='Ad Sales', ascending=False)

                # Replace all None/NaN values with 0 for display
                display_df = display_df.fillna(0)
                if not totals_df.empty:
                    totals_df = totals_df.fillna(0)

                # Formatting
                def currency_fmt(x):
                    return f"${x:,.2f}" if pd.notnull(x) and x != 0 else "$0.00"
                def percent_fmt(x):
                    return f"{x*100:.1f}%" if pd.notnull(x) and x != 0 else "0.0%"
                def ratio_fmt(x):
                    return f"{x:.2f}" if pd.notnull(x) and x != 0 else "0.00"
                def count_fmt(x):
                    return f"{int(x):,}" if pd.notnull(x) and x != 0 else "0"

                fmt_dict = {
                    'Spend': currency_fmt,
                    'Ad Sales': currency_fmt,
                    'Total Sales': currency_fmt,
                    '% of Spend': percent_fmt,
                    '% of Ad Sales': percent_fmt,
                    '% of Total Sales': percent_fmt,
                    'ACoS': percent_fmt,
                    'ROAS': ratio_fmt,
                    'TACoS': percent_fmt,
                    'Ad Sales % of Total': percent_fmt,
                    'Ad Traffic % of Total': percent_fmt,
                    'CPC': currency_fmt,
                    'CVR': percent_fmt,
                    'AOV': currency_fmt,
                    'ASIN Count': count_fmt,
                    'Sessions': count_fmt,
                    'Clicks': count_fmt,
                }

                # Display the totals row separately (detached from the main table)
                if not totals_df.empty:

                    # Use simple styling to match ASIN section
                    st.dataframe(totals_df.style.format(fmt_dict), use_container_width=True, hide_index=True)
                    st.markdown("<div style='margin-bottom:1rem;'></div>", unsafe_allow_html=True)

                # Display the main data table
                if len(display_df) > 0:

                    styled_df = display_df.style.format(fmt_dict)
            
                    # Apply gradients only if columns exist and have more than one unique value
                    if '% of Spend' in display_df.columns:
                        styled_df = styled_df.apply(lambda x: [color_gradient_blue(v*100, 0, 100) if not pd.isna(v) else '' 
                                                            for v in x] if x.name == '% of Spend' else [''] * len(x), axis=0)
            
                    if '% of Ad Sales' in display_df.columns:
                        styled_df = styled_df.apply(lambda x: [color_gradient_green(v*100, 0, 100) if not pd.isna(v) else '' 
                                                            for v in x] if x.name == '% of Ad Sales' else [''] * len(x), axis=0)
            
                    if '% of Total Sales' in display_df.columns:
                        styled_df = styled_df.apply(lambda x: [color_gradient_green(v*100, 0, 100) if not pd.isna(v) else '' 
                                                            for v in x] if x.name == '% of Total Sales' else [''] * len(x), axis=0)
            
                    st.dataframe(styled_df, use_container_width=True, hide_index=True)
                else:

                    st.info("No data to display. Please select one or more Product Groups to analyze.")
        
                # --- HORIZONTAL STACKED BAR CHARTS FOR EACH PRODUCT GROUP ---
                # Update chart data to use filtered data
                if len(filtered_group_perf_df) > 0 and all(col in filtered_group_perf_df.columns for col in ['Spend', 'Ad Sales', 'Total Sales']):
            
                    # --- Chart Display Options ---
                    # Count unique product groups for chart filtering
                    unique_groups = filtered_group_perf_df['Product Group'].unique()
                    total_groups = len(unique_groups)
            
                    # Initialize session state for product group display limit
                    if 'product_group_chart_display_limit' not in st.session_state:
                        # Default to 10, including Untagged Group if it exists
                        st.session_state.product_group_chart_display_limit = min(10, total_groups)
            
                    # Show Chart Display Options only if there are more than 10 unique product groups
                    if total_groups > 10:
                        with st.expander("Chart Display Options", expanded=False):
                            # Create options for the dropdown
                            options = list(range(5, total_groups + 1, 5))  # 5, 10, 15, 20, etc.
                            if 10 not in options:
                                options.append(10)
                            options.append(total_groups)  # Add "All" option
                            options = sorted(set(options))  # Remove duplicates and sort
                    
                            # Create labels for the dropdown
                            option_labels = []
                            for opt in options[:-1]:  # All except the last one
                                option_labels.append(f"Top {opt} Groups")
                            option_labels.append("All Groups")  # Label for the last option
                    
                            # Create the selectbox
                            selected_index = st.selectbox(
                                "Number of Product Groups to Display in Charts:",
                                range(len(options)),
                                format_func=lambda x: option_labels[x],
                                index=options.index(st.session_state.product_group_chart_display_limit) if st.session_state.product_group_chart_display_limit in options else 1,  # Default to "Top 10 Groups"
                                key="product_group_chart_display_selectbox"
                            )
                    
                            # Update session state with selected value
                            st.session_state.product_group_chart_display_limit = options[selected_index]
            
                    # Apply the filtering based on the selected limit for charts only
                    chart_display_df = filtered_group_perf_df.copy()
                    if st.session_state.product_group_chart_display_limit < total_groups:
                        # For charts, include Untagged Group as part of the limit
                        # Sort by Ad Sales and take top N groups (including Untagged if it's in top N)
                        chart_display_df = chart_display_df.sort_values(by='Ad Sales', ascending=False)
                        chart_display_df = chart_display_df.head(st.session_state.product_group_chart_display_limit)
            
                    # Create a copy of the dataframe with only the columns we need for the charts
                    chart_df = chart_display_df[['Product Group', 'Spend', 'Ad Sales', 'Total Sales']].copy()
            
                    # Calculate the total for each product group to use for percentages
                    chart_df['Total'] = chart_df['Spend'] + chart_df['Ad Sales'] + chart_df['Total Sales']
            
                    # Sort by Total Sales (descending)
                    chart_df = chart_df.sort_values('Total Sales', ascending=False)
            
                    # Create tabs for different view types
                    tab1, tab2, tab3 = st.tabs(["Stacked Bar View", "Percentage View", "Pie Charts"])
            

                    # Function to create the chart with either percentage or dollar values
                    def create_product_group_chart(show_percentages=True, combined_view=False):
                        # Create a figure
                        fig = go.Figure()
                
                        # For each product group, add a trace to the figure
                        y_positions = []
                        annotations = []
            
                        for idx, row in chart_df.iterrows():
                            product_group = row['Product Group']
                            # Skip if product group is empty or null
                            if pd.isna(product_group) or str(product_group).strip() == '':
                                continue
                    
                            y_pos = len(y_positions)
                            y_positions.append(y_pos)
                    
                            # Calculate percentages for each product group's internal distribution
                            spend_pct = row['Spend'] / row['Total'] * 100 if row['Total'] > 0 else 0
                            ad_sales_pct = row['Ad Sales'] / row['Total'] * 100 if row['Total'] > 0 else 0
                            total_sales_pct = row['Total Sales'] / row['Total'] * 100 if row['Total'] > 0 else 0
                
                            if show_percentages:
                                # For percentage view, normalize to 100%
                                x_spend = spend_pct
                                x_ad_sales = ad_sales_pct
                                x_total_sales = total_sales_pct
                        
                                # Format text for percentage view - only show if value is significant
                                spend_text = f"{spend_pct:.0f}%" if spend_pct >= 3 else ""
                                ad_sales_text = f"{ad_sales_pct:.0f}%" if ad_sales_pct >= 3 else ""
                                total_sales_text = f"{total_sales_pct:.0f}%" if total_sales_pct >= 3 else ""
                        
                                # Format hover text with more detailed information
                                spend_hover = f"<b>{product_group}</b><br>Spend: ${row['Spend']:,.2f} ({spend_pct:.1f}%)"
                                ad_sales_hover = f"<b>{product_group}</b><br>Ad Sales: ${row['Ad Sales']:,.2f} ({ad_sales_pct:.1f}%)"
                                total_sales_hover = f"<b>{product_group}</b><br>Total Sales: ${row['Total Sales']:,.2f} ({total_sales_pct:.1f}%)"
                        
                                # Set tick format for axis
                                tick_format = ',.0f'
                                tick_suffix = '%'
                                tick_prefix = ''
                            else:
                                # For dollar view, use actual values
                                x_spend = row['Spend']
                                x_ad_sales = row['Ad Sales']
                                x_total_sales = row['Total Sales']
                        
                                # Format text for dollar view - only show if value is significant
                                min_value_for_label = max(row['Total Sales'] * 0.03, 1000)  # At least 3% of total or $1000
                                spend_text = f"${row['Spend']:,.0f}" if row['Spend'] >= min_value_for_label else ""
                                ad_sales_text = f"${row['Ad Sales']:,.0f}" if row['Ad Sales'] >= min_value_for_label else ""
                                total_sales_text = f"${row['Total Sales']:,.0f}" if row['Total Sales'] >= min_value_for_label else ""
                        
                                # Format hover text with more detailed information
                                spend_hover = f"<b>{product_group}</b><br>Spend: ${row['Spend']:,.2f} ({spend_pct:.1f}%)"
                                ad_sales_hover = f"<b>{product_group}</b><br>Ad Sales: ${row['Ad Sales']:,.2f} ({ad_sales_pct:.1f}%)"
                                total_sales_hover = f"<b>{product_group}</b><br>Total Sales: ${row['Total Sales']:,.2f} ({total_sales_pct:.1f}%)"
                        
                                # Set tick format for axis
                                tick_format = ',.0f'
                                tick_suffix = ''
                                tick_prefix = '$'
                
                            # Add the stacked bar segments with improved styling
                            # Spend bar (red)
                            fig.add_trace(go.Bar(
                                x=[x_spend],
                                y=[y_pos],
                                orientation='h',
                                name='Spend',
                                marker_color='rgba(239, 68, 68, 0.9)',  # Bright red with slight transparency
                                marker_line=dict(width=1, color='rgba(255,255,255,0.3)'),
                                text=[spend_text],
                                textposition='inside',
                                insidetextanchor='middle',
                                textfont=dict(color='#fff', size=13, family='Inter, Arial, sans-serif', weight='bold'),
                                showlegend=idx==0,  # Only show in legend for first row
                                hoverinfo='text',
                                hovertext=spend_hover,
                                width=0.75,
                                opacity=0.95
                            ))
                    
                            # Ad Sales bar (blue)
                            fig.add_trace(go.Bar(
                                x=[x_ad_sales],
                                y=[y_pos],
                                orientation='h',
                                name='Ad Sales',
                                marker_color='rgba(59, 130, 246, 0.9)',  # Bright blue with slight transparency
                                marker_line=dict(width=1, color='rgba(255,255,255,0.3)'),
                                text=[ad_sales_text],
                                textposition='inside',
                                insidetextanchor='middle',
                                textfont=dict(color='#fff', size=13, family='Inter, Arial, sans-serif', weight='bold'),
                                showlegend=idx==0,  # Only show in legend for first row
                                hoverinfo='text',
                                hovertext=ad_sales_hover,
                                width=0.75,
                                opacity=0.95
                            ))
                    
                            # Total Sales bar (green)
                            fig.add_trace(go.Bar(
                                x=[x_total_sales],
                                y=[y_pos],
                                orientation='h',
                                name='Total Sales',
                                marker_color='rgba(34, 197, 94, 0.9)',  # Bright green with slight transparency
                                marker_line=dict(width=1, color='rgba(255,255,255,0.3)'),
                                text=[total_sales_text],
                                textposition='inside',
                                insidetextanchor='middle',
                                textfont=dict(color='#fff', size=13, family='Inter, Arial, sans-serif', weight='bold'),
                                showlegend=idx==0,  # Only show in legend for first row
                                hoverinfo='text',
                                hovertext=total_sales_hover,
                                width=0.75,
                                opacity=0.95
                            ))
                    
                        # Create a list of product group names for y-axis labels
                        product_group_labels = [chart_df.iloc[y_positions.index(pos)]['Product Group'] for pos in y_positions]
                
                        # Calculate dynamic height based on number of product groups
                        chart_height = max(350, 55 * (len(y_positions) + 1))  # Increased height per row for better spacing
                
                        # Determine chart title based on view type
                        if combined_view:
                            chart_title = '<b>Product Group Allocation (Combined View)</b>'
                        else:
                            chart_title = '<b>Product Group Allocation (% of Total)</b>' if show_percentages else '<b>Product Group Allocation ($)</b>'
                
                        # Update the layout with enhanced dark mode aesthetic
                        fig.update_layout(
                            barmode='stack',
                            height=chart_height,
                            margin=dict(l=200, r=40, t=70, b=50),  # Adjusted margins for better layout
                            yaxis=dict(
                                showticklabels=True,
                                ticktext=product_group_labels,
                                tickvals=y_positions,
                                tickfont=dict(size=13, color='#ffffff', family='Inter, Arial, sans-serif'),
                                showgrid=False,
                                zeroline=False,
                                domain=[0, 0.95],
                                title='',
                                ticklabelposition='outside'
                            ),
                            xaxis=dict(
                                title='',
                                showgrid=True,
                                gridcolor='rgba(255,255,255,0.15)',  # Slightly more visible grid lines
                                zeroline=False,
                                tickprefix=tick_prefix,
                                ticksuffix=tick_suffix,
                                tickformat=tick_format,
                                color='#ffffff',
                                tickfont=dict(size=12),
                                range=[0, 105] if show_percentages else None,  # Slightly wider range for better spacing
                                showline=True,
                                linecolor='rgba(255,255,255,0.3)',
                                mirror=True  # Add top axis line for better framing
                            ),
                            legend=dict(
                                orientation='h',
                                yanchor='bottom',
                                y=1.02,
                                xanchor='right',
                                x=1.0,  # Right-aligned legend
                                bgcolor='rgba(30,30,30,0.7)',  # Darker, more visible background
                                bordercolor='rgba(255,255,255,0.3)',
                                borderwidth=1,
                                font=dict(color='#ffffff', size=12, family='Inter, Arial, sans-serif'),
                                traceorder='reversed'  # Reverse order to match visual stacking (Spend, Ad Sales, Total Sales)
                            ),
                            legend_traceorder='reversed',
                            annotations=annotations,
                            plot_bgcolor='rgba(17,17,17,0.3)',  # Very subtle dark background
                            paper_bgcolor='rgba(0,0,0,0)',
                            bargap=0.4,  # Larger gap for better separation between product groups
                            font=dict(color='#ffffff', family='Inter, Arial, sans-serif'),
                            title=dict(
                                text=chart_title,
                                font=dict(size=18, color='#ffffff', family='Inter, Arial, sans-serif'),
                                x=0.0,
                                xanchor='left',
                                y=0.98,  # Position title slightly higher
                                yanchor='top'
                            ),
                            hoverlabel=dict(
                                bgcolor='rgba(50,50,50,0.9)',  # Dark hover label background
                                bordercolor='rgba(255,255,255,0.3)',
                                font=dict(family='Inter, Arial, sans-serif', size=13, color='white')
                            ),
                            hovermode='closest',
                            uniformtext=dict(minsize=10, mode='hide')  # Hide text that doesn't fit
                        )
                
                        return fig
            
                    # Function to create the combined view chart showing both dollar values and percentages
                    def create_combined_view_chart():
                        # Create a figure
                        fig = go.Figure()
                
                        # For each product group, add a trace to the figure
                        y_positions = []
                        annotations = []
                
                        # Get total spend across all ASINs for percentage calculations
                        total_spend = chart_df['Spend'].sum()
                        total_ad_sales = chart_df['Ad Sales'].sum()
                        total_sales = chart_df['Total Sales'].sum()
                
                        for idx, row in chart_df.iterrows():
                            product_group = row['Product Group']
                            # Skip if product group is empty or null
                            if pd.isna(product_group) or str(product_group).strip() == '':
                                continue
                    
                            y_pos = len(y_positions)
                            y_positions.append(y_pos)
                    
                            # Calculate percentages of total for each metric
                            spend_pct = row['Spend'] / total_spend * 100 if total_spend > 0 else 0
                            ad_sales_pct = row['Ad Sales'] / total_ad_sales * 100 if total_ad_sales > 0 else 0
                            total_sales_pct = row['Total Sales'] / total_sales * 100 if total_sales > 0 else 0
                    
                            # Use actual dollar values
                            x_spend = row['Spend']
                            x_ad_sales = row['Ad Sales']
                            x_total_sales = row['Total Sales']
                    
                            # Format text to show both dollar value and percentage
                            min_value_for_label = max(row['Total Sales'] * 0.03, 1000)  # At least 3% of total or $1000
                    
                            spend_text = f"${row['Spend']:,.0f} ({spend_pct:.1f}%)" if row['Spend'] >= min_value_for_label else ""
                            ad_sales_text = f"${row['Ad Sales']:,.0f} ({ad_sales_pct:.1f}%)" if row['Ad Sales'] >= min_value_for_label else ""
                            total_sales_text = f"${row['Total Sales']:,.0f} ({total_sales_pct:.1f}%)" if row['Total Sales'] >= min_value_for_label else ""
                    
                            # Format hover text with detailed information
                            spend_hover = f"<b>{product_group}</b><br>Spend: ${row['Spend']:,.2f} ({spend_pct:.1f}%)"
                            ad_sales_hover = f"<b>{product_group}</b><br>Ad Sales: ${row['Ad Sales']:,.2f} ({ad_sales_pct:.1f}%)"
                            total_sales_hover = f"<b>{product_group}</b><br>Total Sales: ${row['Total Sales']:,.2f} ({total_sales_pct:.1f}%)"
                    
                            # Set tick format for axis
                            tick_format = ',.0f'
                            tick_suffix = ''
                            tick_prefix = '$'
                    
                            # Add the stacked bar segments with improved styling
                            # Spend bar (red)
                            fig.add_trace(go.Bar(
                                x=[x_spend],
                                y=[y_pos],
                                orientation='h',
                                name='Spend',
                                marker_color='rgba(239, 68, 68, 0.9)',  # Bright red with slight transparency
                                marker_line=dict(width=1, color='rgba(255,255,255,0.3)'),
                                text=[spend_text],
                                textposition='inside',
                                insidetextanchor='middle',
                                textfont=dict(color='#fff', size=13, family='Inter, Arial, sans-serif', weight='bold'),
                                showlegend=idx==0,  # Only show in legend for first row
                                hoverinfo='text',
                                hovertext=spend_hover,
                                width=0.75,
                                opacity=0.95
                            ))
                    
                            # Ad Sales bar (blue)
                            fig.add_trace(go.Bar(
                                x=[x_ad_sales],
                                y=[y_pos],
                                orientation='h',
                                name='Ad Sales',
                                marker_color='rgba(59, 130, 246, 0.9)',  # Bright blue with slight transparency
                                marker_line=dict(width=1, color='rgba(255,255,255,0.3)'),
                                text=[ad_sales_text],
                                textposition='inside',
                                insidetextanchor='middle',
                                textfont=dict(color='#fff', size=13, family='Inter, Arial, sans-serif', weight='bold'),
                                showlegend=idx==0,  # Only show in legend for first row
                                hoverinfo='text',
                                hovertext=ad_sales_hover,
                                width=0.75,
                                opacity=0.95
                            ))
                    
                            # Total Sales bar (green)
                            fig.add_trace(go.Bar(
                                x=[x_total_sales],
                                y=[y_pos],
                                orientation='h',
                                name='Total Sales',
                                marker_color='rgba(34, 197, 94, 0.9)',  # Bright green with slight transparency
                                marker_line=dict(width=1, color='rgba(255,255,255,0.3)'),
                                text=[total_sales_text],
                                textposition='inside',
                                insidetextanchor='middle',
                                textfont=dict(color='#fff', size=13, family='Inter, Arial, sans-serif', weight='bold'),
                                showlegend=idx==0,  # Only show in legend for first row
                                hoverinfo='text',
                                hovertext=total_sales_hover,
                                width=0.75,
                                opacity=0.95
                            ))
                
                        # Create a list of product group names for y-axis labels
                        product_group_labels = [chart_df.iloc[y_positions.index(pos)]['Product Group'] for pos in y_positions]
                
                        # Calculate dynamic height based on number of product groups
                        chart_height = max(350, 55 * (len(y_positions) + 1))  # Increased height per row for better spacing
                
                        # Update the layout with enhanced dark mode aesthetic
                        fig.update_layout(
                            barmode='stack',
                            height=chart_height,
                            margin=dict(l=200, r=40, t=70, b=50),  # Adjusted margins for better layout
                            yaxis=dict(
                                showticklabels=True,
                                ticktext=product_group_labels,
                                tickvals=y_positions,
                                tickfont=dict(size=13, color='#ffffff', family='Inter, Arial, sans-serif'),
                                showgrid=False,
                                zeroline=False,
                                domain=[0, 0.95],
                                title='',
                                ticklabelposition='outside'
                            ),
                            xaxis=dict(
                                title='',
                                showgrid=True,
                                gridcolor='rgba(255,255,255,0.15)',  # Slightly more visible grid lines
                                zeroline=False,
                                tickprefix=tick_prefix,
                                ticksuffix=tick_suffix,
                                tickformat=tick_format,
                                color='#ffffff',
                                tickfont=dict(size=12),
                                showline=True,
                                linecolor='rgba(255,255,255,0.3)',
                                mirror=True  # Add top axis line for better framing
                            ),
                            legend=dict(
                                orientation='h',
                                yanchor='bottom',
                                y=1.02,
                                xanchor='right',
                                x=1.0,  # Right-aligned legend
                                bgcolor='rgba(30,30,30,0.7)',  # Darker, more visible background
                                bordercolor='rgba(255,255,255,0.3)',
                                borderwidth=1,
                                font=dict(color='#ffffff', size=12, family='Inter, Arial, sans-serif'),
                                traceorder='reversed'  # Reverse order to match visual stacking (Spend, Ad Sales, Total Sales)
                            ),
                            legend_traceorder='reversed',
                            annotations=annotations,
                            plot_bgcolor='rgba(17,17,17,0.3)',  # Very subtle dark background
                            paper_bgcolor='rgba(0,0,0,0)',
                            bargap=0.4,  # Larger gap for better separation between product groups
                            font=dict(color='#ffffff', family='Inter, Arial, sans-serif'),
                            title=dict(
                                text='<b>Product Group Allocation (Stacked Bar View)</b>',
                                font=dict(size=18, color='#ffffff', family='Inter, Arial, sans-serif'),
                                x=0.0,
                                xanchor='left',
                                y=0.98,  # Position title slightly higher
                                yanchor='top'
                            ),
                            hoverlabel=dict(
                                bgcolor='rgba(50,50,50,0.9)',  # Dark hover label background
                                bordercolor='rgba(255,255,255,0.3)',
                                font=dict(family='Inter, Arial, sans-serif', size=13, color='white')
                            ),
                            hovermode='closest',
                            uniformtext=dict(minsize=10, mode='hide')  # Hide text that doesn't fit
                        )
                
                        return fig
            
                    # Display the charts in their respective tabs
                    with tab1:  # Stacked Bar View (default)
                        combined_fig = create_combined_view_chart()
                        st.plotly_chart(combined_fig, use_container_width=True)
            
                    with tab2:  # Percentage View
                        percentage_fig = create_product_group_chart(show_percentages=True)
                        st.plotly_chart(percentage_fig, use_container_width=True)
            
                    with tab3:  # Pie Charts
                        import plotly.express as px
                        import plotly.graph_objects as go
                        from plotly.subplots import make_subplots
                        
                        # Define consistent color palette for product groups
                        product_group_colors = [
                            '#FF6B6B',  # Coral Red
                            '#4A90E2',  # Blue  
                            '#50C878',  # Emerald Green
                            '#FF9F43',  # Orange
                            '#9B59B6',  # Purple
                            '#1ABC9C',  # Teal
                            '#F39C12',  # Golden Yellow
                            '#E74C3C',  # Red
                            '#3498DB',  # Light Blue
                            '#2ECC71',  # Green
                            '#E67E22',  # Dark Orange
                            '#8E44AD',  # Dark Purple
                            '#16A085',  # Dark Teal
                            '#F1C40F',  # Yellow
                            '#C0392B',  # Dark Red
                            '#2980B9',  # Dark Blue
                            '#27AE60',  # Dark Green
                            '#D35400',  # Pumpkin
                            '#7D3C98',  # Violet
                            '#138D75'   # Dark Cyan
                        ]
                        
                        # Create consistent color mapping for product groups
                        unique_groups = chart_df['Product Group'].unique()
                        color_mapping = {}
                        for i, group in enumerate(unique_groups):
                            color_mapping[group] = product_group_colors[i % len(product_group_colors)]
                        
                        pie_columns = [
                            ("Spend", "Ad Spend by Product Group"),
                            ("Ad Sales", "Ad Sales by Product Group"),
                            ("Total Sales", "Total Sales by Product Group")
                        ]
                        
                        # Create tabs including the new "All" tab
                        pie_tabs = st.tabs(["All", "Ad Spend", "Ad Sales", "Total Sales"])
                        
                        # Function to create individual pie chart
                        def create_pie_chart(col, title, chart_data):
                            # Filter out zero values
                            filtered_data = chart_data[chart_data[col] > 0].copy()
                            if len(filtered_data) == 0:
                                return None
                                
                            # Create colors list based on the product groups in this chart
                            colors = [color_mapping[group] for group in filtered_data['Product Group']]
                            
                            fig = px.pie(
                                filtered_data,
                                names="Product Group",
                                values=col,
                                title=title,
                                color_discrete_sequence=colors
                            )
                            fig.update_traces(
                                textinfo='percent+label',
                                pull=[0.05]*len(filtered_data),
                                marker=dict(line=dict(color='#222', width=1)),
                                textfont_color='white',
                                insidetextorientation='auto'
                            )
                            fig.update_layout(
                                template='plotly_dark',
                                height=500,
                                margin=dict(t=80, b=20, l=20, r=20),
                                legend_title_text='Product Group',
                                legend=dict(
                                    font=dict(color='#fff', size=12),
                                    bgcolor='rgba(30,30,30,0.7)',
                                    bordercolor='rgba(255,255,255,0.3)',
                                    borderwidth=1
                                ),
                                title=dict(
                                    font=dict(size=18, color='#fff'),
                                    x=0.5
                                ),
                                hoverlabel=dict(
                                    bgcolor='rgba(0,0,0,0.8)',
                                    font_size=12,
                                    font=dict(color='white')
                                )
                            )
                            return fig
                        # "All" tab - shows all three charts side by side
                        with pie_tabs[0]:
                            st.markdown("### Compare All Metrics")
                            # Create 3 columns for the charts
                            col1, col2, col3 = st.columns(3)
                            
                            # Function to create pie chart without legend
                            def create_pie_chart_no_legend(col, title, chart_data):
                                # Filter out zero values
                                filtered_data = chart_data[chart_data[col] > 0].copy()
                                if len(filtered_data) == 0:
                                    return None
                                    
                                # Create colors list based on the product groups in this chart
                                colors = [color_mapping[group] for group in filtered_data['Product Group']]
                                
                                fig = px.pie(
                                    filtered_data,
                                    names="Product Group",
                                    values=col,
                                    title=title,
                                    color_discrete_sequence=colors
                                )
                                fig.update_traces(
                                    textinfo='percent+label',
                                    pull=[0.05]*len(filtered_data),
                                    marker=dict(line=dict(color='#222', width=1)),
                                    textfont_color='white',
                                    insidetextorientation='auto',
                                    showlegend=False  # Hide individual legends
                                )
                                fig.update_layout(
                                    template='plotly_dark',
                                    height=400,
                                    margin=dict(t=80, b=20, l=20, r=20),
                                    title=dict(
                                        font=dict(size=16, color='#fff'),
                                        x=0.5
                                    ),
                                    hoverlabel=dict(
                                        bgcolor='rgba(0,0,0,0.8)',
                                        font_size=12,
                                        font=dict(color='white')
                                    )
                                )
                                return fig
                            
                            with col1:
                                spend_fig = create_pie_chart_no_legend("Spend", "Ad Spend by Product Group", chart_df)
                                if spend_fig:
                                    st.plotly_chart(spend_fig, use_container_width=True)
                                else:
                                    st.info("No spend data to display")
                            
                            with col2:
                                ad_sales_fig = create_pie_chart_no_legend("Ad Sales", "Ad Sales by Product Group", chart_df)
                                if ad_sales_fig:
                                    st.plotly_chart(ad_sales_fig, use_container_width=True)
                                else:
                                    st.info("No ad sales data to display")
                            
                            with col3:
                                total_sales_fig = create_pie_chart_no_legend("Total Sales", "Total Sales by Product Group", chart_df)
                                if total_sales_fig:
                                    st.plotly_chart(total_sales_fig, use_container_width=True)
                                else:
                                    st.info("No total sales data to display")
                            
                        for i, (col, title) in enumerate(pie_columns):
                            with pie_tabs[i + 1]:  # +1 because "All" tab is first
                                fig = create_pie_chart(col, title, chart_df)
                                if fig:
                                    st.plotly_chart(fig, use_container_width=True)
                                else:
                                    st.info(f"No {col.lower()} data to display")             
                elif len(filtered_group_perf_df) > 0:
                    missing_cols = [col for col in ['Spend', 'Ad Sales', 'Total Sales'] if col not in filtered_group_perf_df.columns]
                    st.info(f"Cannot display charts: Missing required columns {', '.join(missing_cols)}")
                elif not selected_groups:
                    st.info("Please select at least one Product Group to view charts.")
            else:
                st.warning("No metrics available for Product Group aggregation")
            # Debug info for Product Analysis
            if 'debug_messages' in st.session_state:
                st.session_state.debug_messages.append('[INFO] Displayed Product Analysis section')
            

        # Only show the 'Select or create a client' message if no client is selected
        elif not st.session_state.client_config:
            st.info("Select or create a client in the sidebar to get started.")

        # Initialize session state variables if they don't exist
        if 'settings_updated' not in st.session_state:
            st.session_state.settings_updated = False
    
        if 'new_branded_asins' not in st.session_state:
            st.session_state.new_branded_asins = []
    
        if 'new_sales_asins' not in st.session_state:
            st.session_state.new_sales_asins = []
    
        if 'new_sales_asins_titles' not in st.session_state:
            st.session_state.new_sales_asins_titles = {}
    
        if 'show_new_sales_asins_prompt' not in st.session_state:
            st.session_state.show_new_sales_asins_prompt = False

# --- Run the App ---
# (No explicit run needed here, Streamlit handles it)

# --- Performance by Placement Section ---
# Only show on the Advertising Audit page and only if NOT using Companion Exports
if st.session_state.current_page == "advertising_audit" and not st.session_state.get('is_companion_data', False):
    # --- Initialize session state for Placement Product Group filter ---
    if 'placement_product_group_filter' not in st.session_state:
        st.session_state.placement_product_group_filter = []
    if 'placement_product_group_filter_active' not in st.session_state:
        st.session_state.placement_product_group_filter_active = False

    available_placement_pgs = []
    PRODUCT_GROUP_COLUMN_NAME_FOR_PLACEMENT = 'tag_1' # As per memory 1c171cfc-a998-4b55-aeb3-6ef5088e1e87
    CAMPAIGN_NAME_COL_IN_TAGS_FOR_PLACEMENT = 'Campaign Name' # Common name in campaign_tags_df
    CAMPAIGN_NAME_COL_IN_BULK_FOR_PLACEMENT = 'Campaign Name (Informational only)' # Correct column name in bulk files (lowercase 'only')

    # Create campaign_tags_df from campaign_tags_data if it doesn't exist
    campaign_tags_df = None
    if (st.session_state.get('client_config') and 
        'campaign_tags_data' in st.session_state.client_config and 
        st.session_state.client_config['campaign_tags_data']):
        
        # Convert campaign_tags_data to DataFrame
        data = []
        for campaign_name, info in st.session_state.client_config['campaign_tags_data'].items():
            data.append({
                'Campaign Name': campaign_name,
                'Campaign Type': info.get('campaign_type', ''),
                'tag_1': info.get('tag_1', ''),
                'tag_2': info.get('tag_2', ''),
                'tag_3': info.get('tag_3', '')
            })
        
        if data:
            campaign_tags_df = pd.DataFrame(data)
            # Store in session state for use in the filter logic below
            st.session_state.campaign_tags_df = campaign_tags_df

    if campaign_tags_df is not None and not campaign_tags_df.empty:
        if PRODUCT_GROUP_COLUMN_NAME_FOR_PLACEMENT in campaign_tags_df.columns:
            # Get unique product groups, excluding empty/null values and blank strings
            all_product_groups = campaign_tags_df[PRODUCT_GROUP_COLUMN_NAME_FOR_PLACEMENT].dropna().unique().tolist()
            # Filter out empty strings and whitespace-only strings
            available_placement_pgs = sorted([pg for pg in all_product_groups if pg and str(pg).strip()])
            
            # Check if there are any campaigns with empty/null product groups
            has_untagged = campaign_tags_df[PRODUCT_GROUP_COLUMN_NAME_FOR_PLACEMENT].isna().any() or \
                          (campaign_tags_df[PRODUCT_GROUP_COLUMN_NAME_FOR_PLACEMENT] == '').any() or \
                          campaign_tags_df[PRODUCT_GROUP_COLUMN_NAME_FOR_PLACEMENT].astype(str).str.strip().eq('').any()
            
            # Add 'Untagged Group' option if there are campaigns without product group tags
            if has_untagged:
                available_placement_pgs.append('Untagged Group')
        else:
            if 'debug_messages' in st.session_state:
                st.session_state.debug_messages.append(f"[WARNING] Product group column '{PRODUCT_GROUP_COLUMN_NAME_FOR_PLACEMENT}' not found in campaign_tags_df for Placement filter.")

    # --- Performance by Placement ---
    st.markdown("<hr style='height:2px;border-width:0;color:gold;background-color:gold;margin-top:25px;margin-bottom:15px;margin-left:10px;margin-right:10px'>", unsafe_allow_html=True)
    st.markdown("<span class='main-section-header dashboard-section'>Performance by Placement</span>", unsafe_allow_html=True)

    # --- Product Group Filter for Placement ---
    if available_placement_pgs:
        prev_selection_placement = list(st.session_state.placement_product_group_filter) # Make a copy

        selected_pgs_placement = st.multiselect(
            "Filter by Product Group(s):",
            options=available_placement_pgs,
            key='placement_pg_multiselect_key' # Unique key
        )
        current_placement_pg_filter_active = bool(selected_pgs_placement)

        # If selection or active status changed, update session state and rerun
        if sorted(prev_selection_placement) != sorted(selected_pgs_placement) or \
           st.session_state.placement_product_group_filter_active != current_placement_pg_filter_active:
            st.session_state.placement_product_group_filter = selected_pgs_placement
            st.session_state.placement_product_group_filter_active = current_placement_pg_filter_active
            st.rerun()
    elif st.session_state.placement_product_group_filter_active: # If no PGs available but filter was active, deactivate
        st.session_state.placement_product_group_filter = []
        st.session_state.placement_product_group_filter_active = False
        st.rerun()

    
    # Add debug messages to track execution
    if 'debug_messages' in st.session_state:
        st.session_state.debug_messages.append('[INFO] Starting Performance by Placement section')
    
    # Check if bulk data exists
    if 'bulk_data' in st.session_state and st.session_state.bulk_data is not None:
        # Add debug message
        if 'debug_messages' in st.session_state:
            st.session_state.debug_messages.append('[INFO] Bulk data found for placement analysis')
        
        # Initialize variables
        has_bidding_adjustment = False
        placement_data = pd.DataFrame()
        entity_columns_found = []
        bidding_adjustment_counts = {}
        
        # Initialize debug tracking for placement filtering
        if 'placement_filter_debug' not in st.session_state:
            st.session_state.placement_filter_debug = {}
    
        # Examine each sheet in the bulk data
        for sheet_name, df in st.session_state.bulk_data.items():
            if isinstance(df, pd.DataFrame) and not df.empty:
                # Initialize debug info for this sheet
                st.session_state.placement_filter_debug[sheet_name] = {
                    'original_rows': len(df),
                    'campaigns_found': 0,
                    'filtered_rows': 0,
                    'bidding_adjustment_rows': 0
                }
                
                # --- Apply Product Group Filter to current sheet df ---
                if st.session_state.get('placement_product_group_filter_active', False) and st.session_state.placement_product_group_filter:
                    if 'campaign_tags_df' in st.session_state and st.session_state.campaign_tags_df is not None and not st.session_state.campaign_tags_df.empty and \
                       CAMPAIGN_NAME_COL_IN_TAGS_FOR_PLACEMENT in st.session_state.campaign_tags_df.columns and \
                       PRODUCT_GROUP_COLUMN_NAME_FOR_PLACEMENT in st.session_state.campaign_tags_df.columns:
                        
                        # Check for campaign column using case-insensitive matching
                        bulk_campaign_col = None
                        for col in df.columns:
                            if col.lower() == CAMPAIGN_NAME_COL_IN_BULK_FOR_PLACEMENT.lower():
                                bulk_campaign_col = col
                                break
                        
                        if bulk_campaign_col is None:
                            if 'debug_messages' in st.session_state:
                                st.session_state.debug_messages.append(f"[WARNING] Campaign column '{CAMPAIGN_NAME_COL_IN_BULK_FOR_PLACEMENT}' not found in sheet '{sheet_name}' for Placement Product Group filtering.")
                        else:
                            # Get campaigns that match the selected product groups
                            campaigns_for_selected_pgs = []
                            
                            # Handle regular product groups
                            regular_pgs = [pg for pg in st.session_state.placement_product_group_filter if pg != 'Untagged Group']
                            if regular_pgs:
                                tagged_campaigns = st.session_state.campaign_tags_df[
                                    st.session_state.campaign_tags_df[PRODUCT_GROUP_COLUMN_NAME_FOR_PLACEMENT].isin(regular_pgs)
                                ][CAMPAIGN_NAME_COL_IN_TAGS_FOR_PLACEMENT].unique().tolist()
                                campaigns_for_selected_pgs.extend(tagged_campaigns)
                            
                            # Handle 'Untagged Group' selection
                            if 'Untagged Group' in st.session_state.placement_product_group_filter:
                                untagged_campaigns = st.session_state.campaign_tags_df[
                                    (st.session_state.campaign_tags_df[PRODUCT_GROUP_COLUMN_NAME_FOR_PLACEMENT].isna()) |
                                    (st.session_state.campaign_tags_df[PRODUCT_GROUP_COLUMN_NAME_FOR_PLACEMENT] == '') |
                                    (st.session_state.campaign_tags_df[PRODUCT_GROUP_COLUMN_NAME_FOR_PLACEMENT].astype(str).str.strip() == '')
                                ][CAMPAIGN_NAME_COL_IN_TAGS_FOR_PLACEMENT].unique().tolist()
                                campaigns_for_selected_pgs.extend(untagged_campaigns)
                            
                            # Remove duplicates and convert to list
                            campaigns_for_selected_pgs = list(set(campaigns_for_selected_pgs))

                            st.session_state.placement_filter_debug[sheet_name]['campaigns_found'] = len(campaigns_for_selected_pgs)
                            
                            # Add detailed debugging for campaign matching
                            if 'debug_messages' in st.session_state:
                                st.session_state.debug_messages.append(f"[DEBUG] Sheet '{sheet_name}' - Selected Product Groups: {st.session_state.placement_product_group_filter}")
                                st.session_state.debug_messages.append(f"[DEBUG] Sheet '{sheet_name}' - Campaigns from tags matching PGs: {campaigns_for_selected_pgs[:5]}...")  # Show first 5
                                
                                # Show sample campaigns from the bulk file
                                if bulk_campaign_col:
                                    bulk_campaigns = df[bulk_campaign_col].dropna().unique()[:5].tolist()
                                    st.session_state.debug_messages.append(f"[DEBUG] Sheet '{sheet_name}' - Sample campaigns in bulk file (using column '{bulk_campaign_col}'): {bulk_campaigns}")
                                    
                                    # Check for case-insensitive matches
                                    campaigns_lower = [c.lower() for c in campaigns_for_selected_pgs]
                                    bulk_campaigns_lower = df[bulk_campaign_col].str.lower().dropna().unique()
                                    matches = [c for c in bulk_campaigns_lower if c in campaigns_lower]
                                    st.session_state.debug_messages.append(f"[DEBUG] Sheet '{sheet_name}' - Case-insensitive campaign matches found: {len(matches)} - {matches[:3]}...")
                                else:
                                    st.session_state.debug_messages.append(f"[DEBUG] Sheet '{sheet_name}' - No campaign column matching '{CAMPAIGN_NAME_COL_IN_BULK_FOR_PLACEMENT}' found (case-insensitive)")

                            if campaigns_for_selected_pgs:
                                df_original_shape = df.shape
                                # Apply case-insensitive filtering using the found column
                                campaigns_for_selected_pgs_lower = [camp.lower() for camp in campaigns_for_selected_pgs]
                                df_campaign_col_lower = df[bulk_campaign_col].str.lower()
                                df = df[df_campaign_col_lower.isin(campaigns_for_selected_pgs_lower)].copy()
                                st.session_state.placement_filter_debug[sheet_name]['filtered_rows'] = len(df)
                                if 'debug_messages' in st.session_state:
                                    st.session_state.debug_messages.append(f"[INFO] Sheet '{sheet_name}' (original shape: {df_original_shape}) filtered by Product Groups for Placement. New shape: {df.shape}. Campaigns found: {len(campaigns_for_selected_pgs)}")
                            else:
                                # No campaigns match the selected product groups, so this sheet will be empty for these PGs
                                df = pd.DataFrame(columns=df.columns) # Make df empty
                                st.session_state.placement_filter_debug[sheet_name]['filtered_rows'] = 0
                                if 'debug_messages' in st.session_state:
                                    st.session_state.debug_messages.append(f"[INFO] No campaigns in sheet '{sheet_name}' match selected Product Groups for Placement. Sheet effectively empty for this filter.")
                    else:
                        if 'debug_messages' in st.session_state:
                            st.session_state.debug_messages.append("[WARNING] Cannot apply Product Group filter for Placements: campaign_tags_df missing or misconfigured.")
                else:
                    # No filtering applied
                    st.session_state.placement_filter_debug[sheet_name]['filtered_rows'] = len(df)
                
                # If df became empty after filtering, skip to the next sheet
                if df.empty:
                    if 'debug_messages' in st.session_state:
                        st.session_state.debug_messages.append(f"[INFO] Sheet '{sheet_name}' is empty after potential Product Group filtering for Placement. Skipping.")
                    continue
                # Check if Entity column exists
                if 'Entity' in df.columns:
                    entity_columns_found.append(sheet_name)
                    
                    # Check for Bidding Adjustment rows - only if Entity column exists
                    bidding_rows = df[df['Entity'].astype(str).str.strip().str.lower() == 'bidding adjustment']
                    bidding_adjustment_counts[sheet_name] = len(bidding_rows)
                    st.session_state.placement_filter_debug[sheet_name]['bidding_adjustment_rows'] = len(bidding_rows)
                else:
                    # Skip this sheet if Entity column doesn't exist
                    bidding_rows = pd.DataFrame()
                    bidding_adjustment_counts[sheet_name] = 0
                    
                    # Debug message for missing Entity column
                    if 'debug_messages' in st.session_state:
                        st.session_state.debug_messages.append(f'[WARNING] No Entity column found in {sheet_name}')
                
                # Initialize bidding_rows as an empty DataFrame if not already defined
                if 'bidding_rows' not in locals():
                    bidding_rows = pd.DataFrame()
                    
                if not bidding_rows.empty:
                    has_bidding_adjustment = True
                    
                    # Debug message for bidding adjustment rows found
                    if 'debug_messages' in st.session_state:
                        st.session_state.debug_messages.append(f'[INFO] Found {len(bidding_rows)} Bidding Adjustment rows in {sheet_name}')
                    
                    # List all columns in the dataframe to help with debugging
                    if 'debug_messages' in st.session_state:
                        st.session_state.debug_messages.append(f'[INFO] Available columns in {sheet_name}: {list(bidding_rows.columns)}')
                    
                    # Check for placement columns with more flexible matching
                    placement_columns = []
                    placement_column_patterns = {
                        'rest of search': 'Placement Rest Of Search',
                        'product page': 'Placement Product Page', 
                        'top': 'Placement Top',
                        'business': 'Placement Amazon Business'
                    }
                    
                    # Debug message to show all available columns
                    if 'debug_messages' in st.session_state:
                        st.session_state.debug_messages.append(f'[INFO] Searching for placement columns in: {list(bidding_rows.columns)}')
                    
                    # First, check if there's a generic 'Placement' column
                    has_generic_placement = False
                    for col in bidding_rows.columns:
                        if col.lower() == 'placement':
                            has_generic_placement = True
                            # Add debug message about finding the generic placement column
                            if 'debug_messages' in st.session_state:
                                st.session_state.debug_messages.append(f'[INFO] Found generic Placement column: {col}')
                            
                            # Check unique values in this column
                            unique_values = bidding_rows[col].dropna().unique()
                            if 'debug_messages' in st.session_state:
                                st.session_state.debug_messages.append(f'[INFO] Unique values in Placement column: {unique_values}')
                            
                            # If we have a generic placement column, we'll use it differently
                            # We'll treat each unique value as a separate placement type
                            break
                    
                    # Find columns that match our patterns or use the generic placement column
                    if has_generic_placement:
                        # Use the generic placement column
                        for col in bidding_rows.columns:
                            if col.lower() == 'placement':
                                placement_columns.append(col)
                                break
                    else:
                        # Look for specific placement columns
                        for col in bidding_rows.columns:
                            col_lower = col.lower()
                            # Very flexible matching - just look for 'placement' and any of our patterns
                            if 'placement' in col_lower:
                                for pattern, standard_name in placement_column_patterns.items():
                                    if pattern in col_lower:
                                        placement_columns.append(col)
                                        # Map the actual column name to our standard name for later use
                                        placement_column_patterns[pattern] = col
                                        break
                    
                    # Debug message for placement columns found
                    if 'debug_messages' in st.session_state:
                        st.session_state.debug_messages.append(f'[INFO] Found placement columns: {placement_columns}')
                    
                    # Only proceed if we found at least one placement column
                    if placement_columns:
                        # Select columns we need for analysis
                        needed_columns = ['Campaign', 'Ad Group'] if all(col in bidding_rows.columns for col in ['Campaign', 'Ad Group']) else []
                        needed_columns.extend(placement_columns)
                        
                        # Look for metric columns with flexible matching - expanded patterns for better detection
                        metric_patterns = {
                            'spend': 'Spend',
                            'sales': 'Sales',
                            'click': 'Clicks',
                            'impression click': 'Clicks',  # Add this pattern for better clicks detection
                            'order': 'Orders'
                        }
                        
                        metric_columns = {}
                        for col in bidding_rows.columns:
                            col_lower = col.lower()
                            for pattern, standard_name in metric_patterns.items():
                                if pattern in col_lower:
                                    metric_columns[standard_name] = col
                                    needed_columns.append(col)
                                    break
                        
                        # Debug message for metric columns found
                        if 'debug_messages' in st.session_state:
                            st.session_state.debug_messages.append(f'[INFO] Found metric columns: {metric_columns}')
                        
                        # Filter columns that exist in the dataframe
                        existing_columns = [col for col in needed_columns if col in bidding_rows.columns]
                        
                        if existing_columns:
                            # Append to our placement data
                            if placement_data.empty:
                                placement_data = bidding_rows[existing_columns].copy()
                            else:
                                placement_data = pd.concat([placement_data, bidding_rows[existing_columns]], ignore_index=True)
    
        # Debug summary of what we found
        if 'debug_messages' in st.session_state:
            if entity_columns_found:
                st.session_state.debug_messages.append(f'[INFO] Entity column found in sheets: {entity_columns_found}')
            else:
                st.session_state.debug_messages.append('[WARNING] No Entity column found in any sheet')
                
            st.session_state.debug_messages.append(f'[INFO] Bidding Adjustment counts by sheet: {bidding_adjustment_counts}')
            st.session_state.debug_messages.append(f'[INFO] Final placement data shape: {placement_data.shape if not placement_data.empty else "Empty"}')
            
            # Add debug info for Product Group filtering
            if st.session_state.get('placement_product_group_filter_active', False) and st.session_state.placement_product_group_filter:
                st.session_state.debug_messages.append(f'[INFO] Product Group filter applied for Placement: {st.session_state.placement_product_group_filter}')
                if not placement_data.empty and 'Campaign' in placement_data.columns:
                    unique_campaigns = placement_data['Campaign'].unique()
                    st.session_state.debug_messages.append(f'[INFO] Campaigns in filtered placement data: {unique_campaigns[:10].tolist()}')  # Show first 10
            else:
                st.session_state.debug_messages.append('[INFO] No Product Group filter active for Placement')
        
        # Display information about what we found
        if not has_bidding_adjustment:
            st.info("No 'Bidding Adjustment' rows found in the bulk file. Upload a bulk file with bidding adjustment data to see placement performance.")
        elif placement_data.empty:
            st.info("Found 'Bidding Adjustment' rows but no placement columns were detected. Check that your bulk file contains placement data.")
        else:
            # Prepare data for display
            # Convert numeric columns to proper numeric type
            for col in placement_data.columns:
                # Try to identify numeric columns that might contain our metrics - expanded patterns
                if any(metric in col.lower() for metric in ['spend', 'sales', 'click', 'impression click', 'order']):
                    placement_data[col] = safe_convert_to_numeric(placement_data[col])
                    # Map actual column names to our standard names for metrics
            metric_mapping = {}
            for col in placement_data.columns:
                col_lower = col.lower()
                if 'spend' in col_lower:
                    metric_mapping['Spend'] = col
                elif 'sales' in col_lower:
                    metric_mapping['Sales'] = col
                # Prioritize exact 'clicks' column over 'click-through rate'
                elif col_lower == 'clicks':
                    metric_mapping['Clicks'] = col
                elif 'order' in col_lower:
                    metric_mapping['Orders'] = col
            
            # If we didn't find an exact 'clicks' column, look for alternatives
            if 'Clicks' not in metric_mapping:
                for col in placement_data.columns:
                    col_lower = col.lower()
                    if 'click' in col_lower and 'rate' not in col_lower:
                        metric_mapping['Clicks'] = col
                        break
                
                # If we still don't have clicks but have impressions and CTR, calculate clicks
                if 'Clicks' not in metric_mapping:
                    ctr_col = None
                    impressions_col = None
                    
                    for col in placement_data.columns:
                        col_lower = col.lower()
                        if 'click-through rate' in col_lower or 'ctr' in col_lower:
                            ctr_col = col
                        elif 'impression' in col_lower:
                            impressions_col = col
                    
                    if ctr_col and impressions_col:
                        # Calculate clicks from impressions and CTR
                        placement_data['Calculated Clicks'] = placement_data.apply(
                            lambda row: int(row[impressions_col] * row[ctr_col] / 100) if pd.notnull(row[impressions_col]) and pd.notnull(row[ctr_col]) else 0,
                            axis=1
                        )
                        metric_mapping['Clicks'] = 'Calculated Clicks'
                    
            # Debug message to show all available columns for better diagnostics
            if 'debug_messages' in st.session_state:
                st.session_state.debug_messages.append(f'[INFO] All columns in placement_data: {list(placement_data.columns)}')
        
            # Debug message for metric mapping
            if 'debug_messages' in st.session_state:
                st.session_state.debug_messages.append(f'[INFO] Metric mapping: {metric_mapping}')
                
                # Add detailed debug for clicks data
                if 'Clicks' in metric_mapping:
                    clicks_col = metric_mapping['Clicks']
                    if clicks_col in placement_data.columns:
                        st.session_state.debug_messages.append(f'[INFO] Clicks column statistics: Min={placement_data[clicks_col].min()}, Max={placement_data[clicks_col].max()}, Sum={placement_data[clicks_col].sum()}')
                        st.session_state.debug_messages.append(f'[INFO] Sample clicks values: {placement_data[clicks_col].head(5).tolist()}')
            
            # Check if we have the necessary columns to calculate metrics
            has_spend = 'Spend' in metric_mapping
            has_sales = 'Sales' in metric_mapping
            has_clicks = 'Clicks' in metric_mapping
            has_orders = 'Orders' in metric_mapping
        
            # Map placement column patterns to display names
            display_mapping = {
                'rest of search': 'Rest Of Search',
                'product page': 'Product Page',
                'top': 'Top of Search',
                'business': 'B2B Modifier'
            }
        
            # Check if we have a generic placement column
            has_generic_placement = False
            generic_placement_col = None
            for col in placement_data.columns:
                if col.lower() == 'placement':
                    has_generic_placement = True
                    generic_placement_col = col
                    # Add debug message
                    if 'debug_messages' in st.session_state:
                        st.session_state.debug_messages.append(f'[INFO] Using generic Placement column: {col}')
                    break
        
            # Map actual column names to our display names
            placement_mapping = {}
            
            if has_generic_placement:
                # For generic placement column, we'll create a special mapping
                # Get unique values from the placement column
                unique_placements = placement_data[generic_placement_col].dropna().unique()
                
                # Debug message for unique placement values
                if 'debug_messages' in st.session_state:
                    st.session_state.debug_messages.append(f'[INFO] Unique placement values: {unique_placements}')
                
                # Map each unique value to a display name
                # We'll use a case-insensitive matching approach
                for value in unique_placements:
                    if isinstance(value, str):
                        value_lower = value.lower()
                        # Try to match to our standard display names
                        matched = False
                        for pattern, display_name in display_mapping.items():
                            if pattern in value_lower:
                                # This is a special mapping where we map (column, value) to display_name
                                placement_mapping[(generic_placement_col, value)] = display_name
                                matched = True
                                break
                        
                        # If no match found, use the value itself as the display name
                        if not matched:
                            placement_mapping[(generic_placement_col, value)] = value
            else:
                # For specific placement columns, use the original approach
                for col in placement_data.columns:
                    col_lower = col.lower()
                    if 'placement' in col_lower:
                        for pattern, display_name in display_mapping.items():
                            if pattern in col_lower:
                                placement_mapping[col] = display_name
                                break

            # Debug message for placement mapping
            if 'debug_messages' in st.session_state:
                st.session_state.debug_messages.append(f'[INFO] Placement mapping: {placement_mapping}')
            
            # Debug message to show which metrics were found
            if 'debug_messages' in st.session_state:
                st.session_state.debug_messages.append(f'[INFO] Metrics found: Spend={has_spend}, Sales={has_sales}, Clicks={has_clicks}, Orders={has_orders}')
                
            if has_spend and has_sales and placement_mapping:  # Make clicks and orders optional
                # Create a dictionary to store our placement performance data
                placement_performance = {}
                for display_name in display_mapping.values():
                    # Initialize with zeros for all metrics
                    placement_performance[display_name] = {
                        'Spend': 0, 
                        'Ad Sales': 0, 
                        'Clicks': 0 if has_clicks else None,  # Use None if clicks data not available
                        'Orders': 0 if has_orders else None   # Use None if orders data not available
                    }
            
                # Aggregate data for each placement type
                if has_generic_placement:
                    # For generic placement column, we need to filter by value
                    for (col, value), display_name in placement_mapping.items():
                        # Get rows where the placement column equals this specific value
                        relevant_rows = placement_data[placement_data[col] == value]
                        
                        if not relevant_rows.empty:
                            # Initialize this placement type if it doesn't exist
                            if display_name not in placement_performance:
                                placement_performance[display_name] = {'Spend': 0, 'Ad Sales': 0, 'Clicks': 0 if has_clicks else None, 'Orders': 0 if has_orders else None}
                            
                            # Sum up the metrics for this placement type
                            if has_spend:
                                placement_performance[display_name]['Spend'] = relevant_rows[metric_mapping['Spend']].sum()
                            if has_sales:
                                placement_performance[display_name]['Ad Sales'] = relevant_rows[metric_mapping['Sales']].sum()
                            if has_clicks:
                                placement_performance[display_name]['Clicks'] = relevant_rows[metric_mapping['Clicks']].sum()
                            if has_orders:
                                placement_performance[display_name]['Orders'] = relevant_rows[metric_mapping['Orders']].sum()
                else:
                    # For specific placement columns
                    for db_col, display_name in placement_mapping.items():
                        # Get rows where this placement column is not null
                        relevant_rows = placement_data[placement_data[db_col].notna()]
                        
                        if not relevant_rows.empty:
                            # Sum up the metrics for this placement type
                            if has_spend:
                                placement_performance[display_name]['Spend'] = relevant_rows[metric_mapping['Spend']].sum()
                            if has_sales:
                                placement_performance[display_name]['Ad Sales'] = relevant_rows[metric_mapping['Sales']].sum()
                            if has_clicks:
                                placement_performance[display_name]['Clicks'] = relevant_rows[metric_mapping['Clicks']].sum()
                            if has_orders:
                                placement_performance[display_name]['Orders'] = relevant_rows[metric_mapping['Orders']].sum()
            
                # Create a DataFrame from our performance data
                performance_df = pd.DataFrame.from_dict(placement_performance, orient='index')
                
                # Debug message for performance data
                if 'debug_messages' in st.session_state:
                    st.session_state.debug_messages.append(f'[INFO] Created performance dataframe with shape: {performance_df.shape}')
                
                # Calculate derived metrics
                performance_df['ACoS'] = performance_df.apply(
                    lambda row: (row['Spend'] / row['Ad Sales'] * 100) if row['Ad Sales'] > 0 else float('inf'),
                    axis=1
                )
                performance_df['ROAS'] = performance_df.apply(
                    lambda row: (row['Ad Sales'] / row['Spend']) if row['Spend'] > 0 else 0,
                    axis=1
                )
                
                # Only calculate CPC and CVR if we have clicks data
                if has_clicks:
                    # Ensure Clicks are properly populated from the bulk file
                    for placement, data in placement_performance.items():
                        if data['Clicks'] is None or pd.isna(data['Clicks']):
                            # Try to get clicks directly from the relevant rows if available
                            if 'Clicks' in metric_mapping and metric_mapping['Clicks'] in placement_data.columns:
                                if has_generic_placement:
                                    for (col, value), display_name in placement_mapping.items():
                                        if display_name == placement:
                                            relevant_rows = placement_data[placement_data[col] == value]
                                            if not relevant_rows.empty:
                                                data['Clicks'] = relevant_rows[metric_mapping['Clicks']].sum()
                                else:
                                    for db_col, display_name in placement_mapping.items():
                                        if display_name == placement:
                                            relevant_rows = placement_data[placement_data[db_col].notna()]
                                            if not relevant_rows.empty:
                                                data['Clicks'] = relevant_rows[metric_mapping['Clicks']].sum()
                    
                    # Now calculate CPC with the updated clicks data
                    performance_df['CPC'] = performance_df.apply(
                        lambda row: (row['Spend'] / row['Clicks']) if pd.notnull(row['Clicks']) and row['Clicks'] > 0 else None,
                        axis=1
                    )
                    
                    if has_orders:
                        performance_df['CVR'] = performance_df.apply(
                            lambda row: (row['Orders'] / row['Clicks'] * 100) if pd.notnull(row['Clicks']) and pd.notnull(row['Orders']) and row['Clicks'] > 0 else None,
                            axis=1
                        )
                
                # Exclude B2B Modifier from totals calculations (avoid double counting)
                # B2B Modifier can be applied across other placements, so including it would double-count
                totals_df = performance_df[~performance_df.index.isin(['B2B Modifier', 'Amazon Business'])].copy()
                
                # Calculate totals for summary cards (excluding B2B Modifier)
                total_spend = totals_df['Spend'].sum()
                total_ad_sales = totals_df['Ad Sales'].sum()
            
                # Calculate ACoS and ROAS for the totals
                total_acos = (total_spend / total_ad_sales * 100) if total_ad_sales > 0 else float('inf')
                total_roas = (total_ad_sales / total_spend) if total_spend > 0 else 0
                
                # Calculate additional metrics for summary cards
                total_clicks = totals_df['Clicks'].sum() if 'Clicks' in totals_df.columns else 0
                total_orders = totals_df['Orders'].sum() if 'Orders' in totals_df.columns else 0
                total_cpc = (total_spend / total_clicks) if total_clicks and total_clicks > 0 else 0
                total_cvr = (total_orders / total_clicks * 100) if total_clicks and total_clicks > 0 and total_orders else 0
                
                # Display summary cards above the table
                st.markdown("### Placement Performance Summary")
                
                # Create columns for summary cards - 6 cards in 2 rows of 3
                summary_cols_1 = st.columns(3)
                
                with summary_cols_1[0]:
                    st.metric(
                        label="Spend",
                        value=f"${total_spend:,.2f}"
                    )
                
                with summary_cols_1[1]:
                    st.metric(
                        label="Ad Sales", 
                        value=f"${total_ad_sales:,.2f}"
                    )
                
                with summary_cols_1[2]:
                    roas_display = f"{total_roas:.2f}x" if total_roas > 0 else 'N/A'
                    st.metric(
                        label="ROAS",
                        value=roas_display
                    )
                
                # Second row of cards
                summary_cols_2 = st.columns(3)
                
                with summary_cols_2[0]:
                    acos_display = f"{total_acos:.1f}%" if total_acos != float('inf') else 'N/A'
                    st.metric(
                        label="ACoS",
                        value=acos_display
                    )
                
                with summary_cols_2[1]:
                    cpc_display = f"${total_cpc:.2f}" if total_cpc > 0 else 'N/A'
                    st.metric(
                        label="CPC",
                        value=cpc_display
                    )
                
                with summary_cols_2[2]:
                    cvr_display = f"{total_cvr:.1f}%" if total_cvr > 0 else 'N/A'
                    st.metric(
                        label="CVR",
                        value=cvr_display
                    )
                
                st.markdown("---")  # Separator line
                
                # Add percentage columns to the performance dataframe (excluding B2B Modifier from totals)
                # Calculate totals excluding B2B Modifier for percentage calculations
                performance_for_percentages = performance_df[~performance_df.index.isin(['B2B Modifier', 'Amazon Business'])].copy()
                total_spend_for_percentages = performance_for_percentages['Spend'].sum()
                total_ad_sales_for_percentages = performance_for_percentages['Ad Sales'].sum()
                
                # Initialize percentage columns
                performance_df['% of Spend'] = 0.0
                performance_df['% of Ad Sales'] = 0.0
                
                # Calculate percentages only for non-B2B Modifier placements
                for placement in performance_df.index:
                    if placement not in ['B2B Modifier', 'Amazon Business']:
                        if total_spend_for_percentages > 0:
                            performance_df.loc[placement, '% of Spend'] = (performance_df.loc[placement, 'Spend'] / total_spend_for_percentages * 100)
                        if total_ad_sales_for_percentages > 0:
                            performance_df.loc[placement, '% of Ad Sales'] = (performance_df.loc[placement, 'Ad Sales'] / total_ad_sales_for_percentages * 100)
                
                # Make a copy for display formatting
                display_df = performance_df.copy()
                
                # Format the metrics for display with improved handling of missing data
                display_df['Spend'] = display_df['Spend'].apply(lambda x: f"${x:,.2f}")
                display_df['Ad Sales'] = display_df['Ad Sales'].apply(lambda x: f"${x:,.2f}")
                display_df['ACoS'] = display_df['ACoS'].apply(lambda x: f"{x:.2f}%" if x != float('inf') else 'N/A')
                display_df['ROAS'] = display_df['ROAS'].apply(lambda x: f"{x:.2f}" if x > 0 else 'N/A')
                display_df['% of Spend'] = display_df['% of Spend'].apply(lambda x: f"{x:.2f}%")
                display_df['% of Ad Sales'] = display_df['% of Ad Sales'].apply(lambda x: f"{x:.2f}%")
                
                # Only format columns that exist in the dataframe
                if 'CPC' in display_df.columns:
                    display_df['CPC'] = display_df['CPC'].apply(lambda x: f"${x:.2f}" if pd.notnull(x) and x > 0 else 'N/A')
                    
                if 'CVR' in display_df.columns:
                    display_df['CVR'] = display_df['CVR'].apply(lambda x: f"{x:.2f}%" if pd.notnull(x) and x > 0 else 'N/A')
                
                if 'Clicks' in display_df.columns:
                    # Improved clicks display with better null handling
                    display_df['Clicks'] = display_df['Clicks'].apply(
                        lambda x: f"{int(x):,}" if pd.notnull(x) and x != 0 else "0"
                    )
                    # Debug message for clicks data
                    # Add debug message
                    if 'debug_messages' in st.session_state:
                        st.session_state.debug_messages.append(f'[INFO] Clicks data in performance_df: {performance_df["Clicks"].to_dict()}')
                        st.session_state.debug_messages.append(f'[INFO] Clicks column used: {metric_mapping.get("Clicks", "None")}')
                    
                if 'Orders' in display_df.columns:
                    display_df['Orders'] = display_df['Orders'].apply(
                        lambda x: f"{int(x):,}" if pd.notnull(x) else "0"
                    )
                
                # Reorder columns for display - only include columns that exist
                # Put Spend and Ad Sales together, followed by their percentage columns
                base_display_cols = ['Spend', 'Ad Sales', '% of Spend', '% of Ad Sales', 'ACoS', 'ROAS']
                optional_cols = []
                
                # Only include optional columns if they exist in the dataframe
                if 'CPC' in display_df.columns:
                    optional_cols.append('CPC')
                if 'CVR' in display_df.columns:
                    optional_cols.append('CVR')
                if 'Clicks' in display_df.columns:
                    optional_cols.append('Clicks')
                if 'Orders' in display_df.columns:
                    optional_cols.append('Orders')
                    
                display_cols = base_display_cols + optional_cols
                display_df = display_df[display_cols]
                
                # Create a styled dataframe with conditional formatting
                styled_df = display_df.copy()
                
                # Extract numeric values for styling (removing % and $ signs)
                numeric_df = performance_df.copy()
                
                # Apply conditional formatting to percentage columns
                # Create a styler object
                styler = styled_df.style
                
                # Apply blue gradient to % of Spend column
                styler = styler.apply(lambda x: [
                    color_gradient_blue(v, 0, 100) if i != len(x)-1 else ''
                    for i, v in enumerate(numeric_df['% of Spend'])
                ], axis=0, subset=['% of Spend'])
                
                # Apply green gradient to % of Ad Sales column
                styler = styler.apply(lambda x: [
                    color_gradient_green(v, 0, 100) if i != len(x)-1 else ''
                    for i, v in enumerate(numeric_df['% of Ad Sales'])
                ], axis=0, subset=['% of Ad Sales'])
                
                # Display the styled table
                st.dataframe(styler, use_container_width=True)

                # --- Display "Filtered by" message for Placement Product Group filter ---
                if st.session_state.get('placement_product_group_filter_active', False) and st.session_state.placement_product_group_filter:
                    st.markdown(f"<p style='font-size: smaller; text-align: left; margin-top: 10px; margin-bottom: 10px;'>Filtered by Product Group(s): {', '.join(st.session_state.placement_product_group_filter)}</p>", unsafe_allow_html=True)
                
                # Add pie charts for Spend and Ad Sales by Placement
                # Exclude B2B Modifier from pie charts to avoid double counting
                pie_chart_df = performance_df[~performance_df.index.isin(['B2B Modifier', 'Amazon Business'])].copy()
                
                # Only create pie charts if we have at least two placements
                if len(pie_chart_df) >= 2:
                    
                    # Define a consistent color mapping for placements
                    placement_types = pie_chart_df.index.tolist()
                    
                    # Define a fixed color mapping for common placement types
                    color_mapping = {
                        'Rest Of Search': '#42A5F5',  # Blue
                        'Product Page': '#66BB6A',   # Green
                        'Top of Search': '#FFA726',  # Orange
                        'B2B Modifier': '#AB47BC',   # Purple
                        'Amazon Business': '#EF5350' # Red
                    }
                    
                    # Create two columns for the pie charts
                    col1, col2 = st.columns(2)
                    
                    with col1:
                        # Create pie chart for Spend
                        fig_spend = px.pie(
                            pie_chart_df,
                            names=pie_chart_df.index,
                            values='Spend',
                            title='% of Total Spend by Placement',
                            color=pie_chart_df.index,
                            color_discrete_map=color_mapping
                        )
                        fig_spend.update_traces(textinfo='percent+label', pull=[0.05]*len(pie_chart_df))
                        st.plotly_chart(fig_spend, use_container_width=True, key="placement_spend_chart")
                    
                    with col2:
                        # Create pie chart for Ad Sales
                        fig_sales = px.pie(
                            pie_chart_df,
                            names=pie_chart_df.index,
                            values='Ad Sales',
                            title='% of Total Ad Sales by Placement',
                            color=pie_chart_df.index,
                            color_discrete_map=color_mapping
                        )
                        fig_sales.update_traces(textinfo='percent+label', pull=[0.05]*len(pie_chart_df))
                        st.plotly_chart(fig_sales, use_container_width=True, key="placement_sales_chart")
                
                # Add debug message
                if 'debug_messages' in st.session_state:
                    st.session_state.debug_messages.append('[INFO] Successfully displayed Performance by Placement section with summary cards and percentage columns')
            else:
                # Identify what's missing
                missing_metrics = []
                if not has_spend: missing_metrics.append('Spend')
                if not has_sales: missing_metrics.append('Sales')
                if not has_clicks: missing_metrics.append('Clicks')
                if not has_orders: missing_metrics.append('Orders')
                
                if missing_metrics:
                    st.info(f"Cannot display Performance by Placement: Missing required metrics {', '.join(missing_metrics)}")
                elif not placement_mapping:
                    st.info("Cannot display Performance by Placement: No placement columns identified")
                else:
                    st.info("Cannot display Performance by Placement: Insufficient data")
                
                # Show what columns we did find to help with debugging
                st.write("Available columns in the bulk file:")
                if not placement_data.empty:
                    st.write(", ".join(placement_data.columns))
                else:
                    st.write("No relevant data found")
                    
                # Add debug message
                if 'debug_messages' in st.session_state:
                    st.session_state.debug_messages.append('[WARNING] Could not display Performance by Placement due to missing data')
    else:
        st.info("No bulk data available. Upload a bulk file to see placement performance.")
        # Add debug message
        if 'debug_messages' in st.session_state:
            st.session_state.debug_messages.append('[INFO] No bulk data available for placement analysis')


# --- Application Status and Session Information ---
# Initialize session tracking if not already present
if 'session_info' not in st.session_state:
    st.session_state.session_info = {
        'session_start': datetime.now(),
        'data_loaded': False,
        'last_action': 'Session started'
    }

# Initialize debug messages list if it doesn't exist
if 'debug_messages' not in st.session_state:
    st.session_state.debug_messages = []
    
# Initialize debug flag if it doesn't exist
if 'debug' not in st.session_state:
    st.session_state.debug = False

# Function to add session tracking messages
def track_session_activity(activity, details=None):
    """Track user activities and application state changes."""
    timestamp = datetime.now().strftime("%H:%M:%S")
    activity_log = {
        'timestamp': timestamp,
        'activity': activity,
        'details': details or ''
    }
    
    if 'session_activities' not in st.session_state:
        st.session_state.session_activities = []
    
    st.session_state.session_activities.append(activity_log)
    st.session_state.session_info['last_action'] = activity
    
    # Keep only the last 20 activities to prevent memory issues
    if len(st.session_state.session_activities) > 20:
        st.session_state.session_activities = st.session_state.session_activities[-20:]

# Create a safe function to add debug messages that won't crash the app
def add_debug_msg(category, message):
    try:
        full_msg = f"[{category}] {message}"
        if full_msg not in st.session_state.debug_messages:
            st.session_state.debug_messages.append(full_msg)
    except Exception as e:
        # If even adding a debug message fails, at least try to record that
        try:
            st.session_state.debug_messages.append(f"[ERROR] Failed to add debug message: {str(e)}")
        except:
            pass  # At this point we can't do anything else

# Track data loading status
if 'bulk_data' in st.session_state and st.session_state.bulk_data is not None:
    if not st.session_state.session_info.get('data_loaded'):
        track_session_activity("Data Upload", "Bulk advertising file processed successfully")
        st.session_state.session_info['data_loaded'] = True

if 'sales_report_data' in st.session_state and st.session_state.sales_report_data is not None:
    track_session_activity("Sales Data", "Sales report processed successfully")

# Display the application information in an expander at the very bottom
with st.expander("ðŸ“Š Application Information", expanded=False):
    st.markdown("### ðŸ“Š Dashboard Status & Application Guide")
    st.markdown("This section provides an overview of your current session, data status, and helpful information for understanding how the dashboard works.")
    
    # === SESSION OVERVIEW ===
    st.markdown("---")
    st.markdown("#### ðŸš€ Current Session")
    
    col1, col2, col3 = st.columns(3)
    
    with col1:
        session_duration = datetime.now() - st.session_state.session_info['session_start']
        hours, remainder = divmod(int(session_duration.total_seconds()), 3600)
        minutes, _ = divmod(remainder, 60)
        if hours > 0:
            duration_str = f"{hours}h {minutes}m"
        else:
            duration_str = f"{minutes}m"
        st.metric("Session Duration", duration_str)
    
    with col2:
        client_name = "None Selected"
        if 'client_config' in st.session_state and st.session_state.client_config:
            client_name = st.session_state.client_config.get('client_name', 'Unknown')
        st.metric("Active Client", client_name)
    
    with col3:
        last_action = st.session_state.session_info.get('last_action', 'No activity')
        st.metric("Last Activity", last_action)
    
    # === DATA STATUS OVERVIEW ===
    st.markdown("#### ðŸ“ Data Status")
    
    # Check what data we have
    has_bulk_data = 'bulk_data' in st.session_state and st.session_state.bulk_data is not None
    has_sales_data = 'sales_report_data' in st.session_state and st.session_state.sales_report_data is not None
    
    if has_bulk_data or has_sales_data:
        col1, col2 = st.columns(2)
        
        with col1:
            if has_bulk_data:
                st.success("âœ… **Advertising Data Loaded**")
                # Count campaigns and ad types
                if isinstance(st.session_state.bulk_data, dict):
                    campaign_count = 0
                    ad_types = set()
                    for sheet_name, df in st.session_state.bulk_data.items():
                        if isinstance(df, pd.DataFrame) and 'Campaign Name' in df.columns:
                            campaign_count += len(df['Campaign Name'].unique())
                        if isinstance(df, pd.DataFrame) and 'Product' in df.columns:
                            sheet_ad_types = set([str(t) for t in df['Product'].unique() if pd.notna(t) and str(t).strip() != ''])
                            ad_types.update(sheet_ad_types)
                    
                    st.write(f"â€¢ **{campaign_count:,} campaigns** across **{len(ad_types)} ad types**")
                    if ad_types:
                        st.write(f"â€¢ Ad types: {', '.join(sorted(ad_types))}")
                
                # Check for companion data
                is_companion = st.session_state.get('is_companion_data', False)
                if is_companion:
                    st.info("ðŸ“± Using Companion app data")
            else:
                st.warning("âš ï¸ **No Advertising Data**")
                st.write("Upload a bulk file or companion data to get started")
        
        with col2:
            if has_sales_data:
                st.success("âœ… **Sales Report Loaded**")
                sales_df = st.session_state.sales_report_data
                if isinstance(sales_df, pd.DataFrame):
                    unique_asins = len(sales_df['ASIN'].unique()) if 'ASIN' in sales_df.columns else 0
                    st.write(f"â€¢ **{unique_asins:,} unique ASINs** with sales data")
                    
                    # Check for sales columns to understand data scope
                    sales_cols = [col for col in sales_df.columns if 'sales' in col.lower() or 'revenue' in col.lower()]
                    if sales_cols:
                        st.write(f"â€¢ Sales metrics: {len(sales_cols)} columns")
            else:
                st.info("â„¹ï¸ **No Sales Report**")
                st.write("Sales reports help match advertising to product performance")
    else:
        st.info("ðŸŒŸ **Getting Started**")
        st.write("Welcome! Upload your advertising data using the sidebar to begin your analysis.")
    
    # === CLIENT CONFIGURATION ===
    if 'client_config' in st.session_state and st.session_state.client_config:
        st.markdown("#### âš™ï¸ Client Configuration")
        
        config = st.session_state.client_config
        
        # Branded terms
        branded_terms_list = config.get('branded_terms', [])
        if not branded_terms_list and 'branded_keywords' in config:
            branded_terms_list = config.get('branded_keywords', [])
        
        if branded_terms_list:
            st.success(f"âœ… **{len(branded_terms_list)} branded terms** configured for campaign classification")
            # Show sample
            sample_size = min(3, len(branded_terms_list))
            sample_terms = branded_terms_list[:sample_size]
            sample_text = ', '.join(sample_terms)
            if len(branded_terms_list) > sample_size:
                sample_text += f" (and {len(branded_terms_list) - sample_size} more)"
            st.write(f"â€¢ Examples: {sample_text}")
        else:
            st.warning("âš ï¸ **No branded terms** - campaigns will not be classified as branded/non-branded")
        
        # Branded ASINs
        branded_asins_data = config.get('branded_asins_data', {})
        if branded_asins_data:
            st.success(f"âœ… **{len(branded_asins_data)} branded ASINs** configured for product analysis")
        else:
            st.info("â„¹ï¸ **No branded ASINs** - product targeting analysis will be limited")
        

    
    # === RECENT ACTIVITY ===
    if 'session_activities' in st.session_state and st.session_state.session_activities:
        st.markdown("#### ðŸ“ Recent Activity")
        
        # Show last 5 activities
        recent_activities = st.session_state.session_activities[-5:]
        for activity in reversed(recent_activities):
            timestamp = activity['timestamp']
            action = activity['activity']
            details = activity['details']
            
            if details:
                st.write(f"**{timestamp}** - {action}: {details}")
            else:
                st.write(f"**{timestamp}** - {action}")
    

    
    # === TECHNICAL INFORMATION FOR DEBUGGING ===
    if st.session_state.get('debug', False):
        st.markdown("---")
        st.markdown("#### ðŸ”§ Technical Details (Debug Mode)")
        
        st.markdown("##### ðŸ–¥ï¸ System Information")
        st.write(f"**App Version**: {APP_VERSION}")
        st.write(f"**User Data Directory**: {USER_DATA_DIR}")
        st.write(f"**Session State Keys**: {len(st.session_state)} active")
        
        # Cache performance
        cache_stats = get_cache_stats()
        if cache_stats['total'] > 0:
            st.write(f"**Cache Performance**: {cache_stats['hits']} hits, {cache_stats['misses']} misses ({cache_stats['hit_rate']:.1f}% hit rate)")
        
        st.markdown("##### ðŸ“Š Data Structure Details")
        if has_bulk_data:
            st.markdown("**Bulk Data Sheets:**")
            for sheet_name, df in st.session_state.bulk_data.items():
                if isinstance(df, pd.DataFrame):
                    st.write(f"â€¢ {sheet_name}: {len(df):,} rows, {len(df.columns)} columns")
        
        if has_sales_data:
            sales_df = st.session_state.sales_report_data
            st.markdown("**Sales Report Structure:**")
            st.write(f"â€¢ {len(sales_df):,} rows, {len(sales_df.columns)} columns")
            st.write(f"â€¢ Columns: {', '.join(list(sales_df.columns)[:10])}{'...' if len(sales_df.columns) > 10 else ''}")
        
        st.markdown("##### âŒ Error Log")
        errors = [msg for msg in st.session_state.get('debug_messages', []) if '[ERROR]' in msg]
        if errors:
            st.markdown("**Recent Errors:**")
            for error in errors[-5:]:
                st.error(error)
        else:
            st.success("No errors detected in current session")
        
        st.markdown("##### âš¡ Performance Messages")
        perf_messages = [msg for msg in st.session_state.get('debug_messages', []) if '[PERFORMANCE]' in msg or '[CACHE' in msg]
        if perf_messages:
            for msg in perf_messages[-10:]:
                st.text(msg)
        else:
            st.info("No performance data available")
            
        # Debug Mode Toggle
        st.markdown("---")
        st.markdown("##### ðŸ› Debug Mode")
        debug_mode = st.checkbox(
            "Enable Debug Output", 
            value=False, 
            key="global_debug_mode",
            help="Show detailed debug information throughout the application"
        )
        
        if debug_mode:
            # Legacy debug information for technical troubleshooting
            st.markdown("##### ðŸ”§ Legacy Debug Information")
            st.markdown("**Product Group Filtering Debug:**")
            st.markdown("#### Product Group Filtering Debug")
        
        # Add Performance by Placement filter debug information
        st.markdown("**Performance by Placement Filter Debug:**")
        st.write(f"Filter Active: {st.session_state.get('placement_product_group_filter_active', False)}")
        st.write(f"Selected Product Groups: {st.session_state.get('placement_product_group_filter', [])}")
        
        # Campaign tags debugging
        if 'campaign_tags_df' in st.session_state and st.session_state.campaign_tags_df is not None:
            st.write(f"Campaign Tags DataFrame Shape: {st.session_state.campaign_tags_df.shape}")
            if not st.session_state.campaign_tags_df.empty:
                if 'tag_1' in st.session_state.campaign_tags_df.columns:
                    unique_product_groups = st.session_state.campaign_tags_df['tag_1'].dropna().unique()
                    st.write(f"Available Product Groups in Campaign Tags: {unique_product_groups}")
                else:
                    st.write("No 'tag_1' column found in campaign_tags_df")
        else:
            st.write("No campaign_tags_df available")
        
                 # Show placement data processing debug information
        if 'placement_filter_debug' in st.session_state:
            debug_info = st.session_state.placement_filter_debug
            st.markdown("**Placement Data Processing Debug:**")
            for sheet_name, info in debug_info.items():
                st.write(f"Sheet: {sheet_name}")
                st.write(f"  - Original rows: {info.get('original_rows', 'N/A')}")
                st.write(f"  - Filtered rows: {info.get('filtered_rows', 'N/A')}")
                st.write(f"  - Campaigns found: {info.get('campaigns_found', 'N/A')}")
                st.write(f"  - Bidding adjustment rows: {info.get('bidding_adjustment_rows', 'N/A')}")
        
        # Show campaign matching debug details
        st.markdown("**Campaign Matching Details:**")
        if 'campaign_tags_df' in st.session_state and st.session_state.campaign_tags_df is not None:
            # Show sample campaigns from campaign tags for selected product groups
            if st.session_state.get('placement_product_group_filter_active', False) and st.session_state.placement_product_group_filter:
                filtered_tags = st.session_state.campaign_tags_df[
                    st.session_state.campaign_tags_df['tag_1'].isin(st.session_state.placement_product_group_filter)
                ]
                st.write(f"Campaign Tags Matching Selected PGs: {len(filtered_tags)} rows")
                if not filtered_tags.empty and 'Campaign Name' in filtered_tags.columns:
                    sample_campaigns = filtered_tags['Campaign Name'].unique()[:10].tolist()
                    st.write(f"Sample Campaign Names from Tags: {sample_campaigns}")
                    
        # Show bulk file campaign names for comparison  
        if 'bulk_data' in st.session_state and st.session_state.bulk_data:
            st.write("Sample Campaign Names from Bulk Files:")
            for sheet_name, df in st.session_state.bulk_data.items():
                if isinstance(df, pd.DataFrame):
                    # Show all available columns
                    st.write(f"  {sheet_name} columns: {list(df.columns)}")
                    
                    # Look for campaign columns
                    campaign_columns = [col for col in df.columns if 'campaign' in col.lower()]
                    if campaign_columns:
                        st.write(f"  {sheet_name} campaign-related columns: {campaign_columns}")
                        
                        # Try to find the target column using case-insensitive matching
                        target_col = None
                        campaign_name_col_target = 'Campaign Name (Informational only)'
                        for col in df.columns:
                            if col.lower() == campaign_name_col_target.lower():
                                target_col = col
                                break
                        
                        if target_col:
                            sample_bulk_campaigns = df[target_col].dropna().unique()[:5].tolist()
                            st.write(f"  {sheet_name}: {sample_bulk_campaigns} (using '{target_col}' column)")
                        else:
                            # Use the first campaign column found
                            campaign_col = campaign_columns[0]
                            sample_bulk_campaigns = df[campaign_col].dropna().unique()[:5].tolist()
                            st.write(f"  {sheet_name}: {sample_bulk_campaigns} (using '{campaign_col}' column - target column not found)")
                    else:
                        st.write(f"  {sheet_name}: No campaign column found")
                    
        # Show column names comparison
        st.markdown("**Column Names Being Used:**")
        st.write(f"Product Group Column (tags): 'tag_1'")
        st.write(f"Campaign Name Column (tags): 'Campaign Name'") 
        st.write(f"Campaign Column (bulk): 'Campaign Name (Informational only)'")
        
        if 'debug_targeting_filter' in st.session_state:
            debug_info = st.session_state.debug_targeting_filter
            
            st.markdown("**Filter State:**")
            st.write(f"Filter Active: {debug_info.get('active', False)}")
            st.write(f"Selected Groups: {debug_info.get('selected_groups', [])}")
            
            if 'before_filtering' in debug_info and 'after_filtering' in debug_info:
                st.markdown("**Before Filtering:**")
                st.write(f"Branded Rows: {debug_info['before_filtering'].get('branded_rows', 0)}")
                st.write(f"Non-Branded Rows: {debug_info['before_filtering'].get('nonbranded_rows', 0)}")
                st.write(f"Branded Product Groups: {debug_info['before_filtering'].get('branded_product_groups', [])}")
                st.write(f"Non-Branded Product Groups: {debug_info['before_filtering'].get('nonbranded_product_groups', [])}")
                
                st.markdown("**After Filtering:**")
                st.write(f"Branded Rows: {debug_info['after_filtering'].get('branded_rows', 0)}")
                st.write(f"Non-Branded Rows: {debug_info['after_filtering'].get('nonbranded_rows', 0)}")
                st.write(f"Branded Product Groups: {debug_info['after_filtering'].get('branded_product_groups', [])}")
                st.write(f"Non-Branded Product Groups: {debug_info['after_filtering'].get('nonbranded_product_groups', [])}")
                st.write(f"Rows Removed (Branded): {debug_info['after_filtering'].get('rows_removed_branded', 0)}")
                st.write(f"Rows Removed (Non-Branded): {debug_info['after_filtering'].get('rows_removed_nonbranded', 0)}")
            
            # Add Sankey diagram debugging
            st.markdown("**Sankey Diagram Filtering:**")
            if 'ad_spend_sankey' in debug_info:
                st.write(f"Ad Spend Sankey - Filter Active: {debug_info['ad_spend_sankey'].get('filter_active', False)}")
                st.write(f"Ad Spend Sankey - Selected Groups: {debug_info['ad_spend_sankey'].get('selected_groups', [])}")
            
            if 'ad_sales_sankey' in debug_info:
                st.write(f"Ad Sales Sankey - Filter Active: {debug_info['ad_sales_sankey'].get('filter_active', False)}")
                st.write(f"Ad Sales Sankey - Selected Groups: {debug_info['ad_sales_sankey'].get('selected_groups', [])}")
            
            if 'spend_sankey' in debug_info:
                st.write(f"Spend Sankey - Pre-filter Count: {debug_info['spend_sankey'].get('pre_filter_count', 0)}")
                st.write(f"Spend Sankey - Post-filter Count: {debug_info['spend_sankey'].get('post_filter_count', 0)}")
                st.write(f"Spend Sankey - Rows Filtered: {debug_info['spend_sankey'].get('rows_filtered', 0)}")
                st.write(f"Spend Sankey - Selected Groups: {debug_info['spend_sankey'].get('selected_groups', [])}")
                
            # Add a section to show the actual filtered dataframes
            st.markdown("**Filtered DataFrames in Session State:**")
            if 'filtered_branded_targets_df' in st.session_state and 'filtered_non_branded_targets_df' in st.session_state:
                st.write(f"Filtered Branded DataFrame Rows: {len(st.session_state.filtered_branded_targets_df) if not st.session_state.filtered_branded_targets_df.empty else 0}")
                st.write(f"Filtered Non-Branded DataFrame Rows: {len(st.session_state.filtered_non_branded_targets_df) if not st.session_state.filtered_non_branded_targets_df.empty else 0}")
                
                # Show the widget key and session state key to verify they're connected
                st.markdown("**Widget and Session State Keys:**")
                st.write(f"Widget Key: targeting_product_group_filter2_widget")
                st.write(f"Session State Key: targeting_product_group_filter2")
                st.write(f"Widget Value: {st.session_state.get('targeting_product_group_filter2_widget', 'Not set')}")
                st.write(f"Session State Value: {st.session_state.get('targeting_product_group_filter2', [])}")
        else:
            st.write("No product group filtering debug information available.")
    else:
        # Show option to enable debug mode
        st.markdown("---")
        if st.button("ðŸ”§ Enable Debug Mode"):
            st.session_state.debug = True
            track_session_activity("Debug Mode", "Enabled technical debugging information")
            st.rerun()
        st.caption("Enable debug mode to see technical details for troubleshooting")

    # --- Custom Debug DataFrames for Product Group Filtering ---
    if 'debug_branded_pg_mapping' in st.session_state and st.session_state.debug_branded_pg_mapping is not None:
        st.markdown('#### Branded Product Group Mapping')
        st.dataframe(st.session_state.debug_branded_pg_mapping, use_container_width=True)
    if 'debug_nonbranded_pg_mapping' in st.session_state and st.session_state.debug_nonbranded_pg_mapping is not None:
        st.markdown('#### Non-Branded Product Group Mapping')
        st.dataframe(st.session_state.debug_nonbranded_pg_mapping, use_container_width=True)
    if 'debug_filtered_branded' in st.session_state and st.session_state.debug_filtered_branded is not None:
        st.markdown('#### Filtered Branded Data')
        st.dataframe(st.session_state.debug_filtered_branded, use_container_width=True)
    if 'debug_filtered_nonbranded' in st.session_state and st.session_state.debug_filtered_nonbranded is not None:
        st.markdown('#### Filtered Non-Branded Data')
        st.dataframe(st.session_state.debug_filtered_nonbranded, use_container_width=True)

    st.markdown("- Analyzing all columns for campaign type indicators")
    st.markdown("- Using column structure to determine campaign type when other methods fail")
    
    # Display bulk file structure if available
    if 'bulk_data' in st.session_state and st.session_state.bulk_data:
        st.markdown("#### Bulk File Structure:")
        for sheet_name, df in st.session_state.bulk_data.items():
            if isinstance(df, pd.DataFrame) and not df.empty:
                st.markdown(f"**Sheet: {sheet_name}** ({len(df)} rows, {len(df.columns)} columns)")
                st.markdown("Sample columns: " + ", ".join(list(df.columns)[:10]) + (" ..." if len(df.columns) > 10 else ""))
                
            # Check specifically for Customer Search Term column
            has_search_term = False
            for col in df.columns:
                if col.lower() == 'search term' or col.lower() == 'customer search term':
                    st.success(f"âœ… Found search term column: '{col}' in sheet '{sheet_name}'")
                    has_search_term = True
                    
                    # Show sample values from this column
                    if len(df) > 0:
                        sample_values = df[col].dropna().head(5).tolist()
                        if sample_values:
                            st.markdown("Sample values: " + ", ".join([f"\"{str(val)}\"" for val in sample_values]))
                    break
            
            # Check for Product column
            if 'Product' in df.columns:
                product_values = df['Product'].dropna().astype(str).unique()
                if len(product_values) > 0:
                    st.markdown(f"Product column values: {', '.join(product_values[:5])}" + 
                               (" ..." if len(product_values) > 5 else ""))
                    
                    # Check if any product values contain sponsored products, brands, or display
                    has_sp = any('sponsored product' in val.lower() for val in product_values)
                    has_sb = any('sponsored brand' in val.lower() for val in product_values)
                    has_sd = any('sponsored display' in val.lower() for val in product_values)
                    
                    if has_sp:
                        st.success("âœ… Found 'Sponsored Products' in Product column")
                    if has_sb:
                        st.success("âœ… Found 'Sponsored Brands' in Product column")
                    if has_sd:
                        st.success("âœ… Found 'Sponsored Display' in Product column")
                    if not has_sp and not has_sb and not has_sd:
                        st.warning("âš ï¸ No campaign type indicators found in Product column")
    else:
        st.info("No bulk file data available yet. Upload a bulk file to see details.")

    


# Clear out any duplicate system messages from previous runs
st.session_state.debug_messages = [msg for msg in st.session_state.debug_messages 
                                if not msg.startswith('[SYSTEM]')]

# Add useful system information
add_debug_msg("SYSTEM", f"Dashboard version: 1.0.0")
add_debug_msg("SYSTEM", f"Last updated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")

# Add information about loaded data
try:
    if 'bulk_data' in st.session_state and st.session_state.bulk_data is not None:
        if isinstance(st.session_state.bulk_data, dict):
            bulk_info = []
            for sheet_name, df in st.session_state.bulk_data.items():
                if isinstance(df, pd.DataFrame):
                    bulk_info.append(f"{sheet_name}: {df.shape[0]} rows, {df.shape[1]} columns")
            if bulk_info:
                add_debug_msg("DATA", f"Bulk file sheets: {', '.join(bulk_info)}")
            else:
                add_debug_msg("DATA", "Bulk file loaded but no valid dataframes found")
        else:
            add_debug_msg("DATA", f"Bulk data is not a dictionary: {type(st.session_state.bulk_data).__name__}")
    else:
        add_debug_msg("DATA", "No bulk data loaded")

    if 'sales_report_data' in st.session_state:
        if isinstance(st.session_state.sales_report_data, pd.DataFrame):
            sales_df = st.session_state.sales_report_data
            add_debug_msg("DATA", f"Sales report: {sales_df.shape[0]} rows, {sales_df.shape[1]} columns")
        else:
            add_debug_msg("DATA", f"Sales report data is not a DataFrame: {type(st.session_state.sales_report_data).__name__}")
    else:
        add_debug_msg("DATA", "No sales report data loaded")

    # Add attribution information
    if 'sd_attribution_choice' in st.session_state:
        add_debug_msg("SETTINGS", f"Current attribution model: {st.session_state.sd_attribution_choice}")
    else:
        add_debug_msg("SETTINGS", "No attribution model selected")
            
    # Check for untagged ASINs in the ASIN performance dataframe
    if 'asin_perf_df' in st.session_state and isinstance(st.session_state.asin_perf_df, pd.DataFrame) and not st.session_state.asin_perf_df.empty:
        untagged_df = st.session_state.asin_perf_df[st.session_state.asin_perf_df['Product Group'] == 'Untagged Group']
        if not untagged_df.empty:
            untagged_count = len(untagged_df)
    
    # Check if we have any branded ASINs configured - with proper None checks
    if 'client_config' in st.session_state and st.session_state.client_config is not None:
        if isinstance(st.session_state.client_config, dict) and 'branded_asins_data' in st.session_state.client_config:
            branded_asins_data = st.session_state.client_config.get('branded_asins_data', {})
            if branded_asins_data is not None and isinstance(branded_asins_data, dict):
                branded_asins_count = len(branded_asins_data)
                pass
                
                # Sample a few ASINs to show
                if branded_asins_count > 0:
                    sample_asins = list(branded_asins_data.keys())[:3]
                    pass
            else:
                pass
        else:
            pass
    else:
        pass
    
    # Check bulk file for ASIN columns
    if 'bulk_data' in st.session_state and st.session_state.bulk_data is not None:
        try:
            if isinstance(st.session_state.bulk_data, dict):
                for sheet_name, df in st.session_state.bulk_data.items():
                    if isinstance(df, pd.DataFrame):
                        # Check for ASIN columns
                        asin_cols = [col for col in df.columns if 'asin' in col.lower()]
                        if asin_cols:
                            pass
                        else:
                            pass
                        
                        # Check for Entity column
                        if 'Entity' in df.columns:
                            entity_values = df['Entity'].value_counts().to_dict()
                            pass
                        else:
                            pass
                        
                        # Check for Product column
                        if 'Product' in df.columns:
                            product_values = df['Product'].value_counts().to_dict()
                            pass
                        else:
                            pass
                        
                        # Check for Sales columns
                        sales_cols = [col for col in df.columns if 'sales' in col.lower()]
            else:
                pass
        except Exception as e:
            import traceback
            tb = traceback.format_exc()
            pass

    # Add client configuration information with proper None checks
    if 'client_config' in st.session_state and st.session_state.client_config is not None:
        try:
            if isinstance(st.session_state.client_config, dict):
                client_name = st.session_state.client_config.get('client_name', 'Unknown')
                
                # Safely get branded terms count
                branded_terms_list = st.session_state.client_config.get('branded_terms', [])
                branded_terms = len(branded_terms_list) if branded_terms_list is not None else 0
                
                # Safely get branded ASINs count
                branded_asins_dict = st.session_state.client_config.get('branded_asins_data', {})
                branded_asins = len(branded_asins_dict) if branded_asins_dict is not None else 0
                
                add_debug_msg("CLIENT", f"Client: {client_name}, {branded_terms} branded terms, {branded_asins} branded ASINs")
            else:
                add_debug_msg("CLIENT", f"client_config is not a dictionary: {type(st.session_state.client_config)}")
        except Exception as e:
            import traceback
            tb = traceback.format_exc()
            add_debug_msg("CLIENT", f"Error processing client config: {str(e)}\n{tb}")
    else:
        add_debug_msg("CLIENT", "No client selected or client_config is None")

    # Add targeting data information
    if 'branded_targets_df' in st.session_state and isinstance(st.session_state.branded_targets_df, pd.DataFrame):
        add_debug_msg("TARGETING", f"Branded targets: {st.session_state.branded_targets_df.shape[0]} rows")
    if 'non_branded_targets_df' in st.session_state and isinstance(st.session_state.non_branded_targets_df, pd.DataFrame):
        add_debug_msg("TARGETING", f"Non-branded targets: {st.session_state.non_branded_targets_df.shape[0]} rows")

except Exception as e:
    import traceback
    tb = traceback.format_exc()
    add_debug_msg("ERROR", f"Error collecting debug information: {str(e)}\n{tb}")

# The audit progress tracking system has been removed
# All user-friendly application information is now provided in the 'Application Information' section above